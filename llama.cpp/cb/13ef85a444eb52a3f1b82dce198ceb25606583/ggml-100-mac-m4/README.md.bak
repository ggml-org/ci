### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.40 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  180.04 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.88 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   26.11 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.60 sec*proc (27 tests)

Total Test time (real) = 222.61 sec

real	3m42.668s
user	7m39.438s
sys	0m6.165s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.21 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.37 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.05 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.21 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.85 sec*proc (27 tests)

Total Test time (real) =  50.86 sec

real	0m50.869s
user	1m11.214s
sys	0m5.504s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.068 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.899 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.208 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.218 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.219 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.219 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.220 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.221 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.222 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.223 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.224 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.224 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.225 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.229 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.229 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.230 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.230 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.231 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.232 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.232 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.603 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.605 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.606 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.606 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.607 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.607 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.607 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.608 I llama_model_loader: - type  f32:  124 tensors
0.00.028.609 I llama_model_loader: - type  f16:   73 tensors
0.00.033.268 I llm_load_vocab: special tokens cache size = 5
0.00.035.696 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.035.701 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.035.702 I llm_load_print_meta: arch             = bert
0.00.035.702 I llm_load_print_meta: vocab type       = WPM
0.00.035.702 I llm_load_print_meta: n_vocab          = 30522
0.00.035.703 I llm_load_print_meta: n_merges         = 0
0.00.035.703 I llm_load_print_meta: vocab_only       = 0
0.00.035.703 I llm_load_print_meta: n_ctx_train      = 512
0.00.035.704 I llm_load_print_meta: n_embd           = 384
0.00.035.704 I llm_load_print_meta: n_layer          = 12
0.00.035.731 I llm_load_print_meta: n_head           = 12
0.00.035.734 I llm_load_print_meta: n_head_kv        = 12
0.00.035.734 I llm_load_print_meta: n_rot            = 32
0.00.035.734 I llm_load_print_meta: n_swa            = 0
0.00.035.735 I llm_load_print_meta: n_embd_head_k    = 32
0.00.035.735 I llm_load_print_meta: n_embd_head_v    = 32
0.00.035.736 I llm_load_print_meta: n_gqa            = 1
0.00.035.737 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.035.738 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.035.739 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.035.739 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.035.740 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.035.740 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.035.740 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.035.741 I llm_load_print_meta: n_ff             = 1536
0.00.035.743 I llm_load_print_meta: n_expert         = 0
0.00.035.744 I llm_load_print_meta: n_expert_used    = 0
0.00.035.744 I llm_load_print_meta: causal attn      = 0
0.00.035.744 I llm_load_print_meta: pooling type     = 2
0.00.035.744 I llm_load_print_meta: rope type        = 2
0.00.035.745 I llm_load_print_meta: rope scaling     = linear
0.00.035.745 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.035.746 I llm_load_print_meta: freq_scale_train = 1
0.00.035.746 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.035.747 I llm_load_print_meta: rope_finetuned   = unknown
0.00.035.747 I llm_load_print_meta: ssm_d_conv       = 0
0.00.035.747 I llm_load_print_meta: ssm_d_inner      = 0
0.00.035.749 I llm_load_print_meta: ssm_d_state      = 0
0.00.035.750 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.035.750 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.035.756 I llm_load_print_meta: model type       = 33M
0.00.035.757 I llm_load_print_meta: model ftype      = F16
0.00.035.757 I llm_load_print_meta: model params     = 33.21 M
0.00.035.758 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.035.759 I llm_load_print_meta: general.name     = Bge Small
0.00.035.759 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.035.760 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.035.760 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.035.760 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.035.761 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.035.761 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.035.761 I llm_load_print_meta: max token length = 21
0.00.037.805 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.037.807 I llm_load_tensors: offloading output layer to GPU
0.00.037.807 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.037.829 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.831 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.404 I llama_new_context_with_model: n_seq_max     = 1
0.00.038.406 I llama_new_context_with_model: n_ctx         = 512
0.00.038.406 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.038.406 I llama_new_context_with_model: n_batch       = 2048
0.00.038.407 I llama_new_context_with_model: n_ubatch      = 2048
0.00.038.407 I llama_new_context_with_model: flash_attn    = 0
0.00.038.408 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.408 I llama_new_context_with_model: freq_scale    = 1
0.00.038.409 I ggml_metal_init: allocating
0.00.038.413 I ggml_metal_init: found device: Apple M4
0.00.038.418 I ggml_metal_init: picking default device: Apple M4
0.00.039.259 I ggml_metal_init: using embedded metal library
0.00.043.497 I ggml_metal_init: GPU name:   Apple M4
0.00.043.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.500 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.501 I ggml_metal_init: simdgroup reduction   = true
0.00.043.501 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.501 I ggml_metal_init: has bfloat            = true
0.00.043.501 I ggml_metal_init: use bfloat            = true
0.00.043.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.503 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.672 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.675 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.676 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.390 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.391 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.392 I llama_new_context_with_model: graph nodes  = 429
0.00.057.392 I llama_new_context_with_model: graph splits = 2
0.00.057.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.072.725 I 
0.00.072.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.073.343 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.077.532 I llama_perf_context_print:        load time =      54.82 ms
0.00.077.533 I llama_perf_context_print: prompt eval time =       4.05 ms /     9 tokens (    0.45 ms per token,  2222.22 tokens per second)
0.00.077.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.077.534 I llama_perf_context_print:       total time =       4.81 ms /    10 tokens
0.00.077.725 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.080s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.052 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.088 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.093 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.094 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.095 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.095 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.096 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.096 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.097 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.097 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.097 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.100 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.100 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.100 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.101 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.101 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.102 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.103 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.110 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.111 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.112 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.112 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.112 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.113 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.113 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.114 I llama_model_loader: - type  f32:  124 tensors
0.00.014.114 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.463 I llm_load_vocab: special tokens cache size = 5
0.00.017.659 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.662 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.662 I llm_load_print_meta: arch             = bert
0.00.017.663 I llm_load_print_meta: vocab type       = WPM
0.00.017.663 I llm_load_print_meta: n_vocab          = 30522
0.00.017.663 I llm_load_print_meta: n_merges         = 0
0.00.017.663 I llm_load_print_meta: vocab_only       = 0
0.00.017.664 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.664 I llm_load_print_meta: n_embd           = 384
0.00.017.664 I llm_load_print_meta: n_layer          = 12
0.00.017.674 I llm_load_print_meta: n_head           = 12
0.00.017.675 I llm_load_print_meta: n_head_kv        = 12
0.00.017.676 I llm_load_print_meta: n_rot            = 32
0.00.017.676 I llm_load_print_meta: n_swa            = 0
0.00.017.676 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.676 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.677 I llm_load_print_meta: n_gqa            = 1
0.00.017.677 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.678 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.679 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.679 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.679 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.679 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.680 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.680 I llm_load_print_meta: n_ff             = 1536
0.00.017.683 I llm_load_print_meta: n_expert         = 0
0.00.017.683 I llm_load_print_meta: n_expert_used    = 0
0.00.017.683 I llm_load_print_meta: causal attn      = 0
0.00.017.683 I llm_load_print_meta: pooling type     = 2
0.00.017.683 I llm_load_print_meta: rope type        = 2
0.00.017.684 I llm_load_print_meta: rope scaling     = linear
0.00.017.684 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.684 I llm_load_print_meta: freq_scale_train = 1
0.00.017.684 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.685 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.685 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.685 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.685 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.685 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.686 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.690 I llm_load_print_meta: model type       = 33M
0.00.017.690 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.690 I llm_load_print_meta: model params     = 33.21 M
0.00.017.693 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.693 I llm_load_print_meta: general.name     = Bge Small
0.00.017.693 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.693 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.694 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.694 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.694 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.694 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.694 I llm_load_print_meta: max token length = 21
0.00.018.963 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.963 I llm_load_tensors: offloading output layer to GPU
0.00.018.963 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.971 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.972 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.309 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.310 I llama_new_context_with_model: n_ctx         = 512
0.00.019.310 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.310 I llama_new_context_with_model: n_batch       = 2048
0.00.019.311 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.311 I llama_new_context_with_model: flash_attn    = 0
0.00.019.311 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.311 I llama_new_context_with_model: freq_scale    = 1
0.00.019.312 I ggml_metal_init: allocating
0.00.019.315 I ggml_metal_init: found device: Apple M4
0.00.019.317 I ggml_metal_init: picking default device: Apple M4
0.00.019.901 I ggml_metal_init: using embedded metal library
0.00.022.208 I ggml_metal_init: GPU name:   Apple M4
0.00.022.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.210 I ggml_metal_init: simdgroup reduction   = true
0.00.022.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.210 I ggml_metal_init: has bfloat            = true
0.00.022.211 I ggml_metal_init: use bfloat            = true
0.00.022.211 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.819 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.821 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.822 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.411 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.412 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.413 I llama_new_context_with_model: graph nodes  = 429
0.00.033.413 I llama_new_context_with_model: graph splits = 2
0.00.033.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.953 I 
0.00.037.978 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.528 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.961 I llama_perf_context_print:        load time =      28.90 ms
0.00.042.962 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2089.62 tokens per second)
0.00.042.963 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.964 I llama_perf_context_print:       total time =       5.01 ms /    10 tokens
0.00.043.151 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.149 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.634 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.443 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.450 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.451 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.457 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.457 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.458 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.459 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.460 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.461 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.462 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.462 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.466 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.467 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.468 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.687 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.688 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.688 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.052.688 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.689 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.689 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.689 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.690 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.052.690 I llama_model_loader: - type  f32:   41 tensors
0.00.052.691 I llama_model_loader: - type  f16:   29 tensors
0.00.071.183 W llm_load_vocab: empty token at index 5
0.00.075.809 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.077.064 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.077.090 I llm_load_vocab: special tokens cache size = 5
0.00.337.081 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.337.088 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.337.089 I llm_load_print_meta: arch             = jina-bert-v2
0.00.337.090 I llm_load_print_meta: vocab type       = BPE
0.00.337.090 I llm_load_print_meta: n_vocab          = 61056
0.00.337.090 I llm_load_print_meta: n_merges         = 39382
0.00.337.090 I llm_load_print_meta: vocab_only       = 0
0.00.337.090 I llm_load_print_meta: n_ctx_train      = 8192
0.00.337.091 I llm_load_print_meta: n_embd           = 384
0.00.337.094 I llm_load_print_meta: n_layer          = 4
0.00.337.129 I llm_load_print_meta: n_head           = 12
0.00.337.130 I llm_load_print_meta: n_head_kv        = 12
0.00.337.130 I llm_load_print_meta: n_rot            = 32
0.00.337.130 I llm_load_print_meta: n_swa            = 0
0.00.337.131 I llm_load_print_meta: n_embd_head_k    = 32
0.00.337.131 I llm_load_print_meta: n_embd_head_v    = 32
0.00.337.131 I llm_load_print_meta: n_gqa            = 1
0.00.337.132 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.337.132 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.337.133 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.337.134 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.337.134 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.337.134 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.337.135 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.337.137 I llm_load_print_meta: n_ff             = 1536
0.00.337.137 I llm_load_print_meta: n_expert         = 0
0.00.337.138 I llm_load_print_meta: n_expert_used    = 0
0.00.337.138 I llm_load_print_meta: causal attn      = 0
0.00.337.138 I llm_load_print_meta: pooling type     = -1
0.00.337.138 I llm_load_print_meta: rope type        = -1
0.00.337.138 I llm_load_print_meta: rope scaling     = linear
0.00.337.139 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.337.139 I llm_load_print_meta: freq_scale_train = 1
0.00.337.139 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.337.139 I llm_load_print_meta: rope_finetuned   = unknown
0.00.337.139 I llm_load_print_meta: ssm_d_conv       = 0
0.00.337.140 I llm_load_print_meta: ssm_d_inner      = 0
0.00.337.140 I llm_load_print_meta: ssm_d_state      = 0
0.00.337.140 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.337.141 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.337.161 I llm_load_print_meta: model type       = 33M
0.00.337.162 I llm_load_print_meta: model ftype      = F16
0.00.337.162 I llm_load_print_meta: model params     = 32.90 M
0.00.337.163 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.337.163 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.337.163 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.337.163 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.337.164 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.337.164 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.337.164 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.337.164 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.337.164 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.337.165 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.337.166 I llm_load_print_meta: max token length = 45
0.00.338.359 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.338.360 I llm_load_tensors: offloading output layer to GPU
0.00.338.360 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.338.387 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.388 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.339.424 I llama_new_context_with_model: n_seq_max     = 1
0.00.339.424 I llama_new_context_with_model: n_ctx         = 8192
0.00.339.425 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.339.425 I llama_new_context_with_model: n_batch       = 2048
0.00.339.425 I llama_new_context_with_model: n_ubatch      = 2048
0.00.339.426 I llama_new_context_with_model: flash_attn    = 0
0.00.339.426 I llama_new_context_with_model: freq_base     = 10000.0
0.00.339.426 I llama_new_context_with_model: freq_scale    = 1
0.00.339.427 I ggml_metal_init: allocating
0.00.339.430 I ggml_metal_init: found device: Apple M4
0.00.339.433 I ggml_metal_init: picking default device: Apple M4
0.00.340.508 I ggml_metal_init: using embedded metal library
0.00.343.224 I ggml_metal_init: GPU name:   Apple M4
0.00.343.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.227 I ggml_metal_init: simdgroup reduction   = true
0.00.343.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.227 I ggml_metal_init: has bfloat            = true
0.00.343.227 I ggml_metal_init: use bfloat            = true
0.00.343.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.355.191 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.355.193 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.355.194 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.355.793 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.355.794 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.355.794 I llama_new_context_with_model: graph nodes  = 154
0.00.355.794 I llama_new_context_with_model: graph splits = 2
0.00.355.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.048 I 
0.00.368.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.368.229 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.368.230 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.368.232 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.368.232 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.234 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.234 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.786 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.598 I llama_perf_context_print:        load time =     342.41 ms
0.00.372.599 I llama_perf_context_print: prompt eval time =       3.80 ms /    62 tokens (    0.06 ms per token, 16307.21 tokens per second)
0.00.372.600 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.601 I llama_perf_context_print:       total time =       4.55 ms /    63 tokens
0.00.372.878 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.341s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.126 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.285 I main: llama backend init
0.00.000.298 I main: load the model and apply lora adapter, if any
0.00.027.841 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.438 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.456 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.457 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.457 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.458 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.461 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.462 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.464 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.076 I llama_model_loader: - type  f32:  194 tensors
0.00.058.077 I llama_model_loader: - type  f16:   98 tensors
0.00.087.168 I llm_load_vocab: special tokens cache size = 25
0.00.093.865 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.868 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.869 I llm_load_print_meta: arch             = gptneox
0.00.093.869 I llm_load_print_meta: vocab type       = BPE
0.00.093.869 I llm_load_print_meta: n_vocab          = 50304
0.00.093.869 I llm_load_print_meta: n_merges         = 50009
0.00.093.870 I llm_load_print_meta: vocab_only       = 0
0.00.093.870 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.870 I llm_load_print_meta: n_embd           = 2048
0.00.093.870 I llm_load_print_meta: n_layer          = 24
0.00.093.889 I llm_load_print_meta: n_head           = 16
0.00.093.891 I llm_load_print_meta: n_head_kv        = 16
0.00.093.891 I llm_load_print_meta: n_rot            = 32
0.00.093.891 I llm_load_print_meta: n_swa            = 0
0.00.093.891 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.891 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.892 I llm_load_print_meta: n_gqa            = 1
0.00.093.893 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.894 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.895 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.896 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.898 I llm_load_print_meta: n_ff             = 8192
0.00.093.898 I llm_load_print_meta: n_expert         = 0
0.00.093.898 I llm_load_print_meta: n_expert_used    = 0
0.00.093.898 I llm_load_print_meta: causal attn      = 1
0.00.093.898 I llm_load_print_meta: pooling type     = 0
0.00.093.899 I llm_load_print_meta: rope type        = 2
0.00.093.899 I llm_load_print_meta: rope scaling     = linear
0.00.093.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.899 I llm_load_print_meta: freq_scale_train = 1
0.00.093.900 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.900 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.901 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.901 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.910 I llm_load_print_meta: model type       = 1.4B
0.00.093.911 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.093.911 I llm_load_print_meta: model params     = 1.41 B
0.00.093.912 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.093.912 I llm_load_print_meta: general.name     = 1.4B
0.00.093.912 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.912 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.912 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.913 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.913 I llm_load_print_meta: LF token         = 128 ''
0.00.093.913 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.913 I llm_load_print_meta: max token length = 1024
0.00.096.447 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.447 I llm_load_tensors: offloading output layer to GPU
0.00.096.448 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.466 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.467 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.410 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.411 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.411 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.411 I llama_new_context_with_model: n_batch       = 2048
0.00.097.411 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.411 I llama_new_context_with_model: flash_attn    = 0
0.00.097.412 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.412 I llama_new_context_with_model: freq_scale    = 1
0.00.097.413 I ggml_metal_init: allocating
0.00.097.422 I ggml_metal_init: found device: Apple M4
0.00.097.424 I ggml_metal_init: picking default device: Apple M4
0.00.098.095 I ggml_metal_init: using embedded metal library
0.00.113.056 I ggml_metal_init: GPU name:   Apple M4
0.00.113.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.059 I ggml_metal_init: simdgroup reduction   = true
0.00.113.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.060 I ggml_metal_init: has bfloat            = true
0.00.113.060 I ggml_metal_init: use bfloat            = true
0.00.113.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.159.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.159.989 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.009 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.160.975 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.160.976 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.160.976 I llama_new_context_with_model: graph nodes  = 967
0.00.160.976 I llama_new_context_with_model: graph splits = 2
0.00.160.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.237.787 I main: llama threadpool init, n_threads = 4
0.00.237.822 I 
0.00.237.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.237.859 I 
0.00.237.941 I sampler seed: 1234
0.00.237.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.237.968 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.237.970 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.237.970 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.078.477 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.02.078.477 I llama_perf_context_print:        load time =     209.93 ms
0.02.078.478 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.92 tokens per second)
0.02.078.480 I llama_perf_context_print:        eval time =    1793.78 ms /    63 runs   (   28.47 ms per token,    35.12 tokens per second)
0.02.078.480 I llama_perf_context_print:       total time =    1840.69 ms /    70 tokens
0.02.078.626 I ggml_metal_free: deallocating

real	0m2.373s
user	0m0.143s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.520 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.192 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.676 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.695 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.696 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.697 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.589 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.473 I llama_model_loader: - type  f32:  194 tensors
0.00.050.474 I llama_model_loader: - type  f16:   98 tensors
0.00.078.652 I llm_load_vocab: special tokens cache size = 25
0.00.085.145 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.148 I llm_load_print_meta: arch             = gptneox
0.00.085.149 I llm_load_print_meta: vocab type       = BPE
0.00.085.149 I llm_load_print_meta: n_vocab          = 50304
0.00.085.149 I llm_load_print_meta: n_merges         = 50009
0.00.085.149 I llm_load_print_meta: vocab_only       = 0
0.00.085.149 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.150 I llm_load_print_meta: n_embd           = 2048
0.00.085.150 I llm_load_print_meta: n_layer          = 24
0.00.085.164 I llm_load_print_meta: n_head           = 16
0.00.085.165 I llm_load_print_meta: n_head_kv        = 16
0.00.085.165 I llm_load_print_meta: n_rot            = 32
0.00.085.166 I llm_load_print_meta: n_swa            = 0
0.00.085.166 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.166 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.166 I llm_load_print_meta: n_gqa            = 1
0.00.085.167 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.168 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.168 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.168 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.169 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.169 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.169 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.169 I llm_load_print_meta: n_ff             = 8192
0.00.085.170 I llm_load_print_meta: n_expert         = 0
0.00.085.170 I llm_load_print_meta: n_expert_used    = 0
0.00.085.170 I llm_load_print_meta: causal attn      = 1
0.00.085.170 I llm_load_print_meta: pooling type     = 0
0.00.085.170 I llm_load_print_meta: rope type        = 2
0.00.085.170 I llm_load_print_meta: rope scaling     = linear
0.00.085.171 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.171 I llm_load_print_meta: freq_scale_train = 1
0.00.085.171 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.171 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.172 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.172 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.181 I llm_load_print_meta: model type       = 1.4B
0.00.085.182 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.182 I llm_load_print_meta: model params     = 1.41 B
0.00.085.183 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.183 I llm_load_print_meta: general.name     = 1.4B
0.00.085.183 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.183 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.183 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.184 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.184 I llm_load_print_meta: LF token         = 128 ''
0.00.085.184 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.184 I llm_load_print_meta: max token length = 1024
0.00.087.642 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.642 I llm_load_tensors: offloading output layer to GPU
0.00.087.642 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.653 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.654 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.580 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.581 I llama_new_context_with_model: n_ctx         = 128
0.00.088.581 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.581 I llama_new_context_with_model: n_batch       = 128
0.00.088.581 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.581 I llama_new_context_with_model: flash_attn    = 0
0.00.088.582 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.582 I llama_new_context_with_model: freq_scale    = 1
0.00.088.583 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.583 I ggml_metal_init: allocating
0.00.088.590 I ggml_metal_init: found device: Apple M4
0.00.088.594 I ggml_metal_init: picking default device: Apple M4
0.00.089.171 I ggml_metal_init: using embedded metal library
0.00.091.645 I ggml_metal_init: GPU name:   Apple M4
0.00.091.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.648 I ggml_metal_init: simdgroup reduction   = true
0.00.091.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.648 I ggml_metal_init: has bfloat            = true
0.00.091.648 I ggml_metal_init: use bfloat            = true
0.00.091.649 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.101 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.115 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.994 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.995 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.995 I llama_new_context_with_model: graph nodes  = 967
0.00.102.996 I llama_new_context_with_model: graph splits = 2
0.00.103.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.099.101 I 
0.01.099.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.099.219 I perplexity: tokenizing the input ..
0.01.112.471 I perplexity: tokenization took 13.248 ms
0.01.112.504 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.234.047 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.235.911 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.235.928 I llama_perf_context_print:        load time =    1078.87 ms
0.01.235.930 I llama_perf_context_print: prompt eval time =     121.00 ms /   128 tokens (    0.95 ms per token,  1057.84 tokens per second)
0.01.235.931 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.235.933 I llama_perf_context_print:       total time =     136.86 ms /   129 tokens
0.01.236.692 I ggml_metal_free: deallocating

real	0m1.428s
user	0m0.122s
sys	0m0.214s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.865 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.744 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.744 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.101 I llama_model_loader: - type  f32:  194 tensors
0.00.039.102 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.384 I llm_load_vocab: special tokens cache size = 25
0.00.071.200 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.204 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.205 I llm_load_print_meta: arch             = gptneox
0.00.071.205 I llm_load_print_meta: vocab type       = BPE
0.00.071.205 I llm_load_print_meta: n_vocab          = 50304
0.00.071.206 I llm_load_print_meta: n_merges         = 50009
0.00.071.206 I llm_load_print_meta: vocab_only       = 0
0.00.071.207 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.208 I llm_load_print_meta: n_embd           = 2048
0.00.071.208 I llm_load_print_meta: n_layer          = 24
0.00.071.228 I llm_load_print_meta: n_head           = 16
0.00.071.229 I llm_load_print_meta: n_head_kv        = 16
0.00.071.229 I llm_load_print_meta: n_rot            = 32
0.00.071.229 I llm_load_print_meta: n_swa            = 0
0.00.071.229 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.230 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.230 I llm_load_print_meta: n_gqa            = 1
0.00.071.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.232 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.233 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.233 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.233 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.233 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.238 I llm_load_print_meta: n_ff             = 8192
0.00.071.238 I llm_load_print_meta: n_expert         = 0
0.00.071.238 I llm_load_print_meta: n_expert_used    = 0
0.00.071.238 I llm_load_print_meta: causal attn      = 1
0.00.071.238 I llm_load_print_meta: pooling type     = 0
0.00.071.239 I llm_load_print_meta: rope type        = 2
0.00.071.239 I llm_load_print_meta: rope scaling     = linear
0.00.071.239 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.240 I llm_load_print_meta: freq_scale_train = 1
0.00.071.240 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.240 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.240 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.240 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.240 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.240 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.241 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.251 I llm_load_print_meta: model type       = 1.4B
0.00.071.252 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.253 I llm_load_print_meta: model params     = 1.41 B
0.00.071.253 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.253 I llm_load_print_meta: general.name     = 1.4B
0.00.071.253 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.254 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.254 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.254 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.254 I llm_load_print_meta: LF token         = 128 ''
0.00.071.255 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.255 I llm_load_print_meta: max token length = 1024
0.00.073.874 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.874 I llm_load_tensors: offloading output layer to GPU
0.00.073.874 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.886 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.887 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.960 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.961 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.961 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.961 I llama_new_context_with_model: n_batch       = 2048
0.00.074.962 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.962 I llama_new_context_with_model: flash_attn    = 0
0.00.074.962 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.963 I llama_new_context_with_model: freq_scale    = 1
0.00.074.963 I ggml_metal_init: allocating
0.00.074.966 I ggml_metal_init: found device: Apple M4
0.00.074.968 I ggml_metal_init: picking default device: Apple M4
0.00.075.741 I ggml_metal_init: using embedded metal library
0.00.078.655 I ggml_metal_init: GPU name:   Apple M4
0.00.078.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.658 I ggml_metal_init: simdgroup reduction   = true
0.00.078.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.658 I ggml_metal_init: has bfloat            = true
0.00.078.659 I ggml_metal_init: use bfloat            = true
0.00.078.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.659 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.724 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.732 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.754 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.790 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.791 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.792 I llama_new_context_with_model: graph nodes  = 967
0.00.115.792 I llama_new_context_with_model: graph splits = 2
0.00.115.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.152.615 I main: llama threadpool init, n_threads = 4
0.01.152.652 I 
0.01.152.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.152.683 I 
0.01.152.922 I sampler seed: 1234
0.01.152.926 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.152.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.152.947 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.152.947 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.252.219 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.02.252.220 I llama_perf_context_print:        load time =    1142.75 ms
0.02.252.221 I llama_perf_context_print: prompt eval time =      45.09 ms /     7 tokens (    6.44 ms per token,   155.25 tokens per second)
0.02.252.222 I llama_perf_context_print:        eval time =    1051.35 ms /    63 runs   (   16.69 ms per token,    59.92 tokens per second)
0.02.252.222 I llama_perf_context_print:       total time =    1099.61 ms /    70 tokens
0.02.252.417 I ggml_metal_free: deallocating

real	0m2.270s
user	0m0.119s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.151 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.785 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.238 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.238 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.239 I llama_model_loader: - type  f32:  194 tensors
0.00.034.240 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.667 I llm_load_vocab: special tokens cache size = 25
0.00.067.178 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.181 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.181 I llm_load_print_meta: arch             = gptneox
0.00.067.182 I llm_load_print_meta: vocab type       = BPE
0.00.067.182 I llm_load_print_meta: n_vocab          = 50304
0.00.067.182 I llm_load_print_meta: n_merges         = 50009
0.00.067.182 I llm_load_print_meta: vocab_only       = 0
0.00.067.183 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.183 I llm_load_print_meta: n_embd           = 2048
0.00.067.183 I llm_load_print_meta: n_layer          = 24
0.00.067.200 I llm_load_print_meta: n_head           = 16
0.00.067.201 I llm_load_print_meta: n_head_kv        = 16
0.00.067.201 I llm_load_print_meta: n_rot            = 32
0.00.067.201 I llm_load_print_meta: n_swa            = 0
0.00.067.201 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.201 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.204 I llm_load_print_meta: n_gqa            = 1
0.00.067.204 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.205 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.205 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.206 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.206 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.206 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.206 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.207 I llm_load_print_meta: n_ff             = 8192
0.00.067.207 I llm_load_print_meta: n_expert         = 0
0.00.067.207 I llm_load_print_meta: n_expert_used    = 0
0.00.067.207 I llm_load_print_meta: causal attn      = 1
0.00.067.207 I llm_load_print_meta: pooling type     = 0
0.00.067.208 I llm_load_print_meta: rope type        = 2
0.00.067.208 I llm_load_print_meta: rope scaling     = linear
0.00.067.208 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.209 I llm_load_print_meta: freq_scale_train = 1
0.00.067.209 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.211 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.211 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.211 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.212 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.222 I llm_load_print_meta: model type       = 1.4B
0.00.067.222 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.222 I llm_load_print_meta: model params     = 1.41 B
0.00.067.223 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.223 I llm_load_print_meta: general.name     = 1.4B
0.00.067.223 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.223 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.224 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.224 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.224 I llm_load_print_meta: LF token         = 128 ''
0.00.067.224 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.224 I llm_load_print_meta: max token length = 1024
0.00.069.585 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.585 I llm_load_tensors: offloading output layer to GPU
0.00.069.586 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.597 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.598 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.573 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.574 I llama_new_context_with_model: n_ctx         = 128
0.00.070.574 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.574 I llama_new_context_with_model: n_batch       = 128
0.00.070.574 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.575 I llama_new_context_with_model: flash_attn    = 0
0.00.070.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.576 I llama_new_context_with_model: freq_scale    = 1
0.00.070.576 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.577 I ggml_metal_init: allocating
0.00.070.583 I ggml_metal_init: found device: Apple M4
0.00.070.589 I ggml_metal_init: picking default device: Apple M4
0.00.071.253 I ggml_metal_init: using embedded metal library
0.00.073.963 I ggml_metal_init: GPU name:   Apple M4
0.00.073.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.966 I ggml_metal_init: simdgroup reduction   = true
0.00.073.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.966 I ggml_metal_init: has bfloat            = true
0.00.073.966 I ggml_metal_init: use bfloat            = true
0.00.073.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.615 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.618 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.637 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.540 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.541 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.542 I llama_new_context_with_model: graph nodes  = 967
0.00.086.542 I llama_new_context_with_model: graph splits = 2
0.00.086.555 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.973 I 
0.00.882.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.882.035 I perplexity: tokenizing the input ..
0.00.890.278 I perplexity: tokenization took 8.24 ms
0.00.890.288 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.014.953 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.016.130 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.016.142 I llama_perf_context_print:        load time =     869.81 ms
0.01.016.143 I llama_perf_context_print: prompt eval time =     124.44 ms /   128 tokens (    0.97 ms per token,  1028.61 tokens per second)
0.01.016.144 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.016.144 I llama_perf_context_print:       total time =     134.18 ms /   129 tokens
0.01.016.660 I ggml_metal_free: deallocating

real	0m1.035s
user	0m0.097s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.012.187 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.678 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.682 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.682 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.729 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.799 I llama_model_loader: - type  f32:  194 tensors
0.00.030.799 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.139 I llm_load_vocab: special tokens cache size = 25
0.00.058.152 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.156 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.156 I llm_load_print_meta: arch             = gptneox
0.00.058.157 I llm_load_print_meta: vocab type       = BPE
0.00.058.157 I llm_load_print_meta: n_vocab          = 50304
0.00.058.157 I llm_load_print_meta: n_merges         = 50009
0.00.058.157 I llm_load_print_meta: vocab_only       = 0
0.00.058.158 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.158 I llm_load_print_meta: n_embd           = 2048
0.00.058.160 I llm_load_print_meta: n_layer          = 24
0.00.058.176 I llm_load_print_meta: n_head           = 16
0.00.058.177 I llm_load_print_meta: n_head_kv        = 16
0.00.058.177 I llm_load_print_meta: n_rot            = 32
0.00.058.178 I llm_load_print_meta: n_swa            = 0
0.00.058.178 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.178 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.179 I llm_load_print_meta: n_gqa            = 1
0.00.058.179 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.180 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.180 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.181 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.181 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.181 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.181 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.181 I llm_load_print_meta: n_ff             = 8192
0.00.058.182 I llm_load_print_meta: n_expert         = 0
0.00.058.182 I llm_load_print_meta: n_expert_used    = 0
0.00.058.182 I llm_load_print_meta: causal attn      = 1
0.00.058.183 I llm_load_print_meta: pooling type     = 0
0.00.058.183 I llm_load_print_meta: rope type        = 2
0.00.058.183 I llm_load_print_meta: rope scaling     = linear
0.00.058.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.184 I llm_load_print_meta: freq_scale_train = 1
0.00.058.184 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.184 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.184 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.184 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.196 I llm_load_print_meta: model type       = 1.4B
0.00.058.196 I llm_load_print_meta: model ftype      = Q4_0
0.00.058.197 I llm_load_print_meta: model params     = 1.41 B
0.00.058.197 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.058.197 I llm_load_print_meta: general.name     = 1.4B
0.00.058.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: LF token         = 128 ''
0.00.058.198 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.199 I llm_load_print_meta: max token length = 1024
0.00.060.454 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.454 I llm_load_tensors: offloading output layer to GPU
0.00.060.455 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.466 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.060.467 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.061.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.562 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.562 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.562 I llama_new_context_with_model: n_batch       = 2048
0.00.061.562 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.563 I llama_new_context_with_model: flash_attn    = 0
0.00.061.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.563 I llama_new_context_with_model: freq_scale    = 1
0.00.061.564 I ggml_metal_init: allocating
0.00.061.570 I ggml_metal_init: found device: Apple M4
0.00.061.573 I ggml_metal_init: picking default device: Apple M4
0.00.062.290 I ggml_metal_init: using embedded metal library
0.00.064.893 I ggml_metal_init: GPU name:   Apple M4
0.00.064.895 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.895 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.896 I ggml_metal_init: simdgroup reduction   = true
0.00.064.896 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.896 I ggml_metal_init: has bfloat            = true
0.00.064.896 I ggml_metal_init: use bfloat            = true
0.00.064.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.519 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.527 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.550 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.673 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.675 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.675 I llama_new_context_with_model: graph nodes  = 967
0.00.103.675 I llama_new_context_with_model: graph splits = 2
0.00.103.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.516 I main: llama threadpool init, n_threads = 4
0.00.652.565 I 
0.00.652.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.602 I 
0.00.652.831 I sampler seed: 1234
0.00.652.836 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.652.848 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.652.848 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.652.848 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.335.252 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.335.254 I llama_perf_context_print:        load time =     640.32 ms
0.01.335.255 I llama_perf_context_print: prompt eval time =      39.81 ms /     7 tokens (    5.69 ms per token,   175.84 tokens per second)
0.01.335.255 I llama_perf_context_print:        eval time =     639.66 ms /    63 runs   (   10.15 ms per token,    98.49 tokens per second)
0.01.335.256 I llama_perf_context_print:       total time =     682.74 ms /    70 tokens
0.01.335.457 I ggml_metal_free: deallocating

real	0m1.356s
user	0m0.112s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.433 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.360 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.375 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.376 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.298 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.371 I llama_model_loader: - type  f32:  194 tensors
0.00.024.371 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.371 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.187 I llm_load_vocab: special tokens cache size = 25
0.00.051.203 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.205 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.206 I llm_load_print_meta: arch             = gptneox
0.00.051.206 I llm_load_print_meta: vocab type       = BPE
0.00.051.206 I llm_load_print_meta: n_vocab          = 50304
0.00.051.207 I llm_load_print_meta: n_merges         = 50009
0.00.051.207 I llm_load_print_meta: vocab_only       = 0
0.00.051.207 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.207 I llm_load_print_meta: n_embd           = 2048
0.00.051.207 I llm_load_print_meta: n_layer          = 24
0.00.051.221 I llm_load_print_meta: n_head           = 16
0.00.051.222 I llm_load_print_meta: n_head_kv        = 16
0.00.051.222 I llm_load_print_meta: n_rot            = 32
0.00.051.223 I llm_load_print_meta: n_swa            = 0
0.00.051.223 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.223 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.224 I llm_load_print_meta: n_gqa            = 1
0.00.051.224 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.225 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.226 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.229 I llm_load_print_meta: n_ff             = 8192
0.00.051.230 I llm_load_print_meta: n_expert         = 0
0.00.051.230 I llm_load_print_meta: n_expert_used    = 0
0.00.051.230 I llm_load_print_meta: causal attn      = 1
0.00.051.231 I llm_load_print_meta: pooling type     = 0
0.00.051.231 I llm_load_print_meta: rope type        = 2
0.00.051.231 I llm_load_print_meta: rope scaling     = linear
0.00.051.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.232 I llm_load_print_meta: freq_scale_train = 1
0.00.051.232 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.232 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.233 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.233 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.233 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.233 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.233 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.242 I llm_load_print_meta: model type       = 1.4B
0.00.051.243 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.243 I llm_load_print_meta: model params     = 1.41 B
0.00.051.244 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.244 I llm_load_print_meta: general.name     = 1.4B
0.00.051.245 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: LF token         = 128 ''
0.00.051.245 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.246 I llm_load_print_meta: max token length = 1024
0.00.053.197 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.197 I llm_load_tensors: offloading output layer to GPU
0.00.053.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.208 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.209 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.154 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.155 I llama_new_context_with_model: n_ctx         = 128
0.00.054.155 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.155 I llama_new_context_with_model: n_batch       = 128
0.00.054.155 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.156 I llama_new_context_with_model: flash_attn    = 0
0.00.054.156 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.156 I llama_new_context_with_model: freq_scale    = 1
0.00.054.157 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.157 I ggml_metal_init: allocating
0.00.054.163 I ggml_metal_init: found device: Apple M4
0.00.054.166 I ggml_metal_init: picking default device: Apple M4
0.00.054.726 I ggml_metal_init: using embedded metal library
0.00.057.091 I ggml_metal_init: GPU name:   Apple M4
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.093 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.093 I ggml_metal_init: simdgroup reduction   = true
0.00.057.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.094 I ggml_metal_init: has bfloat            = true
0.00.057.094 I ggml_metal_init: use bfloat            = true
0.00.057.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.710 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.715 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.731 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.578 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.579 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.579 I llama_new_context_with_model: graph nodes  = 967
0.00.068.580 I llama_new_context_with_model: graph splits = 2
0.00.068.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.298 I 
0.00.592.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.347 I perplexity: tokenizing the input ..
0.00.599.911 I perplexity: tokenization took 7.562 ms
0.00.599.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.722.269 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.723.400 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.723.417 I llama_perf_context_print:        load time =     582.85 ms
0.00.723.418 I llama_perf_context_print: prompt eval time =     122.09 ms /   128 tokens (    0.95 ms per token,  1048.44 tokens per second)
0.00.723.419 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.723.419 I llama_perf_context_print:       total time =     131.13 ms /   129 tokens
0.00.724.019 I ggml_metal_free: deallocating

real	0m0.739s
user	0m0.079s
sys	0m0.102s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.155 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.860 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.813 I llama_model_loader: - type  f32:  194 tensors
0.00.026.814 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.940 I llm_load_vocab: special tokens cache size = 25
0.00.053.975 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.978 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.978 I llm_load_print_meta: arch             = gptneox
0.00.053.979 I llm_load_print_meta: vocab type       = BPE
0.00.053.979 I llm_load_print_meta: n_vocab          = 50304
0.00.053.979 I llm_load_print_meta: n_merges         = 50009
0.00.053.979 I llm_load_print_meta: vocab_only       = 0
0.00.053.980 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.980 I llm_load_print_meta: n_embd           = 2048
0.00.053.980 I llm_load_print_meta: n_layer          = 24
0.00.053.995 I llm_load_print_meta: n_head           = 16
0.00.053.996 I llm_load_print_meta: n_head_kv        = 16
0.00.053.996 I llm_load_print_meta: n_rot            = 32
0.00.053.996 I llm_load_print_meta: n_swa            = 0
0.00.053.997 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.997 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.998 I llm_load_print_meta: n_gqa            = 1
0.00.053.998 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.999 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.000 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.000 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.000 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.000 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.000 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.001 I llm_load_print_meta: n_ff             = 8192
0.00.054.001 I llm_load_print_meta: n_expert         = 0
0.00.054.001 I llm_load_print_meta: n_expert_used    = 0
0.00.054.002 I llm_load_print_meta: causal attn      = 1
0.00.054.002 I llm_load_print_meta: pooling type     = 0
0.00.054.002 I llm_load_print_meta: rope type        = 2
0.00.054.003 I llm_load_print_meta: rope scaling     = linear
0.00.054.003 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.003 I llm_load_print_meta: freq_scale_train = 1
0.00.054.003 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.004 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.004 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.004 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.004 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.004 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.004 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.014 I llm_load_print_meta: model type       = 1.4B
0.00.054.014 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.014 I llm_load_print_meta: model params     = 1.41 B
0.00.054.015 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.015 I llm_load_print_meta: general.name     = 1.4B
0.00.054.015 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.015 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.016 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.016 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.016 I llm_load_print_meta: LF token         = 128 ''
0.00.054.016 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.016 I llm_load_print_meta: max token length = 1024
0.00.056.008 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.009 I llm_load_tensors: offloading output layer to GPU
0.00.056.009 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.019 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.021 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.057.402 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.403 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.403 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.403 I llama_new_context_with_model: n_batch       = 2048
0.00.057.404 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.404 I llama_new_context_with_model: flash_attn    = 0
0.00.057.404 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.405 I llama_new_context_with_model: freq_scale    = 1
0.00.057.405 I ggml_metal_init: allocating
0.00.057.411 I ggml_metal_init: found device: Apple M4
0.00.057.414 I ggml_metal_init: picking default device: Apple M4
0.00.058.022 I ggml_metal_init: using embedded metal library
0.00.060.370 I ggml_metal_init: GPU name:   Apple M4
0.00.060.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.372 I ggml_metal_init: simdgroup reduction   = true
0.00.060.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.372 I ggml_metal_init: has bfloat            = true
0.00.060.374 I ggml_metal_init: use bfloat            = true
0.00.060.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.395 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.402 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.420 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.503 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.504 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.505 I llama_new_context_with_model: graph nodes  = 967
0.00.090.505 I llama_new_context_with_model: graph splits = 2
0.00.090.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.338 I main: llama threadpool init, n_threads = 4
0.00.705.377 I 
0.00.705.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.408 I 
0.00.705.651 I sampler seed: 1234
0.00.705.656 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.675 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.675 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.675 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.430.721 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64253.39 tokens per second)
0.01.430.722 I llama_perf_context_print:        load time =     696.18 ms
0.01.430.723 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.75 tokens per second)
0.01.430.724 I llama_perf_context_print:        eval time =     682.61 ms /    63 runs   (   10.84 ms per token,    92.29 tokens per second)
0.01.430.724 I llama_perf_context_print:       total time =     725.39 ms /    70 tokens
0.01.430.911 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.234 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.098 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.101 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.101 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.839 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.840 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.840 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.840 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.841 I llama_model_loader: - type  f32:  194 tensors
0.00.026.841 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.842 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.804 I llm_load_vocab: special tokens cache size = 25
0.00.052.615 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.618 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.619 I llm_load_print_meta: arch             = gptneox
0.00.052.619 I llm_load_print_meta: vocab type       = BPE
0.00.052.619 I llm_load_print_meta: n_vocab          = 50304
0.00.052.620 I llm_load_print_meta: n_merges         = 50009
0.00.052.620 I llm_load_print_meta: vocab_only       = 0
0.00.052.620 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.620 I llm_load_print_meta: n_embd           = 2048
0.00.052.621 I llm_load_print_meta: n_layer          = 24
0.00.052.636 I llm_load_print_meta: n_head           = 16
0.00.052.636 I llm_load_print_meta: n_head_kv        = 16
0.00.052.637 I llm_load_print_meta: n_rot            = 32
0.00.052.637 I llm_load_print_meta: n_swa            = 0
0.00.052.637 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.637 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.638 I llm_load_print_meta: n_gqa            = 1
0.00.052.640 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.641 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.641 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.642 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.642 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.642 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.642 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.643 I llm_load_print_meta: n_ff             = 8192
0.00.052.643 I llm_load_print_meta: n_expert         = 0
0.00.052.643 I llm_load_print_meta: n_expert_used    = 0
0.00.052.643 I llm_load_print_meta: causal attn      = 1
0.00.052.643 I llm_load_print_meta: pooling type     = 0
0.00.052.643 I llm_load_print_meta: rope type        = 2
0.00.052.643 I llm_load_print_meta: rope scaling     = linear
0.00.052.645 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.645 I llm_load_print_meta: freq_scale_train = 1
0.00.052.645 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.645 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.646 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.646 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.646 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.646 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.655 I llm_load_print_meta: model type       = 1.4B
0.00.052.655 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.656 I llm_load_print_meta: model params     = 1.41 B
0.00.052.656 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.656 I llm_load_print_meta: general.name     = 1.4B
0.00.052.657 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.657 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.657 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.657 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.657 I llm_load_print_meta: LF token         = 128 ''
0.00.052.658 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.658 I llm_load_print_meta: max token length = 1024
0.00.054.258 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.258 I llm_load_tensors: offloading output layer to GPU
0.00.054.258 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.268 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.269 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.111 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.112 I llama_new_context_with_model: n_ctx         = 128
0.00.055.112 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.112 I llama_new_context_with_model: n_batch       = 128
0.00.055.112 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.113 I llama_new_context_with_model: flash_attn    = 0
0.00.055.113 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.113 I llama_new_context_with_model: freq_scale    = 1
0.00.055.114 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.114 I ggml_metal_init: allocating
0.00.055.117 I ggml_metal_init: found device: Apple M4
0.00.055.119 I ggml_metal_init: picking default device: Apple M4
0.00.055.705 I ggml_metal_init: using embedded metal library
0.00.058.027 I ggml_metal_init: GPU name:   Apple M4
0.00.058.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.030 I ggml_metal_init: simdgroup reduction   = true
0.00.058.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.030 I ggml_metal_init: has bfloat            = true
0.00.058.030 I ggml_metal_init: use bfloat            = true
0.00.058.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.818 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.821 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.835 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.726 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.727 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.727 I llama_new_context_with_model: graph nodes  = 967
0.00.069.727 I llama_new_context_with_model: graph splits = 2
0.00.069.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.164 I 
0.00.645.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.209 I perplexity: tokenizing the input ..
0.00.652.984 I perplexity: tokenization took 7.774 ms
0.00.652.994 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.594 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.776.730 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.776.741 I llama_perf_context_print:        load time =     635.92 ms
0.00.776.742 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.27 tokens per second)
0.00.776.743 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.743 I llama_perf_context_print:       total time =     131.59 ms /   129 tokens
0.00.777.179 I ggml_metal_free: deallocating

real	0m0.789s
user	0m0.081s
sys	0m0.113s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.011.341 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.083 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.038.088 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.099 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.204 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.206 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.206 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.207 I llama_model_loader: - type  f32:  194 tensors
0.00.048.208 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.208 I llama_model_loader: - type q6_K:    1 tensors
0.00.076.695 I llm_load_vocab: special tokens cache size = 25
0.00.086.326 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.330 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.331 I llm_load_print_meta: arch             = gptneox
0.00.086.331 I llm_load_print_meta: vocab type       = BPE
0.00.086.331 I llm_load_print_meta: n_vocab          = 50304
0.00.086.332 I llm_load_print_meta: n_merges         = 50009
0.00.086.332 I llm_load_print_meta: vocab_only       = 0
0.00.086.332 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.332 I llm_load_print_meta: n_embd           = 2048
0.00.086.332 I llm_load_print_meta: n_layer          = 24
0.00.086.342 I llm_load_print_meta: n_head           = 16
0.00.086.343 I llm_load_print_meta: n_head_kv        = 16
0.00.086.344 I llm_load_print_meta: n_rot            = 32
0.00.086.344 I llm_load_print_meta: n_swa            = 0
0.00.086.344 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.344 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.345 I llm_load_print_meta: n_gqa            = 1
0.00.086.346 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.347 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.348 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.350 I llm_load_print_meta: n_ff             = 8192
0.00.086.350 I llm_load_print_meta: n_expert         = 0
0.00.086.351 I llm_load_print_meta: n_expert_used    = 0
0.00.086.351 I llm_load_print_meta: causal attn      = 1
0.00.086.351 I llm_load_print_meta: pooling type     = 0
0.00.086.351 I llm_load_print_meta: rope type        = 2
0.00.086.351 I llm_load_print_meta: rope scaling     = linear
0.00.086.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.352 I llm_load_print_meta: freq_scale_train = 1
0.00.086.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.353 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.353 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.353 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.354 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.359 I llm_load_print_meta: model type       = 1.4B
0.00.086.359 I llm_load_print_meta: model ftype      = Q5_0
0.00.086.360 I llm_load_print_meta: model params     = 1.41 B
0.00.086.361 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.086.361 I llm_load_print_meta: general.name     = 1.4B
0.00.086.362 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.362 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.362 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.363 I llm_load_print_meta: LF token         = 128 ''
0.00.086.363 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.363 I llm_load_print_meta: max token length = 1024
0.00.089.000 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.000 I llm_load_tensors: offloading output layer to GPU
0.00.089.000 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.007 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.089.007 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.090.497 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.498 I llama_new_context_with_model: n_ctx         = 2048
0.00.090.499 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.090.499 I llama_new_context_with_model: n_batch       = 2048
0.00.090.499 I llama_new_context_with_model: n_ubatch      = 512
0.00.090.500 I llama_new_context_with_model: flash_attn    = 0
0.00.090.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.501 I llama_new_context_with_model: freq_scale    = 1
0.00.090.501 I ggml_metal_init: allocating
0.00.090.510 I ggml_metal_init: found device: Apple M4
0.00.090.514 I ggml_metal_init: picking default device: Apple M4
0.00.091.382 I ggml_metal_init: using embedded metal library
0.00.095.402 I ggml_metal_init: GPU name:   Apple M4
0.00.095.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.405 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.405 I ggml_metal_init: simdgroup reduction   = true
0.00.095.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.406 I ggml_metal_init: has bfloat            = true
0.00.095.406 I ggml_metal_init: use bfloat            = true
0.00.095.406 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.128.489 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.128.494 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.522 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.129.454 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.129.456 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.129.456 I llama_new_context_with_model: graph nodes  = 967
0.00.129.456 I llama_new_context_with_model: graph splits = 2
0.00.129.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.720 I main: llama threadpool init, n_threads = 4
0.00.847.759 I 
0.00.847.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.793 I 
0.00.848.023 I sampler seed: 1234
0.00.848.027 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.068 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.068 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.068 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.647.686 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.647.687 I llama_perf_context_print:        load time =     836.37 ms
0.01.647.688 I llama_perf_context_print: prompt eval time =      49.89 ms /     7 tokens (    7.13 ms per token,   140.30 tokens per second)
0.01.647.688 I llama_perf_context_print:        eval time =     746.68 ms /    63 runs   (   11.85 ms per token,    84.37 tokens per second)
0.01.647.689 I llama_perf_context_print:       total time =     799.97 ms /    70 tokens
0.01.647.908 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.130s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.662 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.530 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.531 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.531 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.531 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.532 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.533 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.533 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.533 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.534 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.534 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.497 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.510 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.511 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.512 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.512 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.513 I llama_model_loader: - type  f32:  194 tensors
0.00.025.514 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.514 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.489 I llm_load_vocab: special tokens cache size = 25
0.00.052.475 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.478 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.478 I llm_load_print_meta: arch             = gptneox
0.00.052.478 I llm_load_print_meta: vocab type       = BPE
0.00.052.479 I llm_load_print_meta: n_vocab          = 50304
0.00.052.479 I llm_load_print_meta: n_merges         = 50009
0.00.052.479 I llm_load_print_meta: vocab_only       = 0
0.00.052.479 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.479 I llm_load_print_meta: n_embd           = 2048
0.00.052.480 I llm_load_print_meta: n_layer          = 24
0.00.052.494 I llm_load_print_meta: n_head           = 16
0.00.052.495 I llm_load_print_meta: n_head_kv        = 16
0.00.052.495 I llm_load_print_meta: n_rot            = 32
0.00.052.495 I llm_load_print_meta: n_swa            = 0
0.00.052.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.496 I llm_load_print_meta: n_gqa            = 1
0.00.052.497 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.497 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.498 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.498 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.499 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.499 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.499 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.500 I llm_load_print_meta: n_ff             = 8192
0.00.052.500 I llm_load_print_meta: n_expert         = 0
0.00.052.500 I llm_load_print_meta: n_expert_used    = 0
0.00.052.500 I llm_load_print_meta: causal attn      = 1
0.00.052.500 I llm_load_print_meta: pooling type     = 0
0.00.052.500 I llm_load_print_meta: rope type        = 2
0.00.052.501 I llm_load_print_meta: rope scaling     = linear
0.00.052.501 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.501 I llm_load_print_meta: freq_scale_train = 1
0.00.052.501 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.502 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.502 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.502 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.502 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.502 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.502 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.511 I llm_load_print_meta: model type       = 1.4B
0.00.052.511 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.512 I llm_load_print_meta: model params     = 1.41 B
0.00.052.512 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.512 I llm_load_print_meta: general.name     = 1.4B
0.00.052.513 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.513 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.514 I llm_load_print_meta: LF token         = 128 ''
0.00.052.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.514 I llm_load_print_meta: max token length = 1024
0.00.054.157 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.157 I llm_load_tensors: offloading output layer to GPU
0.00.054.158 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.168 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.169 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.053 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.054 I llama_new_context_with_model: n_ctx         = 128
0.00.055.054 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.054 I llama_new_context_with_model: n_batch       = 128
0.00.055.054 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.054 I llama_new_context_with_model: flash_attn    = 0
0.00.055.055 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.055 I llama_new_context_with_model: freq_scale    = 1
0.00.055.056 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.056 I ggml_metal_init: allocating
0.00.055.063 I ggml_metal_init: found device: Apple M4
0.00.055.065 I ggml_metal_init: picking default device: Apple M4
0.00.055.632 I ggml_metal_init: using embedded metal library
0.00.057.956 I ggml_metal_init: GPU name:   Apple M4
0.00.057.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.958 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.958 I ggml_metal_init: simdgroup reduction   = true
0.00.057.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.959 I ggml_metal_init: has bfloat            = true
0.00.057.959 I ggml_metal_init: use bfloat            = true
0.00.057.959 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.741 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.744 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.759 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.657 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.658 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.658 I llama_new_context_with_model: graph nodes  = 967
0.00.069.658 I llama_new_context_with_model: graph splits = 2
0.00.069.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.532 I 
0.00.737.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.568 I perplexity: tokenizing the input ..
0.00.744.923 I perplexity: tokenization took 7.353 ms
0.00.744.936 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.692 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.880.826 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.880.836 I llama_perf_context_print:        load time =     726.86 ms
0.00.880.837 I llama_perf_context_print: prompt eval time =     134.50 ms /   128 tokens (    1.05 ms per token,   951.69 tokens per second)
0.00.880.838 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.880.839 I llama_perf_context_print:       total time =     143.31 ms /   129 tokens
0.00.881.171 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.079s
sys	0m0.118s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.325 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.806 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.025.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.811 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.813 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.814 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.034.706 I llama_model_loader: - type  f32:  194 tensors
0.00.034.707 I llama_model_loader: - type q5_1:   97 tensors
0.00.034.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.887 I llm_load_vocab: special tokens cache size = 25
0.00.064.014 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.016 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.017 I llm_load_print_meta: arch             = gptneox
0.00.064.017 I llm_load_print_meta: vocab type       = BPE
0.00.064.017 I llm_load_print_meta: n_vocab          = 50304
0.00.064.017 I llm_load_print_meta: n_merges         = 50009
0.00.064.018 I llm_load_print_meta: vocab_only       = 0
0.00.064.018 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.018 I llm_load_print_meta: n_embd           = 2048
0.00.064.018 I llm_load_print_meta: n_layer          = 24
0.00.064.032 I llm_load_print_meta: n_head           = 16
0.00.064.033 I llm_load_print_meta: n_head_kv        = 16
0.00.064.034 I llm_load_print_meta: n_rot            = 32
0.00.064.034 I llm_load_print_meta: n_swa            = 0
0.00.064.034 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.034 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.035 I llm_load_print_meta: n_gqa            = 1
0.00.064.035 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.036 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.036 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.037 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.037 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.037 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.037 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.038 I llm_load_print_meta: n_ff             = 8192
0.00.064.038 I llm_load_print_meta: n_expert         = 0
0.00.064.038 I llm_load_print_meta: n_expert_used    = 0
0.00.064.039 I llm_load_print_meta: causal attn      = 1
0.00.064.041 I llm_load_print_meta: pooling type     = 0
0.00.064.041 I llm_load_print_meta: rope type        = 2
0.00.064.041 I llm_load_print_meta: rope scaling     = linear
0.00.064.042 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.042 I llm_load_print_meta: freq_scale_train = 1
0.00.064.042 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.042 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.042 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.042 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.042 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.043 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.043 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.052 I llm_load_print_meta: model type       = 1.4B
0.00.064.052 I llm_load_print_meta: model ftype      = Q5_1
0.00.064.052 I llm_load_print_meta: model params     = 1.41 B
0.00.064.053 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.064.053 I llm_load_print_meta: general.name     = 1.4B
0.00.064.053 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.054 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.054 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.054 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.054 I llm_load_print_meta: LF token         = 128 ''
0.00.064.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.055 I llm_load_print_meta: max token length = 1024
0.00.066.135 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.135 I llm_load_tensors: offloading output layer to GPU
0.00.066.136 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.146 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.066.147 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.067.113 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.114 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.114 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.114 I llama_new_context_with_model: n_batch       = 2048
0.00.067.114 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.114 I llama_new_context_with_model: flash_attn    = 0
0.00.067.115 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.115 I llama_new_context_with_model: freq_scale    = 1
0.00.067.115 I ggml_metal_init: allocating
0.00.067.118 I ggml_metal_init: found device: Apple M4
0.00.067.120 I ggml_metal_init: picking default device: Apple M4
0.00.067.720 I ggml_metal_init: using embedded metal library
0.00.070.241 I ggml_metal_init: GPU name:   Apple M4
0.00.070.242 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.242 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.243 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.243 I ggml_metal_init: simdgroup reduction   = true
0.00.070.244 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.245 I ggml_metal_init: has bfloat            = true
0.00.070.245 I ggml_metal_init: use bfloat            = true
0.00.070.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.750 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.755 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.773 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.839 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.841 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.841 I llama_new_context_with_model: graph nodes  = 967
0.00.100.841 I llama_new_context_with_model: graph splits = 2
0.00.100.855 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.871.198 I main: llama threadpool init, n_threads = 4
0.00.871.234 I 
0.00.871.265 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.871.265 I 
0.00.871.495 I sampler seed: 1234
0.00.871.501 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.871.540 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.871.544 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.871.545 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.718.470 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.718.470 I llama_perf_context_print:        load time =     861.87 ms
0.01.718.471 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.61 tokens per second)
0.01.718.471 I llama_perf_context_print:        eval time =     801.66 ms /    63 runs   (   12.72 ms per token,    78.59 tokens per second)
0.01.718.472 I llama_perf_context_print:       total time =     847.27 ms /    70 tokens
0.01.718.673 I ggml_metal_free: deallocating

real	0m1.733s
user	0m0.112s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.259 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.129 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.135 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.958 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.749 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.749 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.749 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.750 I llama_model_loader: - type  f32:  194 tensors
0.00.023.751 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.751 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.880 I llm_load_vocab: special tokens cache size = 25
0.00.049.937 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.940 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.940 I llm_load_print_meta: arch             = gptneox
0.00.049.940 I llm_load_print_meta: vocab type       = BPE
0.00.049.941 I llm_load_print_meta: n_vocab          = 50304
0.00.049.941 I llm_load_print_meta: n_merges         = 50009
0.00.049.941 I llm_load_print_meta: vocab_only       = 0
0.00.049.941 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.941 I llm_load_print_meta: n_embd           = 2048
0.00.049.941 I llm_load_print_meta: n_layer          = 24
0.00.049.955 I llm_load_print_meta: n_head           = 16
0.00.049.956 I llm_load_print_meta: n_head_kv        = 16
0.00.049.957 I llm_load_print_meta: n_rot            = 32
0.00.049.957 I llm_load_print_meta: n_swa            = 0
0.00.049.957 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.957 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.958 I llm_load_print_meta: n_gqa            = 1
0.00.049.958 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.961 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.961 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.962 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.962 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.962 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.963 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.963 I llm_load_print_meta: n_ff             = 8192
0.00.049.963 I llm_load_print_meta: n_expert         = 0
0.00.049.964 I llm_load_print_meta: n_expert_used    = 0
0.00.049.964 I llm_load_print_meta: causal attn      = 1
0.00.049.964 I llm_load_print_meta: pooling type     = 0
0.00.049.964 I llm_load_print_meta: rope type        = 2
0.00.049.964 I llm_load_print_meta: rope scaling     = linear
0.00.049.965 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.965 I llm_load_print_meta: freq_scale_train = 1
0.00.049.965 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.965 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.966 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.966 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.966 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.966 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.966 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.975 I llm_load_print_meta: model type       = 1.4B
0.00.049.976 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.978 I llm_load_print_meta: model params     = 1.41 B
0.00.049.978 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.978 I llm_load_print_meta: general.name     = 1.4B
0.00.049.979 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.979 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.979 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.979 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.979 I llm_load_print_meta: LF token         = 128 ''
0.00.049.980 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.980 I llm_load_print_meta: max token length = 1024
0.00.051.579 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.579 I llm_load_tensors: offloading output layer to GPU
0.00.051.579 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.589 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.590 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.463 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.464 I llama_new_context_with_model: n_ctx         = 128
0.00.052.464 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.464 I llama_new_context_with_model: n_batch       = 128
0.00.052.464 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.465 I llama_new_context_with_model: flash_attn    = 0
0.00.052.465 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.465 I llama_new_context_with_model: freq_scale    = 1
0.00.052.466 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.466 I ggml_metal_init: allocating
0.00.052.472 I ggml_metal_init: found device: Apple M4
0.00.052.475 I ggml_metal_init: picking default device: Apple M4
0.00.053.038 I ggml_metal_init: using embedded metal library
0.00.055.365 I ggml_metal_init: GPU name:   Apple M4
0.00.055.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.367 I ggml_metal_init: simdgroup reduction   = true
0.00.055.367 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.367 I ggml_metal_init: has bfloat            = true
0.00.055.367 I ggml_metal_init: use bfloat            = true
0.00.055.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.039 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.043 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.058 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.875 I llama_new_context_with_model: graph nodes  = 967
0.00.066.876 I llama_new_context_with_model: graph splits = 2
0.00.066.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.798 I 
0.00.742.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.832 I perplexity: tokenizing the input ..
0.00.750.594 I perplexity: tokenization took 7.761 ms
0.00.750.605 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.885.385 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.886.559 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.886.578 I llama_perf_context_print:        load time =     733.53 ms
0.00.886.579 I llama_perf_context_print: prompt eval time =     134.52 ms /   128 tokens (    1.05 ms per token,   951.52 tokens per second)
0.00.886.580 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.886.581 I llama_perf_context_print:       total time =     143.79 ms /   129 tokens
0.00.887.161 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.078s
sys	0m0.133s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.015.886 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.022.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.786 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.786 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.787 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.789 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.789 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.789 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.790 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.791 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.791 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.792 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.794 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.794 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.794 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.595 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.595 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.596 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.596 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.596 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.032.597 I llama_model_loader: - type  f32:  194 tensors
0.00.032.597 I llama_model_loader: - type q2_K:   49 tensors
0.00.032.597 I llama_model_loader: - type q3_K:   48 tensors
0.00.032.597 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.237 I llm_load_vocab: special tokens cache size = 25
0.00.065.119 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.122 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.122 I llm_load_print_meta: arch             = gptneox
0.00.065.123 I llm_load_print_meta: vocab type       = BPE
0.00.065.123 I llm_load_print_meta: n_vocab          = 50304
0.00.065.123 I llm_load_print_meta: n_merges         = 50009
0.00.065.123 I llm_load_print_meta: vocab_only       = 0
0.00.065.123 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.124 I llm_load_print_meta: n_embd           = 2048
0.00.065.124 I llm_load_print_meta: n_layer          = 24
0.00.065.132 I llm_load_print_meta: n_head           = 16
0.00.065.133 I llm_load_print_meta: n_head_kv        = 16
0.00.065.133 I llm_load_print_meta: n_rot            = 32
0.00.065.134 I llm_load_print_meta: n_swa            = 0
0.00.065.134 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.134 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.135 I llm_load_print_meta: n_gqa            = 1
0.00.065.135 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.136 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.137 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.137 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.137 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.137 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.137 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.139 I llm_load_print_meta: n_ff             = 8192
0.00.065.139 I llm_load_print_meta: n_expert         = 0
0.00.065.139 I llm_load_print_meta: n_expert_used    = 0
0.00.065.139 I llm_load_print_meta: causal attn      = 1
0.00.065.140 I llm_load_print_meta: pooling type     = 0
0.00.065.140 I llm_load_print_meta: rope type        = 2
0.00.065.140 I llm_load_print_meta: rope scaling     = linear
0.00.065.141 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.142 I llm_load_print_meta: freq_scale_train = 1
0.00.065.142 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.142 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.142 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.142 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.143 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.143 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.147 I llm_load_print_meta: model type       = 1.4B
0.00.065.148 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.065.148 I llm_load_print_meta: model params     = 1.41 B
0.00.065.149 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.065.149 I llm_load_print_meta: general.name     = 1.4B
0.00.065.149 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.149 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.149 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.150 I llm_load_print_meta: LF token         = 128 ''
0.00.065.150 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.150 I llm_load_print_meta: max token length = 1024
0.00.067.102 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.103 I llm_load_tensors: offloading output layer to GPU
0.00.067.103 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.108 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.067.109 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.068.172 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.173 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.173 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.173 I llama_new_context_with_model: n_batch       = 2048
0.00.068.174 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.174 I llama_new_context_with_model: flash_attn    = 0
0.00.068.174 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.175 I llama_new_context_with_model: freq_scale    = 1
0.00.068.175 I ggml_metal_init: allocating
0.00.068.180 I ggml_metal_init: found device: Apple M4
0.00.068.182 I ggml_metal_init: picking default device: Apple M4
0.00.068.834 I ggml_metal_init: using embedded metal library
0.00.071.360 I ggml_metal_init: GPU name:   Apple M4
0.00.071.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.363 I ggml_metal_init: simdgroup reduction   = true
0.00.071.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.363 I ggml_metal_init: has bfloat            = true
0.00.071.363 I ggml_metal_init: use bfloat            = true
0.00.071.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.852 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.866 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.887 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.986 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.988 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.988 I llama_new_context_with_model: graph nodes  = 967
0.00.103.988 I llama_new_context_with_model: graph splits = 2
0.00.104.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.500 I main: llama threadpool init, n_threads = 4
0.00.508.541 I 
0.00.508.572 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.574 I 
0.00.508.796 I sampler seed: 1234
0.00.508.801 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.508.866 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.508.871 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.508.871 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.194.757 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62665.49 tokens per second)
0.01.194.758 I llama_perf_context_print:        load time =     492.61 ms
0.01.194.758 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.62 tokens per second)
0.01.194.759 I llama_perf_context_print:        eval time =     643.40 ms /    63 runs   (   10.21 ms per token,    97.92 tokens per second)
0.01.194.760 I llama_perf_context_print:       total time =     686.26 ms /    70 tokens
0.01.194.967 I ggml_metal_free: deallocating

real	0m1.218s
user	0m0.117s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.506 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.823 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.824 I llama_model_loader: - type  f32:  194 tensors
0.00.024.824 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.825 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.732 I llm_load_vocab: special tokens cache size = 25
0.00.051.663 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.666 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.666 I llm_load_print_meta: arch             = gptneox
0.00.051.666 I llm_load_print_meta: vocab type       = BPE
0.00.051.667 I llm_load_print_meta: n_vocab          = 50304
0.00.051.667 I llm_load_print_meta: n_merges         = 50009
0.00.051.667 I llm_load_print_meta: vocab_only       = 0
0.00.051.667 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.667 I llm_load_print_meta: n_embd           = 2048
0.00.051.668 I llm_load_print_meta: n_layer          = 24
0.00.051.681 I llm_load_print_meta: n_head           = 16
0.00.051.682 I llm_load_print_meta: n_head_kv        = 16
0.00.051.682 I llm_load_print_meta: n_rot            = 32
0.00.051.683 I llm_load_print_meta: n_swa            = 0
0.00.051.683 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.683 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.684 I llm_load_print_meta: n_gqa            = 1
0.00.051.684 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.685 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.686 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.686 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.687 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.687 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.687 I llm_load_print_meta: n_ff             = 8192
0.00.051.688 I llm_load_print_meta: n_expert         = 0
0.00.051.688 I llm_load_print_meta: n_expert_used    = 0
0.00.051.688 I llm_load_print_meta: causal attn      = 1
0.00.051.688 I llm_load_print_meta: pooling type     = 0
0.00.051.688 I llm_load_print_meta: rope type        = 2
0.00.051.688 I llm_load_print_meta: rope scaling     = linear
0.00.051.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.691 I llm_load_print_meta: freq_scale_train = 1
0.00.051.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.691 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.691 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.691 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.692 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.692 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.692 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.702 I llm_load_print_meta: model type       = 1.4B
0.00.051.703 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.703 I llm_load_print_meta: model params     = 1.41 B
0.00.051.704 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.704 I llm_load_print_meta: general.name     = 1.4B
0.00.051.704 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.705 I llm_load_print_meta: LF token         = 128 ''
0.00.051.705 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.705 I llm_load_print_meta: max token length = 1024
0.00.053.319 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.319 I llm_load_tensors: offloading output layer to GPU
0.00.053.319 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.330 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.331 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.186 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.187 I llama_new_context_with_model: n_ctx         = 128
0.00.054.187 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.187 I llama_new_context_with_model: n_batch       = 128
0.00.054.187 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.188 I llama_new_context_with_model: flash_attn    = 0
0.00.054.188 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.188 I llama_new_context_with_model: freq_scale    = 1
0.00.054.189 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.189 I ggml_metal_init: allocating
0.00.054.195 I ggml_metal_init: found device: Apple M4
0.00.054.197 I ggml_metal_init: picking default device: Apple M4
0.00.054.745 I ggml_metal_init: using embedded metal library
0.00.057.062 I ggml_metal_init: GPU name:   Apple M4
0.00.057.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.063 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.063 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.064 I ggml_metal_init: simdgroup reduction   = true
0.00.057.064 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.064 I ggml_metal_init: has bfloat            = true
0.00.057.064 I ggml_metal_init: use bfloat            = true
0.00.057.064 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.866 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.870 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.895 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.761 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.762 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.762 I llama_new_context_with_model: graph nodes  = 967
0.00.068.762 I llama_new_context_with_model: graph splits = 2
0.00.068.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.029 I 
0.00.437.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.071 I perplexity: tokenizing the input ..
0.00.444.520 I perplexity: tokenization took 7.447 ms
0.00.444.531 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.576.783 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.577.923 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.577.936 I llama_perf_context_print:        load time =     426.51 ms
0.00.577.937 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.74 tokens per second)
0.00.577.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.577.938 I llama_perf_context_print:       total time =     140.91 ms /   129 tokens
0.00.578.495 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.079s
sys	0m0.079s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.411 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.731 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.732 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.733 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.733 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.736 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.736 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.737 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.741 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.634 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.546 I llama_model_loader: - type  f32:  194 tensors
0.00.024.547 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.547 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.547 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.537 I llm_load_vocab: special tokens cache size = 25
0.00.051.383 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.385 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.386 I llm_load_print_meta: arch             = gptneox
0.00.051.386 I llm_load_print_meta: vocab type       = BPE
0.00.051.386 I llm_load_print_meta: n_vocab          = 50304
0.00.051.387 I llm_load_print_meta: n_merges         = 50009
0.00.051.387 I llm_load_print_meta: vocab_only       = 0
0.00.051.387 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.387 I llm_load_print_meta: n_embd           = 2048
0.00.051.387 I llm_load_print_meta: n_layer          = 24
0.00.051.401 I llm_load_print_meta: n_head           = 16
0.00.051.403 I llm_load_print_meta: n_head_kv        = 16
0.00.051.403 I llm_load_print_meta: n_rot            = 32
0.00.051.404 I llm_load_print_meta: n_swa            = 0
0.00.051.404 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.404 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.405 I llm_load_print_meta: n_gqa            = 1
0.00.051.406 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.406 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.407 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.407 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.408 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.408 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.408 I llm_load_print_meta: n_ff             = 8192
0.00.051.408 I llm_load_print_meta: n_expert         = 0
0.00.051.409 I llm_load_print_meta: n_expert_used    = 0
0.00.051.409 I llm_load_print_meta: causal attn      = 1
0.00.051.409 I llm_load_print_meta: pooling type     = 0
0.00.051.409 I llm_load_print_meta: rope type        = 2
0.00.051.409 I llm_load_print_meta: rope scaling     = linear
0.00.051.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.410 I llm_load_print_meta: freq_scale_train = 1
0.00.051.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.420 I llm_load_print_meta: model type       = 1.4B
0.00.051.420 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.421 I llm_load_print_meta: model params     = 1.41 B
0.00.051.421 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.421 I llm_load_print_meta: general.name     = 1.4B
0.00.051.422 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.422 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.424 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.424 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.424 I llm_load_print_meta: LF token         = 128 ''
0.00.051.424 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.424 I llm_load_print_meta: max token length = 1024
0.00.053.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.343 I llm_load_tensors: offloading output layer to GPU
0.00.053.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.354 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.355 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.315 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.316 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.316 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.316 I llama_new_context_with_model: n_batch       = 2048
0.00.054.316 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.317 I llama_new_context_with_model: flash_attn    = 0
0.00.054.317 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.317 I llama_new_context_with_model: freq_scale    = 1
0.00.054.318 I ggml_metal_init: allocating
0.00.054.323 I ggml_metal_init: found device: Apple M4
0.00.054.325 I ggml_metal_init: picking default device: Apple M4
0.00.054.925 I ggml_metal_init: using embedded metal library
0.00.057.298 I ggml_metal_init: GPU name:   Apple M4
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.300 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.301 I ggml_metal_init: simdgroup reduction   = true
0.00.057.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.301 I ggml_metal_init: has bfloat            = true
0.00.057.301 I ggml_metal_init: use bfloat            = true
0.00.057.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.635 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.643 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.711 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.712 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.712 I llama_new_context_with_model: graph nodes  = 967
0.00.087.713 I llama_new_context_with_model: graph splits = 2
0.00.087.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.349 I main: llama threadpool init, n_threads = 4
0.00.600.387 I 
0.00.600.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.430 I 
0.00.600.656 I sampler seed: 1234
0.00.600.660 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.600.711 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.600.713 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.600.713 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.350.445 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.350.447 I llama_perf_context_print:        load time =     590.94 ms
0.01.350.448 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.62 tokens per second)
0.01.350.448 I llama_perf_context_print:        eval time =     702.28 ms /    63 runs   (   11.15 ms per token,    89.71 tokens per second)
0.01.350.449 I llama_perf_context_print:       total time =     750.10 ms /    70 tokens
0.01.350.638 I ggml_metal_free: deallocating

real	0m1.367s
user	0m0.110s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.386 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.013.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.964 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.965 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.645 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.646 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.647 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.647 I llama_model_loader: - type  f32:  194 tensors
0.00.022.648 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.648 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.648 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.648 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.742 I llm_load_vocab: special tokens cache size = 25
0.00.048.792 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.795 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.795 I llm_load_print_meta: arch             = gptneox
0.00.048.795 I llm_load_print_meta: vocab type       = BPE
0.00.048.795 I llm_load_print_meta: n_vocab          = 50304
0.00.048.796 I llm_load_print_meta: n_merges         = 50009
0.00.048.796 I llm_load_print_meta: vocab_only       = 0
0.00.048.796 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.796 I llm_load_print_meta: n_embd           = 2048
0.00.048.796 I llm_load_print_meta: n_layer          = 24
0.00.048.811 I llm_load_print_meta: n_head           = 16
0.00.048.813 I llm_load_print_meta: n_head_kv        = 16
0.00.048.813 I llm_load_print_meta: n_rot            = 32
0.00.048.813 I llm_load_print_meta: n_swa            = 0
0.00.048.813 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.813 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.814 I llm_load_print_meta: n_gqa            = 1
0.00.048.815 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.816 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.816 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.817 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.817 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.817 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.817 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.818 I llm_load_print_meta: n_ff             = 8192
0.00.048.818 I llm_load_print_meta: n_expert         = 0
0.00.048.820 I llm_load_print_meta: n_expert_used    = 0
0.00.048.820 I llm_load_print_meta: causal attn      = 1
0.00.048.820 I llm_load_print_meta: pooling type     = 0
0.00.048.820 I llm_load_print_meta: rope type        = 2
0.00.048.820 I llm_load_print_meta: rope scaling     = linear
0.00.048.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.821 I llm_load_print_meta: freq_scale_train = 1
0.00.048.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.831 I llm_load_print_meta: model type       = 1.4B
0.00.048.831 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.832 I llm_load_print_meta: model params     = 1.41 B
0.00.048.832 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.832 I llm_load_print_meta: general.name     = 1.4B
0.00.048.833 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.833 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.834 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.834 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.834 I llm_load_print_meta: LF token         = 128 ''
0.00.048.835 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.835 I llm_load_print_meta: max token length = 1024
0.00.050.373 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.373 I llm_load_tensors: offloading output layer to GPU
0.00.050.373 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.383 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.384 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.234 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.234 I llama_new_context_with_model: n_ctx         = 128
0.00.051.234 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.235 I llama_new_context_with_model: n_batch       = 128
0.00.051.235 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.235 I llama_new_context_with_model: flash_attn    = 0
0.00.051.236 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.236 I llama_new_context_with_model: freq_scale    = 1
0.00.051.236 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.237 I ggml_metal_init: allocating
0.00.051.243 I ggml_metal_init: found device: Apple M4
0.00.051.245 I ggml_metal_init: picking default device: Apple M4
0.00.051.850 I ggml_metal_init: using embedded metal library
0.00.054.181 I ggml_metal_init: GPU name:   Apple M4
0.00.054.182 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.183 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.183 I ggml_metal_init: simdgroup reduction   = true
0.00.054.183 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.183 I ggml_metal_init: has bfloat            = true
0.00.054.184 I ggml_metal_init: use bfloat            = true
0.00.054.184 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.185 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.873 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.876 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.892 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.790 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.791 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.791 I llama_new_context_with_model: graph nodes  = 967
0.00.065.791 I llama_new_context_with_model: graph splits = 2
0.00.065.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.142 I 
0.00.498.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.197 I perplexity: tokenizing the input ..
0.00.506.563 I perplexity: tokenization took 8.364 ms
0.00.506.578 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.638.080 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.639.243 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.639.264 I llama_perf_context_print:        load time =     489.74 ms
0.00.639.265 I llama_perf_context_print: prompt eval time =     131.26 ms /   128 tokens (    1.03 ms per token,   975.19 tokens per second)
0.00.639.266 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.266 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.639.582 I ggml_metal_free: deallocating

real	0m0.652s
user	0m0.078s
sys	0m0.094s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.012.632 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.030.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.137 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.137 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.138 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.874 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.296 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.043.300 I llama_model_loader: - type  f32:  194 tensors
0.00.043.301 I llama_model_loader: - type q4_K:   61 tensors
0.00.043.301 I llama_model_loader: - type q5_K:   24 tensors
0.00.043.301 I llama_model_loader: - type q6_K:   13 tensors
0.00.083.304 I llm_load_vocab: special tokens cache size = 25
0.00.092.791 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.795 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.795 I llm_load_print_meta: arch             = gptneox
0.00.092.796 I llm_load_print_meta: vocab type       = BPE
0.00.092.796 I llm_load_print_meta: n_vocab          = 50304
0.00.092.796 I llm_load_print_meta: n_merges         = 50009
0.00.092.796 I llm_load_print_meta: vocab_only       = 0
0.00.092.797 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.797 I llm_load_print_meta: n_embd           = 2048
0.00.092.797 I llm_load_print_meta: n_layer          = 24
0.00.092.813 I llm_load_print_meta: n_head           = 16
0.00.092.814 I llm_load_print_meta: n_head_kv        = 16
0.00.092.814 I llm_load_print_meta: n_rot            = 32
0.00.092.815 I llm_load_print_meta: n_swa            = 0
0.00.092.815 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.815 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.816 I llm_load_print_meta: n_gqa            = 1
0.00.092.816 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.817 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.818 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.818 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.818 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.819 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.819 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.820 I llm_load_print_meta: n_ff             = 8192
0.00.092.820 I llm_load_print_meta: n_expert         = 0
0.00.092.820 I llm_load_print_meta: n_expert_used    = 0
0.00.092.820 I llm_load_print_meta: causal attn      = 1
0.00.092.821 I llm_load_print_meta: pooling type     = 0
0.00.092.821 I llm_load_print_meta: rope type        = 2
0.00.092.821 I llm_load_print_meta: rope scaling     = linear
0.00.092.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.822 I llm_load_print_meta: freq_scale_train = 1
0.00.092.822 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.822 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.823 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.823 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.823 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.823 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.823 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.834 I llm_load_print_meta: model type       = 1.4B
0.00.092.834 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.092.834 I llm_load_print_meta: model params     = 1.41 B
0.00.092.835 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.092.835 I llm_load_print_meta: general.name     = 1.4B
0.00.092.836 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.836 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.836 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.839 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.839 I llm_load_print_meta: LF token         = 128 ''
0.00.092.839 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.840 I llm_load_print_meta: max token length = 1024
0.00.095.463 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.463 I llm_load_tensors: offloading output layer to GPU
0.00.095.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.475 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.095.476 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.096.800 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.801 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.801 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.801 I llama_new_context_with_model: n_batch       = 2048
0.00.096.802 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.802 I llama_new_context_with_model: flash_attn    = 0
0.00.096.803 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.803 I llama_new_context_with_model: freq_scale    = 1
0.00.096.803 I ggml_metal_init: allocating
0.00.096.807 I ggml_metal_init: found device: Apple M4
0.00.096.809 I ggml_metal_init: picking default device: Apple M4
0.00.097.591 I ggml_metal_init: using embedded metal library
0.00.100.845 I ggml_metal_init: GPU name:   Apple M4
0.00.100.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.848 I ggml_metal_init: simdgroup reduction   = true
0.00.100.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.849 I ggml_metal_init: has bfloat            = true
0.00.100.849 I ggml_metal_init: use bfloat            = true
0.00.100.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.133.576 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.582 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.604 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.134.602 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.134.603 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.134.604 I llama_new_context_with_model: graph nodes  = 967
0.00.134.604 I llama_new_context_with_model: graph splits = 2
0.00.134.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.496 I main: llama threadpool init, n_threads = 4
0.00.737.585 I 
0.00.737.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.658 I 
0.00.738.216 I sampler seed: 1234
0.00.738.223 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.302 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.302 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.517.767 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.517.769 I llama_perf_context_print:        load time =     724.86 ms
0.01.517.769 I llama_perf_context_print: prompt eval time =      59.14 ms /     7 tokens (    8.45 ms per token,   118.36 tokens per second)
0.01.517.773 I llama_perf_context_print:        eval time =     717.14 ms /    63 runs   (   11.38 ms per token,    87.85 tokens per second)
0.01.517.773 I llama_perf_context_print:       total time =     780.28 ms /    70 tokens
0.01.517.991 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.153s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.602 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.519 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.514 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.514 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.515 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.515 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.516 I llama_model_loader: - type  f32:  194 tensors
0.00.023.516 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.516 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.517 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.435 I llm_load_vocab: special tokens cache size = 25
0.00.050.302 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.304 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.305 I llm_load_print_meta: arch             = gptneox
0.00.050.305 I llm_load_print_meta: vocab type       = BPE
0.00.050.305 I llm_load_print_meta: n_vocab          = 50304
0.00.050.306 I llm_load_print_meta: n_merges         = 50009
0.00.050.306 I llm_load_print_meta: vocab_only       = 0
0.00.050.306 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.306 I llm_load_print_meta: n_embd           = 2048
0.00.050.306 I llm_load_print_meta: n_layer          = 24
0.00.050.321 I llm_load_print_meta: n_head           = 16
0.00.050.322 I llm_load_print_meta: n_head_kv        = 16
0.00.050.323 I llm_load_print_meta: n_rot            = 32
0.00.050.323 I llm_load_print_meta: n_swa            = 0
0.00.050.323 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.323 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.324 I llm_load_print_meta: n_gqa            = 1
0.00.050.325 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.325 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.326 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.326 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.326 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.327 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.327 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.327 I llm_load_print_meta: n_ff             = 8192
0.00.050.327 I llm_load_print_meta: n_expert         = 0
0.00.050.328 I llm_load_print_meta: n_expert_used    = 0
0.00.050.328 I llm_load_print_meta: causal attn      = 1
0.00.050.328 I llm_load_print_meta: pooling type     = 0
0.00.050.328 I llm_load_print_meta: rope type        = 2
0.00.050.328 I llm_load_print_meta: rope scaling     = linear
0.00.050.329 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.329 I llm_load_print_meta: freq_scale_train = 1
0.00.050.329 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.329 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.331 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.331 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.331 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.331 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.331 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.341 I llm_load_print_meta: model type       = 1.4B
0.00.050.341 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.341 I llm_load_print_meta: model params     = 1.41 B
0.00.050.342 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.342 I llm_load_print_meta: general.name     = 1.4B
0.00.050.342 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.343 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.343 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.343 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.344 I llm_load_print_meta: LF token         = 128 ''
0.00.050.344 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.345 I llm_load_print_meta: max token length = 1024
0.00.052.297 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.298 I llm_load_tensors: offloading output layer to GPU
0.00.052.298 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.308 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.310 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.192 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.192 I llama_new_context_with_model: n_ctx         = 128
0.00.053.192 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.193 I llama_new_context_with_model: n_batch       = 128
0.00.053.193 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.193 I llama_new_context_with_model: flash_attn    = 0
0.00.053.193 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.194 I llama_new_context_with_model: freq_scale    = 1
0.00.053.194 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.194 I ggml_metal_init: allocating
0.00.053.200 I ggml_metal_init: found device: Apple M4
0.00.053.202 I ggml_metal_init: picking default device: Apple M4
0.00.053.761 I ggml_metal_init: using embedded metal library
0.00.056.058 I ggml_metal_init: GPU name:   Apple M4
0.00.056.059 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.060 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.060 I ggml_metal_init: simdgroup reduction   = true
0.00.056.060 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.060 I ggml_metal_init: has bfloat            = true
0.00.056.060 I ggml_metal_init: use bfloat            = true
0.00.056.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.755 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.761 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.777 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.651 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.652 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.652 I llama_new_context_with_model: graph nodes  = 967
0.00.067.653 I llama_new_context_with_model: graph splits = 2
0.00.067.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.091 I 
0.00.559.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.136 I perplexity: tokenizing the input ..
0.00.566.808 I perplexity: tokenization took 7.672 ms
0.00.566.819 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.427 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.702.676 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.702.695 I llama_perf_context_print:        load time =     550.47 ms
0.00.702.696 I llama_perf_context_print: prompt eval time =     134.38 ms /   128 tokens (    1.05 ms per token,   952.56 tokens per second)
0.00.702.697 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.698 I llama_perf_context_print:       total time =     143.62 ms /   129 tokens
0.00.703.167 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.078s
sys	0m0.095s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.587 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.345 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.025.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.352 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.353 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.355 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.355 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.262 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.034.263 I llama_model_loader: - type  f32:  194 tensors
0.00.034.264 I llama_model_loader: - type q5_K:   61 tensors
0.00.034.264 I llama_model_loader: - type q6_K:   37 tensors
0.00.056.207 I llm_load_vocab: special tokens cache size = 25
0.00.062.184 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.187 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.188 I llm_load_print_meta: arch             = gptneox
0.00.062.188 I llm_load_print_meta: vocab type       = BPE
0.00.062.189 I llm_load_print_meta: n_vocab          = 50304
0.00.062.189 I llm_load_print_meta: n_merges         = 50009
0.00.062.189 I llm_load_print_meta: vocab_only       = 0
0.00.062.189 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.189 I llm_load_print_meta: n_embd           = 2048
0.00.062.189 I llm_load_print_meta: n_layer          = 24
0.00.062.203 I llm_load_print_meta: n_head           = 16
0.00.062.204 I llm_load_print_meta: n_head_kv        = 16
0.00.062.204 I llm_load_print_meta: n_rot            = 32
0.00.062.204 I llm_load_print_meta: n_swa            = 0
0.00.062.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.205 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.208 I llm_load_print_meta: n_gqa            = 1
0.00.062.209 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.209 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.210 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.212 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.212 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.213 I llm_load_print_meta: n_ff             = 8192
0.00.062.213 I llm_load_print_meta: n_expert         = 0
0.00.062.213 I llm_load_print_meta: n_expert_used    = 0
0.00.062.214 I llm_load_print_meta: causal attn      = 1
0.00.062.215 I llm_load_print_meta: pooling type     = 0
0.00.062.215 I llm_load_print_meta: rope type        = 2
0.00.062.215 I llm_load_print_meta: rope scaling     = linear
0.00.062.216 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.216 I llm_load_print_meta: freq_scale_train = 1
0.00.062.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.217 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.217 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.217 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.226 I llm_load_print_meta: model type       = 1.4B
0.00.062.226 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.062.226 I llm_load_print_meta: model params     = 1.41 B
0.00.062.227 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.062.227 I llm_load_print_meta: general.name     = 1.4B
0.00.062.227 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.227 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.227 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.228 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.228 I llm_load_print_meta: LF token         = 128 ''
0.00.062.228 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.228 I llm_load_print_meta: max token length = 1024
0.00.063.866 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.866 I llm_load_tensors: offloading output layer to GPU
0.00.063.866 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.877 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.063.878 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.064.717 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.718 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.718 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.718 I llama_new_context_with_model: n_batch       = 2048
0.00.064.718 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.718 I llama_new_context_with_model: flash_attn    = 0
0.00.064.719 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.719 I llama_new_context_with_model: freq_scale    = 1
0.00.064.720 I ggml_metal_init: allocating
0.00.064.723 I ggml_metal_init: found device: Apple M4
0.00.064.725 I ggml_metal_init: picking default device: Apple M4
0.00.065.314 I ggml_metal_init: using embedded metal library
0.00.067.787 I ggml_metal_init: GPU name:   Apple M4
0.00.067.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.791 I ggml_metal_init: simdgroup reduction   = true
0.00.067.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.791 I ggml_metal_init: has bfloat            = true
0.00.067.791 I ggml_metal_init: use bfloat            = true
0.00.067.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.573 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.578 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.596 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.641 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.643 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.643 I llama_new_context_with_model: graph nodes  = 967
0.00.098.643 I llama_new_context_with_model: graph splits = 2
0.00.098.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.853.009 I main: llama threadpool init, n_threads = 4
0.00.853.087 I 
0.00.853.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.161 I 
0.00.853.721 I sampler seed: 1234
0.00.853.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.853.788 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.853.790 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.853.790 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.706.317 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.706.318 I llama_perf_context_print:        load time =     844.41 ms
0.01.706.319 I llama_perf_context_print: prompt eval time =      52.47 ms /     7 tokens (    7.50 ms per token,   133.42 tokens per second)
0.01.706.320 I llama_perf_context_print:        eval time =     796.94 ms /    63 runs   (   12.65 ms per token,    79.05 tokens per second)
0.01.706.320 I llama_perf_context_print:       total time =     853.32 ms /    70 tokens
0.01.706.507 I ggml_metal_free: deallocating

real	0m1.722s
user	0m0.121s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.677 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.590 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.592 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.594 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.460 I llama_model_loader: - type  f32:  194 tensors
0.00.024.460 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.460 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.395 I llm_load_vocab: special tokens cache size = 25
0.00.051.380 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.383 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.384 I llm_load_print_meta: arch             = gptneox
0.00.051.384 I llm_load_print_meta: vocab type       = BPE
0.00.051.384 I llm_load_print_meta: n_vocab          = 50304
0.00.051.384 I llm_load_print_meta: n_merges         = 50009
0.00.051.385 I llm_load_print_meta: vocab_only       = 0
0.00.051.385 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.385 I llm_load_print_meta: n_embd           = 2048
0.00.051.385 I llm_load_print_meta: n_layer          = 24
0.00.051.395 I llm_load_print_meta: n_head           = 16
0.00.051.395 I llm_load_print_meta: n_head_kv        = 16
0.00.051.396 I llm_load_print_meta: n_rot            = 32
0.00.051.396 I llm_load_print_meta: n_swa            = 0
0.00.051.396 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.396 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.397 I llm_load_print_meta: n_gqa            = 1
0.00.051.398 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.398 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.399 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.399 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.399 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.400 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.400 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.401 I llm_load_print_meta: n_ff             = 8192
0.00.051.401 I llm_load_print_meta: n_expert         = 0
0.00.051.401 I llm_load_print_meta: n_expert_used    = 0
0.00.051.401 I llm_load_print_meta: causal attn      = 1
0.00.051.401 I llm_load_print_meta: pooling type     = 0
0.00.051.401 I llm_load_print_meta: rope type        = 2
0.00.051.402 I llm_load_print_meta: rope scaling     = linear
0.00.051.402 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.402 I llm_load_print_meta: freq_scale_train = 1
0.00.051.403 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.403 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.403 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.403 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.403 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.403 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.404 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.408 I llm_load_print_meta: model type       = 1.4B
0.00.051.408 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.409 I llm_load_print_meta: model params     = 1.41 B
0.00.051.409 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.409 I llm_load_print_meta: general.name     = 1.4B
0.00.051.409 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: LF token         = 128 ''
0.00.051.410 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.411 I llm_load_print_meta: max token length = 1024
0.00.053.156 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.157 I llm_load_tensors: offloading output layer to GPU
0.00.053.157 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.162 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.162 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.015 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.015 I llama_new_context_with_model: n_ctx         = 128
0.00.054.016 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.016 I llama_new_context_with_model: n_batch       = 128
0.00.054.016 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.016 I llama_new_context_with_model: flash_attn    = 0
0.00.054.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.017 I llama_new_context_with_model: freq_scale    = 1
0.00.054.017 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.018 I ggml_metal_init: allocating
0.00.054.021 I ggml_metal_init: found device: Apple M4
0.00.054.023 I ggml_metal_init: picking default device: Apple M4
0.00.054.617 I ggml_metal_init: using embedded metal library
0.00.056.947 I ggml_metal_init: GPU name:   Apple M4
0.00.056.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.949 I ggml_metal_init: simdgroup reduction   = true
0.00.056.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.949 I ggml_metal_init: has bfloat            = true
0.00.056.950 I ggml_metal_init: use bfloat            = true
0.00.056.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.952 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.898 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.900 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.916 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.865 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.866 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.866 I llama_new_context_with_model: graph nodes  = 967
0.00.068.867 I llama_new_context_with_model: graph splits = 2
0.00.068.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.224 I 
0.00.632.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.307 I perplexity: tokenizing the input ..
0.00.640.171 I perplexity: tokenization took 7.868 ms
0.00.640.182 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.970 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.782.238 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.782.264 I llama_perf_context_print:        load time =     622.53 ms
0.00.782.265 I llama_perf_context_print: prompt eval time =     140.56 ms /   128 tokens (    1.10 ms per token,   910.65 tokens per second)
0.00.782.266 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.266 I llama_perf_context_print:       total time =     150.06 ms /   129 tokens
0.00.782.789 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.079s
sys	0m0.111s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.978 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.517 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.540 I llama_model_loader: - type  f32:  194 tensors
0.00.028.540 I llama_model_loader: - type q6_K:   98 tensors
0.00.048.928 I llm_load_vocab: special tokens cache size = 25
0.00.054.900 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.903 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.903 I llm_load_print_meta: arch             = gptneox
0.00.054.904 I llm_load_print_meta: vocab type       = BPE
0.00.054.904 I llm_load_print_meta: n_vocab          = 50304
0.00.054.904 I llm_load_print_meta: n_merges         = 50009
0.00.054.904 I llm_load_print_meta: vocab_only       = 0
0.00.054.904 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.905 I llm_load_print_meta: n_embd           = 2048
0.00.054.905 I llm_load_print_meta: n_layer          = 24
0.00.054.919 I llm_load_print_meta: n_head           = 16
0.00.054.919 I llm_load_print_meta: n_head_kv        = 16
0.00.054.920 I llm_load_print_meta: n_rot            = 32
0.00.054.920 I llm_load_print_meta: n_swa            = 0
0.00.054.920 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.920 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.921 I llm_load_print_meta: n_gqa            = 1
0.00.054.922 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.922 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.923 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.923 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.924 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.924 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.924 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.925 I llm_load_print_meta: n_ff             = 8192
0.00.054.925 I llm_load_print_meta: n_expert         = 0
0.00.054.925 I llm_load_print_meta: n_expert_used    = 0
0.00.054.925 I llm_load_print_meta: causal attn      = 1
0.00.054.927 I llm_load_print_meta: pooling type     = 0
0.00.054.928 I llm_load_print_meta: rope type        = 2
0.00.054.929 I llm_load_print_meta: rope scaling     = linear
0.00.054.929 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.929 I llm_load_print_meta: freq_scale_train = 1
0.00.054.929 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.930 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.930 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.930 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.930 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.930 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.930 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.940 I llm_load_print_meta: model type       = 1.4B
0.00.054.940 I llm_load_print_meta: model ftype      = Q6_K
0.00.054.940 I llm_load_print_meta: model params     = 1.41 B
0.00.054.941 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.054.941 I llm_load_print_meta: general.name     = 1.4B
0.00.054.941 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.941 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.941 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.942 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.942 I llm_load_print_meta: LF token         = 128 ''
0.00.054.942 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.942 I llm_load_print_meta: max token length = 1024
0.00.056.978 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.978 I llm_load_tensors: offloading output layer to GPU
0.00.056.978 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.988 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.989 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.057.906 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.906 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.907 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.907 I llama_new_context_with_model: n_batch       = 2048
0.00.057.907 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.907 I llama_new_context_with_model: flash_attn    = 0
0.00.057.908 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.908 I llama_new_context_with_model: freq_scale    = 1
0.00.057.908 I ggml_metal_init: allocating
0.00.057.912 I ggml_metal_init: found device: Apple M4
0.00.057.914 I ggml_metal_init: picking default device: Apple M4
0.00.058.507 I ggml_metal_init: using embedded metal library
0.00.060.785 I ggml_metal_init: GPU name:   Apple M4
0.00.060.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.788 I ggml_metal_init: simdgroup reduction   = true
0.00.060.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.789 I ggml_metal_init: has bfloat            = true
0.00.060.789 I ggml_metal_init: use bfloat            = true
0.00.060.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.703 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.709 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.727 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.746 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.747 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.748 I llama_new_context_with_model: graph nodes  = 967
0.00.091.748 I llama_new_context_with_model: graph splits = 2
0.00.091.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.860 I main: llama threadpool init, n_threads = 4
0.00.770.895 I 
0.00.770.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.926 I 
0.00.771.188 I sampler seed: 1234
0.00.771.194 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.232 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.232 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.232 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.652.426 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.01.652.427 I llama_perf_context_print:        load time =     760.88 ms
0.01.652.428 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.53 tokens per second)
0.01.652.428 I llama_perf_context_print:        eval time =     823.76 ms /    63 runs   (   13.08 ms per token,    76.48 tokens per second)
0.01.652.432 I llama_perf_context_print:       total time =     881.57 ms /    70 tokens
0.01.652.637 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.113s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4314 (cb13ef85) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.998 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.870 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.871 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.874 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.875 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.875 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.882 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.719 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.720 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.721 I llama_model_loader: - type  f32:  194 tensors
0.00.023.721 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.797 I llm_load_vocab: special tokens cache size = 25
0.00.051.152 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.155 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.156 I llm_load_print_meta: arch             = gptneox
0.00.051.156 I llm_load_print_meta: vocab type       = BPE
0.00.051.156 I llm_load_print_meta: n_vocab          = 50304
0.00.051.157 I llm_load_print_meta: n_merges         = 50009
0.00.051.157 I llm_load_print_meta: vocab_only       = 0
0.00.051.157 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.157 I llm_load_print_meta: n_embd           = 2048
0.00.051.157 I llm_load_print_meta: n_layer          = 24
0.00.051.172 I llm_load_print_meta: n_head           = 16
0.00.051.173 I llm_load_print_meta: n_head_kv        = 16
0.00.051.173 I llm_load_print_meta: n_rot            = 32
0.00.051.173 I llm_load_print_meta: n_swa            = 0
0.00.051.173 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.173 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.174 I llm_load_print_meta: n_gqa            = 1
0.00.051.175 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.175 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.176 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.177 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.177 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.177 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.177 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.178 I llm_load_print_meta: n_ff             = 8192
0.00.051.178 I llm_load_print_meta: n_expert         = 0
0.00.051.178 I llm_load_print_meta: n_expert_used    = 0
0.00.051.178 I llm_load_print_meta: causal attn      = 1
0.00.051.178 I llm_load_print_meta: pooling type     = 0
0.00.051.178 I llm_load_print_meta: rope type        = 2
0.00.051.179 I llm_load_print_meta: rope scaling     = linear
0.00.051.179 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.181 I llm_load_print_meta: freq_scale_train = 1
0.00.051.182 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.182 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.182 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.182 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.182 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.182 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.183 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.193 I llm_load_print_meta: model type       = 1.4B
0.00.051.193 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.193 I llm_load_print_meta: model params     = 1.41 B
0.00.051.194 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.194 I llm_load_print_meta: general.name     = 1.4B
0.00.051.194 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.194 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.194 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.195 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.195 I llm_load_print_meta: LF token         = 128 ''
0.00.051.195 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.195 I llm_load_print_meta: max token length = 1024
0.00.053.289 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.289 I llm_load_tensors: offloading output layer to GPU
0.00.053.289 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.299 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.301 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.311 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.312 I llama_new_context_with_model: n_ctx         = 128
0.00.054.312 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.312 I llama_new_context_with_model: n_batch       = 128
0.00.054.312 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.312 I llama_new_context_with_model: flash_attn    = 0
0.00.054.313 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.313 I llama_new_context_with_model: freq_scale    = 1
0.00.054.313 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.314 I ggml_metal_init: allocating
0.00.054.317 I ggml_metal_init: found device: Apple M4
0.00.054.319 I ggml_metal_init: picking default device: Apple M4
0.00.054.912 I ggml_metal_init: using embedded metal library
0.00.057.254 I ggml_metal_init: GPU name:   Apple M4
0.00.057.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.256 I ggml_metal_init: simdgroup reduction   = true
0.00.057.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.257 I ggml_metal_init: has bfloat            = true
0.00.057.257 I ggml_metal_init: use bfloat            = true
0.00.057.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.373 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.375 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.390 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.357 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.358 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.358 I llama_new_context_with_model: graph nodes  = 967
0.00.069.359 I llama_new_context_with_model: graph splits = 2
0.00.069.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.393.805 I 
0.00.393.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.393.869 I perplexity: tokenizing the input ..
0.00.401.758 I perplexity: tokenization took 7.888 ms
0.00.401.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.541.778 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.542.939 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.542.960 I llama_perf_context_print:        load time =     384.79 ms
0.00.542.961 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.75 tokens per second)
0.00.542.962 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.542.966 I llama_perf_context_print:       total time =     149.17 ms /   129 tokens
0.00.543.428 I ggml_metal_free: deallocating

real	0m0.556s
user	0m0.079s
sys	0m0.086s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4314 (cb13ef85)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151c0a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151c0a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151c0af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151c0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151c0ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151c0c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151c0c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151c0cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151c0d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151c0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151c0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151c0e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151c0eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151c0f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151c0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151c10250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151c10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151c11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151c117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151c11f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151c126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151c12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151c134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151c13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151c144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151c14760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151c14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151c159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151c15f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151c161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151c16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151c16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151c171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151c17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151c179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151c17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151c18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151c187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151c18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151c190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151c19590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151c19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151c19ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151c1a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151c1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151c1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151c1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151c1bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151c1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151c1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151c1cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151c1d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151c1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151c1dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151c1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151c1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151c1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151c1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151c1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151c201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151c20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151c20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151c20dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151c21260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151c21700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151c21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151c22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151c224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151c22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151c22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151c232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151c23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151c23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151c24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151c246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151c24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151c25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151c25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151c25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151c26130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151c26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151c26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151c27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151c27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151c27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151c28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151c28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151c28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151c29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151c29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151c29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151c2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151c2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151c2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151c2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151c2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151c2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151c1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151c2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151c2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151c2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151c2d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151c2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151c2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151c2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151c2e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151c2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151c2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151c2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151c2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151c30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151c30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151c30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151c31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151c315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151c31a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151c31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151c323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151c32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151c32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151c331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151c33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151c33af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151c33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151c34430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151c348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151c34d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151c35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151c356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151c35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151c35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151c36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151c36930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151c36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151c37270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151c37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151c37bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151c38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151c384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151c38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151c38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151c392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151c39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151c39c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151c3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151c3a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151c3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151c3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151c3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151c3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151c3bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151c3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151c3c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151c3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151c3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151c3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151c3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151c3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151c3e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151c3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151c3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151c3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151c3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151c3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151c3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151c401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151c40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151c40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151c40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151c41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151c418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151c41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151c42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151c426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151c42b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151c43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151c434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151c43950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151c43df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151c44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151c44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151c44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151c45070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151c45510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151c459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151c45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151c462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151c46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151c46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151c470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151c47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151c47a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151c47eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151c48400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151c48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151c48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151c493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151c496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151c49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151c4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151c4a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151c4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151c4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151c4b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151c4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151c4c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151c4cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151c4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151c4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151c4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151c4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151c4e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151c4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151c4f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151c4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151c4fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151c501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151c50700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151c50c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151c511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151c516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151c51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151c52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151c526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151c52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151c53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151c536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151c53c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151c54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151c546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151c54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151c55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151c556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151c55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151c56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151c566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151c56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151c57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151c57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151c57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151c58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151c58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151c58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151c59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151c59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151c59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151c5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151c5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151c5abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151c5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151c5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151c5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151c5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151c5c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151c5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151c5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151c5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151c5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151c5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151c5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151c5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151c5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151c5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151c5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151c600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151c60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151c60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151c60ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151c61490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151c61930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151c61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151c62270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151c62710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151c62bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151c63050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151c634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151c63990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151c63e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151c642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151c64770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151c64c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151c650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151c65600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151c65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151c66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151c66b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151c67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151c67540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151c67d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151c67ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151c68600 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.148.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151d04d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151d051d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151d05640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151d05ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151d05f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151d06390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151d06800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151d06c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151d070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151d07550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151d079c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151d080b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151d08bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151d09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151d09b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151d0a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151d0a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151d0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151d0b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151d0bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151d0c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151d0cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151d0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151d0dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151d0e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151d0e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151d0e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151d0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151d0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151d0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151d0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151d0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151d103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151d10680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151d10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151d10f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151d113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151d11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151d11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151d12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151d12590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151d12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151d12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151d132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151d13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151d13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151d14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151d144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151d14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151d14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151d151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151d15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151d15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151d15f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151d163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151d16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151d16d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151d17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151d17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151d17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151d17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151d18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151d188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151d18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151d191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151d19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151d19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151d19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151d1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151d1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151d1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151d1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151d1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151d1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151d1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151d1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151d1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151d1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151d1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151d1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151d1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151d1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151d1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151d1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151d1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151d1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151d1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151d1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151d1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151d20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151d20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151d20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151d20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151d21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151d216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151d21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151d21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151d22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151d22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151d22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151d23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151d235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151d23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151d23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151d24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151d24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151d24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151d25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151d254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151d25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151d25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151d26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151d266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151d26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151d26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151d273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151d27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151d27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151d28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151d285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151d28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151d28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151d29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151d29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151d29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151d2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151d2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151d2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151d2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151d2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151d2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151d2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151d2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151d2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151d2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151d2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151d2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151d2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151d2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151d2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151d2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151d2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151d2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151d2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151d2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151d2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151d2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151d301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151d30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151d30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151d30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151d313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151d31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151d31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151d32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151d32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151d329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151d32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151d332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151d33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151d33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151d34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151d34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151d348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151d34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151d351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151d35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151d35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151d35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151d36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151d36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151d36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151d370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151d37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151d379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151d37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151d382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151d38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151d38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151d38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151d39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151d398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151d39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151d3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151d3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151d3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151d3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151d3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151d3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151d3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151d3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151d3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151d3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151d3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151d3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151d3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151d3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151d3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151d3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151d3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151d3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151d3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151d3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151d3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151d3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151d40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151d407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151d40d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151d411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151d41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151d42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151d42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151d42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151d42b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151d42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151d43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151d438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151d43d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151d441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151d44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151d44a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151d44ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151d45360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151d457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151d45c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151d460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151d46520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151d46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151d46e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151d47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151d476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151d47b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151d47fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151d48430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151d488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151d48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151d49180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151d495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151d49a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151d49ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151d4a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151d4a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151d4ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151d4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151d4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151d4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151d4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151d4c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151d4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151d4cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151d4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151d4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151d4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151d4dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151d4e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151d4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151d4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151d4eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151d4f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151d4f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151d4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151d50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151d504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151d50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151d50dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151d51230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151d516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151d51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151d51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151d523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151d52860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151d52cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151d53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151d535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151d53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151d53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151d54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151d54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151d54be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151d55050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151d554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151d55930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151d55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151d56810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151d56f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151d57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151d57d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151d58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151d584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151d58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151d590b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151c24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151c25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151c255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151c25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151c25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151c26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151c267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151c26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151c27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151c27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151c27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151c27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151c28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151c28fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151c297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151c29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151c2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151c2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151c2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151c2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151c2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151c2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151c2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151c2d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151c2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151c2e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151c2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151c2ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151c2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151c2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151c2fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151c2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151c30310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151c305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151c30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151c30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151c31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151c31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151c31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151c32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151c324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151c32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151c32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151c33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151c336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151c33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151c33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151c343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151c34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151c34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151c35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151c355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151c35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151c35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151c36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151c36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151c36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151c37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151c374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151c37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151c37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151c38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151c38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151c38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151c38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151c393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151c39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151c39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151c3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151c3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151c3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151c3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151c3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151c3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151c3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151c3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151c3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151c3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151c3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151c3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151c3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151c3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151c3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151c3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151c3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151c3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151c3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151c3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151c3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151c3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151c402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151c40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151c40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151c41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151c41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151c418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151c41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151c421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151c42640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151c42ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151c42f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151c43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151c43800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151c43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151c440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151c44550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151c449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151c44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151c452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151c45710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151c45b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151c45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151c46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151c468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151c46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151c471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151c47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151c47a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151c47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151c48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151c487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151c48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151c490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151c49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151c499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151c49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151c4a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151c4a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151c4ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151c4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151c4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151c4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151c4bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151c4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151c4c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151c4ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151c4cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151c4d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151c4d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151c4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151c4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151c4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151c4e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151c4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151c4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151c4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151c4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151c4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151c50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151c50890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151c50d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151c51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151c515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151c51a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151c51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151c52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151c527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151c52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151c53080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151c534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151c53960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151c53dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151c54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151c546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151c54b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151c54f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151c55400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151c55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151c55ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151c56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151c565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151c56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151c56ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151c57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151c57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151c57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151c58060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151c584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151c58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151c58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151c59220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151c59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151c59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151c59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151c5a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151c5a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151c5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151c5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151c5b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151c5ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151c5be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151c5c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151c5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151c5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151c5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151c5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151c5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151c5dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151c5e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151c5e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151c5eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151c5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151c5f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151c5f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151c5fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151c60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151c60580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151c609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151c60e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151c612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151c61a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151c61ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151c62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151c627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151c62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151c63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151c634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151c63960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151c63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151c64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151c646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151c64b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151c64f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151c65400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151c65870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151c65ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151c66150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151c665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151c66a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151c66ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151c67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151c67780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151c67bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151c68060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151c684d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151c0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151c0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151c098a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151c0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151c178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151c17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151c18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151c18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151c18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151c18ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151c19350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151c197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151c19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151c1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151c1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151c1a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151c1adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151c1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151c1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151c1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151c1bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151c1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151c1c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151c1cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151c1d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151c1d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151c1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151c1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151c1e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151c1e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151c1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151c1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151c1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151c1f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151c1fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151c20240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151c206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151c20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151c20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151c21400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151c21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151c21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151c22150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151c225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151c22a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151c22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151c23310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151c23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151c23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151c24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151c16340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151c16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151c16ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151c0d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151c0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151c0def0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.785s
user	0m0.293s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4314 (cb13ef85)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12070f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12070fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120710480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120710a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120710fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120711590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120711b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1207120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1207126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120712ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1207130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1207135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1207140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120714870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120715080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1207157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1207165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120716d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1207174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120717bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120718310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1207192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1207199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120719cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12071a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12071af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12071b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12071b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12071bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12071be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12071c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12071cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12071cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12071d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12071d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12071dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12071e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12071e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12071eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12071ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12071f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12071f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12071fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120720190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1207207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1207210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1207216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120721ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1207222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120722900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120722f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120723520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120723d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1207241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120724650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120724910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120724f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120725710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1207259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120725e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120726310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1207267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120726c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1207270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120727590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120727a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120727ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120728810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120728cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120729150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1207296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120729bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12072a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12072a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12072abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12072b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12072b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12072bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12072c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12072c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12072cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12072d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12072d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12072dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12072e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12072e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12072eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12072f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12072f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12072fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1207300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120730630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120730b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1207310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120720db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120731540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120731cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120732240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120732790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120733780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120733cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120734220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120734770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120734cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120735210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120735760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120735cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120736200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1207366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120736b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120736fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120737480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120737920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120737dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120738260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120738700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120738ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120739040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1207394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120739980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120739e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12073a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12073a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12073ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12073b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12073b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12073b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12073be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12073c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12073c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12073cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12073d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12073d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12073da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12073dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12073e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12073e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12073ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12073f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12073f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12073faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12073ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1207403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120740880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120740d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1207411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120741660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120741b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120741fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120742440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1207428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120742d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120743220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1207436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120743b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120744000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1207444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120744940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120744de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120745280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120745720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120745bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120746060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120746500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1207469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120746e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1207472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120747780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120747c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1207480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120748560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120748a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120748ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120749340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1207497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120749c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12074a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12074a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12074aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12074af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12074b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12074b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12074bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12074c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12074c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12074cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12074cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12074d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12074d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12074dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12074e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12074e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12074ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12074f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12074f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12074fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120750620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120750ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120750d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120751390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1207519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120752190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120752630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120752ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120752f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120753720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120753c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1207541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120754710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120754c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1207551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120755700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120755c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1207561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1207566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120756c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120757190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1207576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120757c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120758180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1207586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120758c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120759170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1207596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120759c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12075a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12075a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12075ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12075b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12075b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12075bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12075c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12075c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12075cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12075d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12075d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12075dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12075e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12075e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12075ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12075f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12075f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12075fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120760100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120760650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120760ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1207610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120761640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120761b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1207620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120762630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120762b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1207630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120763620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120763b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1207640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120764610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120764b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1207650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120765600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120765b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1207660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120766540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1207669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120766e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120767320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1207677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120767c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120768100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1207685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120768a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120768ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120769380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120769820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120769cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12076a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12076a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12076ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12076b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12076b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12076c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12076c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12076ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12076d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12076d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12076db50 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121804ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121805150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1218055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121805a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121805ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121806310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121806780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121806bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121807060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1218074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121807940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121808020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1218092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121809b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12180a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12180a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12180b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12180b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12180bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12180c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12180cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12180d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12180dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12180e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12180e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12180e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12180ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12180f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12180f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12180fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12180ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1218103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121810690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121810b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121810f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1218113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121811850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121811cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121812130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1218125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121812a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1218132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121813760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121813bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121814040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1218144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121814920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121814d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121815200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121815670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121815ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121815f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1218163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121816830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121816da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1218172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121817710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121817b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121817ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121818460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1218188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121818d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1218191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121819a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121819f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12181a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12181a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12181ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12181b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12181b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12181b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12181be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12181c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12181c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12181cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12181cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12181d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12181d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12181dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12181e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12181e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12181ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12181eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12181f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12181f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12181fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1218200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121820510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121820980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121820df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121821260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1218216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121821b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121821fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121822420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121822890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121822d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1218235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121823ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121824330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1218247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121824c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121825080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1218254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121825960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121825dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121826240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1218266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121826b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121826f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121827400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121827870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121827ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121828150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1218285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121828a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121828ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121829310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121829780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121829bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12182a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12182a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12182a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12182adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12182b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12182b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12182bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12182bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12182c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12182c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12182ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12182d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12182d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12182da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12182de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12182e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12182e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12182ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12182f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12182f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12182f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12182fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121830200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121830670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121830ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121830f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1218313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121831ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121832110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121832580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1218329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121832e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1218332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121833740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121833bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121834020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121834490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121834900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121834d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1218351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121835650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121835ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121835f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1218363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121836810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1218370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121837560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1218379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1218382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121838720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121838b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121839470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1218398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12183a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12183a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12183aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12183af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12183b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12183b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12183bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12183c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12183c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12183c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12183ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12183d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12183d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12183db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12183dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12183e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12183e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12183ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12183f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12183f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12183fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12183fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121840360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1218407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121840d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1218411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121841640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121842450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121842710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121842b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121842ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121843460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1218438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121843d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1218441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121844620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121844a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121844f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121845370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1218457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1218460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121846530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1218469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121846e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121847280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1218476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121847b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121847fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121848440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1218488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121848d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121849190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121849600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121849a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121849ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12184a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12184a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12184ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12184b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12184b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12184b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12184bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12184c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12184c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12184cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12184cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12184d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12184d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12184dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12184e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12184e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12184ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12184eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12184f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12184f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12184fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121850080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1218504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121850960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121850dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121851240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1218516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121851b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121851f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121852400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121852870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121852ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121853150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1218535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121853a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121853ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121854310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121854780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121854bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121855060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1218554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121855940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121855db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121856820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121856f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121857660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121857d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121858040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1218584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121858ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1218590c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1163044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116304dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116305230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1163056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116305b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116305f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1163063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116306860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116306cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116307140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116307860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116308380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116308b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116309340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116309a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11630a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11630a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11630afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11630b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11630be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11630c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11630cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11630d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11630da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11630dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11630e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11630e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11630e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11630ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11630f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11630f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11630fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11630fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1163102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116310710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116310b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116310ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116311460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1163118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116311d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1163121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116312a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116312f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1163137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116313c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1163140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116314530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1163149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116314e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116315280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1163156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116316a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116316eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116317320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116317790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116317c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116318070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1163184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116318950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116318dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116319230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1163196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116319b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116319f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11631a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11631a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11631acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11631b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11631b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11631ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11631be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11631c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11631c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11631cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11631d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11631d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11631d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11631dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11631e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11631e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11631eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11631ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11631f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11631f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11631fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116320120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116320590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116320a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116320e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1163212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116321750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116321bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116322030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1163224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116322910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116322d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1163231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116323660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116323ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116323f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1163243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116324820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116324c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116325100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116325570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1163259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116325e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1163262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116326730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116326ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116327010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116327480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1163278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116327d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1163281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116328640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116328ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116328f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116329390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116329800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116329c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11632a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11632a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11632a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11632ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11632b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11632b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11632bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11632bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11632c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11632c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11632cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11632d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11632d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11632da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11632df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11632e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11632e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11632ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11632f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11632f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11632f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11632fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116330280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1163306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116330b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116330fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116331440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1163318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116331d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116332190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116332600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116332a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116332ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116333350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1163337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116333c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1163340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116334510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116334980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116334df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116335260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1163356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116335b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116335fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116336420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116336890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116336d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116337170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1163375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116337a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116337ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116338330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1163387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116338c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116339080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1163394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116339960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116339dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11633a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11633a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11633ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11633af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11633b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11633b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11633bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11633c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11633c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11633ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11633cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11633d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11633d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11633dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11633e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11633e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11633e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11633edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11633f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11633f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11633fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11633ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116340500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116340970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116340de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116341930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116341bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116341eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116342320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116342790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116342c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116343070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1163434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116343950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116343dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116344230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1163446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116344b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116344f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1163453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116345860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116345cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116346140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1163465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116346a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116346e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116347300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116347770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116347be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116348050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1163484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116348930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116348da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116349210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116349680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116349af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116349f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11634a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11634a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11634b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11634b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11634b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11634bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11634c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11634c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11634caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11634cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11634d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11634d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11634dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11634e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11634e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11634e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11634ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11634f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11634f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11634fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11634ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116350450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1163508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116350d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1163511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116351610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116351a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116351ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116352360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1163527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116352c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1163530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116353520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116353990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116353e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116354270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1163546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116354b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116354fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116355430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1163558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116356310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116356a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116357150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116357870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116357b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116357fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1163585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116358bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.244s
sys	0m0.148s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.54 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.30 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.57 sec*proc (2 tests)

Total Test time (real) =   0.58 sec
        0.58 real         0.16 user         0.05 sys
```
