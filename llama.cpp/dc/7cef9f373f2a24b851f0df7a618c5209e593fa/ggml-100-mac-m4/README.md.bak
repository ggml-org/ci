### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.22 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.48 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  176.17 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.92 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.88 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 218.89 sec*proc (28 tests)

Total Test time (real) = 218.90 sec

real	3m38.932s
user	7m30.315s
sys	0m6.163s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.12 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.20 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.33 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.15 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.20 sec*proc (28 tests)

Total Test time (real) =  51.21 sec

real	0m51.221s
user	1m11.146s
sys	0m5.594s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.138 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.924 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.847 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.854 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.856 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.857 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.858 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.858 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.859 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.860 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.861 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.862 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.862 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.863 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.867 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.867 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.868 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.868 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.869 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.870 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.870 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.709 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.711 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.712 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.712 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.713 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.027.713 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.714 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.027.714 I llama_model_loader: - type  f32:  124 tensors
0.00.027.715 I llama_model_loader: - type  f16:   73 tensors
0.00.031.694 I llm_load_vocab: special tokens cache size = 5
0.00.033.860 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.033.864 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.033.865 I llm_load_print_meta: arch             = bert
0.00.033.865 I llm_load_print_meta: vocab type       = WPM
0.00.033.865 I llm_load_print_meta: n_vocab          = 30522
0.00.033.866 I llm_load_print_meta: n_merges         = 0
0.00.033.866 I llm_load_print_meta: vocab_only       = 0
0.00.033.866 I llm_load_print_meta: n_ctx_train      = 512
0.00.033.866 I llm_load_print_meta: n_embd           = 384
0.00.033.867 I llm_load_print_meta: n_layer          = 12
0.00.033.870 I llm_load_print_meta: n_head           = 12
0.00.033.871 I llm_load_print_meta: n_head_kv        = 12
0.00.033.871 I llm_load_print_meta: n_rot            = 32
0.00.033.871 I llm_load_print_meta: n_swa            = 0
0.00.033.872 I llm_load_print_meta: n_embd_head_k    = 32
0.00.033.872 I llm_load_print_meta: n_embd_head_v    = 32
0.00.033.873 I llm_load_print_meta: n_gqa            = 1
0.00.033.873 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.033.874 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.033.875 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.033.876 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.033.876 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.033.876 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.033.876 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.033.877 I llm_load_print_meta: n_ff             = 1536
0.00.033.878 I llm_load_print_meta: n_expert         = 0
0.00.033.878 I llm_load_print_meta: n_expert_used    = 0
0.00.033.878 I llm_load_print_meta: causal attn      = 0
0.00.033.878 I llm_load_print_meta: pooling type     = 2
0.00.033.878 I llm_load_print_meta: rope type        = 2
0.00.033.880 I llm_load_print_meta: rope scaling     = linear
0.00.033.884 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.033.884 I llm_load_print_meta: freq_scale_train = 1
0.00.033.884 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.033.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.033.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.033.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.033.885 I llm_load_print_meta: ssm_d_state      = 0
0.00.033.885 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.033.886 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.033.886 I llm_load_print_meta: model type       = 33M
0.00.033.886 I llm_load_print_meta: model ftype      = F16
0.00.033.887 I llm_load_print_meta: model params     = 33.21 M
0.00.033.892 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.033.893 I llm_load_print_meta: general.name     = Bge Small
0.00.033.893 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.033.894 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.033.894 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.033.895 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.033.896 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.033.896 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.033.896 I llm_load_print_meta: max token length = 21
0.00.035.964 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.035.964 I llm_load_tensors: offloading output layer to GPU
0.00.035.970 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.035.997 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.998 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.036.556 I llama_new_context_with_model: n_seq_max     = 1
0.00.036.558 I llama_new_context_with_model: n_ctx         = 512
0.00.036.558 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.036.558 I llama_new_context_with_model: n_batch       = 2048
0.00.036.558 I llama_new_context_with_model: n_ubatch      = 2048
0.00.036.559 I llama_new_context_with_model: flash_attn    = 0
0.00.036.559 I llama_new_context_with_model: freq_base     = 10000.0
0.00.036.560 I llama_new_context_with_model: freq_scale    = 1
0.00.036.560 I ggml_metal_init: allocating
0.00.036.564 I ggml_metal_init: found device: Apple M4
0.00.036.567 I ggml_metal_init: picking default device: Apple M4
0.00.037.418 I ggml_metal_init: using embedded metal library
0.00.041.447 I ggml_metal_init: GPU name:   Apple M4
0.00.041.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.451 I ggml_metal_init: simdgroup reduction   = true
0.00.041.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.451 I ggml_metal_init: has bfloat            = true
0.00.041.451 I ggml_metal_init: use bfloat            = true
0.00.041.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.607 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.173 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.176 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.178 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.999 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.055.000 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.055.000 I llama_new_context_with_model: graph nodes  = 429
0.00.055.001 I llama_new_context_with_model: graph splits = 2
0.00.055.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.055.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.347 I 
0.00.065.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.065.977 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.070.760 I llama_perf_context_print:        load time =      47.41 ms
0.00.070.761 I llama_perf_context_print: prompt eval time =       4.64 ms /     9 tokens (    0.52 ms per token,  1939.24 tokens per second)
0.00.070.762 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.070.762 I llama_perf_context_print:       total time =       5.42 ms /    10 tokens
0.00.070.900 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.059s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.949 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.954 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.955 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.956 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.956 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.957 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.957 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.957 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.958 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.958 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.960 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.961 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.961 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.962 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.962 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.962 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.963 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.289 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.927 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.928 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.928 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.929 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.929 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.929 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.929 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.930 I llama_model_loader: - type  f32:  124 tensors
0.00.013.930 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.273 I llm_load_vocab: special tokens cache size = 5
0.00.017.550 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.553 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.553 I llm_load_print_meta: arch             = bert
0.00.017.554 I llm_load_print_meta: vocab type       = WPM
0.00.017.554 I llm_load_print_meta: n_vocab          = 30522
0.00.017.554 I llm_load_print_meta: n_merges         = 0
0.00.017.554 I llm_load_print_meta: vocab_only       = 0
0.00.017.555 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.555 I llm_load_print_meta: n_embd           = 384
0.00.017.555 I llm_load_print_meta: n_layer          = 12
0.00.017.558 I llm_load_print_meta: n_head           = 12
0.00.017.559 I llm_load_print_meta: n_head_kv        = 12
0.00.017.559 I llm_load_print_meta: n_rot            = 32
0.00.017.559 I llm_load_print_meta: n_swa            = 0
0.00.017.559 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.559 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.560 I llm_load_print_meta: n_gqa            = 1
0.00.017.560 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.561 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.562 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.562 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.563 I llm_load_print_meta: n_ff             = 1536
0.00.017.564 I llm_load_print_meta: n_expert         = 0
0.00.017.565 I llm_load_print_meta: n_expert_used    = 0
0.00.017.565 I llm_load_print_meta: causal attn      = 0
0.00.017.565 I llm_load_print_meta: pooling type     = 2
0.00.017.565 I llm_load_print_meta: rope type        = 2
0.00.017.565 I llm_load_print_meta: rope scaling     = linear
0.00.017.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.566 I llm_load_print_meta: freq_scale_train = 1
0.00.017.566 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.566 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.566 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.567 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.567 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.567 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.567 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.567 I llm_load_print_meta: model type       = 33M
0.00.017.568 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.568 I llm_load_print_meta: model params     = 33.21 M
0.00.017.568 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.569 I llm_load_print_meta: general.name     = Bge Small
0.00.017.569 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.569 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.569 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.569 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.570 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.570 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.570 I llm_load_print_meta: max token length = 21
0.00.018.846 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.846 I llm_load_tensors: offloading output layer to GPU
0.00.018.847 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.855 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.856 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.195 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.196 I llama_new_context_with_model: n_ctx         = 512
0.00.019.196 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.196 I llama_new_context_with_model: n_batch       = 2048
0.00.019.196 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.196 I llama_new_context_with_model: flash_attn    = 0
0.00.019.197 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.197 I llama_new_context_with_model: freq_scale    = 1
0.00.019.198 I ggml_metal_init: allocating
0.00.019.201 I ggml_metal_init: found device: Apple M4
0.00.019.203 I ggml_metal_init: picking default device: Apple M4
0.00.019.797 I ggml_metal_init: using embedded metal library
0.00.022.130 I ggml_metal_init: GPU name:   Apple M4
0.00.022.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.133 I ggml_metal_init: simdgroup reduction   = true
0.00.022.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.133 I ggml_metal_init: has bfloat            = true
0.00.022.133 I ggml_metal_init: use bfloat            = true
0.00.022.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.280 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.772 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.774 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.776 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.339 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.340 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.340 I llama_new_context_with_model: graph nodes  = 429
0.00.033.340 I llama_new_context_with_model: graph splits = 2
0.00.033.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.920 I 
0.00.037.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.523 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.888 I llama_perf_context_print:        load time =      28.91 ms
0.00.042.889 I llama_perf_context_print: prompt eval time =       4.22 ms /     9 tokens (    0.47 ms per token,  2130.68 tokens per second)
0.00.042.890 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.891 I llama_perf_context_print:       total time =       4.97 ms /    10 tokens
0.00.043.094 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.204 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.757 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.809 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.817 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.818 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.819 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.819 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.820 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.821 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.822 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.823 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.824 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.824 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.827 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.828 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.829 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.905 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.906 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.906 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.906 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.906 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.907 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.907 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.907 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.908 I llama_model_loader: - type  f32:   40 tensors
0.00.049.908 I llama_model_loader: - type  f16:   30 tensors
0.00.068.534 W llm_load_vocab: empty token at index 5
0.00.073.279 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.647 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.679 I llm_load_vocab: special tokens cache size = 5
0.00.325.074 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.325.080 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.325.080 I llm_load_print_meta: arch             = jina-bert-v2
0.00.325.081 I llm_load_print_meta: vocab type       = BPE
0.00.325.082 I llm_load_print_meta: n_vocab          = 61056
0.00.325.082 I llm_load_print_meta: n_merges         = 39382
0.00.325.082 I llm_load_print_meta: vocab_only       = 0
0.00.325.082 I llm_load_print_meta: n_ctx_train      = 8192
0.00.325.084 I llm_load_print_meta: n_embd           = 384
0.00.325.084 I llm_load_print_meta: n_layer          = 4
0.00.325.090 I llm_load_print_meta: n_head           = 12
0.00.325.091 I llm_load_print_meta: n_head_kv        = 12
0.00.325.092 I llm_load_print_meta: n_rot            = 32
0.00.325.092 I llm_load_print_meta: n_swa            = 0
0.00.325.092 I llm_load_print_meta: n_embd_head_k    = 32
0.00.325.092 I llm_load_print_meta: n_embd_head_v    = 32
0.00.325.095 I llm_load_print_meta: n_gqa            = 1
0.00.325.096 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.325.096 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.325.097 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.325.099 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.325.100 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.325.100 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.325.101 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.325.101 I llm_load_print_meta: n_ff             = 1536
0.00.325.101 I llm_load_print_meta: n_expert         = 0
0.00.325.101 I llm_load_print_meta: n_expert_used    = 0
0.00.325.102 I llm_load_print_meta: causal attn      = 0
0.00.325.102 I llm_load_print_meta: pooling type     = -1
0.00.325.102 I llm_load_print_meta: rope type        = -1
0.00.325.102 I llm_load_print_meta: rope scaling     = linear
0.00.325.103 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.325.103 I llm_load_print_meta: freq_scale_train = 1
0.00.325.103 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.325.103 I llm_load_print_meta: rope_finetuned   = unknown
0.00.325.104 I llm_load_print_meta: ssm_d_conv       = 0
0.00.325.104 I llm_load_print_meta: ssm_d_inner      = 0
0.00.325.104 I llm_load_print_meta: ssm_d_state      = 0
0.00.325.104 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.325.104 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.325.105 I llm_load_print_meta: model type       = 33M
0.00.325.105 I llm_load_print_meta: model ftype      = F16
0.00.325.106 I llm_load_print_meta: model params     = 32.90 M
0.00.325.106 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.325.106 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.325.106 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.325.107 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.325.107 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.325.107 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.325.107 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.325.107 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.325.107 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.325.109 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.325.109 I llm_load_print_meta: max token length = 45
0.00.326.319 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.326.320 I llm_load_tensors: offloading output layer to GPU
0.00.326.320 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.326.347 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.348 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.327.196 I llama_new_context_with_model: n_seq_max     = 1
0.00.327.198 I llama_new_context_with_model: n_ctx         = 8192
0.00.327.198 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.327.198 I llama_new_context_with_model: n_batch       = 2048
0.00.327.199 I llama_new_context_with_model: n_ubatch      = 2048
0.00.327.199 I llama_new_context_with_model: flash_attn    = 0
0.00.327.199 I llama_new_context_with_model: freq_base     = 10000.0
0.00.327.199 I llama_new_context_with_model: freq_scale    = 1
0.00.327.200 I ggml_metal_init: allocating
0.00.327.209 I ggml_metal_init: found device: Apple M4
0.00.327.211 I ggml_metal_init: picking default device: Apple M4
0.00.328.020 I ggml_metal_init: using embedded metal library
0.00.330.882 I ggml_metal_init: GPU name:   Apple M4
0.00.330.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.884 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.885 I ggml_metal_init: simdgroup reduction   = true
0.00.330.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.885 I ggml_metal_init: has bfloat            = true
0.00.330.885 I ggml_metal_init: use bfloat            = true
0.00.330.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.340.550 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.343.020 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.343.023 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.343.028 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.343.597 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.343.598 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.343.599 I llama_new_context_with_model: graph nodes  = 154
0.00.343.599 I llama_new_context_with_model: graph splits = 2
0.00.343.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.343.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.285 I 
0.00.356.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.456 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.457 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.468 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.469 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.475 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.475 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.972 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.709 I llama_perf_context_print:        load time =     332.52 ms
0.00.360.710 I llama_perf_context_print: prompt eval time =       3.73 ms /    62 tokens (    0.06 ms per token, 16635.36 tokens per second)
0.00.360.711 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.711 I llama_perf_context_print:       total time =       4.42 ms /    63 tokens
0.00.360.905 I ggml_metal_free: deallocating

real	0m1.075s
user	0m0.332s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.176 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.289 I main: llama backend init
0.00.000.296 I main: load the model and apply lora adapter, if any
0.00.083.026 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.093.935 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.093.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.093.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.093.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.093.953 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.093.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.093.954 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.093.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.093.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.093.958 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.093.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.093.959 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.093.968 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.093.969 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.093.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.093.974 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.093.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.100.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.103.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.110.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.110.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.110.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.110.027 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.110.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.110.030 I llama_model_loader: - type  f32:  194 tensors
0.00.110.030 I llama_model_loader: - type  f16:   98 tensors
0.00.148.590 I llm_load_vocab: special tokens cache size = 25
0.00.156.688 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.156.692 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.156.693 I llm_load_print_meta: arch             = gptneox
0.00.156.693 I llm_load_print_meta: vocab type       = BPE
0.00.156.693 I llm_load_print_meta: n_vocab          = 50304
0.00.156.694 I llm_load_print_meta: n_merges         = 50009
0.00.156.694 I llm_load_print_meta: vocab_only       = 0
0.00.156.694 I llm_load_print_meta: n_ctx_train      = 2048
0.00.156.694 I llm_load_print_meta: n_embd           = 2048
0.00.156.696 I llm_load_print_meta: n_layer          = 24
0.00.156.700 I llm_load_print_meta: n_head           = 16
0.00.156.701 I llm_load_print_meta: n_head_kv        = 16
0.00.156.701 I llm_load_print_meta: n_rot            = 32
0.00.156.701 I llm_load_print_meta: n_swa            = 0
0.00.156.702 I llm_load_print_meta: n_embd_head_k    = 128
0.00.156.702 I llm_load_print_meta: n_embd_head_v    = 128
0.00.156.702 I llm_load_print_meta: n_gqa            = 1
0.00.156.703 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.156.704 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.156.705 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.156.707 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.156.707 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.156.707 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.156.707 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.156.708 I llm_load_print_meta: n_ff             = 8192
0.00.156.708 I llm_load_print_meta: n_expert         = 0
0.00.156.708 I llm_load_print_meta: n_expert_used    = 0
0.00.156.708 I llm_load_print_meta: causal attn      = 1
0.00.156.709 I llm_load_print_meta: pooling type     = 0
0.00.156.709 I llm_load_print_meta: rope type        = 2
0.00.156.709 I llm_load_print_meta: rope scaling     = linear
0.00.156.710 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.156.710 I llm_load_print_meta: freq_scale_train = 1
0.00.156.710 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.156.710 I llm_load_print_meta: rope_finetuned   = unknown
0.00.156.710 I llm_load_print_meta: ssm_d_conv       = 0
0.00.156.711 I llm_load_print_meta: ssm_d_inner      = 0
0.00.156.711 I llm_load_print_meta: ssm_d_state      = 0
0.00.156.711 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.156.711 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.156.712 I llm_load_print_meta: model type       = 1.4B
0.00.156.712 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.156.712 I llm_load_print_meta: model params     = 1.41 B
0.00.156.713 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.156.713 I llm_load_print_meta: general.name     = 1.4B
0.00.156.714 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.156.714 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.156.714 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.156.714 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.156.715 I llm_load_print_meta: LF token         = 128 ''
0.00.156.715 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.156.715 I llm_load_print_meta: max token length = 1024
0.00.159.472 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.159.473 I llm_load_tensors: offloading output layer to GPU
0.00.159.473 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.159.493 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.159.494 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.160.532 I llama_new_context_with_model: n_seq_max     = 1
0.00.160.533 I llama_new_context_with_model: n_ctx         = 2048
0.00.160.534 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.160.534 I llama_new_context_with_model: n_batch       = 2048
0.00.160.534 I llama_new_context_with_model: n_ubatch      = 512
0.00.160.534 I llama_new_context_with_model: flash_attn    = 0
0.00.160.535 I llama_new_context_with_model: freq_base     = 10000.0
0.00.160.535 I llama_new_context_with_model: freq_scale    = 1
0.00.160.535 I ggml_metal_init: allocating
0.00.160.538 I ggml_metal_init: found device: Apple M4
0.00.160.542 I ggml_metal_init: picking default device: Apple M4
0.00.161.243 I ggml_metal_init: using embedded metal library
0.00.173.293 I ggml_metal_init: GPU name:   Apple M4
0.00.173.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.173.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.173.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.173.296 I ggml_metal_init: simdgroup reduction   = true
0.00.173.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.173.296 I ggml_metal_init: has bfloat            = true
0.00.173.296 I ggml_metal_init: use bfloat            = true
0.00.173.297 I ggml_metal_init: hasUnifiedMemory      = true
0.00.173.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.219.301 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.242.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.242.946 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.242.968 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.244.075 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.244.077 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.244.077 I llama_new_context_with_model: graph nodes  = 967
0.00.244.078 I llama_new_context_with_model: graph splits = 2
0.00.244.081 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.244.234 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.244.235 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.333.326 I main: llama threadpool init, n_threads = 4
0.00.333.364 I 
0.00.333.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.333.390 I 
0.00.333.464 I sampler seed: 1234
0.00.333.468 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.333.503 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.333.504 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.333.504 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.170.297 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.02.170.297 I llama_perf_context_print:        load time =     250.29 ms
0.02.170.298 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.26 tokens per second)
0.02.170.299 I llama_perf_context_print:        eval time =    1790.20 ms /    63 runs   (   28.42 ms per token,    35.19 tokens per second)
0.02.170.299 I llama_perf_context_print:       total time =    1836.97 ms /    70 tokens
0.02.170.541 I ggml_metal_free: deallocating

real	0m2.496s
user	0m0.153s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.836 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.952 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.981 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.995 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.995 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.995 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.996 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.997 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.999 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.209 I llama_model_loader: - type  f32:  194 tensors
0.00.056.210 I llama_model_loader: - type  f16:   98 tensors
0.00.084.377 I llm_load_vocab: special tokens cache size = 25
0.00.090.861 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.864 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.864 I llm_load_print_meta: arch             = gptneox
0.00.090.864 I llm_load_print_meta: vocab type       = BPE
0.00.090.865 I llm_load_print_meta: n_vocab          = 50304
0.00.090.865 I llm_load_print_meta: n_merges         = 50009
0.00.090.865 I llm_load_print_meta: vocab_only       = 0
0.00.090.865 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.865 I llm_load_print_meta: n_embd           = 2048
0.00.090.865 I llm_load_print_meta: n_layer          = 24
0.00.090.868 I llm_load_print_meta: n_head           = 16
0.00.090.869 I llm_load_print_meta: n_head_kv        = 16
0.00.090.869 I llm_load_print_meta: n_rot            = 32
0.00.090.869 I llm_load_print_meta: n_swa            = 0
0.00.090.869 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.869 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.870 I llm_load_print_meta: n_gqa            = 1
0.00.090.870 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.871 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.872 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.872 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.872 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.873 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.873 I llm_load_print_meta: n_ff             = 8192
0.00.090.873 I llm_load_print_meta: n_expert         = 0
0.00.090.874 I llm_load_print_meta: n_expert_used    = 0
0.00.090.874 I llm_load_print_meta: causal attn      = 1
0.00.090.875 I llm_load_print_meta: pooling type     = 0
0.00.090.875 I llm_load_print_meta: rope type        = 2
0.00.090.875 I llm_load_print_meta: rope scaling     = linear
0.00.090.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.876 I llm_load_print_meta: freq_scale_train = 1
0.00.090.876 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.878 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.878 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.878 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.878 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.878 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.878 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.878 I llm_load_print_meta: model type       = 1.4B
0.00.090.879 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.879 I llm_load_print_meta: model params     = 1.41 B
0.00.090.880 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.880 I llm_load_print_meta: general.name     = 1.4B
0.00.090.883 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.884 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.884 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.884 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.884 I llm_load_print_meta: LF token         = 128 ''
0.00.090.885 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.885 I llm_load_print_meta: max token length = 1024
0.00.093.418 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.418 I llm_load_tensors: offloading output layer to GPU
0.00.093.418 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.429 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.430 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.338 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.338 I llama_new_context_with_model: n_ctx         = 128
0.00.094.338 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.339 I llama_new_context_with_model: n_batch       = 128
0.00.094.339 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.339 I llama_new_context_with_model: flash_attn    = 0
0.00.094.339 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.340 I llama_new_context_with_model: freq_scale    = 1
0.00.094.340 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.341 I ggml_metal_init: allocating
0.00.094.344 I ggml_metal_init: found device: Apple M4
0.00.094.346 I ggml_metal_init: picking default device: Apple M4
0.00.094.974 I ggml_metal_init: using embedded metal library
0.00.097.486 I ggml_metal_init: GPU name:   Apple M4
0.00.097.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.488 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.488 I ggml_metal_init: simdgroup reduction   = true
0.00.097.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.489 I ggml_metal_init: has bfloat            = true
0.00.097.489 I ggml_metal_init: use bfloat            = true
0.00.097.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.490 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.285 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.528 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.530 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.547 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.471 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.472 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.473 I llama_new_context_with_model: graph nodes  = 967
0.00.108.473 I llama_new_context_with_model: graph splits = 2
0.00.108.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.329.824 I 
0.01.329.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.329.992 I perplexity: tokenizing the input ..
0.01.343.622 I perplexity: tokenization took 13.627 ms
0.01.343.629 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.465.859 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.467.598 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.467.631 I llama_perf_context_print:        load time =    1304.84 ms
0.01.467.633 I llama_perf_context_print: prompt eval time =     121.34 ms /   128 tokens (    0.95 ms per token,  1054.85 tokens per second)
0.01.467.634 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.467.635 I llama_perf_context_print:       total time =     137.83 ms /   129 tokens
0.01.468.447 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.124s
sys	0m0.234s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.843 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.231 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.240 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.246 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.921 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.974 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.785 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.786 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.786 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.787 I llama_model_loader: - type  f32:  194 tensors
0.00.031.788 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.057 I llm_load_vocab: special tokens cache size = 25
0.00.059.048 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.053 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.053 I llm_load_print_meta: arch             = gptneox
0.00.059.054 I llm_load_print_meta: vocab type       = BPE
0.00.059.056 I llm_load_print_meta: n_vocab          = 50304
0.00.059.056 I llm_load_print_meta: n_merges         = 50009
0.00.059.056 I llm_load_print_meta: vocab_only       = 0
0.00.059.056 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.056 I llm_load_print_meta: n_embd           = 2048
0.00.059.057 I llm_load_print_meta: n_layer          = 24
0.00.059.061 I llm_load_print_meta: n_head           = 16
0.00.059.062 I llm_load_print_meta: n_head_kv        = 16
0.00.059.062 I llm_load_print_meta: n_rot            = 32
0.00.059.063 I llm_load_print_meta: n_swa            = 0
0.00.059.063 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.063 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.064 I llm_load_print_meta: n_gqa            = 1
0.00.059.064 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.065 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.069 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.071 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.071 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.071 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.072 I llm_load_print_meta: n_ff             = 8192
0.00.059.072 I llm_load_print_meta: n_expert         = 0
0.00.059.072 I llm_load_print_meta: n_expert_used    = 0
0.00.059.072 I llm_load_print_meta: causal attn      = 1
0.00.059.072 I llm_load_print_meta: pooling type     = 0
0.00.059.073 I llm_load_print_meta: rope type        = 2
0.00.059.074 I llm_load_print_meta: rope scaling     = linear
0.00.059.075 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.075 I llm_load_print_meta: freq_scale_train = 1
0.00.059.075 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.075 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.075 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.075 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.076 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.076 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.076 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.076 I llm_load_print_meta: model type       = 1.4B
0.00.059.077 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.077 I llm_load_print_meta: model params     = 1.41 B
0.00.059.077 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.077 I llm_load_print_meta: general.name     = 1.4B
0.00.059.081 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.081 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.082 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.082 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.082 I llm_load_print_meta: LF token         = 128 ''
0.00.059.083 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.083 I llm_load_print_meta: max token length = 1024
0.00.061.593 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.594 I llm_load_tensors: offloading output layer to GPU
0.00.061.594 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.606 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.607 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.625 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.626 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.626 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.626 I llama_new_context_with_model: n_batch       = 2048
0.00.062.626 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.626 I llama_new_context_with_model: flash_attn    = 0
0.00.062.627 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.627 I llama_new_context_with_model: freq_scale    = 1
0.00.062.628 I ggml_metal_init: allocating
0.00.062.631 I ggml_metal_init: found device: Apple M4
0.00.062.633 I ggml_metal_init: picking default device: Apple M4
0.00.063.358 I ggml_metal_init: using embedded metal library
0.00.065.898 I ggml_metal_init: GPU name:   Apple M4
0.00.065.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.900 I ggml_metal_init: simdgroup reduction   = true
0.00.065.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.901 I ggml_metal_init: has bfloat            = true
0.00.065.901 I ggml_metal_init: use bfloat            = true
0.00.065.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.360 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.130 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.138 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.162 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.382 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.385 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.385 I llama_new_context_with_model: graph nodes  = 967
0.00.104.385 I llama_new_context_with_model: graph splits = 2
0.00.104.390 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.421.060 I main: llama threadpool init, n_threads = 4
0.01.421.104 I 
0.01.421.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.421.143 I 
0.01.421.378 I sampler seed: 1234
0.01.421.383 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.421.398 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.421.399 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.421.400 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.507.741 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.02.507.742 I llama_perf_context_print:        load time =    1411.21 ms
0.02.507.742 I llama_perf_context_print: prompt eval time =      39.81 ms /     7 tokens (    5.69 ms per token,   175.83 tokens per second)
0.02.507.743 I llama_perf_context_print:        eval time =    1043.64 ms /    63 runs   (   16.57 ms per token,    60.37 tokens per second)
0.02.507.743 I llama_perf_context_print:       total time =    1086.68 ms /    70 tokens
0.02.508.040 I ggml_metal_free: deallocating

real	0m2.528s
user	0m0.112s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.375 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.400 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.445 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.892 I llama_model_loader: - type  f32:  194 tensors
0.00.032.892 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.492 I llm_load_vocab: special tokens cache size = 25
0.00.064.894 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.897 I llm_load_print_meta: arch             = gptneox
0.00.064.897 I llm_load_print_meta: vocab type       = BPE
0.00.064.898 I llm_load_print_meta: n_vocab          = 50304
0.00.064.898 I llm_load_print_meta: n_merges         = 50009
0.00.064.898 I llm_load_print_meta: vocab_only       = 0
0.00.064.898 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.898 I llm_load_print_meta: n_embd           = 2048
0.00.064.898 I llm_load_print_meta: n_layer          = 24
0.00.064.903 I llm_load_print_meta: n_head           = 16
0.00.064.904 I llm_load_print_meta: n_head_kv        = 16
0.00.064.906 I llm_load_print_meta: n_rot            = 32
0.00.064.906 I llm_load_print_meta: n_swa            = 0
0.00.064.906 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.906 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.907 I llm_load_print_meta: n_gqa            = 1
0.00.064.907 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.908 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.908 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.909 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.909 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.909 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.910 I llm_load_print_meta: n_ff             = 8192
0.00.064.910 I llm_load_print_meta: n_expert         = 0
0.00.064.910 I llm_load_print_meta: n_expert_used    = 0
0.00.064.910 I llm_load_print_meta: causal attn      = 1
0.00.064.910 I llm_load_print_meta: pooling type     = 0
0.00.064.911 I llm_load_print_meta: rope type        = 2
0.00.064.911 I llm_load_print_meta: rope scaling     = linear
0.00.064.911 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.911 I llm_load_print_meta: freq_scale_train = 1
0.00.064.912 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.912 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.912 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.912 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.913 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.915 I llm_load_print_meta: model type       = 1.4B
0.00.064.915 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.915 I llm_load_print_meta: model params     = 1.41 B
0.00.064.916 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.916 I llm_load_print_meta: general.name     = 1.4B
0.00.064.916 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.916 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.916 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.917 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.917 I llm_load_print_meta: LF token         = 128 ''
0.00.064.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.918 I llm_load_print_meta: max token length = 1024
0.00.067.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.343 I llm_load_tensors: offloading output layer to GPU
0.00.067.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.354 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.355 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.293 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.293 I llama_new_context_with_model: n_ctx         = 128
0.00.068.294 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.294 I llama_new_context_with_model: n_batch       = 128
0.00.068.294 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.294 I llama_new_context_with_model: flash_attn    = 0
0.00.068.295 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.295 I llama_new_context_with_model: freq_scale    = 1
0.00.068.296 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.296 I ggml_metal_init: allocating
0.00.068.301 I ggml_metal_init: found device: Apple M4
0.00.068.304 I ggml_metal_init: picking default device: Apple M4
0.00.068.972 I ggml_metal_init: using embedded metal library
0.00.072.201 I ggml_metal_init: GPU name:   Apple M4
0.00.072.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.204 I ggml_metal_init: simdgroup reduction   = true
0.00.072.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.207 I ggml_metal_init: has bfloat            = true
0.00.072.207 I ggml_metal_init: use bfloat            = true
0.00.072.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.826 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.830 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.844 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.795 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.796 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.796 I llama_new_context_with_model: graph nodes  = 967
0.00.084.797 I llama_new_context_with_model: graph splits = 2
0.00.084.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.011.644 I 
0.01.011.687 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.011.703 I perplexity: tokenizing the input ..
0.01.019.752 I perplexity: tokenization took 8.047 ms
0.01.019.756 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.142.311 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.143.638 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.143.649 I llama_perf_context_print:        load time =    1000.26 ms
0.01.143.650 I llama_perf_context_print: prompt eval time =     122.32 ms /   128 tokens (    0.96 ms per token,  1046.42 tokens per second)
0.01.143.651 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.143.651 I llama_perf_context_print:       total time =     132.01 ms /   129 tokens
0.01.144.045 I ggml_metal_free: deallocating

real	0m1.161s
user	0m0.095s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.066 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.103 I main: llama backend init
0.00.000.106 I main: load the model and apply lora adapter, if any
0.00.023.071 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.059.031 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.059.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.044 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.045 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.062.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.063.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.067.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.067.895 I llama_model_loader: - type  f32:  194 tensors
0.00.067.895 I llama_model_loader: - type q4_0:   97 tensors
0.00.067.895 I llama_model_loader: - type q6_K:    1 tensors
0.00.097.253 I llm_load_vocab: special tokens cache size = 25
0.00.103.188 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.103.194 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.103.195 I llm_load_print_meta: arch             = gptneox
0.00.103.195 I llm_load_print_meta: vocab type       = BPE
0.00.103.196 I llm_load_print_meta: n_vocab          = 50304
0.00.103.196 I llm_load_print_meta: n_merges         = 50009
0.00.103.196 I llm_load_print_meta: vocab_only       = 0
0.00.103.196 I llm_load_print_meta: n_ctx_train      = 2048
0.00.103.196 I llm_load_print_meta: n_embd           = 2048
0.00.103.197 I llm_load_print_meta: n_layer          = 24
0.00.103.202 I llm_load_print_meta: n_head           = 16
0.00.103.203 I llm_load_print_meta: n_head_kv        = 16
0.00.103.204 I llm_load_print_meta: n_rot            = 32
0.00.103.204 I llm_load_print_meta: n_swa            = 0
0.00.103.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.103.204 I llm_load_print_meta: n_embd_head_v    = 128
0.00.103.205 I llm_load_print_meta: n_gqa            = 1
0.00.103.206 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.103.206 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.103.207 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.103.208 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.103.208 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.103.208 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.103.208 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.103.209 I llm_load_print_meta: n_ff             = 8192
0.00.103.209 I llm_load_print_meta: n_expert         = 0
0.00.103.209 I llm_load_print_meta: n_expert_used    = 0
0.00.103.210 I llm_load_print_meta: causal attn      = 1
0.00.103.210 I llm_load_print_meta: pooling type     = 0
0.00.103.210 I llm_load_print_meta: rope type        = 2
0.00.103.210 I llm_load_print_meta: rope scaling     = linear
0.00.103.211 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.103.211 I llm_load_print_meta: freq_scale_train = 1
0.00.103.211 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.103.212 I llm_load_print_meta: rope_finetuned   = unknown
0.00.103.212 I llm_load_print_meta: ssm_d_conv       = 0
0.00.103.212 I llm_load_print_meta: ssm_d_inner      = 0
0.00.103.212 I llm_load_print_meta: ssm_d_state      = 0
0.00.103.212 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.103.212 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.103.212 I llm_load_print_meta: model type       = 1.4B
0.00.103.213 I llm_load_print_meta: model ftype      = Q4_0
0.00.103.213 I llm_load_print_meta: model params     = 1.41 B
0.00.103.214 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.103.214 I llm_load_print_meta: general.name     = 1.4B
0.00.103.214 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.103.215 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.103.215 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.103.215 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.103.215 I llm_load_print_meta: LF token         = 128 ''
0.00.103.216 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.103.216 I llm_load_print_meta: max token length = 1024
0.00.105.724 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.724 I llm_load_tensors: offloading output layer to GPU
0.00.105.725 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.737 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.105.739 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.106.754 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.754 I llama_new_context_with_model: n_ctx         = 2048
0.00.106.755 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.106.755 I llama_new_context_with_model: n_batch       = 2048
0.00.106.755 I llama_new_context_with_model: n_ubatch      = 512
0.00.106.755 I llama_new_context_with_model: flash_attn    = 0
0.00.106.756 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.756 I llama_new_context_with_model: freq_scale    = 1
0.00.106.756 I ggml_metal_init: allocating
0.00.106.760 I ggml_metal_init: found device: Apple M4
0.00.106.762 I ggml_metal_init: picking default device: Apple M4
0.00.107.507 I ggml_metal_init: using embedded metal library
0.00.110.428 I ggml_metal_init: GPU name:   Apple M4
0.00.110.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.430 I ggml_metal_init: simdgroup reduction   = true
0.00.110.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.430 I ggml_metal_init: has bfloat            = true
0.00.110.430 I ggml_metal_init: use bfloat            = true
0.00.110.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.418 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.157.774 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.779 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.804 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.894 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.895 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.896 I llama_new_context_with_model: graph nodes  = 967
0.00.158.896 I llama_new_context_with_model: graph splits = 2
0.00.158.901 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.159.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.913.138 I main: llama threadpool init, n_threads = 4
0.00.913.246 I 
0.00.913.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.913.316 I 
0.00.913.574 I sampler seed: 1234
0.00.913.586 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.913.678 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.913.684 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.913.684 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.611.009 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49067.04 tokens per second)
0.01.611.009 I llama_perf_context_print:        load time =     890.05 ms
0.01.611.010 I llama_perf_context_print: prompt eval time =      50.55 ms /     7 tokens (    7.22 ms per token,   138.47 tokens per second)
0.01.611.011 I llama_perf_context_print:        eval time =     643.60 ms /    63 runs   (   10.22 ms per token,    97.89 tokens per second)
0.01.611.011 I llama_perf_context_print:       total time =     697.88 ms /    70 tokens
0.01.611.232 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.134s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.031 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.564 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.564 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.565 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.566 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.566 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.567 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.568 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.628 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.629 I llama_model_loader: - type  f32:  194 tensors
0.00.024.630 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.630 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.792 I llm_load_vocab: special tokens cache size = 25
0.00.051.789 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.794 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.795 I llm_load_print_meta: arch             = gptneox
0.00.051.795 I llm_load_print_meta: vocab type       = BPE
0.00.051.795 I llm_load_print_meta: n_vocab          = 50304
0.00.051.796 I llm_load_print_meta: n_merges         = 50009
0.00.051.802 I llm_load_print_meta: vocab_only       = 0
0.00.051.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.802 I llm_load_print_meta: n_embd           = 2048
0.00.051.802 I llm_load_print_meta: n_layer          = 24
0.00.051.806 I llm_load_print_meta: n_head           = 16
0.00.051.807 I llm_load_print_meta: n_head_kv        = 16
0.00.051.807 I llm_load_print_meta: n_rot            = 32
0.00.051.807 I llm_load_print_meta: n_swa            = 0
0.00.051.807 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.807 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.808 I llm_load_print_meta: n_gqa            = 1
0.00.051.809 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.810 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.810 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.811 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.811 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.811 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.813 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.814 I llm_load_print_meta: n_ff             = 8192
0.00.051.814 I llm_load_print_meta: n_expert         = 0
0.00.051.814 I llm_load_print_meta: n_expert_used    = 0
0.00.051.815 I llm_load_print_meta: causal attn      = 1
0.00.051.841 I llm_load_print_meta: pooling type     = 0
0.00.051.844 I llm_load_print_meta: rope type        = 2
0.00.051.844 I llm_load_print_meta: rope scaling     = linear
0.00.051.845 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.845 I llm_load_print_meta: freq_scale_train = 1
0.00.051.845 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.845 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.845 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.845 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.846 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.846 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.849 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.849 I llm_load_print_meta: model type       = 1.4B
0.00.051.850 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.850 I llm_load_print_meta: model params     = 1.41 B
0.00.051.851 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.851 I llm_load_print_meta: general.name     = 1.4B
0.00.051.852 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.852 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.853 I llm_load_print_meta: LF token         = 128 ''
0.00.051.855 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.855 I llm_load_print_meta: max token length = 1024
0.00.053.732 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.732 I llm_load_tensors: offloading output layer to GPU
0.00.053.732 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.744 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.745 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.729 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.731 I llama_new_context_with_model: n_ctx         = 128
0.00.054.731 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.731 I llama_new_context_with_model: n_batch       = 128
0.00.054.731 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.731 I llama_new_context_with_model: flash_attn    = 0
0.00.054.732 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.732 I llama_new_context_with_model: freq_scale    = 1
0.00.054.732 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.733 I ggml_metal_init: allocating
0.00.054.740 I ggml_metal_init: found device: Apple M4
0.00.054.742 I ggml_metal_init: picking default device: Apple M4
0.00.055.387 I ggml_metal_init: using embedded metal library
0.00.057.788 I ggml_metal_init: GPU name:   Apple M4
0.00.057.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.791 I ggml_metal_init: simdgroup reduction   = true
0.00.057.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.791 I ggml_metal_init: has bfloat            = true
0.00.057.791 I ggml_metal_init: use bfloat            = true
0.00.057.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.230 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.569 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.576 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.475 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.476 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.476 I llama_new_context_with_model: graph nodes  = 967
0.00.069.477 I llama_new_context_with_model: graph splits = 2
0.00.069.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.576 I 
0.00.598.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.612 I perplexity: tokenizing the input ..
0.00.606.041 I perplexity: tokenization took 7.427 ms
0.00.606.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.070 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.730.434 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.730.453 I llama_perf_context_print:        load time =     588.54 ms
0.00.730.455 I llama_perf_context_print: prompt eval time =     122.81 ms /   128 tokens (    0.96 ms per token,  1042.30 tokens per second)
0.00.730.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.456 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.730.927 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.078s
sys	0m0.092s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.820 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.344 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.351 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.272 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.273 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.275 I llama_model_loader: - type  f32:  194 tensors
0.00.025.275 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.851 I llm_load_vocab: special tokens cache size = 25
0.00.052.072 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.075 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.075 I llm_load_print_meta: arch             = gptneox
0.00.052.075 I llm_load_print_meta: vocab type       = BPE
0.00.052.076 I llm_load_print_meta: n_vocab          = 50304
0.00.052.076 I llm_load_print_meta: n_merges         = 50009
0.00.052.076 I llm_load_print_meta: vocab_only       = 0
0.00.052.076 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.076 I llm_load_print_meta: n_embd           = 2048
0.00.052.076 I llm_load_print_meta: n_layer          = 24
0.00.052.080 I llm_load_print_meta: n_head           = 16
0.00.052.080 I llm_load_print_meta: n_head_kv        = 16
0.00.052.081 I llm_load_print_meta: n_rot            = 32
0.00.052.083 I llm_load_print_meta: n_swa            = 0
0.00.052.083 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.083 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.084 I llm_load_print_meta: n_gqa            = 1
0.00.052.084 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.085 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.086 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.086 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.087 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.087 I llm_load_print_meta: n_ff             = 8192
0.00.052.087 I llm_load_print_meta: n_expert         = 0
0.00.052.088 I llm_load_print_meta: n_expert_used    = 0
0.00.052.089 I llm_load_print_meta: causal attn      = 1
0.00.052.090 I llm_load_print_meta: pooling type     = 0
0.00.052.090 I llm_load_print_meta: rope type        = 2
0.00.052.091 I llm_load_print_meta: rope scaling     = linear
0.00.052.091 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.091 I llm_load_print_meta: freq_scale_train = 1
0.00.052.091 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.092 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.092 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.092 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.092 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.092 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.092 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.093 I llm_load_print_meta: model type       = 1.4B
0.00.052.093 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.094 I llm_load_print_meta: model params     = 1.41 B
0.00.052.094 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.095 I llm_load_print_meta: general.name     = 1.4B
0.00.052.095 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.095 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.095 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.095 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.096 I llm_load_print_meta: LF token         = 128 ''
0.00.052.096 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.100 I llm_load_print_meta: max token length = 1024
0.00.054.077 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.078 I llm_load_tensors: offloading output layer to GPU
0.00.054.078 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.089 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.090 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.978 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.978 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.979 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.979 I llama_new_context_with_model: n_batch       = 2048
0.00.054.979 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.979 I llama_new_context_with_model: flash_attn    = 0
0.00.054.980 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.980 I llama_new_context_with_model: freq_scale    = 1
0.00.054.980 I ggml_metal_init: allocating
0.00.054.983 I ggml_metal_init: found device: Apple M4
0.00.054.985 I ggml_metal_init: picking default device: Apple M4
0.00.055.573 I ggml_metal_init: using embedded metal library
0.00.058.154 I ggml_metal_init: GPU name:   Apple M4
0.00.058.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.155 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.156 I ggml_metal_init: simdgroup reduction   = true
0.00.058.156 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.156 I ggml_metal_init: has bfloat            = true
0.00.058.158 I ggml_metal_init: use bfloat            = true
0.00.058.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.160 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.466 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.119 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.130 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.148 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.353 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.355 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.355 I llama_new_context_with_model: graph nodes  = 967
0.00.089.356 I llama_new_context_with_model: graph splits = 2
0.00.089.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.118.184 I main: llama threadpool init, n_threads = 4
0.01.118.224 I 
0.01.118.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.118.248 I 
0.01.118.490 I sampler seed: 1234
0.01.118.496 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.118.519 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.118.519 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.118.519 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.840.021 I llama_perf_sampler_print:    sampling time =       1.05 ms /    71 runs   (    0.01 ms per token, 67748.09 tokens per second)
0.01.840.022 I llama_perf_context_print:        load time =    1109.36 ms
0.01.840.022 I llama_perf_context_print: prompt eval time =      43.49 ms /     7 tokens (    6.21 ms per token,   160.95 tokens per second)
0.01.840.023 I llama_perf_context_print:        eval time =     675.27 ms /    63 runs   (   10.72 ms per token,    93.30 tokens per second)
0.01.840.023 I llama_perf_context_print:       total time =     721.84 ms /    70 tokens
0.01.840.273 I ggml_metal_free: deallocating

real	0m1.856s
user	0m0.109s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.186 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.362 I llama_model_loader: - type  f32:  194 tensors
0.00.023.363 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.363 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.727 I llm_load_vocab: special tokens cache size = 25
0.00.049.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.682 I llm_load_print_meta: arch             = gptneox
0.00.049.683 I llm_load_print_meta: vocab type       = BPE
0.00.049.683 I llm_load_print_meta: n_vocab          = 50304
0.00.049.683 I llm_load_print_meta: n_merges         = 50009
0.00.049.683 I llm_load_print_meta: vocab_only       = 0
0.00.049.684 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.684 I llm_load_print_meta: n_embd           = 2048
0.00.049.684 I llm_load_print_meta: n_layer          = 24
0.00.049.697 I llm_load_print_meta: n_head           = 16
0.00.049.705 I llm_load_print_meta: n_head_kv        = 16
0.00.049.706 I llm_load_print_meta: n_rot            = 32
0.00.049.706 I llm_load_print_meta: n_swa            = 0
0.00.049.706 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.706 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.707 I llm_load_print_meta: n_gqa            = 1
0.00.049.707 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.708 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.709 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.709 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.709 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.709 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.709 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.710 I llm_load_print_meta: n_ff             = 8192
0.00.049.710 I llm_load_print_meta: n_expert         = 0
0.00.049.710 I llm_load_print_meta: n_expert_used    = 0
0.00.049.710 I llm_load_print_meta: causal attn      = 1
0.00.049.710 I llm_load_print_meta: pooling type     = 0
0.00.049.711 I llm_load_print_meta: rope type        = 2
0.00.049.711 I llm_load_print_meta: rope scaling     = linear
0.00.049.711 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.711 I llm_load_print_meta: freq_scale_train = 1
0.00.049.712 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.712 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.712 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.712 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.712 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.712 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.713 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.713 I llm_load_print_meta: model type       = 1.4B
0.00.049.713 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.714 I llm_load_print_meta: model params     = 1.41 B
0.00.049.714 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.715 I llm_load_print_meta: general.name     = 1.4B
0.00.049.715 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.715 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.715 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.715 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.716 I llm_load_print_meta: LF token         = 128 ''
0.00.049.718 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: max token length = 1024
0.00.051.674 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.674 I llm_load_tensors: offloading output layer to GPU
0.00.051.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.685 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.686 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.610 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.611 I llama_new_context_with_model: n_ctx         = 128
0.00.052.611 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.611 I llama_new_context_with_model: n_batch       = 128
0.00.052.612 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.612 I llama_new_context_with_model: flash_attn    = 0
0.00.052.612 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.612 I llama_new_context_with_model: freq_scale    = 1
0.00.052.613 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.614 I ggml_metal_init: allocating
0.00.052.620 I ggml_metal_init: found device: Apple M4
0.00.052.623 I ggml_metal_init: picking default device: Apple M4
0.00.053.234 I ggml_metal_init: using embedded metal library
0.00.055.570 I ggml_metal_init: GPU name:   Apple M4
0.00.055.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.573 I ggml_metal_init: simdgroup reduction   = true
0.00.055.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.573 I ggml_metal_init: has bfloat            = true
0.00.055.573 I ggml_metal_init: use bfloat            = true
0.00.055.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.156 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.558 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.561 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.576 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.470 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.472 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.472 I llama_new_context_with_model: graph nodes  = 967
0.00.067.472 I llama_new_context_with_model: graph splits = 2
0.00.067.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.282 I 
0.00.698.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.328 I perplexity: tokenizing the input ..
0.00.706.272 I perplexity: tokenization took 7.942 ms
0.00.706.275 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.089 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.830.274 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.830.291 I llama_perf_context_print:        load time =     689.09 ms
0.00.830.292 I llama_perf_context_print: prompt eval time =     122.59 ms /   128 tokens (    0.96 ms per token,  1044.14 tokens per second)
0.00.830.293 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.293 I llama_perf_context_print:       total time =     132.01 ms /   129 tokens
0.00.830.746 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.078s
sys	0m0.109s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.012.155 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.004 I llama_model_loader: - type  f32:  194 tensors
0.00.039.004 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.945 I llm_load_vocab: special tokens cache size = 25
0.00.080.470 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.474 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.474 I llm_load_print_meta: arch             = gptneox
0.00.080.475 I llm_load_print_meta: vocab type       = BPE
0.00.080.475 I llm_load_print_meta: n_vocab          = 50304
0.00.080.475 I llm_load_print_meta: n_merges         = 50009
0.00.080.476 I llm_load_print_meta: vocab_only       = 0
0.00.080.476 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.476 I llm_load_print_meta: n_embd           = 2048
0.00.080.476 I llm_load_print_meta: n_layer          = 24
0.00.080.479 I llm_load_print_meta: n_head           = 16
0.00.080.481 I llm_load_print_meta: n_head_kv        = 16
0.00.080.481 I llm_load_print_meta: n_rot            = 32
0.00.080.481 I llm_load_print_meta: n_swa            = 0
0.00.080.482 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.482 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.483 I llm_load_print_meta: n_gqa            = 1
0.00.080.484 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.485 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.485 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.486 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.486 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.486 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.486 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.487 I llm_load_print_meta: n_ff             = 8192
0.00.080.487 I llm_load_print_meta: n_expert         = 0
0.00.080.488 I llm_load_print_meta: n_expert_used    = 0
0.00.080.489 I llm_load_print_meta: causal attn      = 1
0.00.080.490 I llm_load_print_meta: pooling type     = 0
0.00.080.490 I llm_load_print_meta: rope type        = 2
0.00.080.490 I llm_load_print_meta: rope scaling     = linear
0.00.080.491 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.491 I llm_load_print_meta: freq_scale_train = 1
0.00.080.492 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.492 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.492 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.492 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.493 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.493 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.493 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.493 I llm_load_print_meta: model type       = 1.4B
0.00.080.494 I llm_load_print_meta: model ftype      = Q5_0
0.00.080.494 I llm_load_print_meta: model params     = 1.41 B
0.00.080.495 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.080.496 I llm_load_print_meta: general.name     = 1.4B
0.00.080.496 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.496 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.498 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.498 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.499 I llm_load_print_meta: LF token         = 128 ''
0.00.080.499 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.499 I llm_load_print_meta: max token length = 1024
0.00.083.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.655 I llm_load_tensors: offloading output layer to GPU
0.00.083.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.662 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.083.663 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.085.193 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.194 I llama_new_context_with_model: n_ctx         = 2048
0.00.085.195 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.085.195 I llama_new_context_with_model: n_batch       = 2048
0.00.085.195 I llama_new_context_with_model: n_ubatch      = 512
0.00.085.195 I llama_new_context_with_model: flash_attn    = 0
0.00.085.196 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.197 I llama_new_context_with_model: freq_scale    = 1
0.00.085.197 I ggml_metal_init: allocating
0.00.085.206 I ggml_metal_init: found device: Apple M4
0.00.085.213 I ggml_metal_init: picking default device: Apple M4
0.00.086.067 I ggml_metal_init: using embedded metal library
0.00.089.751 I ggml_metal_init: GPU name:   Apple M4
0.00.089.753 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.754 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.755 I ggml_metal_init: simdgroup reduction   = true
0.00.089.755 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.755 I ggml_metal_init: has bfloat            = true
0.00.089.755 I ggml_metal_init: use bfloat            = true
0.00.089.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.598 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.124.317 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.326 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.344 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.380 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.382 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.382 I llama_new_context_with_model: graph nodes  = 967
0.00.125.383 I llama_new_context_with_model: graph splits = 2
0.00.125.388 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.125.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.125.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.665 I main: llama threadpool init, n_threads = 4
0.00.832.718 I 
0.00.832.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.832.757 I 
0.00.832.994 I sampler seed: 1234
0.00.833.002 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.034 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.036 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.036 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.620.650 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.01.620.650 I llama_perf_context_print:        load time =     820.50 ms
0.01.620.653 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.17 tokens per second)
0.01.620.655 I llama_perf_context_print:        eval time =     741.58 ms /    63 runs   (   11.77 ms per token,    84.95 tokens per second)
0.01.620.655 I llama_perf_context_print:       total time =     787.99 ms /    70 tokens
0.01.620.853 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.136s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.920 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.968 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.969 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.970 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.970 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.970 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.799 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.647 I llama_model_loader: - type  f32:  194 tensors
0.00.030.647 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.359 I llm_load_vocab: special tokens cache size = 25
0.00.058.283 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.286 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.286 I llm_load_print_meta: arch             = gptneox
0.00.058.286 I llm_load_print_meta: vocab type       = BPE
0.00.058.287 I llm_load_print_meta: n_vocab          = 50304
0.00.058.287 I llm_load_print_meta: n_merges         = 50009
0.00.058.287 I llm_load_print_meta: vocab_only       = 0
0.00.058.287 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.287 I llm_load_print_meta: n_embd           = 2048
0.00.058.287 I llm_load_print_meta: n_layer          = 24
0.00.058.290 I llm_load_print_meta: n_head           = 16
0.00.058.291 I llm_load_print_meta: n_head_kv        = 16
0.00.058.291 I llm_load_print_meta: n_rot            = 32
0.00.058.291 I llm_load_print_meta: n_swa            = 0
0.00.058.292 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.292 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.293 I llm_load_print_meta: n_gqa            = 1
0.00.058.294 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.295 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.295 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.296 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.296 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.296 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.296 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.297 I llm_load_print_meta: n_ff             = 8192
0.00.058.297 I llm_load_print_meta: n_expert         = 0
0.00.058.297 I llm_load_print_meta: n_expert_used    = 0
0.00.058.298 I llm_load_print_meta: causal attn      = 1
0.00.058.298 I llm_load_print_meta: pooling type     = 0
0.00.058.298 I llm_load_print_meta: rope type        = 2
0.00.058.298 I llm_load_print_meta: rope scaling     = linear
0.00.058.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.299 I llm_load_print_meta: freq_scale_train = 1
0.00.058.299 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.300 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.300 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.300 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.300 I llm_load_print_meta: model type       = 1.4B
0.00.058.301 I llm_load_print_meta: model ftype      = Q5_0
0.00.058.301 I llm_load_print_meta: model params     = 1.41 B
0.00.058.302 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.058.302 I llm_load_print_meta: general.name     = 1.4B
0.00.058.302 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.302 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.302 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.303 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.303 I llm_load_print_meta: LF token         = 128 ''
0.00.058.303 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.303 I llm_load_print_meta: max token length = 1024
0.00.060.302 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.302 I llm_load_tensors: offloading output layer to GPU
0.00.060.303 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.314 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.060.315 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.061.171 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.172 I llama_new_context_with_model: n_ctx         = 128
0.00.061.172 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.061.172 I llama_new_context_with_model: n_batch       = 128
0.00.061.172 I llama_new_context_with_model: n_ubatch      = 128
0.00.061.172 I llama_new_context_with_model: flash_attn    = 0
0.00.061.173 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.173 I llama_new_context_with_model: freq_scale    = 1
0.00.061.173 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.174 I ggml_metal_init: allocating
0.00.061.177 I ggml_metal_init: found device: Apple M4
0.00.061.179 I ggml_metal_init: picking default device: Apple M4
0.00.061.753 I ggml_metal_init: using embedded metal library
0.00.064.027 I ggml_metal_init: GPU name:   Apple M4
0.00.064.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.030 I ggml_metal_init: simdgroup reduction   = true
0.00.064.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.030 I ggml_metal_init: has bfloat            = true
0.00.064.030 I ggml_metal_init: use bfloat            = true
0.00.064.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.311 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.883 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.887 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.911 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.075.840 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.075.841 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.075.841 I llama_new_context_with_model: graph nodes  = 967
0.00.075.841 I llama_new_context_with_model: graph splits = 2
0.00.075.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.340 I 
0.00.709.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.399 I perplexity: tokenizing the input ..
0.00.717.485 I perplexity: tokenization took 8.084 ms
0.00.717.489 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.100 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.280 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.297 I llama_perf_context_print:        load time =     697.41 ms
0.00.854.298 I llama_perf_context_print: prompt eval time =     135.37 ms /   128 tokens (    1.06 ms per token,   945.53 tokens per second)
0.00.854.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.299 I llama_perf_context_print:       total time =     144.96 ms /   129 tokens
0.00.854.854 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.079s
sys	0m0.129s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.868 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.968 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.972 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.972 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.824 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.825 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.826 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.826 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.826 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.827 I llama_model_loader: - type  f32:  194 tensors
0.00.023.827 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.827 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.301 I llm_load_vocab: special tokens cache size = 25
0.00.050.280 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.282 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.282 I llm_load_print_meta: arch             = gptneox
0.00.050.283 I llm_load_print_meta: vocab type       = BPE
0.00.050.283 I llm_load_print_meta: n_vocab          = 50304
0.00.050.283 I llm_load_print_meta: n_merges         = 50009
0.00.050.283 I llm_load_print_meta: vocab_only       = 0
0.00.050.283 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.284 I llm_load_print_meta: n_embd           = 2048
0.00.050.284 I llm_load_print_meta: n_layer          = 24
0.00.050.287 I llm_load_print_meta: n_head           = 16
0.00.050.288 I llm_load_print_meta: n_head_kv        = 16
0.00.050.288 I llm_load_print_meta: n_rot            = 32
0.00.050.288 I llm_load_print_meta: n_swa            = 0
0.00.050.288 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.288 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.290 I llm_load_print_meta: n_gqa            = 1
0.00.050.291 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.292 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.292 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.294 I llm_load_print_meta: n_ff             = 8192
0.00.050.296 I llm_load_print_meta: n_expert         = 0
0.00.050.296 I llm_load_print_meta: n_expert_used    = 0
0.00.050.296 I llm_load_print_meta: causal attn      = 1
0.00.050.296 I llm_load_print_meta: pooling type     = 0
0.00.050.296 I llm_load_print_meta: rope type        = 2
0.00.050.296 I llm_load_print_meta: rope scaling     = linear
0.00.050.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.298 I llm_load_print_meta: freq_scale_train = 1
0.00.050.299 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.300 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.300 I llm_load_print_meta: model type       = 1.4B
0.00.050.300 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.300 I llm_load_print_meta: model params     = 1.41 B
0.00.050.301 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.301 I llm_load_print_meta: general.name     = 1.4B
0.00.050.302 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: LF token         = 128 ''
0.00.050.306 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.307 I llm_load_print_meta: max token length = 1024
0.00.052.354 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.354 I llm_load_tensors: offloading output layer to GPU
0.00.052.355 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.366 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.367 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.265 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.266 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.266 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.266 I llama_new_context_with_model: n_batch       = 2048
0.00.053.267 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.267 I llama_new_context_with_model: flash_attn    = 0
0.00.053.267 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.268 I llama_new_context_with_model: freq_scale    = 1
0.00.053.268 I ggml_metal_init: allocating
0.00.053.274 I ggml_metal_init: found device: Apple M4
0.00.053.276 I ggml_metal_init: picking default device: Apple M4
0.00.053.854 I ggml_metal_init: using embedded metal library
0.00.056.179 I ggml_metal_init: GPU name:   Apple M4
0.00.056.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.182 I ggml_metal_init: simdgroup reduction   = true
0.00.056.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.182 I ggml_metal_init: has bfloat            = true
0.00.056.182 I ggml_metal_init: use bfloat            = true
0.00.056.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.473 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.466 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.472 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.491 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.394 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.395 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.395 I llama_new_context_with_model: graph nodes  = 967
0.00.086.396 I llama_new_context_with_model: graph splits = 2
0.00.086.399 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.037 I main: llama threadpool init, n_threads = 4
0.00.684.075 I 
0.00.684.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.123 I 
0.00.684.347 I sampler seed: 1234
0.00.684.352 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.368 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.368 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.523.114 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.523.115 I llama_perf_context_print:        load time =     675.16 ms
0.01.523.116 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.72 tokens per second)
0.01.523.120 I llama_perf_context_print:        eval time =     793.47 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.523.121 I llama_perf_context_print:       total time =     839.08 ms /    70 tokens
0.01.523.304 I ggml_metal_free: deallocating

real	0m1.540s
user	0m0.109s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.518 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.200 I llama_model_loader: - type  f32:  194 tensors
0.00.023.200 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.200 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.574 I llm_load_vocab: special tokens cache size = 25
0.00.049.671 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.674 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.674 I llm_load_print_meta: arch             = gptneox
0.00.049.675 I llm_load_print_meta: vocab type       = BPE
0.00.049.675 I llm_load_print_meta: n_vocab          = 50304
0.00.049.675 I llm_load_print_meta: n_merges         = 50009
0.00.049.675 I llm_load_print_meta: vocab_only       = 0
0.00.049.675 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.675 I llm_load_print_meta: n_embd           = 2048
0.00.049.676 I llm_load_print_meta: n_layer          = 24
0.00.049.679 I llm_load_print_meta: n_head           = 16
0.00.049.679 I llm_load_print_meta: n_head_kv        = 16
0.00.049.680 I llm_load_print_meta: n_rot            = 32
0.00.049.680 I llm_load_print_meta: n_swa            = 0
0.00.049.680 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.680 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.681 I llm_load_print_meta: n_gqa            = 1
0.00.049.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.682 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.683 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.684 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.684 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.684 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.684 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.685 I llm_load_print_meta: n_ff             = 8192
0.00.049.685 I llm_load_print_meta: n_expert         = 0
0.00.049.685 I llm_load_print_meta: n_expert_used    = 0
0.00.049.685 I llm_load_print_meta: causal attn      = 1
0.00.049.687 I llm_load_print_meta: pooling type     = 0
0.00.049.687 I llm_load_print_meta: rope type        = 2
0.00.049.687 I llm_load_print_meta: rope scaling     = linear
0.00.049.688 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.688 I llm_load_print_meta: freq_scale_train = 1
0.00.049.688 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.689 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.689 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.689 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.689 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.689 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.689 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.689 I llm_load_print_meta: model type       = 1.4B
0.00.049.690 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.690 I llm_load_print_meta: model params     = 1.41 B
0.00.049.691 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.691 I llm_load_print_meta: general.name     = 1.4B
0.00.049.691 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.693 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.694 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.694 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.694 I llm_load_print_meta: LF token         = 128 ''
0.00.049.694 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.694 I llm_load_print_meta: max token length = 1024
0.00.051.671 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.672 I llm_load_tensors: offloading output layer to GPU
0.00.051.672 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.682 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.683 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.576 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.576 I llama_new_context_with_model: n_ctx         = 128
0.00.052.577 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.577 I llama_new_context_with_model: n_batch       = 128
0.00.052.577 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.577 I llama_new_context_with_model: flash_attn    = 0
0.00.052.577 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.578 I llama_new_context_with_model: freq_scale    = 1
0.00.052.578 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.579 I ggml_metal_init: allocating
0.00.052.582 I ggml_metal_init: found device: Apple M4
0.00.052.583 I ggml_metal_init: picking default device: Apple M4
0.00.053.161 I ggml_metal_init: using embedded metal library
0.00.055.545 I ggml_metal_init: GPU name:   Apple M4
0.00.055.546 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.547 I ggml_metal_init: simdgroup reduction   = true
0.00.055.547 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.548 I ggml_metal_init: has bfloat            = true
0.00.055.548 I ggml_metal_init: use bfloat            = true
0.00.055.548 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.549 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.029 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.325 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.340 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.211 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.212 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.212 I llama_new_context_with_model: graph nodes  = 967
0.00.067.212 I llama_new_context_with_model: graph splits = 2
0.00.067.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.920 I 
0.00.620.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.993 I perplexity: tokenizing the input ..
0.00.628.969 I perplexity: tokenization took 7.974 ms
0.00.628.972 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.108 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.765.279 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.765.294 I llama_perf_context_print:        load time =     611.91 ms
0.00.765.294 I llama_perf_context_print: prompt eval time =     134.91 ms /   128 tokens (    1.05 ms per token,   948.77 tokens per second)
0.00.765.295 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.296 I llama_perf_context_print:       total time =     144.38 ms /   129 tokens
0.00.765.702 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.078s
sys	0m0.104s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.845 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.167 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.168 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.169 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.170 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.788 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.791 I llama_model_loader: - type  f32:  194 tensors
0.00.023.791 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.791 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.628 I llm_load_vocab: special tokens cache size = 25
0.00.049.520 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.523 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.523 I llm_load_print_meta: arch             = gptneox
0.00.049.524 I llm_load_print_meta: vocab type       = BPE
0.00.049.524 I llm_load_print_meta: n_vocab          = 50304
0.00.049.524 I llm_load_print_meta: n_merges         = 50009
0.00.049.524 I llm_load_print_meta: vocab_only       = 0
0.00.049.524 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.525 I llm_load_print_meta: n_embd           = 2048
0.00.049.525 I llm_load_print_meta: n_layer          = 24
0.00.049.528 I llm_load_print_meta: n_head           = 16
0.00.049.528 I llm_load_print_meta: n_head_kv        = 16
0.00.049.528 I llm_load_print_meta: n_rot            = 32
0.00.049.529 I llm_load_print_meta: n_swa            = 0
0.00.049.529 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.529 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.530 I llm_load_print_meta: n_gqa            = 1
0.00.049.531 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.531 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.534 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.534 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.534 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.534 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.534 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.535 I llm_load_print_meta: n_ff             = 8192
0.00.049.535 I llm_load_print_meta: n_expert         = 0
0.00.049.535 I llm_load_print_meta: n_expert_used    = 0
0.00.049.536 I llm_load_print_meta: causal attn      = 1
0.00.049.536 I llm_load_print_meta: pooling type     = 0
0.00.049.536 I llm_load_print_meta: rope type        = 2
0.00.049.536 I llm_load_print_meta: rope scaling     = linear
0.00.049.538 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.538 I llm_load_print_meta: freq_scale_train = 1
0.00.049.539 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.539 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.539 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.539 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.539 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.539 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.539 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.540 I llm_load_print_meta: model type       = 1.4B
0.00.049.540 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.541 I llm_load_print_meta: model params     = 1.41 B
0.00.049.541 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.541 I llm_load_print_meta: general.name     = 1.4B
0.00.049.542 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.542 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.542 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.546 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.546 I llm_load_print_meta: LF token         = 128 ''
0.00.049.546 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.547 I llm_load_print_meta: max token length = 1024
0.00.051.395 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.395 I llm_load_tensors: offloading output layer to GPU
0.00.051.395 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.405 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.406 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.278 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.279 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.279 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.279 I llama_new_context_with_model: n_batch       = 2048
0.00.052.280 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.280 I llama_new_context_with_model: flash_attn    = 0
0.00.052.280 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.280 I llama_new_context_with_model: freq_scale    = 1
0.00.052.281 I ggml_metal_init: allocating
0.00.052.286 I ggml_metal_init: found device: Apple M4
0.00.052.289 I ggml_metal_init: picking default device: Apple M4
0.00.052.881 I ggml_metal_init: using embedded metal library
0.00.055.191 I ggml_metal_init: GPU name:   Apple M4
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.194 I ggml_metal_init: simdgroup reduction   = true
0.00.055.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.194 I ggml_metal_init: has bfloat            = true
0.00.055.194 I ggml_metal_init: use bfloat            = true
0.00.055.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.643 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.995 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.015 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.013 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.014 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.015 I llama_new_context_with_model: graph nodes  = 967
0.00.089.015 I llama_new_context_with_model: graph splits = 2
0.00.089.018 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.465.327 I main: llama threadpool init, n_threads = 4
0.00.465.367 I 
0.00.465.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.465.388 I 
0.00.465.536 I sampler seed: 1234
0.00.465.542 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.465.555 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.465.556 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.465.557 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.139.653 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.139.654 I llama_perf_context_print:        load time =     455.48 ms
0.01.139.659 I llama_perf_context_print: prompt eval time =      35.71 ms /     7 tokens (    5.10 ms per token,   196.04 tokens per second)
0.01.139.660 I llama_perf_context_print:        eval time =     635.70 ms /    63 runs   (   10.09 ms per token,    99.10 tokens per second)
0.01.139.660 I llama_perf_context_print:       total time =     674.33 ms /    70 tokens
0.01.139.927 I ggml_metal_free: deallocating

real	0m1.158s
user	0m0.109s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.761 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.937 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.943 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.944 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.946 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.947 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.949 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.507 I llama_model_loader: - type  f32:  194 tensors
0.00.024.507 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.507 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.508 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.040 I llm_load_vocab: special tokens cache size = 25
0.00.050.205 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.208 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.208 I llm_load_print_meta: arch             = gptneox
0.00.050.209 I llm_load_print_meta: vocab type       = BPE
0.00.050.209 I llm_load_print_meta: n_vocab          = 50304
0.00.050.209 I llm_load_print_meta: n_merges         = 50009
0.00.050.209 I llm_load_print_meta: vocab_only       = 0
0.00.050.210 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.210 I llm_load_print_meta: n_embd           = 2048
0.00.050.210 I llm_load_print_meta: n_layer          = 24
0.00.050.212 I llm_load_print_meta: n_head           = 16
0.00.050.213 I llm_load_print_meta: n_head_kv        = 16
0.00.050.213 I llm_load_print_meta: n_rot            = 32
0.00.050.214 I llm_load_print_meta: n_swa            = 0
0.00.050.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.215 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.216 I llm_load_print_meta: n_gqa            = 1
0.00.050.216 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.217 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.218 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.220 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.220 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.220 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.221 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.221 I llm_load_print_meta: n_ff             = 8192
0.00.050.221 I llm_load_print_meta: n_expert         = 0
0.00.050.221 I llm_load_print_meta: n_expert_used    = 0
0.00.050.222 I llm_load_print_meta: causal attn      = 1
0.00.050.222 I llm_load_print_meta: pooling type     = 0
0.00.050.222 I llm_load_print_meta: rope type        = 2
0.00.050.222 I llm_load_print_meta: rope scaling     = linear
0.00.050.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.223 I llm_load_print_meta: freq_scale_train = 1
0.00.050.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.223 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.224 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.224 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.224 I llm_load_print_meta: model type       = 1.4B
0.00.050.230 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.230 I llm_load_print_meta: model params     = 1.41 B
0.00.050.230 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.231 I llm_load_print_meta: general.name     = 1.4B
0.00.050.231 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.233 I llm_load_print_meta: LF token         = 128 ''
0.00.050.233 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.233 I llm_load_print_meta: max token length = 1024
0.00.052.101 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.101 I llm_load_tensors: offloading output layer to GPU
0.00.052.101 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.112 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.113 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.993 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.993 I llama_new_context_with_model: n_ctx         = 128
0.00.052.994 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.994 I llama_new_context_with_model: n_batch       = 128
0.00.052.994 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.994 I llama_new_context_with_model: flash_attn    = 0
0.00.052.995 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.995 I llama_new_context_with_model: freq_scale    = 1
0.00.052.995 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.996 I ggml_metal_init: allocating
0.00.053.002 I ggml_metal_init: found device: Apple M4
0.00.053.005 I ggml_metal_init: picking default device: Apple M4
0.00.053.586 I ggml_metal_init: using embedded metal library
0.00.055.977 I ggml_metal_init: GPU name:   Apple M4
0.00.055.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.980 I ggml_metal_init: simdgroup reduction   = true
0.00.055.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.980 I ggml_metal_init: has bfloat            = true
0.00.055.980 I ggml_metal_init: use bfloat            = true
0.00.055.980 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.301 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.532 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.534 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.548 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.428 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.429 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.430 I llama_new_context_with_model: graph nodes  = 967
0.00.067.430 I llama_new_context_with_model: graph splits = 2
0.00.067.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.744 I 
0.00.387.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.793 I perplexity: tokenizing the input ..
0.00.396.069 I perplexity: tokenization took 8.274 ms
0.00.396.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.659 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.829 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.848 I llama_perf_context_print:        load time =     376.98 ms
0.00.529.849 I llama_perf_context_print: prompt eval time =     132.34 ms /   128 tokens (    1.03 ms per token,   967.23 tokens per second)
0.00.529.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.850 I llama_perf_context_print:       total time =     142.11 ms /   129 tokens
0.00.530.346 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.077s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.800 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.914 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.920 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.923 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.923 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.924 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.921 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.892 I llama_model_loader: - type  f32:  194 tensors
0.00.023.898 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.903 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.903 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.903 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.089 I llm_load_vocab: special tokens cache size = 25
0.00.051.309 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.314 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.314 I llm_load_print_meta: arch             = gptneox
0.00.051.314 I llm_load_print_meta: vocab type       = BPE
0.00.051.315 I llm_load_print_meta: n_vocab          = 50304
0.00.051.315 I llm_load_print_meta: n_merges         = 50009
0.00.051.315 I llm_load_print_meta: vocab_only       = 0
0.00.051.315 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.315 I llm_load_print_meta: n_embd           = 2048
0.00.051.315 I llm_load_print_meta: n_layer          = 24
0.00.051.319 I llm_load_print_meta: n_head           = 16
0.00.051.320 I llm_load_print_meta: n_head_kv        = 16
0.00.051.320 I llm_load_print_meta: n_rot            = 32
0.00.051.320 I llm_load_print_meta: n_swa            = 0
0.00.051.320 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.320 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.321 I llm_load_print_meta: n_gqa            = 1
0.00.051.322 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.324 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.324 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.325 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.325 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.325 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.325 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.326 I llm_load_print_meta: n_ff             = 8192
0.00.051.326 I llm_load_print_meta: n_expert         = 0
0.00.051.326 I llm_load_print_meta: n_expert_used    = 0
0.00.051.326 I llm_load_print_meta: causal attn      = 1
0.00.051.326 I llm_load_print_meta: pooling type     = 0
0.00.051.326 I llm_load_print_meta: rope type        = 2
0.00.051.326 I llm_load_print_meta: rope scaling     = linear
0.00.051.327 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.327 I llm_load_print_meta: freq_scale_train = 1
0.00.051.327 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.327 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.327 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.327 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.328 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.330 I llm_load_print_meta: model type       = 1.4B
0.00.051.330 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.331 I llm_load_print_meta: model params     = 1.41 B
0.00.051.331 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.331 I llm_load_print_meta: general.name     = 1.4B
0.00.051.331 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.331 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: LF token         = 128 ''
0.00.051.332 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: max token length = 1024
0.00.053.417 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.417 I llm_load_tensors: offloading output layer to GPU
0.00.053.417 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.428 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.429 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.330 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.330 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.331 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.331 I llama_new_context_with_model: n_batch       = 2048
0.00.054.331 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.331 I llama_new_context_with_model: flash_attn    = 0
0.00.054.333 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.333 I llama_new_context_with_model: freq_scale    = 1
0.00.054.334 I ggml_metal_init: allocating
0.00.054.337 I ggml_metal_init: found device: Apple M4
0.00.054.339 I ggml_metal_init: picking default device: Apple M4
0.00.054.999 I ggml_metal_init: using embedded metal library
0.00.057.458 I ggml_metal_init: GPU name:   Apple M4
0.00.057.460 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.460 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.461 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.461 I ggml_metal_init: simdgroup reduction   = true
0.00.057.461 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.461 I ggml_metal_init: has bfloat            = true
0.00.057.462 I ggml_metal_init: use bfloat            = true
0.00.057.462 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.675 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.421 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.429 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.453 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.380 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.381 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.381 I llama_new_context_with_model: graph nodes  = 967
0.00.087.381 I llama_new_context_with_model: graph splits = 2
0.00.087.386 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.520 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.211 I main: llama threadpool init, n_threads = 4
0.00.522.257 I 
0.00.522.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.283 I 
0.00.522.526 I sampler seed: 1234
0.00.522.532 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.579 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.582 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.582 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.261.420 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.01.261.420 I llama_perf_context_print:        load time =     513.40 ms
0.01.261.421 I llama_perf_context_print: prompt eval time =      40.77 ms /     7 tokens (    5.82 ms per token,   171.69 tokens per second)
0.01.261.422 I llama_perf_context_print:        eval time =     695.32 ms /    63 runs   (   11.04 ms per token,    90.61 tokens per second)
0.01.261.422 I llama_perf_context_print:       total time =     739.21 ms /    70 tokens
0.01.261.695 I ggml_metal_free: deallocating

real	0m1.278s
user	0m0.110s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.762 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.254 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.683 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.685 I llama_model_loader: - type  f32:  194 tensors
0.00.022.685 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.686 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.686 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.686 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.415 I llm_load_vocab: special tokens cache size = 25
0.00.048.317 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.319 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.320 I llm_load_print_meta: arch             = gptneox
0.00.048.320 I llm_load_print_meta: vocab type       = BPE
0.00.048.320 I llm_load_print_meta: n_vocab          = 50304
0.00.048.321 I llm_load_print_meta: n_merges         = 50009
0.00.048.321 I llm_load_print_meta: vocab_only       = 0
0.00.048.321 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.321 I llm_load_print_meta: n_embd           = 2048
0.00.048.321 I llm_load_print_meta: n_layer          = 24
0.00.048.324 I llm_load_print_meta: n_head           = 16
0.00.048.325 I llm_load_print_meta: n_head_kv        = 16
0.00.048.325 I llm_load_print_meta: n_rot            = 32
0.00.048.325 I llm_load_print_meta: n_swa            = 0
0.00.048.325 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.325 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.326 I llm_load_print_meta: n_gqa            = 1
0.00.048.327 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.329 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.330 I llm_load_print_meta: n_ff             = 8192
0.00.048.330 I llm_load_print_meta: n_expert         = 0
0.00.048.330 I llm_load_print_meta: n_expert_used    = 0
0.00.048.330 I llm_load_print_meta: causal attn      = 1
0.00.048.330 I llm_load_print_meta: pooling type     = 0
0.00.048.330 I llm_load_print_meta: rope type        = 2
0.00.048.333 I llm_load_print_meta: rope scaling     = linear
0.00.048.333 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.334 I llm_load_print_meta: freq_scale_train = 1
0.00.048.334 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.334 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.334 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.334 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.334 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.335 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.335 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.335 I llm_load_print_meta: model type       = 1.4B
0.00.048.336 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.336 I llm_load_print_meta: model params     = 1.41 B
0.00.048.336 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.337 I llm_load_print_meta: general.name     = 1.4B
0.00.048.341 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.341 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.341 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.342 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.342 I llm_load_print_meta: LF token         = 128 ''
0.00.048.342 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.342 I llm_load_print_meta: max token length = 1024
0.00.050.260 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.260 I llm_load_tensors: offloading output layer to GPU
0.00.050.260 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.271 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.272 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.130 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.131 I llama_new_context_with_model: n_ctx         = 128
0.00.051.131 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.131 I llama_new_context_with_model: n_batch       = 128
0.00.051.131 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.131 I llama_new_context_with_model: flash_attn    = 0
0.00.051.132 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.132 I llama_new_context_with_model: freq_scale    = 1
0.00.051.132 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.133 I ggml_metal_init: allocating
0.00.051.136 I ggml_metal_init: found device: Apple M4
0.00.051.138 I ggml_metal_init: picking default device: Apple M4
0.00.051.684 I ggml_metal_init: using embedded metal library
0.00.053.951 I ggml_metal_init: GPU name:   Apple M4
0.00.053.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.953 I ggml_metal_init: simdgroup reduction   = true
0.00.053.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.954 I ggml_metal_init: has bfloat            = true
0.00.053.954 I ggml_metal_init: use bfloat            = true
0.00.053.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.317 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.595 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.597 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.503 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.505 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.505 I llama_new_context_with_model: graph nodes  = 967
0.00.065.505 I llama_new_context_with_model: graph splits = 2
0.00.065.506 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.463.512 I 
0.00.463.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.463.581 I perplexity: tokenizing the input ..
0.00.471.439 I perplexity: tokenization took 7.856 ms
0.00.471.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.603.575 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.604.773 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.604.785 I llama_perf_context_print:        load time =     454.74 ms
0.00.604.793 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.40 tokens per second)
0.00.604.794 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.604.794 I llama_perf_context_print:       total time =     141.28 ms /   129 tokens
0.00.605.273 I ggml_metal_free: deallocating

real	0m0.618s
user	0m0.077s
sys	0m0.078s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.757 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.209 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.214 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.832 I llama_model_loader: - type  f32:  194 tensors
0.00.022.832 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.832 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.832 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.572 I llm_load_vocab: special tokens cache size = 25
0.00.048.474 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.477 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.477 I llm_load_print_meta: arch             = gptneox
0.00.048.477 I llm_load_print_meta: vocab type       = BPE
0.00.048.478 I llm_load_print_meta: n_vocab          = 50304
0.00.048.478 I llm_load_print_meta: n_merges         = 50009
0.00.048.478 I llm_load_print_meta: vocab_only       = 0
0.00.048.478 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.478 I llm_load_print_meta: n_embd           = 2048
0.00.048.479 I llm_load_print_meta: n_layer          = 24
0.00.048.482 I llm_load_print_meta: n_head           = 16
0.00.048.483 I llm_load_print_meta: n_head_kv        = 16
0.00.048.483 I llm_load_print_meta: n_rot            = 32
0.00.048.483 I llm_load_print_meta: n_swa            = 0
0.00.048.483 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.483 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.484 I llm_load_print_meta: n_gqa            = 1
0.00.048.486 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.487 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.488 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.488 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.488 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.489 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.489 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.490 I llm_load_print_meta: n_ff             = 8192
0.00.048.490 I llm_load_print_meta: n_expert         = 0
0.00.048.491 I llm_load_print_meta: n_expert_used    = 0
0.00.048.492 I llm_load_print_meta: causal attn      = 1
0.00.048.492 I llm_load_print_meta: pooling type     = 0
0.00.048.492 I llm_load_print_meta: rope type        = 2
0.00.048.492 I llm_load_print_meta: rope scaling     = linear
0.00.048.492 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.493 I llm_load_print_meta: freq_scale_train = 1
0.00.048.493 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.493 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.493 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.493 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.494 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.494 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.494 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.494 I llm_load_print_meta: model type       = 1.4B
0.00.048.494 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.495 I llm_load_print_meta: model params     = 1.41 B
0.00.048.496 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.496 I llm_load_print_meta: general.name     = 1.4B
0.00.048.496 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.497 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.497 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.498 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.499 I llm_load_print_meta: LF token         = 128 ''
0.00.048.499 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.499 I llm_load_print_meta: max token length = 1024
0.00.050.108 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.108 I llm_load_tensors: offloading output layer to GPU
0.00.050.109 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.119 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.120 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.050.946 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.947 I llama_new_context_with_model: n_ctx         = 2048
0.00.050.947 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.050.947 I llama_new_context_with_model: n_batch       = 2048
0.00.050.947 I llama_new_context_with_model: n_ubatch      = 512
0.00.050.948 I llama_new_context_with_model: flash_attn    = 0
0.00.050.948 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.948 I llama_new_context_with_model: freq_scale    = 1
0.00.050.949 I ggml_metal_init: allocating
0.00.050.955 I ggml_metal_init: found device: Apple M4
0.00.050.958 I ggml_metal_init: picking default device: Apple M4
0.00.051.567 I ggml_metal_init: using embedded metal library
0.00.053.877 I ggml_metal_init: GPU name:   Apple M4
0.00.053.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.880 I ggml_metal_init: simdgroup reduction   = true
0.00.053.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.880 I ggml_metal_init: has bfloat            = true
0.00.053.880 I ggml_metal_init: use bfloat            = true
0.00.053.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.305 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.171 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.179 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.199 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.067 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.069 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.069 I llama_new_context_with_model: graph nodes  = 967
0.00.085.069 I llama_new_context_with_model: graph splits = 2
0.00.085.071 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.614 I main: llama threadpool init, n_threads = 4
0.00.602.665 I 
0.00.602.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.709 I 
0.00.602.949 I sampler seed: 1234
0.00.602.954 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.992 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.993 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.356.725 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.356.726 I llama_perf_context_print:        load time =     593.85 ms
0.01.356.727 I llama_perf_context_print: prompt eval time =      47.09 ms /     7 tokens (    6.73 ms per token,   148.66 tokens per second)
0.01.356.728 I llama_perf_context_print:        eval time =     703.62 ms /    63 runs   (   11.17 ms per token,    89.54 tokens per second)
0.01.356.728 I llama_perf_context_print:       total time =     754.12 ms /    70 tokens
0.01.356.921 I ggml_metal_free: deallocating

real	0m1.373s
user	0m0.108s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.807 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.192 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.212 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.012 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.013 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.014 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.014 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.014 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.015 I llama_model_loader: - type  f32:  194 tensors
0.00.023.015 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.015 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.016 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.445 I llm_load_vocab: special tokens cache size = 25
0.00.049.559 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.561 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.562 I llm_load_print_meta: arch             = gptneox
0.00.049.562 I llm_load_print_meta: vocab type       = BPE
0.00.049.562 I llm_load_print_meta: n_vocab          = 50304
0.00.049.562 I llm_load_print_meta: n_merges         = 50009
0.00.049.563 I llm_load_print_meta: vocab_only       = 0
0.00.049.563 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.563 I llm_load_print_meta: n_embd           = 2048
0.00.049.563 I llm_load_print_meta: n_layer          = 24
0.00.049.566 I llm_load_print_meta: n_head           = 16
0.00.049.567 I llm_load_print_meta: n_head_kv        = 16
0.00.049.567 I llm_load_print_meta: n_rot            = 32
0.00.049.567 I llm_load_print_meta: n_swa            = 0
0.00.049.567 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.567 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.568 I llm_load_print_meta: n_gqa            = 1
0.00.049.569 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.569 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.570 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.571 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.571 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.571 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.571 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.572 I llm_load_print_meta: n_ff             = 8192
0.00.049.572 I llm_load_print_meta: n_expert         = 0
0.00.049.572 I llm_load_print_meta: n_expert_used    = 0
0.00.049.572 I llm_load_print_meta: causal attn      = 1
0.00.049.572 I llm_load_print_meta: pooling type     = 0
0.00.049.572 I llm_load_print_meta: rope type        = 2
0.00.049.573 I llm_load_print_meta: rope scaling     = linear
0.00.049.573 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.573 I llm_load_print_meta: freq_scale_train = 1
0.00.049.574 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.574 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.574 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.574 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.574 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.575 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.575 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.575 I llm_load_print_meta: model type       = 1.4B
0.00.049.576 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.576 I llm_load_print_meta: model params     = 1.41 B
0.00.049.577 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.579 I llm_load_print_meta: general.name     = 1.4B
0.00.049.579 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.579 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.579 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.580 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.580 I llm_load_print_meta: LF token         = 128 ''
0.00.049.580 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.580 I llm_load_print_meta: max token length = 1024
0.00.051.498 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.498 I llm_load_tensors: offloading output layer to GPU
0.00.051.498 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.509 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.510 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.371 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.372 I llama_new_context_with_model: n_ctx         = 128
0.00.052.372 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.372 I llama_new_context_with_model: n_batch       = 128
0.00.052.372 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.373 I llama_new_context_with_model: flash_attn    = 0
0.00.052.373 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.373 I llama_new_context_with_model: freq_scale    = 1
0.00.052.374 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.374 I ggml_metal_init: allocating
0.00.052.380 I ggml_metal_init: found device: Apple M4
0.00.052.382 I ggml_metal_init: picking default device: Apple M4
0.00.052.958 I ggml_metal_init: using embedded metal library
0.00.055.295 I ggml_metal_init: GPU name:   Apple M4
0.00.055.296 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.297 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.297 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.297 I ggml_metal_init: simdgroup reduction   = true
0.00.055.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.298 I ggml_metal_init: has bfloat            = true
0.00.055.298 I ggml_metal_init: use bfloat            = true
0.00.055.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.299 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.595 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.881 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.885 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.898 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.701 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.702 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.702 I llama_new_context_with_model: graph nodes  = 967
0.00.066.702 I llama_new_context_with_model: graph splits = 2
0.00.066.703 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.161 I 
0.00.574.251 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.287 I perplexity: tokenizing the input ..
0.00.581.881 I perplexity: tokenization took 7.593 ms
0.00.581.884 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.476 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.663 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.683 I llama_perf_context_print:        load time =     565.35 ms
0.00.717.685 I llama_perf_context_print: prompt eval time =     134.37 ms /   128 tokens (    1.05 ms per token,   952.63 tokens per second)
0.00.717.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.686 I llama_perf_context_print:       total time =     143.53 ms /   129 tokens
0.00.718.173 I ggml_metal_free: deallocating

real	0m0.732s
user	0m0.078s
sys	0m0.110s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.070 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.560 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.136 I llama_model_loader: - type  f32:  194 tensors
0.00.025.136 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.136 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.040 I llm_load_vocab: special tokens cache size = 25
0.00.051.234 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.237 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.238 I llm_load_print_meta: arch             = gptneox
0.00.051.238 I llm_load_print_meta: vocab type       = BPE
0.00.051.238 I llm_load_print_meta: n_vocab          = 50304
0.00.051.238 I llm_load_print_meta: n_merges         = 50009
0.00.051.239 I llm_load_print_meta: vocab_only       = 0
0.00.051.239 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.239 I llm_load_print_meta: n_embd           = 2048
0.00.051.239 I llm_load_print_meta: n_layer          = 24
0.00.051.242 I llm_load_print_meta: n_head           = 16
0.00.051.243 I llm_load_print_meta: n_head_kv        = 16
0.00.051.243 I llm_load_print_meta: n_rot            = 32
0.00.051.244 I llm_load_print_meta: n_swa            = 0
0.00.051.244 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.244 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.245 I llm_load_print_meta: n_gqa            = 1
0.00.051.245 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.246 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.247 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.247 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.248 I llm_load_print_meta: n_ff             = 8192
0.00.051.248 I llm_load_print_meta: n_expert         = 0
0.00.051.248 I llm_load_print_meta: n_expert_used    = 0
0.00.051.250 I llm_load_print_meta: causal attn      = 1
0.00.051.252 I llm_load_print_meta: pooling type     = 0
0.00.051.252 I llm_load_print_meta: rope type        = 2
0.00.051.252 I llm_load_print_meta: rope scaling     = linear
0.00.051.252 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.253 I llm_load_print_meta: freq_scale_train = 1
0.00.051.253 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.253 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.254 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.254 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.254 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.254 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.256 I llm_load_print_meta: model type       = 1.4B
0.00.051.256 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.257 I llm_load_print_meta: model params     = 1.41 B
0.00.051.257 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.257 I llm_load_print_meta: general.name     = 1.4B
0.00.051.258 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.258 I llm_load_print_meta: LF token         = 128 ''
0.00.051.262 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: max token length = 1024
0.00.053.323 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.324 I llm_load_tensors: offloading output layer to GPU
0.00.053.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.335 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.336 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.315 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.315 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.315 I llama_new_context_with_model: n_batch       = 2048
0.00.054.315 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.315 I llama_new_context_with_model: flash_attn    = 0
0.00.054.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.316 I llama_new_context_with_model: freq_scale    = 1
0.00.054.317 I ggml_metal_init: allocating
0.00.054.324 I ggml_metal_init: found device: Apple M4
0.00.054.327 I ggml_metal_init: picking default device: Apple M4
0.00.054.961 I ggml_metal_init: using embedded metal library
0.00.057.339 I ggml_metal_init: GPU name:   Apple M4
0.00.057.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.344 I ggml_metal_init: simdgroup reduction   = true
0.00.057.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.344 I ggml_metal_init: has bfloat            = true
0.00.057.344 I ggml_metal_init: use bfloat            = true
0.00.057.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.879 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.107 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.112 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.132 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.187 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.189 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.189 I llama_new_context_with_model: graph nodes  = 967
0.00.087.189 I llama_new_context_with_model: graph splits = 2
0.00.087.192 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.466 I main: llama threadpool init, n_threads = 4
0.00.730.505 I 
0.00.730.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.527 I 
0.00.730.750 I sampler seed: 1234
0.00.730.754 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.811 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.815 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.815 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.578.948 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.578.949 I llama_perf_context_print:        load time =     719.39 ms
0.01.578.950 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.64 tokens per second)
0.01.578.950 I llama_perf_context_print:        eval time =     793.48 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.578.951 I llama_perf_context_print:       total time =     848.49 ms /    70 tokens
0.01.579.177 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.108s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.118 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.154 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.173 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.882 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.883 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.884 I llama_model_loader: - type  f32:  194 tensors
0.00.023.884 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.884 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.377 I llm_load_vocab: special tokens cache size = 25
0.00.049.288 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.291 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.291 I llm_load_print_meta: arch             = gptneox
0.00.049.291 I llm_load_print_meta: vocab type       = BPE
0.00.049.292 I llm_load_print_meta: n_vocab          = 50304
0.00.049.292 I llm_load_print_meta: n_merges         = 50009
0.00.049.292 I llm_load_print_meta: vocab_only       = 0
0.00.049.292 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.292 I llm_load_print_meta: n_embd           = 2048
0.00.049.293 I llm_load_print_meta: n_layer          = 24
0.00.049.296 I llm_load_print_meta: n_head           = 16
0.00.049.297 I llm_load_print_meta: n_head_kv        = 16
0.00.049.297 I llm_load_print_meta: n_rot            = 32
0.00.049.297 I llm_load_print_meta: n_swa            = 0
0.00.049.297 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.298 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.298 I llm_load_print_meta: n_gqa            = 1
0.00.049.299 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.300 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.300 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.301 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.301 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.301 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.301 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.302 I llm_load_print_meta: n_ff             = 8192
0.00.049.305 I llm_load_print_meta: n_expert         = 0
0.00.049.305 I llm_load_print_meta: n_expert_used    = 0
0.00.049.305 I llm_load_print_meta: causal attn      = 1
0.00.049.305 I llm_load_print_meta: pooling type     = 0
0.00.049.305 I llm_load_print_meta: rope type        = 2
0.00.049.305 I llm_load_print_meta: rope scaling     = linear
0.00.049.306 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.306 I llm_load_print_meta: freq_scale_train = 1
0.00.049.306 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.307 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.307 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.307 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.307 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.307 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.307 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.308 I llm_load_print_meta: model type       = 1.4B
0.00.049.308 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.309 I llm_load_print_meta: model params     = 1.41 B
0.00.049.309 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.309 I llm_load_print_meta: general.name     = 1.4B
0.00.049.310 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.310 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.310 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.310 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.311 I llm_load_print_meta: LF token         = 128 ''
0.00.049.311 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.311 I llm_load_print_meta: max token length = 1024
0.00.051.283 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.283 I llm_load_tensors: offloading output layer to GPU
0.00.051.283 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.294 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.295 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.212 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.213 I llama_new_context_with_model: n_ctx         = 128
0.00.052.214 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.214 I llama_new_context_with_model: n_batch       = 128
0.00.052.214 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.214 I llama_new_context_with_model: flash_attn    = 0
0.00.052.215 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.215 I llama_new_context_with_model: freq_scale    = 1
0.00.052.215 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.216 I ggml_metal_init: allocating
0.00.052.221 I ggml_metal_init: found device: Apple M4
0.00.052.223 I ggml_metal_init: picking default device: Apple M4
0.00.052.771 I ggml_metal_init: using embedded metal library
0.00.055.117 I ggml_metal_init: GPU name:   Apple M4
0.00.055.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.120 I ggml_metal_init: simdgroup reduction   = true
0.00.055.120 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.120 I ggml_metal_init: has bfloat            = true
0.00.055.120 I ggml_metal_init: use bfloat            = true
0.00.055.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.362 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.630 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.636 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.651 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.516 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.517 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.517 I llama_new_context_with_model: graph nodes  = 967
0.00.066.518 I llama_new_context_with_model: graph splits = 2
0.00.066.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.360 I 
0.00.679.403 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.419 I perplexity: tokenizing the input ..
0.00.687.446 I perplexity: tokenization took 8.025 ms
0.00.687.450 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.310 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.829.581 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.829.596 I llama_perf_context_print:        load time =     669.24 ms
0.00.829.597 I llama_perf_context_print: prompt eval time =     140.63 ms /   128 tokens (    1.10 ms per token,   910.16 tokens per second)
0.00.829.598 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.598 I llama_perf_context_print:       total time =     150.24 ms /   129 tokens
0.00.830.107 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.076s
sys	0m0.107s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.537 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.013.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.833 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.839 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.332 I llama_model_loader: - type  f32:  194 tensors
0.00.022.332 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.176 I llm_load_vocab: special tokens cache size = 25
0.00.048.065 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.068 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.068 I llm_load_print_meta: arch             = gptneox
0.00.048.069 I llm_load_print_meta: vocab type       = BPE
0.00.048.069 I llm_load_print_meta: n_vocab          = 50304
0.00.048.069 I llm_load_print_meta: n_merges         = 50009
0.00.048.069 I llm_load_print_meta: vocab_only       = 0
0.00.048.070 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.070 I llm_load_print_meta: n_embd           = 2048
0.00.048.070 I llm_load_print_meta: n_layer          = 24
0.00.048.073 I llm_load_print_meta: n_head           = 16
0.00.048.076 I llm_load_print_meta: n_head_kv        = 16
0.00.048.076 I llm_load_print_meta: n_rot            = 32
0.00.048.076 I llm_load_print_meta: n_swa            = 0
0.00.048.076 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.077 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.077 I llm_load_print_meta: n_gqa            = 1
0.00.048.078 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.079 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.079 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.080 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.080 I llm_load_print_meta: n_ff             = 8192
0.00.048.080 I llm_load_print_meta: n_expert         = 0
0.00.048.080 I llm_load_print_meta: n_expert_used    = 0
0.00.048.080 I llm_load_print_meta: causal attn      = 1
0.00.048.084 I llm_load_print_meta: pooling type     = 0
0.00.048.084 I llm_load_print_meta: rope type        = 2
0.00.048.084 I llm_load_print_meta: rope scaling     = linear
0.00.048.085 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.085 I llm_load_print_meta: freq_scale_train = 1
0.00.048.085 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.085 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.085 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.085 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.086 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.086 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.086 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.087 I llm_load_print_meta: model type       = 1.4B
0.00.048.087 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.088 I llm_load_print_meta: model params     = 1.41 B
0.00.048.091 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.091 I llm_load_print_meta: general.name     = 1.4B
0.00.048.091 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.091 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.091 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.092 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.092 I llm_load_print_meta: LF token         = 128 ''
0.00.048.092 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.092 I llm_load_print_meta: max token length = 1024
0.00.050.076 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.076 I llm_load_tensors: offloading output layer to GPU
0.00.050.077 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.087 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.089 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.050.990 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.990 I llama_new_context_with_model: n_ctx         = 2048
0.00.050.990 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.050.991 I llama_new_context_with_model: n_batch       = 2048
0.00.050.991 I llama_new_context_with_model: n_ubatch      = 512
0.00.050.991 I llama_new_context_with_model: flash_attn    = 0
0.00.050.991 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.992 I llama_new_context_with_model: freq_scale    = 1
0.00.050.992 I ggml_metal_init: allocating
0.00.050.995 I ggml_metal_init: found device: Apple M4
0.00.050.997 I ggml_metal_init: picking default device: Apple M4
0.00.051.596 I ggml_metal_init: using embedded metal library
0.00.053.867 I ggml_metal_init: GPU name:   Apple M4
0.00.053.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.869 I ggml_metal_init: simdgroup reduction   = true
0.00.053.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.870 I ggml_metal_init: has bfloat            = true
0.00.053.870 I ggml_metal_init: use bfloat            = true
0.00.053.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.354 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.902 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.908 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.932 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.893 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.894 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.895 I llama_new_context_with_model: graph nodes  = 967
0.00.084.895 I llama_new_context_with_model: graph splits = 2
0.00.084.897 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.040 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.123 I main: llama threadpool init, n_threads = 4
0.00.743.158 I 
0.00.743.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.181 I 
0.00.743.347 I sampler seed: 1234
0.00.743.353 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.367 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.367 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.623.288 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60787.67 tokens per second)
0.01.623.288 I llama_perf_context_print:        load time =     734.58 ms
0.01.623.289 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.41 tokens per second)
0.01.623.290 I llama_perf_context_print:        eval time =     822.46 ms /    63 runs   (   13.05 ms per token,    76.60 tokens per second)
0.01.623.291 I llama_perf_context_print:       total time =     880.17 ms /    70 tokens
0.01.623.490 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.108s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4431 (dc7cef9f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.634 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.998 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.004 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.005 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.006 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.006 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.007 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.823 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.588 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.590 I llama_model_loader: - type  f32:  194 tensors
0.00.022.590 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.002 I llm_load_vocab: special tokens cache size = 25
0.00.049.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.014 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.014 I llm_load_print_meta: arch             = gptneox
0.00.049.014 I llm_load_print_meta: vocab type       = BPE
0.00.049.015 I llm_load_print_meta: n_vocab          = 50304
0.00.049.015 I llm_load_print_meta: n_merges         = 50009
0.00.049.015 I llm_load_print_meta: vocab_only       = 0
0.00.049.015 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.015 I llm_load_print_meta: n_embd           = 2048
0.00.049.016 I llm_load_print_meta: n_layer          = 24
0.00.049.018 I llm_load_print_meta: n_head           = 16
0.00.049.020 I llm_load_print_meta: n_head_kv        = 16
0.00.049.021 I llm_load_print_meta: n_rot            = 32
0.00.049.021 I llm_load_print_meta: n_swa            = 0
0.00.049.021 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.021 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.022 I llm_load_print_meta: n_gqa            = 1
0.00.049.022 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.023 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.024 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.024 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.025 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.025 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.025 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.026 I llm_load_print_meta: n_ff             = 8192
0.00.049.026 I llm_load_print_meta: n_expert         = 0
0.00.049.027 I llm_load_print_meta: n_expert_used    = 0
0.00.049.027 I llm_load_print_meta: causal attn      = 1
0.00.049.027 I llm_load_print_meta: pooling type     = 0
0.00.049.027 I llm_load_print_meta: rope type        = 2
0.00.049.027 I llm_load_print_meta: rope scaling     = linear
0.00.049.028 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.030 I llm_load_print_meta: freq_scale_train = 1
0.00.049.031 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.031 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.031 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.031 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.032 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.032 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.032 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.032 I llm_load_print_meta: model type       = 1.4B
0.00.049.033 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.033 I llm_load_print_meta: model params     = 1.41 B
0.00.049.034 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.034 I llm_load_print_meta: general.name     = 1.4B
0.00.049.034 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.034 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.034 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.036 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.036 I llm_load_print_meta: LF token         = 128 ''
0.00.049.036 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.036 I llm_load_print_meta: max token length = 1024
0.00.051.100 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.101 I llm_load_tensors: offloading output layer to GPU
0.00.051.101 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.111 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.113 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.111 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.111 I llama_new_context_with_model: n_ctx         = 128
0.00.052.112 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.112 I llama_new_context_with_model: n_batch       = 128
0.00.052.112 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.112 I llama_new_context_with_model: flash_attn    = 0
0.00.052.112 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.113 I llama_new_context_with_model: freq_scale    = 1
0.00.052.113 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.114 I ggml_metal_init: allocating
0.00.052.117 I ggml_metal_init: found device: Apple M4
0.00.052.119 I ggml_metal_init: picking default device: Apple M4
0.00.052.693 I ggml_metal_init: using embedded metal library
0.00.054.995 I ggml_metal_init: GPU name:   Apple M4
0.00.054.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.998 I ggml_metal_init: simdgroup reduction   = true
0.00.054.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.998 I ggml_metal_init: has bfloat            = true
0.00.054.998 I ggml_metal_init: use bfloat            = true
0.00.055.000 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.582 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.012 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.014 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.015 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.016 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.016 I llama_new_context_with_model: graph nodes  = 967
0.00.067.016 I llama_new_context_with_model: graph splits = 2
0.00.067.017 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.649 I 
0.00.618.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.686 I perplexity: tokenizing the input ..
0.00.626.733 I perplexity: tokenization took 8.045 ms
0.00.626.740 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.895 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.768.039 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.768.056 I llama_perf_context_print:        load time =     610.01 ms
0.00.768.056 I llama_perf_context_print: prompt eval time =     139.93 ms /   128 tokens (    1.09 ms per token,   914.75 tokens per second)
0.00.768.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.058 I llama_perf_context_print:       total time =     149.41 ms /   129 tokens
0.00.768.526 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.078s
sys	0m0.112s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4431 (dc7cef9f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d607590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d607ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d608250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d608800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d608db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d609360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d609910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d609ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d60a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d60a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d60ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d60b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d60be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d60c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d60ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d60d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d60dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d60e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d60ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d60f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d60f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d6100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d6110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d6117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d612090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d612d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d613240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d6139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d613c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d6144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d614cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d615190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d615630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d615ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d615f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d6168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d616d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d6171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d617950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d6194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d619ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d61a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d61a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d61ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d61b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d61bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d61bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d61c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d61c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d61ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d61d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d61d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d61dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d61e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d61e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d61ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d61eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d61f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d61f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d61fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d6205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d620f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d6219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d6229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d6239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d623ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d624ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d626970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d627410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d627960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d627eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d628950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d628ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d618b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d629310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d62a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d62a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d62aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d62b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d62b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d62baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d62bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d62c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d62ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d62cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d62d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d62da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d62dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d62e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d62e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d62edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d62f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d62f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d62fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d630030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d6304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d630970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d630e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d6312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d631750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d631bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d632530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d6329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d632e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d633310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d6337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d6340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d634a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d634ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d635370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d635810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d636150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d6365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d636f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d6373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d637870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d637d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d6381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d638650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d638af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d638f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d639430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d6398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d639d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d63a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d63a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d63ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d63aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d63b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d63b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d63bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d63c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d63c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d63cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d63d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d63d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d63d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d63de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d63e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d63e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d63ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d63f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d63f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d63f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d63fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d640330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d6407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d640c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d6415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d641ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d642cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d643170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d643610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d643ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d6443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d644d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d6451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d645720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d6461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d6469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d6475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d647c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d6483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d648890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d648b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d649f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d64a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d64a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d64ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d64b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d64ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d64bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d64c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d64ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d64cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d64d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d64da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d64df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d64e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d64ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d64ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d64f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d64fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d64ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d6504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d6509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d651490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d6519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d651f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d652480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d6529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d652f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d653470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d6539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d653f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d654460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d6549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d654f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d655450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d6559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d655ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d656440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d656990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d656ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d657430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d657980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d657ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d658420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d658970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d658ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d659410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d659960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d659eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d65a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d65a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d65aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d65b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d65b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d65be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d65c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d65c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d65ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d65d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d65d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d65de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d65e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d65e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d65ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d65f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d65f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d65fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d65fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d660810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d660cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d661150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d6615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d661a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d661f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d6623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d662920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d663040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d663760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d6645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d664860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d665310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d665920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.139.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.139.673 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d504b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d504fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d505430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d5058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d505d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d506180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d5065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d506a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d506ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d507340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d5077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d507ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d5089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d509170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d509980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d50a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d50a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d50aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d50b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d50bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d50c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d50cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d50d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d50d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d50e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d50e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d50e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d50eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d50ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d50f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d50f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d50fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d5101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d510470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d5108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d510d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d5111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d511630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d511aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d511f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d512380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d5127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d512c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d5130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d513540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d5139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d513e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d514290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d514700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d514b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d514fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d515450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d5158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d515d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d5161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d516610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d516b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d517080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d5174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d517960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d517dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d518240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d5186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d518b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d518f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d519400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d519870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d519ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d51a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d51a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d51aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d51aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d51b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d51b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d51bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d51c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d51c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d51c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d51cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d51d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d51d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d51db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d51df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d51e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d51e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d51ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d51f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d51f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d51fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d51fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d5202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d520760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d520bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d521040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d5214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d521920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d521d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d522200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d522670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d522ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d522f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d5233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d523830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d523ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d524110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d524580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d5249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d524e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d5252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d525740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d525bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d526020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d526490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d526900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d526d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d5271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d527650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d527ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d527f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d5283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d528810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d528c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d5290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d529560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d5299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d529e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d52a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d52a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d52ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d52b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d52b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d52b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d52bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d52c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d52c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d52caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d52cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d52d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d52d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d52dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d52e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d52e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d52e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d52ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d52f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d52f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d52fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d52ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d530450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d5308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d530d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d5311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d531610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d531a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d531ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d532360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d5327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d532c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d5330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d533520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d533990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d533e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d534270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d5346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d534b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d534fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d535bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d535eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d536170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d5365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d536a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d536ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d537330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d5377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d537c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d538080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d5384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d538960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d538dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d539240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d5396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d539b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d539f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d53a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d53a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d53ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d53b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d53b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d53ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d53bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d53c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d53c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d53cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d53d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d53d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d53d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d53ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d53e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d53e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d53eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d53ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d53f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d53f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d53fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d5402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d540730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d540ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d541010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d541530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d541a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d5425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d542870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d542e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d5433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d5439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d543f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d544530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d544af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d5450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d545670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d545c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d5461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d5467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d546d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d547330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d5478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d547eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d548470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d548a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d548ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d5495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d549b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d54a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d54a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d54acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d54b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d54b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d54bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d54c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d54c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d54cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d54d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d54dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d54e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d54e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d54ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d54f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d54f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d54fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d5502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d5508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d550e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d551430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d5519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d551fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d552570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d552b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d5530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d5536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d553c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d554230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d5547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d554db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d555370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d555930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d555ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d5564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d556a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d556f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d557470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d557970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d557e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d558370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d558870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d558d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d559270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d559770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d559c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d55a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d55a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d55ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d55b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d55b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d55bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d55c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d55cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d55d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d55d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d55df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d55e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d55e860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e307290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e307700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e307b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e307fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e308450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e3088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e308d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e3091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e309610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e309b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e309fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e30a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e30b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e30b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e30c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e30c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e30cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e30d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e30dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e30e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e30ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e30f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e30fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e3101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e310910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e310bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e310e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e311300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e311770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e311be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e312050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e312580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e3129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e312cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e313120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e313590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e313a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e313e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e3142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e314750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e314bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e315030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e3154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e315910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e315d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e3161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e316660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e316ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e316f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e3173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e317820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e317c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e318100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e318570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e3189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e318e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e3193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e3198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e319d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e31a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e31a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e31aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e31aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e31b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e31b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e31bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e31c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e31c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e31c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e31ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e31d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e31d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e31db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e31dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e31e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e31e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e31ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e31f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e31f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e31fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e31fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e320340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e3207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e320c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e321090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e321500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e321970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e321de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e322250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e3226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e322b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e322fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e323410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e323880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e323cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e324160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e3245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e324a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e324eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e325320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e325790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e325c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e326070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e326900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e326bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e327030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e3274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e327910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e327d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e3281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e328660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e328ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e328f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e3293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e329820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e329c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e32a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e32a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e32a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e32ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e32b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e32b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e32bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e32c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e32c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e32c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e32cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e32d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e32d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e32dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e32df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e32e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e32e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e32ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e32f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e32f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e32f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e32fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e3302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e330710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e330b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e330ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e331460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e3318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e331d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e3321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e332620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e332a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e332f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e333370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e3337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e333c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e3340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e334530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e3349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e334e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e335280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e3356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e335b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e335fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e336440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e3368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e336d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e337190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e337600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e337a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e337ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e338350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e3387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e338c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e3390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e339510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e339980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e339df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e33a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e33a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e33ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e33afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e33b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e33b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e33bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e33c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e33c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e33ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e33cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e33d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e33d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e33dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e33e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e33e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e33e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e33edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e33f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e33f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e33fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e33ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e340400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e340870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e340ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e341150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e3415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e341a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e341ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e342310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e342780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e342bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e343060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e3434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e343940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e343db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e344930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e344bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e344eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e345320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e345790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e346070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e3464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e346950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e346dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e347230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e3476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e347b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e347f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e3483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e348860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e348cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e349140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e3495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e349a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e349e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e34a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e34a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e34abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e34b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e34b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e34b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e34bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e34c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e34c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e34caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e34cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e34d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e34d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e34dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e34e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e34e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e34ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e34ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e34f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e34f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e34fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e350030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e3504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e350910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e350d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e3511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e351660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e351ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e351f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e3523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e352820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e352c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e353100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e353570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e3539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e353e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e3542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e354730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e354ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e355010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e355480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e3558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e355d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e3561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e356640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e356ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e356f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e357390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e357800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e357c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e3580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e358550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e358fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e3596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e359e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e35a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e35a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e35ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e35b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e35b860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.130s
user	0m0.286s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4431 (dc7cef9f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126f0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126f0d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126f0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126f0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126f0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126f0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126f0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126f0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126f10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126f10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126f10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126f11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126f11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126f122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126f12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126f13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126f13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126f14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126f14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126f14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126f15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126f15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126f164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126f16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126f17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126f17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126f17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126f189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126f18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126f191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126f19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126f19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126f1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126f1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126f1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126f1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126f1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126f1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126f1c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126f1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126f1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126f1cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126f1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126f1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126f1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126f1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126f1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126f1f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126f1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126f20380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126f20990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126f20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126f21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126f21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126f220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126f22390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126f229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126f23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126f23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126f238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126f23d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126f24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126f246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126f24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126f25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126f254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126f25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126f26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126f26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126f26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126f27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126f27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126f27bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126f28110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126f28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126f28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126f29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126f29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126f29ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126f2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126f2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126f2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126f2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126f2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126f2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126f2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126f2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126f2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126f2d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126f2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126f2db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126f2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126f2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126f2eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126f1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126f2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126f2f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126f2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126f30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126f30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126f30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126f31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126f31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126f31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126f321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126f32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126f32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126f331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126f33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126f34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126f345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126f34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126f34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126f353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126f35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126f35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126f36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126f36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126f36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126f36f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126f37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126f378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126f37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126f381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126f38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126f38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126f38fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126f39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126f39900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126f39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126f3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126f3a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126f3ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126f3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126f3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126f3b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126f3be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126f3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126f3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126f3cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126f3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126f3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126f3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126f3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126f3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126f3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126f3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126f3f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126f3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126f3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126f3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126f40360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126f40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126f40ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126f41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126f415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126f41a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126f41f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126f423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126f42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126f42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126f431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126f43640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126f43ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126f43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126f44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126f448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126f44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126f45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126f456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126f45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126f45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126f46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126f46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126f46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126f47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126f47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126f47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126f48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126f484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126f48980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126f48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126f492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126f49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126f49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126f4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126f4a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126f4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126f4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126f4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126f4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126f4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126f4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126f4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126f4cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126f4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126f4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126f4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126f4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126f4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126f4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126f4f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126f4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126f500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126f50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126f509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126f511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126f516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126f51c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126f52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126f526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126f52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126f53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126f536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126f53c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126f54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126f546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126f54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126f55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126f556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126f55c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126f56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126f566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126f56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126f57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126f57690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126f57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126f58130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126f58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126f58bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126f59120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126f59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126f59bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126f5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126f5a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126f5abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126f5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126f5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126f5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126f5c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126f5c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126f5cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126f5d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126f5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126f5db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126f5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126f5e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126f5eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126f5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126f5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126f5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126f600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126f60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126f60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126f610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126f615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126f61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126f62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126f625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126f62b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126f63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126f635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126f63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126f63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126f64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126f64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126f64da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126f65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126f656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126f65b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126f66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126f664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126f66960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126f66e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126f672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126f67740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126f67be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126f68080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126f685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126f68cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126f69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126f69b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126f6a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126f6a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126f6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126f6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126f6b5d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124a04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124a05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124a054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124a05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124a05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124a06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124a06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124a06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124a06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124a073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124a07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124a07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124a08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124a091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124a09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124a0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124a0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124a0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124a0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124a0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124a0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124a0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124a0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124a0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124a0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124a0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124a0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124a0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124a0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124a0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124a0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124a0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124a10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124a104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124a10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124a10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124a11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124a116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124a11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124a11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124a12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124a12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124a12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124a13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124a135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124a13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124a13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124a14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124a14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124a14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124a15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124a154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124a15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124a15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124a16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124a16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124a16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124a17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124a17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124a179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124a17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124a182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124a18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124a18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124a19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124a19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124a198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124a19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124a1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124a1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124a1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124a1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124a1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124a1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124a1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124a1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124a1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124a1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124a1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124a1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124a1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124a1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124a1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124a1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124a1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124a1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124a1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124a1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124a1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124a1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124a20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124a207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124a20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124a210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124a21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124a219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124a21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124a22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124a226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124a22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124a22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124a23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124a238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124a23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124a24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124a24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124a24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124a24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124a25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124a257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124a25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124a260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124a26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124a26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124a26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124a27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124a276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124a27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124a27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124a28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124a28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124a28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124a29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124a295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124a29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124a29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124a2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124a2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124a2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124a2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124a2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124a2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124a2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124a2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124a2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124a2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124a2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124a2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124a2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124a2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124a2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124a2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124a2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124a2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124a2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124a2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124a2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124a30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124a304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124a30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124a30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124a31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124a31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124a31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124a31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124a323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124a32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124a32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124a33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124a335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124a33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124a33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124a342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124a34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124a34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124a35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124a35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124a35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124a361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124a36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124a36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124a36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124a373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124a37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124a37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124a38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124a38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124a389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124a38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124a392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124a39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124a39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124a3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124a3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124a3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124a3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124a3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124a3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124a3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124a3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124a3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124a3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124a3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124a3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124a3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124a3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124a3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124a3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124a3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124a3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124a3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124a3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124a3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124a3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124a40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124a407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124a40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124a41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124a415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124a41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124a42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124a428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124a42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124a43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124a43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124a43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124a445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124a44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124a45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124a456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124a45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124a46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124a46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124a46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124a473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124a47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124a47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124a484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124a48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124a49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124a49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124a49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124a4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124a4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124a4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124a4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124a4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124a4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124a4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124a4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124a4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124a4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124a4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124a4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124a4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124a4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124a4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124a4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124a4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124a50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124a50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124a50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124a514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124a51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124a52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124a525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124a52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124a53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124a53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124a53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124a542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124a54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124a54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124a553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124a559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124a55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124a56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124a56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124a56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124a574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124a579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124a57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124a583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124a588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124a58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124a592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124a597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124a59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124a5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124a5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124a5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124a5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124a5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124a5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124a5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124a5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124a5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124a5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124a5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124a5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124a5e8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e08910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e08d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e09660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e09ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e09f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e0ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e0b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e0b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e0bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e0c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e0d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e0e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e10250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e11ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e15120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e16a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e16f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e1a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e1b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e1bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e1d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e1e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e23940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e23e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e24930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e24e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e25920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e26910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e28e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e29e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e2a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e2ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e2b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e2b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e2ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e2dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e2e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e2e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e2ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e2f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e30190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e31410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e32690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e32b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e33db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e34b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e35970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e35e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e36750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e36bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e37090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e37e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e39a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e39ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e3a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e3b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e3b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e3bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e3c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e3cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e3d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e3d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e3e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e3fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e40dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e41710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e42990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e42e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e43c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e44550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e44ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e45a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e46970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e47c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e48af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e4a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e4adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e4b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e4b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e4bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e4cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e4d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e4dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e4e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e4f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e4fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e50810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e51800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e51d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e53290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e56260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e56d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e57250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e57cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e58790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e59780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e59cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e5b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e5b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e5cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e5d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e5dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e5edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e5f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e5f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e5fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e60030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e60970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e60e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e61750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e61ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e62ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e63200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e63920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e63be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e64690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e64ca0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.924s
user	0m0.243s
sys	0m0.137s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.14 user         0.04 sys
```
