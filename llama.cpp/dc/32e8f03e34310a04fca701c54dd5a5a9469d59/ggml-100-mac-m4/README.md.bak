### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.48 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  176.72 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.71 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 219.11 sec*proc (28 tests)

Total Test time (real) = 219.12 sec

real	3m39.146s
user	7m31.297s
sys	0m6.297s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.24 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.39 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.30 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.10 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.38 sec*proc (28 tests)

Total Test time (real) =  51.39 sec

real	0m51.402s
user	1m12.063s
sys	0m5.508s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.077 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.825 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.516 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.529 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.530 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.530 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.531 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.532 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.533 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.533 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.534 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.534 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.538 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.539 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.539 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.540 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.540 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.541 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.541 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.023.671 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.672 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.023.672 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.023.673 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.023.673 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.023.673 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.023.674 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.023.674 I llama_model_loader: - type  f32:  124 tensors
0.00.023.674 I llama_model_loader: - type  f16:   73 tensors
0.00.025.895 I llm_load_vocab: special tokens cache size = 5
0.00.027.006 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.027.010 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.027.010 I llm_load_print_meta: arch             = bert
0.00.027.011 I llm_load_print_meta: vocab type       = WPM
0.00.027.011 I llm_load_print_meta: n_vocab          = 30522
0.00.027.013 I llm_load_print_meta: n_merges         = 0
0.00.027.013 I llm_load_print_meta: vocab_only       = 0
0.00.027.013 I llm_load_print_meta: n_ctx_train      = 512
0.00.027.013 I llm_load_print_meta: n_embd           = 384
0.00.027.013 I llm_load_print_meta: n_layer          = 12
0.00.027.016 I llm_load_print_meta: n_head           = 12
0.00.027.017 I llm_load_print_meta: n_head_kv        = 12
0.00.027.017 I llm_load_print_meta: n_rot            = 32
0.00.027.017 I llm_load_print_meta: n_swa            = 0
0.00.027.018 I llm_load_print_meta: n_embd_head_k    = 32
0.00.027.018 I llm_load_print_meta: n_embd_head_v    = 32
0.00.027.018 I llm_load_print_meta: n_gqa            = 1
0.00.027.019 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.027.020 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.027.023 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.027.023 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.027.024 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.027.024 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.027.024 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.027.025 I llm_load_print_meta: n_ff             = 1536
0.00.027.025 I llm_load_print_meta: n_expert         = 0
0.00.027.025 I llm_load_print_meta: n_expert_used    = 0
0.00.027.025 I llm_load_print_meta: causal attn      = 0
0.00.027.025 I llm_load_print_meta: pooling type     = 2
0.00.027.026 I llm_load_print_meta: rope type        = 2
0.00.027.026 I llm_load_print_meta: rope scaling     = linear
0.00.027.026 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.027.027 I llm_load_print_meta: freq_scale_train = 1
0.00.027.028 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.027.029 I llm_load_print_meta: rope_finetuned   = unknown
0.00.027.029 I llm_load_print_meta: ssm_d_conv       = 0
0.00.027.029 I llm_load_print_meta: ssm_d_inner      = 0
0.00.027.029 I llm_load_print_meta: ssm_d_state      = 0
0.00.027.029 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.027.029 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.027.030 I llm_load_print_meta: model type       = 33M
0.00.027.030 I llm_load_print_meta: model ftype      = F16
0.00.027.030 I llm_load_print_meta: model params     = 33.21 M
0.00.027.031 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.027.031 I llm_load_print_meta: general.name     = Bge Small
0.00.027.032 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.027.032 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.027.032 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.027.032 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.027.032 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.027.033 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.027.033 I llm_load_print_meta: max token length = 21
0.00.028.333 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.028.333 I llm_load_tensors: offloading output layer to GPU
0.00.028.333 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.028.356 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.028.357 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.028.722 I llama_new_context_with_model: n_seq_max     = 1
0.00.028.723 I llama_new_context_with_model: n_ctx         = 512
0.00.028.723 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.028.723 I llama_new_context_with_model: n_batch       = 2048
0.00.028.723 I llama_new_context_with_model: n_ubatch      = 2048
0.00.028.723 I llama_new_context_with_model: flash_attn    = 0
0.00.028.724 I llama_new_context_with_model: freq_base     = 10000.0
0.00.028.724 I llama_new_context_with_model: freq_scale    = 1
0.00.028.725 I ggml_metal_init: allocating
0.00.028.733 I ggml_metal_init: found device: Apple M4
0.00.028.736 I ggml_metal_init: picking default device: Apple M4
0.00.029.359 I ggml_metal_init: using embedded metal library
0.00.031.840 I ggml_metal_init: GPU name:   Apple M4
0.00.031.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.031.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.031.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.031.843 I ggml_metal_init: simdgroup reduction   = true
0.00.031.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.031.843 I ggml_metal_init: has bfloat            = true
0.00.031.843 I ggml_metal_init: use bfloat            = true
0.00.031.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.031.845 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.041.954 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.042.436 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.042.438 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.042.440 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.043.025 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.043.026 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.043.027 I llama_new_context_with_model: graph nodes  = 429
0.00.043.027 I llama_new_context_with_model: graph splits = 2
0.00.043.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.043.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.047.989 I 
0.00.048.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.048.572 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.052.965 I llama_perf_context_print:        load time =      31.16 ms
0.00.052.966 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2108.22 tokens per second)
0.00.052.967 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.052.967 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.053.174 I ggml_metal_free: deallocating

real	0m0.224s
user	0m0.036s
sys	0m0.024s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.429 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.434 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.434 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.435 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.436 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.438 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.439 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.439 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.443 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.444 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.446 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.447 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.447 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.447 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.448 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.448 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.448 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.012.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.036 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.037 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.037 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.038 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.038 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.038 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.039 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.039 I llama_model_loader: - type  f32:  124 tensors
0.00.013.039 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.224 I llm_load_vocab: special tokens cache size = 5
0.00.016.368 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.016.371 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.016.371 I llm_load_print_meta: arch             = bert
0.00.016.372 I llm_load_print_meta: vocab type       = WPM
0.00.016.372 I llm_load_print_meta: n_vocab          = 30522
0.00.016.372 I llm_load_print_meta: n_merges         = 0
0.00.016.372 I llm_load_print_meta: vocab_only       = 0
0.00.016.373 I llm_load_print_meta: n_ctx_train      = 512
0.00.016.373 I llm_load_print_meta: n_embd           = 384
0.00.016.373 I llm_load_print_meta: n_layer          = 12
0.00.016.376 I llm_load_print_meta: n_head           = 12
0.00.016.376 I llm_load_print_meta: n_head_kv        = 12
0.00.016.377 I llm_load_print_meta: n_rot            = 32
0.00.016.377 I llm_load_print_meta: n_swa            = 0
0.00.016.377 I llm_load_print_meta: n_embd_head_k    = 32
0.00.016.377 I llm_load_print_meta: n_embd_head_v    = 32
0.00.016.378 I llm_load_print_meta: n_gqa            = 1
0.00.016.378 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.016.379 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.016.379 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.016.380 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.016.380 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.016.380 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.016.380 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.016.383 I llm_load_print_meta: n_ff             = 1536
0.00.016.383 I llm_load_print_meta: n_expert         = 0
0.00.016.383 I llm_load_print_meta: n_expert_used    = 0
0.00.016.383 I llm_load_print_meta: causal attn      = 0
0.00.016.383 I llm_load_print_meta: pooling type     = 2
0.00.016.384 I llm_load_print_meta: rope type        = 2
0.00.016.384 I llm_load_print_meta: rope scaling     = linear
0.00.016.386 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.016.386 I llm_load_print_meta: freq_scale_train = 1
0.00.016.386 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.016.386 I llm_load_print_meta: rope_finetuned   = unknown
0.00.016.387 I llm_load_print_meta: ssm_d_conv       = 0
0.00.016.387 I llm_load_print_meta: ssm_d_inner      = 0
0.00.016.387 I llm_load_print_meta: ssm_d_state      = 0
0.00.016.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.016.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.016.387 I llm_load_print_meta: model type       = 33M
0.00.016.388 I llm_load_print_meta: model ftype      = Q8_0
0.00.016.388 I llm_load_print_meta: model params     = 33.21 M
0.00.016.389 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.016.389 I llm_load_print_meta: general.name     = Bge Small
0.00.016.389 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.016.391 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.016.391 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.016.391 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.016.391 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.016.391 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.016.392 I llm_load_print_meta: max token length = 21
0.00.017.739 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.017.739 I llm_load_tensors: offloading output layer to GPU
0.00.017.739 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.017.748 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.017.749 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.114 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.115 I llama_new_context_with_model: n_ctx         = 512
0.00.018.115 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.116 I llama_new_context_with_model: n_batch       = 2048
0.00.018.116 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.116 I llama_new_context_with_model: flash_attn    = 0
0.00.018.116 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.116 I llama_new_context_with_model: freq_scale    = 1
0.00.018.117 I ggml_metal_init: allocating
0.00.018.120 I ggml_metal_init: found device: Apple M4
0.00.018.122 I ggml_metal_init: picking default device: Apple M4
0.00.018.767 I ggml_metal_init: using embedded metal library
0.00.021.352 I ggml_metal_init: GPU name:   Apple M4
0.00.021.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.357 I ggml_metal_init: simdgroup reduction   = true
0.00.021.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.357 I ggml_metal_init: has bfloat            = true
0.00.021.357 I ggml_metal_init: use bfloat            = true
0.00.021.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.030.818 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.031.364 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.366 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.368 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.051 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.052 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.052 I llama_new_context_with_model: graph nodes  = 429
0.00.032.052 I llama_new_context_with_model: graph splits = 2
0.00.032.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.032.054 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.632 I 
0.00.037.659 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.290 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.999 I llama_perf_context_print:        load time =      28.99 ms
0.00.043.002 I llama_perf_context_print: prompt eval time =       4.58 ms /     9 tokens (    0.51 ms per token,  1967.21 tokens per second)
0.00.043.003 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.004 I llama_perf_context_print:       total time =       5.37 ms /    10 tokens
0.00.043.267 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.028s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.232 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.907 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.313 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.320 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.322 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.323 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.323 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.324 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.325 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.325 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.326 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.326 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.330 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.330 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.331 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.767 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.769 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.769 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.770 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.770 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.770 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.771 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.771 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.771 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.772 I llama_model_loader: - type  f32:   40 tensors
0.00.048.772 I llama_model_loader: - type  f16:   30 tensors
0.00.066.514 W llm_load_vocab: empty token at index 5
0.00.071.030 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.355 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.384 I llm_load_vocab: special tokens cache size = 5
0.00.331.900 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.905 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.905 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.906 I llm_load_print_meta: vocab type       = BPE
0.00.331.906 I llm_load_print_meta: n_vocab          = 61056
0.00.331.906 I llm_load_print_meta: n_merges         = 39382
0.00.331.906 I llm_load_print_meta: vocab_only       = 0
0.00.331.911 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.911 I llm_load_print_meta: n_embd           = 384
0.00.331.911 I llm_load_print_meta: n_layer          = 4
0.00.331.917 I llm_load_print_meta: n_head           = 12
0.00.331.917 I llm_load_print_meta: n_head_kv        = 12
0.00.331.917 I llm_load_print_meta: n_rot            = 32
0.00.331.918 I llm_load_print_meta: n_swa            = 0
0.00.331.918 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.919 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.919 I llm_load_print_meta: n_gqa            = 1
0.00.331.920 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.920 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.921 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.922 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.922 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.922 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.922 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.923 I llm_load_print_meta: n_ff             = 1536
0.00.331.923 I llm_load_print_meta: n_expert         = 0
0.00.331.923 I llm_load_print_meta: n_expert_used    = 0
0.00.331.923 I llm_load_print_meta: causal attn      = 0
0.00.331.924 I llm_load_print_meta: pooling type     = -1
0.00.331.924 I llm_load_print_meta: rope type        = -1
0.00.331.924 I llm_load_print_meta: rope scaling     = linear
0.00.331.925 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.925 I llm_load_print_meta: freq_scale_train = 1
0.00.331.925 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.925 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.926 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.926 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.926 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.926 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.926 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.926 I llm_load_print_meta: model type       = 33M
0.00.331.927 I llm_load_print_meta: model ftype      = F16
0.00.331.927 I llm_load_print_meta: model params     = 32.90 M
0.00.331.928 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.928 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.928 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.928 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.929 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.929 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.929 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.929 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.929 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.930 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.930 I llm_load_print_meta: max token length = 45
0.00.332.959 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.959 I llm_load_tensors: offloading output layer to GPU
0.00.332.961 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.986 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.987 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.906 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.907 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.907 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.908 I llama_new_context_with_model: n_batch       = 2048
0.00.333.908 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.908 I llama_new_context_with_model: flash_attn    = 0
0.00.333.908 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.909 I llama_new_context_with_model: freq_scale    = 1
0.00.333.909 I ggml_metal_init: allocating
0.00.333.916 I ggml_metal_init: found device: Apple M4
0.00.333.919 I ggml_metal_init: picking default device: Apple M4
0.00.334.703 I ggml_metal_init: using embedded metal library
0.00.337.573 I ggml_metal_init: GPU name:   Apple M4
0.00.337.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.576 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.577 I ggml_metal_init: simdgroup reduction   = true
0.00.337.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.577 I ggml_metal_init: has bfloat            = true
0.00.337.577 I ggml_metal_init: use bfloat            = true
0.00.337.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.099 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.349.637 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.641 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.642 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.220 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.221 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.221 I llama_new_context_with_model: graph nodes  = 154
0.00.350.221 I llama_new_context_with_model: graph splits = 2
0.00.350.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.759 I 
0.00.362.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.039 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.363.040 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.363.044 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.363.044 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.363.051 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.363.051 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.363.539 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.367.287 I llama_perf_context_print:        load time =     338.85 ms
0.00.367.288 I llama_perf_context_print: prompt eval time =       3.73 ms /    62 tokens (    0.06 ms per token, 16599.73 tokens per second)
0.00.367.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.367.291 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.367.534 I ggml_metal_free: deallocating

real	0m1.077s
user	0m0.338s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.259 I main: llama backend init
0.00.000.266 I main: load the model and apply lora adapter, if any
0.00.030.700 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.527 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.546 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.547 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.060.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.460 I llama_model_loader: - type  f32:  194 tensors
0.00.060.461 I llama_model_loader: - type  f16:   98 tensors
0.00.089.162 I llm_load_vocab: special tokens cache size = 25
0.00.095.683 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.686 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.686 I llm_load_print_meta: arch             = gptneox
0.00.095.686 I llm_load_print_meta: vocab type       = BPE
0.00.095.687 I llm_load_print_meta: n_vocab          = 50304
0.00.095.687 I llm_load_print_meta: n_merges         = 50009
0.00.095.687 I llm_load_print_meta: vocab_only       = 0
0.00.095.687 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.687 I llm_load_print_meta: n_embd           = 2048
0.00.095.687 I llm_load_print_meta: n_layer          = 24
0.00.095.690 I llm_load_print_meta: n_head           = 16
0.00.095.690 I llm_load_print_meta: n_head_kv        = 16
0.00.095.692 I llm_load_print_meta: n_rot            = 32
0.00.095.692 I llm_load_print_meta: n_swa            = 0
0.00.095.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.692 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.693 I llm_load_print_meta: n_gqa            = 1
0.00.095.694 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.694 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.695 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.695 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.695 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.695 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.695 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.696 I llm_load_print_meta: n_ff             = 8192
0.00.095.696 I llm_load_print_meta: n_expert         = 0
0.00.095.696 I llm_load_print_meta: n_expert_used    = 0
0.00.095.696 I llm_load_print_meta: causal attn      = 1
0.00.095.696 I llm_load_print_meta: pooling type     = 0
0.00.095.697 I llm_load_print_meta: rope type        = 2
0.00.095.697 I llm_load_print_meta: rope scaling     = linear
0.00.095.698 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.698 I llm_load_print_meta: freq_scale_train = 1
0.00.095.698 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.699 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.699 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.699 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.699 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.699 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.699 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.699 I llm_load_print_meta: model type       = 1.4B
0.00.095.700 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.700 I llm_load_print_meta: model params     = 1.41 B
0.00.095.701 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.701 I llm_load_print_meta: general.name     = 1.4B
0.00.095.701 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.701 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.702 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.702 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.702 I llm_load_print_meta: LF token         = 128 ''
0.00.095.702 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.702 I llm_load_print_meta: max token length = 1024
0.00.097.694 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.694 I llm_load_tensors: offloading output layer to GPU
0.00.097.694 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.712 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.713 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.599 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.600 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.600 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.600 I llama_new_context_with_model: n_batch       = 2048
0.00.098.600 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.600 I llama_new_context_with_model: flash_attn    = 0
0.00.098.601 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.601 I llama_new_context_with_model: freq_scale    = 1
0.00.098.602 I ggml_metal_init: allocating
0.00.098.610 I ggml_metal_init: found device: Apple M4
0.00.098.612 I ggml_metal_init: picking default device: Apple M4
0.00.099.291 I ggml_metal_init: using embedded metal library
0.00.109.404 I ggml_metal_init: GPU name:   Apple M4
0.00.109.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.407 I ggml_metal_init: simdgroup reduction   = true
0.00.109.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.407 I ggml_metal_init: has bfloat            = true
0.00.109.408 I ggml_metal_init: use bfloat            = true
0.00.109.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.132.719 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.151.824 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.151.832 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.151.861 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.152.876 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.152.879 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.152.879 I llama_new_context_with_model: graph nodes  = 967
0.00.152.879 I llama_new_context_with_model: graph splits = 2
0.00.152.882 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.153.018 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.153.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.228.275 I main: llama threadpool init, n_threads = 4
0.00.228.322 I 
0.00.228.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.228.344 I 
0.00.228.422 I sampler seed: 1234
0.00.228.427 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.228.450 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.228.453 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.228.453 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.029.325 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.02.029.326 I llama_perf_context_print:        load time =     197.56 ms
0.02.029.327 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.50 tokens per second)
0.02.029.328 I llama_perf_context_print:        eval time =    1754.53 ms /    63 runs   (   27.85 ms per token,    35.91 tokens per second)
0.02.029.328 I llama_perf_context_print:       total time =    1801.06 ms /    70 tokens
0.02.029.606 I ggml_metal_free: deallocating

real	0m2.324s
user	0m0.145s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.459 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.598 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.602 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.606 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.606 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.611 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.612 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.715 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.716 I llama_model_loader: - type  f32:  194 tensors
0.00.047.716 I llama_model_loader: - type  f16:   98 tensors
0.00.075.428 I llm_load_vocab: special tokens cache size = 25
0.00.081.764 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.767 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.767 I llm_load_print_meta: arch             = gptneox
0.00.081.768 I llm_load_print_meta: vocab type       = BPE
0.00.081.768 I llm_load_print_meta: n_vocab          = 50304
0.00.081.768 I llm_load_print_meta: n_merges         = 50009
0.00.081.768 I llm_load_print_meta: vocab_only       = 0
0.00.081.768 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.769 I llm_load_print_meta: n_embd           = 2048
0.00.081.769 I llm_load_print_meta: n_layer          = 24
0.00.081.771 I llm_load_print_meta: n_head           = 16
0.00.081.772 I llm_load_print_meta: n_head_kv        = 16
0.00.081.772 I llm_load_print_meta: n_rot            = 32
0.00.081.772 I llm_load_print_meta: n_swa            = 0
0.00.081.772 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.773 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.773 I llm_load_print_meta: n_gqa            = 1
0.00.081.774 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.775 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.775 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.775 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.776 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.776 I llm_load_print_meta: n_ff             = 8192
0.00.081.777 I llm_load_print_meta: n_expert         = 0
0.00.081.777 I llm_load_print_meta: n_expert_used    = 0
0.00.081.777 I llm_load_print_meta: causal attn      = 1
0.00.081.777 I llm_load_print_meta: pooling type     = 0
0.00.081.777 I llm_load_print_meta: rope type        = 2
0.00.081.777 I llm_load_print_meta: rope scaling     = linear
0.00.081.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.778 I llm_load_print_meta: freq_scale_train = 1
0.00.081.778 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.778 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.779 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.779 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.781 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.781 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.781 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.781 I llm_load_print_meta: model type       = 1.4B
0.00.081.782 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.081.782 I llm_load_print_meta: model params     = 1.41 B
0.00.081.782 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.081.783 I llm_load_print_meta: general.name     = 1.4B
0.00.081.783 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.783 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.783 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.783 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.783 I llm_load_print_meta: LF token         = 128 ''
0.00.081.784 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.784 I llm_load_print_meta: max token length = 1024
0.00.083.723 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.723 I llm_load_tensors: offloading output layer to GPU
0.00.083.724 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.734 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.083.735 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.084.598 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.599 I llama_new_context_with_model: n_ctx         = 128
0.00.084.599 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.084.599 I llama_new_context_with_model: n_batch       = 128
0.00.084.599 I llama_new_context_with_model: n_ubatch      = 128
0.00.084.599 I llama_new_context_with_model: flash_attn    = 0
0.00.084.600 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.600 I llama_new_context_with_model: freq_scale    = 1
0.00.084.601 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.601 I ggml_metal_init: allocating
0.00.084.608 I ggml_metal_init: found device: Apple M4
0.00.084.610 I ggml_metal_init: picking default device: Apple M4
0.00.085.190 I ggml_metal_init: using embedded metal library
0.00.087.745 I ggml_metal_init: GPU name:   Apple M4
0.00.087.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.748 I ggml_metal_init: simdgroup reduction   = true
0.00.087.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.748 I ggml_metal_init: has bfloat            = true
0.00.087.748 I ggml_metal_init: use bfloat            = true
0.00.087.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.749 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.466 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.866 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.098.868 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.098.882 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.785 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.099.786 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.099.786 I llama_new_context_with_model: graph nodes  = 967
0.00.099.786 I llama_new_context_with_model: graph splits = 2
0.00.099.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.099.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.391.575 I 
0.01.391.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.391.636 I perplexity: tokenizing the input ..
0.01.401.927 I perplexity: tokenization took 10.289 ms
0.01.401.933 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.522.657 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.524.314 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.524.334 I llama_perf_context_print:        load time =    1371.88 ms
0.01.524.336 I llama_perf_context_print: prompt eval time =     120.31 ms /   128 tokens (    0.94 ms per token,  1063.94 tokens per second)
0.01.524.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.524.337 I llama_perf_context_print:       total time =     132.76 ms /   129 tokens
0.01.524.928 I ggml_metal_free: deallocating

real	0m1.724s
user	0m0.120s
sys	0m0.306s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.015 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.023 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.029 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.036 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.036 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.037 I llama_model_loader: - type  f32:  194 tensors
0.00.035.037 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.519 I llm_load_vocab: special tokens cache size = 25
0.00.064.539 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.543 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.544 I llm_load_print_meta: arch             = gptneox
0.00.064.544 I llm_load_print_meta: vocab type       = BPE
0.00.064.544 I llm_load_print_meta: n_vocab          = 50304
0.00.064.544 I llm_load_print_meta: n_merges         = 50009
0.00.064.545 I llm_load_print_meta: vocab_only       = 0
0.00.064.545 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.545 I llm_load_print_meta: n_embd           = 2048
0.00.064.545 I llm_load_print_meta: n_layer          = 24
0.00.064.550 I llm_load_print_meta: n_head           = 16
0.00.064.551 I llm_load_print_meta: n_head_kv        = 16
0.00.064.551 I llm_load_print_meta: n_rot            = 32
0.00.064.552 I llm_load_print_meta: n_swa            = 0
0.00.064.552 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.552 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.553 I llm_load_print_meta: n_gqa            = 1
0.00.064.553 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.554 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.554 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.558 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.558 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.558 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.558 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.559 I llm_load_print_meta: n_ff             = 8192
0.00.064.559 I llm_load_print_meta: n_expert         = 0
0.00.064.559 I llm_load_print_meta: n_expert_used    = 0
0.00.064.559 I llm_load_print_meta: causal attn      = 1
0.00.064.560 I llm_load_print_meta: pooling type     = 0
0.00.064.560 I llm_load_print_meta: rope type        = 2
0.00.064.561 I llm_load_print_meta: rope scaling     = linear
0.00.064.562 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.562 I llm_load_print_meta: freq_scale_train = 1
0.00.064.562 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.562 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.562 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.563 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.563 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.563 I llm_load_print_meta: model type       = 1.4B
0.00.064.564 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.564 I llm_load_print_meta: model params     = 1.41 B
0.00.064.564 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.564 I llm_load_print_meta: general.name     = 1.4B
0.00.064.565 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.568 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.568 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.568 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.569 I llm_load_print_meta: LF token         = 128 ''
0.00.064.569 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.569 I llm_load_print_meta: max token length = 1024
0.00.066.726 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.726 I llm_load_tensors: offloading output layer to GPU
0.00.066.727 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.737 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.738 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.691 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.692 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.692 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.692 I llama_new_context_with_model: n_batch       = 2048
0.00.067.693 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.693 I llama_new_context_with_model: flash_attn    = 0
0.00.067.693 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.693 I llama_new_context_with_model: freq_scale    = 1
0.00.067.694 I ggml_metal_init: allocating
0.00.067.702 I ggml_metal_init: found device: Apple M4
0.00.067.705 I ggml_metal_init: picking default device: Apple M4
0.00.068.498 I ggml_metal_init: using embedded metal library
0.00.071.126 I ggml_metal_init: GPU name:   Apple M4
0.00.071.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.129 I ggml_metal_init: simdgroup reduction   = true
0.00.071.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.130 I ggml_metal_init: has bfloat            = true
0.00.071.130 I ggml_metal_init: use bfloat            = true
0.00.071.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.131 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.913 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.043 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.067 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.279 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.281 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.282 I llama_new_context_with_model: graph nodes  = 967
0.00.109.282 I llama_new_context_with_model: graph splits = 2
0.00.109.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.395.838 I main: llama threadpool init, n_threads = 4
0.01.395.872 I 
0.01.395.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.395.899 I 
0.01.396.062 I sampler seed: 1234
0.01.396.067 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.396.085 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.396.085 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.396.085 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.480.304 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62390.16 tokens per second)
0.02.480.305 I llama_perf_context_print:        load time =    1386.40 ms
0.02.480.306 I llama_perf_context_print: prompt eval time =      40.19 ms /     7 tokens (    5.74 ms per token,   174.19 tokens per second)
0.02.480.306 I llama_perf_context_print:        eval time =    1041.20 ms /    63 runs   (   16.53 ms per token,    60.51 tokens per second)
0.02.480.307 I llama_perf_context_print:       total time =    1084.47 ms /    70 tokens
0.02.480.547 I ggml_metal_free: deallocating

real	0m2.498s
user	0m0.116s
sys	0m0.275s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.759 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.932 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.636 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.637 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.638 I llama_model_loader: - type  f32:  194 tensors
0.00.034.638 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.476 I llm_load_vocab: special tokens cache size = 25
0.00.066.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.443 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.444 I llm_load_print_meta: arch             = gptneox
0.00.066.444 I llm_load_print_meta: vocab type       = BPE
0.00.066.444 I llm_load_print_meta: n_vocab          = 50304
0.00.066.445 I llm_load_print_meta: n_merges         = 50009
0.00.066.445 I llm_load_print_meta: vocab_only       = 0
0.00.066.445 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.445 I llm_load_print_meta: n_embd           = 2048
0.00.066.445 I llm_load_print_meta: n_layer          = 24
0.00.066.450 I llm_load_print_meta: n_head           = 16
0.00.066.451 I llm_load_print_meta: n_head_kv        = 16
0.00.066.451 I llm_load_print_meta: n_rot            = 32
0.00.066.451 I llm_load_print_meta: n_swa            = 0
0.00.066.451 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.454 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.455 I llm_load_print_meta: n_gqa            = 1
0.00.066.456 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.456 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.457 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.459 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.461 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.461 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.461 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.462 I llm_load_print_meta: n_ff             = 8192
0.00.066.462 I llm_load_print_meta: n_expert         = 0
0.00.066.462 I llm_load_print_meta: n_expert_used    = 0
0.00.066.462 I llm_load_print_meta: causal attn      = 1
0.00.066.463 I llm_load_print_meta: pooling type     = 0
0.00.066.463 I llm_load_print_meta: rope type        = 2
0.00.066.463 I llm_load_print_meta: rope scaling     = linear
0.00.066.463 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.464 I llm_load_print_meta: freq_scale_train = 1
0.00.066.464 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.464 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.464 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.464 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.465 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.465 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.465 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.469 I llm_load_print_meta: model type       = 1.4B
0.00.066.469 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.470 I llm_load_print_meta: model params     = 1.41 B
0.00.066.470 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.470 I llm_load_print_meta: general.name     = 1.4B
0.00.066.471 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.471 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.471 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.471 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.471 I llm_load_print_meta: LF token         = 128 ''
0.00.066.473 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.473 I llm_load_print_meta: max token length = 1024
0.00.068.547 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.547 I llm_load_tensors: offloading output layer to GPU
0.00.068.547 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.558 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.559 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.444 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.445 I llama_new_context_with_model: n_ctx         = 128
0.00.069.445 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.446 I llama_new_context_with_model: n_batch       = 128
0.00.069.446 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.446 I llama_new_context_with_model: flash_attn    = 0
0.00.069.446 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.447 I llama_new_context_with_model: freq_scale    = 1
0.00.069.447 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.447 I ggml_metal_init: allocating
0.00.069.458 I ggml_metal_init: found device: Apple M4
0.00.069.479 I ggml_metal_init: picking default device: Apple M4
0.00.070.200 I ggml_metal_init: using embedded metal library
0.00.072.795 I ggml_metal_init: GPU name:   Apple M4
0.00.072.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.798 I ggml_metal_init: simdgroup reduction   = true
0.00.072.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.798 I ggml_metal_init: has bfloat            = true
0.00.072.798 I ggml_metal_init: use bfloat            = true
0.00.072.799 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.450 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.070 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.073 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.089 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.080 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.082 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.082 I llama_new_context_with_model: graph nodes  = 967
0.00.086.082 I llama_new_context_with_model: graph splits = 2
0.00.086.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.005.368 I 
0.01.005.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.005.435 I perplexity: tokenizing the input ..
0.01.012.998 I perplexity: tokenization took 7.561 ms
0.01.013.005 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.137.711 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.138.805 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.138.818 I llama_perf_context_print:        load time =     993.61 ms
0.01.138.819 I llama_perf_context_print: prompt eval time =     124.46 ms /   128 tokens (    0.97 ms per token,  1028.47 tokens per second)
0.01.138.820 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.138.820 I llama_perf_context_print:       total time =     133.45 ms /   129 tokens
0.01.139.226 I ggml_metal_free: deallocating

real	0m1.158s
user	0m0.096s
sys	0m0.199s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.020.837 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.740 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.432 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.899 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.900 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.901 I llama_model_loader: - type  f32:  194 tensors
0.00.046.902 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.902 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.609 I llm_load_vocab: special tokens cache size = 25
0.00.085.108 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.113 I llm_load_print_meta: arch             = gptneox
0.00.085.114 I llm_load_print_meta: vocab type       = BPE
0.00.085.114 I llm_load_print_meta: n_vocab          = 50304
0.00.085.114 I llm_load_print_meta: n_merges         = 50009
0.00.085.114 I llm_load_print_meta: vocab_only       = 0
0.00.085.115 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.115 I llm_load_print_meta: n_embd           = 2048
0.00.085.115 I llm_load_print_meta: n_layer          = 24
0.00.085.122 I llm_load_print_meta: n_head           = 16
0.00.085.123 I llm_load_print_meta: n_head_kv        = 16
0.00.085.125 I llm_load_print_meta: n_rot            = 32
0.00.085.125 I llm_load_print_meta: n_swa            = 0
0.00.085.126 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.126 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.127 I llm_load_print_meta: n_gqa            = 1
0.00.085.128 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.135 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.135 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.136 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.136 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.137 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.137 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.138 I llm_load_print_meta: n_ff             = 8192
0.00.085.146 I llm_load_print_meta: n_expert         = 0
0.00.085.146 I llm_load_print_meta: n_expert_used    = 0
0.00.085.146 I llm_load_print_meta: causal attn      = 1
0.00.085.146 I llm_load_print_meta: pooling type     = 0
0.00.085.147 I llm_load_print_meta: rope type        = 2
0.00.085.147 I llm_load_print_meta: rope scaling     = linear
0.00.085.147 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.148 I llm_load_print_meta: freq_scale_train = 1
0.00.085.150 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.150 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.151 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.151 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.151 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.151 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.151 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.152 I llm_load_print_meta: model type       = 1.4B
0.00.085.152 I llm_load_print_meta: model ftype      = Q4_0
0.00.085.153 I llm_load_print_meta: model params     = 1.41 B
0.00.085.154 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.085.154 I llm_load_print_meta: general.name     = 1.4B
0.00.085.154 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.155 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.155 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.155 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.156 I llm_load_print_meta: LF token         = 128 ''
0.00.085.156 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.156 I llm_load_print_meta: max token length = 1024
0.00.088.031 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.031 I llm_load_tensors: offloading output layer to GPU
0.00.088.032 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.044 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.045 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.089.479 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.485 I llama_new_context_with_model: n_ctx         = 2048
0.00.089.485 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.089.485 I llama_new_context_with_model: n_batch       = 2048
0.00.089.486 I llama_new_context_with_model: n_ubatch      = 512
0.00.089.486 I llama_new_context_with_model: flash_attn    = 0
0.00.089.486 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.487 I llama_new_context_with_model: freq_scale    = 1
0.00.089.487 I ggml_metal_init: allocating
0.00.089.492 I ggml_metal_init: found device: Apple M4
0.00.089.495 I ggml_metal_init: picking default device: Apple M4
0.00.090.486 I ggml_metal_init: using embedded metal library
0.00.094.581 I ggml_metal_init: GPU name:   Apple M4
0.00.094.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.585 I ggml_metal_init: simdgroup reduction   = true
0.00.094.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.585 I ggml_metal_init: has bfloat            = true
0.00.094.585 I ggml_metal_init: use bfloat            = true
0.00.094.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.128 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.131.412 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.420 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.443 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.132.554 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.132.556 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.132.556 I llama_new_context_with_model: graph nodes  = 967
0.00.132.557 I llama_new_context_with_model: graph splits = 2
0.00.132.561 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.132.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.132.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.475 I main: llama threadpool init, n_threads = 4
0.00.789.555 I 
0.00.789.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.598 I 
0.00.789.949 I sampler seed: 1234
0.00.789.958 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.976 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.979 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.979 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.431 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.01.483.432 I llama_perf_context_print:        load time =     768.63 ms
0.01.483.433 I llama_perf_context_print: prompt eval time =      50.48 ms /     7 tokens (    7.21 ms per token,   138.68 tokens per second)
0.01.483.434 I llama_perf_context_print:        eval time =     639.92 ms /    63 runs   (   10.16 ms per token,    98.45 tokens per second)
0.01.483.435 I llama_perf_context_print:       total time =     693.96 ms /    70 tokens
0.01.483.675 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.144s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.954 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.372 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.373 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.373 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.111 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.022.842 I llama_model_loader: - type  f32:  194 tensors
0.00.022.842 I llama_model_loader: - type q4_0:   97 tensors
0.00.022.842 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.428 I llm_load_vocab: special tokens cache size = 25
0.00.049.298 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.301 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.301 I llm_load_print_meta: arch             = gptneox
0.00.049.302 I llm_load_print_meta: vocab type       = BPE
0.00.049.302 I llm_load_print_meta: n_vocab          = 50304
0.00.049.302 I llm_load_print_meta: n_merges         = 50009
0.00.049.302 I llm_load_print_meta: vocab_only       = 0
0.00.049.302 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.303 I llm_load_print_meta: n_embd           = 2048
0.00.049.303 I llm_load_print_meta: n_layer          = 24
0.00.049.306 I llm_load_print_meta: n_head           = 16
0.00.049.306 I llm_load_print_meta: n_head_kv        = 16
0.00.049.307 I llm_load_print_meta: n_rot            = 32
0.00.049.307 I llm_load_print_meta: n_swa            = 0
0.00.049.307 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.307 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.308 I llm_load_print_meta: n_gqa            = 1
0.00.049.309 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.309 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.310 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.310 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.310 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.310 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.311 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.311 I llm_load_print_meta: n_ff             = 8192
0.00.049.312 I llm_load_print_meta: n_expert         = 0
0.00.049.312 I llm_load_print_meta: n_expert_used    = 0
0.00.049.312 I llm_load_print_meta: causal attn      = 1
0.00.049.312 I llm_load_print_meta: pooling type     = 0
0.00.049.312 I llm_load_print_meta: rope type        = 2
0.00.049.312 I llm_load_print_meta: rope scaling     = linear
0.00.049.313 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.313 I llm_load_print_meta: freq_scale_train = 1
0.00.049.313 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.313 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.314 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.314 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.317 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.317 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.317 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.317 I llm_load_print_meta: model type       = 1.4B
0.00.049.318 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.318 I llm_load_print_meta: model params     = 1.41 B
0.00.049.319 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.319 I llm_load_print_meta: general.name     = 1.4B
0.00.049.319 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.319 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.319 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.320 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.320 I llm_load_print_meta: LF token         = 128 ''
0.00.049.320 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.320 I llm_load_print_meta: max token length = 1024
0.00.051.008 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.008 I llm_load_tensors: offloading output layer to GPU
0.00.051.008 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.018 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.019 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.051.825 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.826 I llama_new_context_with_model: n_ctx         = 128
0.00.051.826 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.826 I llama_new_context_with_model: n_batch       = 128
0.00.051.827 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.827 I llama_new_context_with_model: flash_attn    = 0
0.00.051.827 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.828 I llama_new_context_with_model: freq_scale    = 1
0.00.051.828 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.828 I ggml_metal_init: allocating
0.00.051.835 I ggml_metal_init: found device: Apple M4
0.00.051.838 I ggml_metal_init: picking default device: Apple M4
0.00.052.403 I ggml_metal_init: using embedded metal library
0.00.054.720 I ggml_metal_init: GPU name:   Apple M4
0.00.054.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.723 I ggml_metal_init: simdgroup reduction   = true
0.00.054.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.723 I ggml_metal_init: has bfloat            = true
0.00.054.723 I ggml_metal_init: use bfloat            = true
0.00.054.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.077 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.351 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.353 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.366 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.221 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.222 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.222 I llama_new_context_with_model: graph nodes  = 967
0.00.066.222 I llama_new_context_with_model: graph splits = 2
0.00.066.223 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.224 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.678 I 
0.00.669.706 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.720 I perplexity: tokenizing the input ..
0.00.677.210 I perplexity: tokenization took 7.489 ms
0.00.677.217 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.338 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.801.436 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.801.449 I llama_perf_context_print:        load time =     660.72 ms
0.00.801.450 I llama_perf_context_print: prompt eval time =     122.90 ms /   128 tokens (    0.96 ms per token,  1041.49 tokens per second)
0.00.801.450 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.451 I llama_perf_context_print:       total time =     131.77 ms /   129 tokens
0.00.801.865 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.130s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.326 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.460 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.909 I llama_model_loader: - type  f32:  194 tensors
0.00.031.909 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.178 I llm_load_vocab: special tokens cache size = 25
0.00.058.913 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.917 I llm_load_print_meta: arch             = gptneox
0.00.058.917 I llm_load_print_meta: vocab type       = BPE
0.00.058.917 I llm_load_print_meta: n_vocab          = 50304
0.00.058.917 I llm_load_print_meta: n_merges         = 50009
0.00.058.918 I llm_load_print_meta: vocab_only       = 0
0.00.058.918 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.918 I llm_load_print_meta: n_embd           = 2048
0.00.058.918 I llm_load_print_meta: n_layer          = 24
0.00.058.921 I llm_load_print_meta: n_head           = 16
0.00.058.921 I llm_load_print_meta: n_head_kv        = 16
0.00.058.922 I llm_load_print_meta: n_rot            = 32
0.00.058.922 I llm_load_print_meta: n_swa            = 0
0.00.058.922 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.922 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.925 I llm_load_print_meta: n_gqa            = 1
0.00.058.926 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.926 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.927 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.927 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.927 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.927 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.928 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.928 I llm_load_print_meta: n_ff             = 8192
0.00.058.928 I llm_load_print_meta: n_expert         = 0
0.00.058.928 I llm_load_print_meta: n_expert_used    = 0
0.00.058.929 I llm_load_print_meta: causal attn      = 1
0.00.058.929 I llm_load_print_meta: pooling type     = 0
0.00.058.929 I llm_load_print_meta: rope type        = 2
0.00.058.929 I llm_load_print_meta: rope scaling     = linear
0.00.058.929 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.930 I llm_load_print_meta: freq_scale_train = 1
0.00.058.930 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.930 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.931 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.933 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.933 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.933 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.934 I llm_load_print_meta: model type       = 1.4B
0.00.058.934 I llm_load_print_meta: model ftype      = Q4_1
0.00.058.934 I llm_load_print_meta: model params     = 1.41 B
0.00.058.935 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.058.935 I llm_load_print_meta: general.name     = 1.4B
0.00.058.935 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.935 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.936 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.936 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.936 I llm_load_print_meta: LF token         = 128 ''
0.00.058.936 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.936 I llm_load_print_meta: max token length = 1024
0.00.060.647 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.648 I llm_load_tensors: offloading output layer to GPU
0.00.060.648 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.658 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.060.659 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.061.491 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.492 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.492 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.492 I llama_new_context_with_model: n_batch       = 2048
0.00.061.492 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.493 I llama_new_context_with_model: flash_attn    = 0
0.00.061.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.493 I llama_new_context_with_model: freq_scale    = 1
0.00.061.494 I ggml_metal_init: allocating
0.00.061.500 I ggml_metal_init: found device: Apple M4
0.00.061.502 I ggml_metal_init: picking default device: Apple M4
0.00.062.084 I ggml_metal_init: using embedded metal library
0.00.064.406 I ggml_metal_init: GPU name:   Apple M4
0.00.064.408 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.408 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.408 I ggml_metal_init: simdgroup reduction   = true
0.00.064.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.409 I ggml_metal_init: has bfloat            = true
0.00.064.409 I ggml_metal_init: use bfloat            = true
0.00.064.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.925 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.934 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.953 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.093 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.094 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.095 I llama_new_context_with_model: graph nodes  = 967
0.00.095.095 I llama_new_context_with_model: graph splits = 2
0.00.095.098 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.937.480 I main: llama threadpool init, n_threads = 4
0.00.937.517 I 
0.00.937.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.937.571 I 
0.00.937.728 I sampler seed: 1234
0.00.937.733 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.937.751 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.937.751 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.937.751 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.662.662 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 64840.18 tokens per second)
0.01.662.662 I llama_perf_context_print:        load time =     929.15 ms
0.01.662.663 I llama_perf_context_print: prompt eval time =      39.89 ms /     7 tokens (    5.70 ms per token,   175.47 tokens per second)
0.01.662.665 I llama_perf_context_print:        eval time =     682.14 ms /    63 runs   (   10.83 ms per token,    92.36 tokens per second)
0.01.662.665 I llama_perf_context_print:       total time =     725.18 ms /    70 tokens
0.01.662.862 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.908 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.387 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.393 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.394 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.397 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.399 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.399 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.868 I llama_model_loader: - type  f32:  194 tensors
0.00.022.869 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.869 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.131 I llm_load_vocab: special tokens cache size = 25
0.00.048.798 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.801 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.801 I llm_load_print_meta: arch             = gptneox
0.00.048.802 I llm_load_print_meta: vocab type       = BPE
0.00.048.802 I llm_load_print_meta: n_vocab          = 50304
0.00.048.802 I llm_load_print_meta: n_merges         = 50009
0.00.048.802 I llm_load_print_meta: vocab_only       = 0
0.00.048.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.803 I llm_load_print_meta: n_embd           = 2048
0.00.048.803 I llm_load_print_meta: n_layer          = 24
0.00.048.805 I llm_load_print_meta: n_head           = 16
0.00.048.806 I llm_load_print_meta: n_head_kv        = 16
0.00.048.806 I llm_load_print_meta: n_rot            = 32
0.00.048.806 I llm_load_print_meta: n_swa            = 0
0.00.048.806 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.806 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.810 I llm_load_print_meta: n_gqa            = 1
0.00.048.811 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.811 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.812 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.812 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.812 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.812 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.813 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.813 I llm_load_print_meta: n_ff             = 8192
0.00.048.813 I llm_load_print_meta: n_expert         = 0
0.00.048.814 I llm_load_print_meta: n_expert_used    = 0
0.00.048.814 I llm_load_print_meta: causal attn      = 1
0.00.048.814 I llm_load_print_meta: pooling type     = 0
0.00.048.814 I llm_load_print_meta: rope type        = 2
0.00.048.814 I llm_load_print_meta: rope scaling     = linear
0.00.048.818 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.819 I llm_load_print_meta: freq_scale_train = 1
0.00.048.819 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.819 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.819 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.820 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.820 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.820 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.820 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.820 I llm_load_print_meta: model type       = 1.4B
0.00.048.821 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.821 I llm_load_print_meta: model params     = 1.41 B
0.00.048.822 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.822 I llm_load_print_meta: general.name     = 1.4B
0.00.048.822 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.825 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.825 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.825 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.825 I llm_load_print_meta: LF token         = 128 ''
0.00.048.826 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.826 I llm_load_print_meta: max token length = 1024
0.00.050.603 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.603 I llm_load_tensors: offloading output layer to GPU
0.00.050.604 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.613 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.615 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.461 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.462 I llama_new_context_with_model: n_ctx         = 128
0.00.051.462 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.462 I llama_new_context_with_model: n_batch       = 128
0.00.051.462 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.462 I llama_new_context_with_model: flash_attn    = 0
0.00.051.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.463 I llama_new_context_with_model: freq_scale    = 1
0.00.051.463 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.464 I ggml_metal_init: allocating
0.00.051.467 I ggml_metal_init: found device: Apple M4
0.00.051.469 I ggml_metal_init: picking default device: Apple M4
0.00.052.029 I ggml_metal_init: using embedded metal library
0.00.054.333 I ggml_metal_init: GPU name:   Apple M4
0.00.054.334 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.335 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.335 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.335 I ggml_metal_init: simdgroup reduction   = true
0.00.054.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.335 I ggml_metal_init: has bfloat            = true
0.00.054.336 I ggml_metal_init: use bfloat            = true
0.00.054.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.337 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.741 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.979 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.983 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.998 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.866 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.867 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.867 I llama_new_context_with_model: graph nodes  = 967
0.00.065.867 I llama_new_context_with_model: graph splits = 2
0.00.065.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.327 I 
0.00.724.351 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.363 I perplexity: tokenizing the input ..
0.00.731.936 I perplexity: tokenization took 7.571 ms
0.00.731.940 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.100 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.856.195 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.856.217 I llama_perf_context_print:        load time =     715.41 ms
0.00.856.218 I llama_perf_context_print: prompt eval time =     122.94 ms /   128 tokens (    0.96 ms per token,  1041.18 tokens per second)
0.00.856.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.219 I llama_perf_context_print:       total time =     131.89 ms /   129 tokens
0.00.856.696 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.078s
sys	0m0.142s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.019.427 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.039.105 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.106 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.107 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.113 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.114 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.115 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.116 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.116 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.117 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.119 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.119 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.120 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.051.392 I llama_model_loader: - type  f32:  194 tensors
0.00.051.392 I llama_model_loader: - type q5_0:   97 tensors
0.00.051.393 I llama_model_loader: - type q6_K:    1 tensors
0.00.089.593 I llm_load_vocab: special tokens cache size = 25
0.00.099.190 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.194 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.194 I llm_load_print_meta: arch             = gptneox
0.00.099.195 I llm_load_print_meta: vocab type       = BPE
0.00.099.195 I llm_load_print_meta: n_vocab          = 50304
0.00.099.195 I llm_load_print_meta: n_merges         = 50009
0.00.099.196 I llm_load_print_meta: vocab_only       = 0
0.00.099.196 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.196 I llm_load_print_meta: n_embd           = 2048
0.00.099.196 I llm_load_print_meta: n_layer          = 24
0.00.099.200 I llm_load_print_meta: n_head           = 16
0.00.099.201 I llm_load_print_meta: n_head_kv        = 16
0.00.099.201 I llm_load_print_meta: n_rot            = 32
0.00.099.202 I llm_load_print_meta: n_swa            = 0
0.00.099.202 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.202 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.203 I llm_load_print_meta: n_gqa            = 1
0.00.099.204 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.205 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.205 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.206 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.206 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.206 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.206 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.207 I llm_load_print_meta: n_ff             = 8192
0.00.099.207 I llm_load_print_meta: n_expert         = 0
0.00.099.208 I llm_load_print_meta: n_expert_used    = 0
0.00.099.208 I llm_load_print_meta: causal attn      = 1
0.00.099.208 I llm_load_print_meta: pooling type     = 0
0.00.099.208 I llm_load_print_meta: rope type        = 2
0.00.099.209 I llm_load_print_meta: rope scaling     = linear
0.00.099.209 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.210 I llm_load_print_meta: freq_scale_train = 1
0.00.099.210 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.210 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.210 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.211 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.211 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.211 I llm_load_print_meta: model type       = 1.4B
0.00.099.212 I llm_load_print_meta: model ftype      = Q5_0
0.00.099.212 I llm_load_print_meta: model params     = 1.41 B
0.00.099.213 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.099.213 I llm_load_print_meta: general.name     = 1.4B
0.00.099.213 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.216 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.217 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.217 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.217 I llm_load_print_meta: LF token         = 128 ''
0.00.099.217 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.218 I llm_load_print_meta: max token length = 1024
0.00.101.662 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.662 I llm_load_tensors: offloading output layer to GPU
0.00.101.663 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.673 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.101.675 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.102.898 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.903 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.903 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.903 I llama_new_context_with_model: n_batch       = 2048
0.00.102.904 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.904 I llama_new_context_with_model: flash_attn    = 0
0.00.102.904 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.905 I llama_new_context_with_model: freq_scale    = 1
0.00.102.905 I ggml_metal_init: allocating
0.00.102.909 I ggml_metal_init: found device: Apple M4
0.00.102.911 I ggml_metal_init: picking default device: Apple M4
0.00.103.734 I ggml_metal_init: using embedded metal library
0.00.107.174 I ggml_metal_init: GPU name:   Apple M4
0.00.107.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.107.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.107.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.107.177 I ggml_metal_init: simdgroup reduction   = true
0.00.107.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.107.178 I ggml_metal_init: has bfloat            = true
0.00.107.178 I ggml_metal_init: use bfloat            = true
0.00.107.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.107.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.118.049 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.140.581 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.140.589 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.140.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.141.544 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.141.545 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.141.545 I llama_new_context_with_model: graph nodes  = 967
0.00.141.546 I llama_new_context_with_model: graph splits = 2
0.00.141.551 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.141.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.950.648 I main: llama threadpool init, n_threads = 4
0.00.950.736 I 
0.00.950.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.950.792 I 
0.00.951.112 I sampler seed: 1234
0.00.951.122 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.951.142 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.951.144 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.951.144 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.740.393 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50859.60 tokens per second)
0.01.740.394 I llama_perf_context_print:        load time =     931.21 ms
0.01.740.396 I llama_perf_context_print: prompt eval time =      44.09 ms /     7 tokens (    6.30 ms per token,   158.75 tokens per second)
0.01.740.397 I llama_perf_context_print:        eval time =     741.98 ms /    63 runs   (   11.78 ms per token,    84.91 tokens per second)
0.01.740.398 I llama_perf_context_print:       total time =     789.75 ms /    70 tokens
0.01.740.588 I ggml_metal_free: deallocating

real	0m1.771s
user	0m0.151s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.103 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.541 I llama_model_loader: - type  f32:  194 tensors
0.00.023.542 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.542 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.135 I llm_load_vocab: special tokens cache size = 25
0.00.048.787 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.790 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.791 I llm_load_print_meta: arch             = gptneox
0.00.048.791 I llm_load_print_meta: vocab type       = BPE
0.00.048.791 I llm_load_print_meta: n_vocab          = 50304
0.00.048.791 I llm_load_print_meta: n_merges         = 50009
0.00.048.792 I llm_load_print_meta: vocab_only       = 0
0.00.048.792 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.792 I llm_load_print_meta: n_embd           = 2048
0.00.048.792 I llm_load_print_meta: n_layer          = 24
0.00.048.796 I llm_load_print_meta: n_head           = 16
0.00.048.797 I llm_load_print_meta: n_head_kv        = 16
0.00.048.797 I llm_load_print_meta: n_rot            = 32
0.00.048.797 I llm_load_print_meta: n_swa            = 0
0.00.048.799 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.799 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.800 I llm_load_print_meta: n_gqa            = 1
0.00.048.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.802 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.802 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.803 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.803 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.808 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.809 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.810 I llm_load_print_meta: n_ff             = 8192
0.00.048.810 I llm_load_print_meta: n_expert         = 0
0.00.048.811 I llm_load_print_meta: n_expert_used    = 0
0.00.048.811 I llm_load_print_meta: causal attn      = 1
0.00.048.811 I llm_load_print_meta: pooling type     = 0
0.00.048.811 I llm_load_print_meta: rope type        = 2
0.00.048.812 I llm_load_print_meta: rope scaling     = linear
0.00.048.812 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.812 I llm_load_print_meta: freq_scale_train = 1
0.00.048.812 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.813 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.814 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.814 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.814 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.814 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.814 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.815 I llm_load_print_meta: model type       = 1.4B
0.00.048.815 I llm_load_print_meta: model ftype      = Q5_0
0.00.048.817 I llm_load_print_meta: model params     = 1.41 B
0.00.048.817 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.048.817 I llm_load_print_meta: general.name     = 1.4B
0.00.048.817 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.818 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.818 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.818 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.818 I llm_load_print_meta: LF token         = 128 ''
0.00.048.821 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: max token length = 1024
0.00.050.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.512 I llm_load_tensors: offloading output layer to GPU
0.00.050.512 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.522 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.050.523 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.051.380 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.381 I llama_new_context_with_model: n_ctx         = 128
0.00.051.381 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.381 I llama_new_context_with_model: n_batch       = 128
0.00.051.381 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.381 I llama_new_context_with_model: flash_attn    = 0
0.00.051.382 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.382 I llama_new_context_with_model: freq_scale    = 1
0.00.051.382 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.383 I ggml_metal_init: allocating
0.00.051.388 I ggml_metal_init: found device: Apple M4
0.00.051.391 I ggml_metal_init: picking default device: Apple M4
0.00.051.966 I ggml_metal_init: using embedded metal library
0.00.054.291 I ggml_metal_init: GPU name:   Apple M4
0.00.054.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.293 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.294 I ggml_metal_init: simdgroup reduction   = true
0.00.054.294 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.294 I ggml_metal_init: has bfloat            = true
0.00.054.294 I ggml_metal_init: use bfloat            = true
0.00.054.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.560 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.832 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.835 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.850 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.712 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.713 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.713 I llama_new_context_with_model: graph nodes  = 967
0.00.064.714 I llama_new_context_with_model: graph splits = 2
0.00.064.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.213 I 
0.00.753.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.259 I perplexity: tokenizing the input ..
0.00.760.977 I perplexity: tokenization took 7.717 ms
0.00.760.981 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.236 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.897.342 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.897.361 I llama_perf_context_print:        load time =     743.38 ms
0.00.897.363 I llama_perf_context_print: prompt eval time =     135.03 ms /   128 tokens (    1.05 ms per token,   947.90 tokens per second)
0.00.897.366 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.366 I llama_perf_context_print:       total time =     144.15 ms /   129 tokens
0.00.897.863 I ggml_metal_free: deallocating

real	0m0.912s
user	0m0.076s
sys	0m0.153s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.351 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.249 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.257 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.258 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.072 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.928 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.929 I llama_model_loader: - type  f32:  194 tensors
0.00.024.929 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.929 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.651 I llm_load_vocab: special tokens cache size = 25
0.00.051.691 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.696 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.696 I llm_load_print_meta: arch             = gptneox
0.00.051.696 I llm_load_print_meta: vocab type       = BPE
0.00.051.697 I llm_load_print_meta: n_vocab          = 50304
0.00.051.697 I llm_load_print_meta: n_merges         = 50009
0.00.051.697 I llm_load_print_meta: vocab_only       = 0
0.00.051.697 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.697 I llm_load_print_meta: n_embd           = 2048
0.00.051.698 I llm_load_print_meta: n_layer          = 24
0.00.051.701 I llm_load_print_meta: n_head           = 16
0.00.051.702 I llm_load_print_meta: n_head_kv        = 16
0.00.051.702 I llm_load_print_meta: n_rot            = 32
0.00.051.703 I llm_load_print_meta: n_swa            = 0
0.00.051.703 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.703 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.704 I llm_load_print_meta: n_gqa            = 1
0.00.051.704 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.705 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.706 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.706 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.709 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.710 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.710 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.710 I llm_load_print_meta: n_ff             = 8192
0.00.051.711 I llm_load_print_meta: n_expert         = 0
0.00.051.711 I llm_load_print_meta: n_expert_used    = 0
0.00.051.711 I llm_load_print_meta: causal attn      = 1
0.00.051.711 I llm_load_print_meta: pooling type     = 0
0.00.051.711 I llm_load_print_meta: rope type        = 2
0.00.051.711 I llm_load_print_meta: rope scaling     = linear
0.00.051.712 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.714 I llm_load_print_meta: freq_scale_train = 1
0.00.051.714 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.714 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.714 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.714 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.715 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.716 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.716 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.716 I llm_load_print_meta: model type       = 1.4B
0.00.051.717 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.717 I llm_load_print_meta: model params     = 1.41 B
0.00.051.718 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.718 I llm_load_print_meta: general.name     = 1.4B
0.00.051.718 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.718 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.720 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.720 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.720 I llm_load_print_meta: LF token         = 128 ''
0.00.051.721 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.721 I llm_load_print_meta: max token length = 1024
0.00.053.487 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.487 I llm_load_tensors: offloading output layer to GPU
0.00.053.488 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.498 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.499 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.389 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.389 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.390 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.390 I llama_new_context_with_model: n_batch       = 2048
0.00.054.390 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.390 I llama_new_context_with_model: flash_attn    = 0
0.00.054.391 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.391 I llama_new_context_with_model: freq_scale    = 1
0.00.054.392 I ggml_metal_init: allocating
0.00.054.395 I ggml_metal_init: found device: Apple M4
0.00.054.397 I ggml_metal_init: picking default device: Apple M4
0.00.055.021 I ggml_metal_init: using embedded metal library
0.00.057.413 I ggml_metal_init: GPU name:   Apple M4
0.00.057.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.417 I ggml_metal_init: simdgroup reduction   = true
0.00.057.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.417 I ggml_metal_init: has bfloat            = true
0.00.057.417 I ggml_metal_init: use bfloat            = true
0.00.057.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.641 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.233 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.240 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.213 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.216 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.216 I llama_new_context_with_model: graph nodes  = 967
0.00.088.217 I llama_new_context_with_model: graph splits = 2
0.00.088.219 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.911 I main: llama threadpool init, n_threads = 4
0.00.778.950 I 
0.00.778.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.970 I 
0.00.779.127 I sampler seed: 1234
0.00.779.131 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.141 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.612.065 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.612.066 I llama_perf_context_print:        load time =     769.56 ms
0.01.612.067 I llama_perf_context_print: prompt eval time =      42.62 ms /     7 tokens (    6.09 ms per token,   164.22 tokens per second)
0.01.612.068 I llama_perf_context_print:        eval time =     787.18 ms /    63 runs   (   12.49 ms per token,    80.03 tokens per second)
0.01.612.068 I llama_perf_context_print:       total time =     833.16 ms /    70 tokens
0.01.612.298 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.110s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.508 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.519 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.520 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.521 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.123 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.124 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.124 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.125 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.125 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.125 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.126 I llama_model_loader: - type  f32:  194 tensors
0.00.023.126 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.126 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.566 I llm_load_vocab: special tokens cache size = 25
0.00.049.473 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.476 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.476 I llm_load_print_meta: arch             = gptneox
0.00.049.476 I llm_load_print_meta: vocab type       = BPE
0.00.049.477 I llm_load_print_meta: n_vocab          = 50304
0.00.049.477 I llm_load_print_meta: n_merges         = 50009
0.00.049.477 I llm_load_print_meta: vocab_only       = 0
0.00.049.477 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.477 I llm_load_print_meta: n_embd           = 2048
0.00.049.477 I llm_load_print_meta: n_layer          = 24
0.00.049.480 I llm_load_print_meta: n_head           = 16
0.00.049.481 I llm_load_print_meta: n_head_kv        = 16
0.00.049.481 I llm_load_print_meta: n_rot            = 32
0.00.049.481 I llm_load_print_meta: n_swa            = 0
0.00.049.481 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.482 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.482 I llm_load_print_meta: n_gqa            = 1
0.00.049.483 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.484 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.484 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.485 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.485 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.485 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.485 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.486 I llm_load_print_meta: n_ff             = 8192
0.00.049.486 I llm_load_print_meta: n_expert         = 0
0.00.049.487 I llm_load_print_meta: n_expert_used    = 0
0.00.049.487 I llm_load_print_meta: causal attn      = 1
0.00.049.487 I llm_load_print_meta: pooling type     = 0
0.00.049.487 I llm_load_print_meta: rope type        = 2
0.00.049.487 I llm_load_print_meta: rope scaling     = linear
0.00.049.488 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.488 I llm_load_print_meta: freq_scale_train = 1
0.00.049.489 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.489 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.490 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.490 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.490 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.491 I llm_load_print_meta: model type       = 1.4B
0.00.049.491 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.492 I llm_load_print_meta: model params     = 1.41 B
0.00.049.494 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.494 I llm_load_print_meta: general.name     = 1.4B
0.00.049.494 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.494 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.495 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.495 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.495 I llm_load_print_meta: LF token         = 128 ''
0.00.049.495 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.495 I llm_load_print_meta: max token length = 1024
0.00.051.231 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.231 I llm_load_tensors: offloading output layer to GPU
0.00.051.231 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.242 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.243 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.082 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.083 I llama_new_context_with_model: n_ctx         = 128
0.00.052.083 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.083 I llama_new_context_with_model: n_batch       = 128
0.00.052.083 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.084 I llama_new_context_with_model: flash_attn    = 0
0.00.052.084 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.084 I llama_new_context_with_model: freq_scale    = 1
0.00.052.085 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.085 I ggml_metal_init: allocating
0.00.052.091 I ggml_metal_init: found device: Apple M4
0.00.052.094 I ggml_metal_init: picking default device: Apple M4
0.00.052.662 I ggml_metal_init: using embedded metal library
0.00.055.002 I ggml_metal_init: GPU name:   Apple M4
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.004 I ggml_metal_init: simdgroup reduction   = true
0.00.055.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.005 I ggml_metal_init: has bfloat            = true
0.00.055.005 I ggml_metal_init: use bfloat            = true
0.00.055.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.390 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.622 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.625 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.648 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.496 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.497 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.498 I llama_new_context_with_model: graph nodes  = 967
0.00.066.498 I llama_new_context_with_model: graph splits = 2
0.00.066.499 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.499 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.022 I 
0.00.691.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.069 I perplexity: tokenizing the input ..
0.00.698.575 I perplexity: tokenization took 7.506 ms
0.00.698.579 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.746 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.834.859 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.834.875 I llama_perf_context_print:        load time =     682.18 ms
0.00.834.876 I llama_perf_context_print: prompt eval time =     134.95 ms /   128 tokens (    1.05 ms per token,   948.51 tokens per second)
0.00.834.877 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.877 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.835.410 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.078s
sys	0m0.154s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.541 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.760 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.775 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.781 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.541 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.589 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.383 I llama_model_loader: - type  f32:  194 tensors
0.00.023.383 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.383 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.384 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.974 I llm_load_vocab: special tokens cache size = 25
0.00.049.796 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.799 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.799 I llm_load_print_meta: arch             = gptneox
0.00.049.800 I llm_load_print_meta: vocab type       = BPE
0.00.049.800 I llm_load_print_meta: n_vocab          = 50304
0.00.049.800 I llm_load_print_meta: n_merges         = 50009
0.00.049.800 I llm_load_print_meta: vocab_only       = 0
0.00.049.801 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.801 I llm_load_print_meta: n_embd           = 2048
0.00.049.801 I llm_load_print_meta: n_layer          = 24
0.00.049.803 I llm_load_print_meta: n_head           = 16
0.00.049.804 I llm_load_print_meta: n_head_kv        = 16
0.00.049.804 I llm_load_print_meta: n_rot            = 32
0.00.049.804 I llm_load_print_meta: n_swa            = 0
0.00.049.805 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.805 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.805 I llm_load_print_meta: n_gqa            = 1
0.00.049.806 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.807 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.807 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.808 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.808 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.808 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.808 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.809 I llm_load_print_meta: n_ff             = 8192
0.00.049.809 I llm_load_print_meta: n_expert         = 0
0.00.049.809 I llm_load_print_meta: n_expert_used    = 0
0.00.049.809 I llm_load_print_meta: causal attn      = 1
0.00.049.809 I llm_load_print_meta: pooling type     = 0
0.00.049.810 I llm_load_print_meta: rope type        = 2
0.00.049.810 I llm_load_print_meta: rope scaling     = linear
0.00.049.810 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.811 I llm_load_print_meta: freq_scale_train = 1
0.00.049.811 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.811 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.811 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.811 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.811 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.812 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.812 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.812 I llm_load_print_meta: model type       = 1.4B
0.00.049.812 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.813 I llm_load_print_meta: model params     = 1.41 B
0.00.049.814 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.815 I llm_load_print_meta: general.name     = 1.4B
0.00.049.815 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.815 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.815 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.815 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.816 I llm_load_print_meta: LF token         = 128 ''
0.00.049.816 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.816 I llm_load_print_meta: max token length = 1024
0.00.051.524 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.524 I llm_load_tensors: offloading output layer to GPU
0.00.051.524 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.534 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.535 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.373 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.374 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.374 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.374 I llama_new_context_with_model: n_batch       = 2048
0.00.052.374 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.375 I llama_new_context_with_model: flash_attn    = 0
0.00.052.375 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.375 I llama_new_context_with_model: freq_scale    = 1
0.00.052.376 I ggml_metal_init: allocating
0.00.052.379 I ggml_metal_init: found device: Apple M4
0.00.052.381 I ggml_metal_init: picking default device: Apple M4
0.00.052.970 I ggml_metal_init: using embedded metal library
0.00.055.297 I ggml_metal_init: GPU name:   Apple M4
0.00.055.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.299 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.299 I ggml_metal_init: simdgroup reduction   = true
0.00.055.299 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.299 I ggml_metal_init: has bfloat            = true
0.00.055.300 I ggml_metal_init: use bfloat            = true
0.00.055.300 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.596 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.548 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.555 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.585 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.692 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.694 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.694 I llama_new_context_with_model: graph nodes  = 967
0.00.085.695 I llama_new_context_with_model: graph splits = 2
0.00.085.697 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.450.543 I main: llama threadpool init, n_threads = 4
0.00.450.592 I 
0.00.450.611 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.450.611 I 
0.00.450.781 I sampler seed: 1234
0.00.450.786 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.450.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.450.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.450.797 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.139.366 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62280.70 tokens per second)
0.01.139.366 I llama_perf_context_print:        load time =     441.00 ms
0.01.139.367 I llama_perf_context_print: prompt eval time =      36.20 ms /     7 tokens (    5.17 ms per token,   193.39 tokens per second)
0.01.139.368 I llama_perf_context_print:        eval time =     649.39 ms /    63 runs   (   10.31 ms per token,    97.01 tokens per second)
0.01.139.368 I llama_perf_context_print:       total time =     688.82 ms /    70 tokens
0.01.139.553 I ggml_metal_free: deallocating

real	0m1.156s
user	0m0.108s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.329 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.724 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.731 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.731 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.732 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.732 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.739 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.133 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.134 I llama_model_loader: - type  f32:  194 tensors
0.00.023.134 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.134 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.135 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.707 I llm_load_vocab: special tokens cache size = 25
0.00.048.329 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.331 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.332 I llm_load_print_meta: arch             = gptneox
0.00.048.332 I llm_load_print_meta: vocab type       = BPE
0.00.048.332 I llm_load_print_meta: n_vocab          = 50304
0.00.048.332 I llm_load_print_meta: n_merges         = 50009
0.00.048.333 I llm_load_print_meta: vocab_only       = 0
0.00.048.333 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.333 I llm_load_print_meta: n_embd           = 2048
0.00.048.333 I llm_load_print_meta: n_layer          = 24
0.00.048.336 I llm_load_print_meta: n_head           = 16
0.00.048.336 I llm_load_print_meta: n_head_kv        = 16
0.00.048.337 I llm_load_print_meta: n_rot            = 32
0.00.048.337 I llm_load_print_meta: n_swa            = 0
0.00.048.337 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.337 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.338 I llm_load_print_meta: n_gqa            = 1
0.00.048.338 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.341 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.342 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.342 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.344 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.344 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.344 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.345 I llm_load_print_meta: n_ff             = 8192
0.00.048.345 I llm_load_print_meta: n_expert         = 0
0.00.048.345 I llm_load_print_meta: n_expert_used    = 0
0.00.048.345 I llm_load_print_meta: causal attn      = 1
0.00.048.345 I llm_load_print_meta: pooling type     = 0
0.00.048.345 I llm_load_print_meta: rope type        = 2
0.00.048.346 I llm_load_print_meta: rope scaling     = linear
0.00.048.346 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.346 I llm_load_print_meta: freq_scale_train = 1
0.00.048.346 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.347 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.347 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.347 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.347 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.347 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.347 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.348 I llm_load_print_meta: model type       = 1.4B
0.00.048.348 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.048.349 I llm_load_print_meta: model params     = 1.41 B
0.00.048.349 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.048.349 I llm_load_print_meta: general.name     = 1.4B
0.00.048.350 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.350 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.350 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.350 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.351 I llm_load_print_meta: LF token         = 128 ''
0.00.048.351 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.351 I llm_load_print_meta: max token length = 1024
0.00.050.037 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.037 I llm_load_tensors: offloading output layer to GPU
0.00.050.038 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.047 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.048 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.050.853 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.854 I llama_new_context_with_model: n_ctx         = 128
0.00.050.854 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.854 I llama_new_context_with_model: n_batch       = 128
0.00.050.855 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.855 I llama_new_context_with_model: flash_attn    = 0
0.00.050.855 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.855 I llama_new_context_with_model: freq_scale    = 1
0.00.050.856 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.856 I ggml_metal_init: allocating
0.00.050.859 I ggml_metal_init: found device: Apple M4
0.00.050.860 I ggml_metal_init: picking default device: Apple M4
0.00.051.432 I ggml_metal_init: using embedded metal library
0.00.053.795 I ggml_metal_init: GPU name:   Apple M4
0.00.053.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.798 I ggml_metal_init: simdgroup reduction   = true
0.00.053.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.798 I ggml_metal_init: has bfloat            = true
0.00.053.798 I ggml_metal_init: use bfloat            = true
0.00.053.798 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.799 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.878 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.173 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.186 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.088 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.089 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.089 I llama_new_context_with_model: graph nodes  = 967
0.00.064.090 I llama_new_context_with_model: graph splits = 2
0.00.064.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.444 I 
0.00.401.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.487 I perplexity: tokenizing the input ..
0.00.409.606 I perplexity: tokenization took 8.117 ms
0.00.409.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.542.459 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.543.641 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.543.655 I llama_perf_context_print:        load time =     392.11 ms
0.00.543.655 I llama_perf_context_print: prompt eval time =     132.63 ms /   128 tokens (    1.04 ms per token,   965.12 tokens per second)
0.00.543.656 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.543.657 I llama_perf_context_print:       total time =     142.21 ms /   129 tokens
0.00.544.155 I ggml_metal_free: deallocating

real	0m0.558s
user	0m0.076s
sys	0m0.083s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.237 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.557 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.561 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.565 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.329 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.008 I llama_model_loader: - type  f32:  194 tensors
0.00.023.009 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.009 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.009 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.009 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.621 I llm_load_vocab: special tokens cache size = 25
0.00.048.538 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.541 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.541 I llm_load_print_meta: arch             = gptneox
0.00.048.542 I llm_load_print_meta: vocab type       = BPE
0.00.048.542 I llm_load_print_meta: n_vocab          = 50304
0.00.048.542 I llm_load_print_meta: n_merges         = 50009
0.00.048.542 I llm_load_print_meta: vocab_only       = 0
0.00.048.542 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.542 I llm_load_print_meta: n_embd           = 2048
0.00.048.543 I llm_load_print_meta: n_layer          = 24
0.00.048.545 I llm_load_print_meta: n_head           = 16
0.00.048.546 I llm_load_print_meta: n_head_kv        = 16
0.00.048.546 I llm_load_print_meta: n_rot            = 32
0.00.048.546 I llm_load_print_meta: n_swa            = 0
0.00.048.546 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.546 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.547 I llm_load_print_meta: n_gqa            = 1
0.00.048.548 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.548 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.549 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.549 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.550 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.550 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.550 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.552 I llm_load_print_meta: n_ff             = 8192
0.00.048.552 I llm_load_print_meta: n_expert         = 0
0.00.048.553 I llm_load_print_meta: n_expert_used    = 0
0.00.048.553 I llm_load_print_meta: causal attn      = 1
0.00.048.553 I llm_load_print_meta: pooling type     = 0
0.00.048.553 I llm_load_print_meta: rope type        = 2
0.00.048.553 I llm_load_print_meta: rope scaling     = linear
0.00.048.555 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.555 I llm_load_print_meta: freq_scale_train = 1
0.00.048.556 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.556 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.556 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.556 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.556 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.556 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.557 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.557 I llm_load_print_meta: model type       = 1.4B
0.00.048.557 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.558 I llm_load_print_meta: model params     = 1.41 B
0.00.048.558 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.558 I llm_load_print_meta: general.name     = 1.4B
0.00.048.559 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.559 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.560 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.561 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.561 I llm_load_print_meta: LF token         = 128 ''
0.00.048.562 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.562 I llm_load_print_meta: max token length = 1024
0.00.050.239 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.239 I llm_load_tensors: offloading output layer to GPU
0.00.050.239 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.249 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.250 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.090 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.090 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.091 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.091 I llama_new_context_with_model: n_batch       = 2048
0.00.051.091 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.091 I llama_new_context_with_model: flash_attn    = 0
0.00.051.092 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.092 I llama_new_context_with_model: freq_scale    = 1
0.00.051.092 I ggml_metal_init: allocating
0.00.051.098 I ggml_metal_init: found device: Apple M4
0.00.051.101 I ggml_metal_init: picking default device: Apple M4
0.00.051.701 I ggml_metal_init: using embedded metal library
0.00.054.027 I ggml_metal_init: GPU name:   Apple M4
0.00.054.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.029 I ggml_metal_init: simdgroup reduction   = true
0.00.054.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.029 I ggml_metal_init: has bfloat            = true
0.00.054.030 I ggml_metal_init: use bfloat            = true
0.00.054.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.629 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.640 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.659 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.653 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.654 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.655 I llama_new_context_with_model: graph nodes  = 967
0.00.083.655 I llama_new_context_with_model: graph splits = 2
0.00.083.659 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.363 I main: llama threadpool init, n_threads = 4
0.00.571.404 I 
0.00.571.426 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.426 I 
0.00.571.587 I sampler seed: 1234
0.00.571.591 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.571.602 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.571.602 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.571.604 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.319.978 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.319.979 I llama_perf_context_print:        load time =     562.12 ms
0.01.319.981 I llama_perf_context_print: prompt eval time =      40.99 ms /     7 tokens (    5.86 ms per token,   170.75 tokens per second)
0.01.319.981 I llama_perf_context_print:        eval time =     704.41 ms /    63 runs   (   11.18 ms per token,    89.44 tokens per second)
0.01.319.982 I llama_perf_context_print:       total time =     748.62 ms /    70 tokens
0.01.320.212 I ggml_metal_free: deallocating

real	0m1.334s
user	0m0.108s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.120 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.546 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.060 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.061 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.062 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.062 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.062 I llama_model_loader: - type  f32:  194 tensors
0.00.024.063 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.063 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.063 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.070 I llm_load_vocab: special tokens cache size = 25
0.00.050.100 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.103 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.103 I llm_load_print_meta: arch             = gptneox
0.00.050.103 I llm_load_print_meta: vocab type       = BPE
0.00.050.104 I llm_load_print_meta: n_vocab          = 50304
0.00.050.104 I llm_load_print_meta: n_merges         = 50009
0.00.050.104 I llm_load_print_meta: vocab_only       = 0
0.00.050.104 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.104 I llm_load_print_meta: n_embd           = 2048
0.00.050.104 I llm_load_print_meta: n_layer          = 24
0.00.050.107 I llm_load_print_meta: n_head           = 16
0.00.050.108 I llm_load_print_meta: n_head_kv        = 16
0.00.050.108 I llm_load_print_meta: n_rot            = 32
0.00.050.109 I llm_load_print_meta: n_swa            = 0
0.00.050.109 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.110 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.111 I llm_load_print_meta: n_gqa            = 1
0.00.050.111 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.112 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.113 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.113 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.114 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.114 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.114 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.123 I llm_load_print_meta: n_ff             = 8192
0.00.050.123 I llm_load_print_meta: n_expert         = 0
0.00.050.123 I llm_load_print_meta: n_expert_used    = 0
0.00.050.123 I llm_load_print_meta: causal attn      = 1
0.00.050.123 I llm_load_print_meta: pooling type     = 0
0.00.050.124 I llm_load_print_meta: rope type        = 2
0.00.050.124 I llm_load_print_meta: rope scaling     = linear
0.00.050.124 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.125 I llm_load_print_meta: freq_scale_train = 1
0.00.050.125 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.125 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.125 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.125 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.126 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.126 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.126 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.126 I llm_load_print_meta: model type       = 1.4B
0.00.050.128 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.128 I llm_load_print_meta: model params     = 1.41 B
0.00.050.129 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.129 I llm_load_print_meta: general.name     = 1.4B
0.00.050.129 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.130 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.130 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.130 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.131 I llm_load_print_meta: LF token         = 128 ''
0.00.050.131 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.131 I llm_load_print_meta: max token length = 1024
0.00.052.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.008 I llm_load_tensors: offloading output layer to GPU
0.00.052.008 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.013 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.013 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.982 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.983 I llama_new_context_with_model: n_ctx         = 128
0.00.052.984 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.984 I llama_new_context_with_model: n_batch       = 128
0.00.052.984 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.984 I llama_new_context_with_model: flash_attn    = 0
0.00.052.984 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.985 I llama_new_context_with_model: freq_scale    = 1
0.00.052.985 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.985 I ggml_metal_init: allocating
0.00.052.990 I ggml_metal_init: found device: Apple M4
0.00.052.993 I ggml_metal_init: picking default device: Apple M4
0.00.053.610 I ggml_metal_init: using embedded metal library
0.00.056.182 I ggml_metal_init: GPU name:   Apple M4
0.00.056.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.185 I ggml_metal_init: simdgroup reduction   = true
0.00.056.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.185 I ggml_metal_init: has bfloat            = true
0.00.056.185 I ggml_metal_init: use bfloat            = true
0.00.056.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.445 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.641 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.655 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.438 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.439 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.439 I llama_new_context_with_model: graph nodes  = 967
0.00.067.439 I llama_new_context_with_model: graph splits = 2
0.00.067.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.390 I 
0.00.507.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.441 I perplexity: tokenizing the input ..
0.00.514.953 I perplexity: tokenization took 7.51 ms
0.00.514.956 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.583 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.931 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.942 I llama_perf_context_print:        load time =     497.26 ms
0.00.647.943 I llama_perf_context_print: prompt eval time =     131.40 ms /   128 tokens (    1.03 ms per token,   974.14 tokens per second)
0.00.647.944 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.945 I llama_perf_context_print:       total time =     140.56 ms /   129 tokens
0.00.648.377 I ggml_metal_free: deallocating

real	0m0.661s
user	0m0.079s
sys	0m0.106s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.036 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.995 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.921 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.725 I llama_model_loader: - type  f32:  194 tensors
0.00.024.725 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.725 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.726 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.375 I llm_load_vocab: special tokens cache size = 25
0.00.050.341 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.344 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.345 I llm_load_print_meta: arch             = gptneox
0.00.050.345 I llm_load_print_meta: vocab type       = BPE
0.00.050.345 I llm_load_print_meta: n_vocab          = 50304
0.00.050.345 I llm_load_print_meta: n_merges         = 50009
0.00.050.345 I llm_load_print_meta: vocab_only       = 0
0.00.050.346 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.346 I llm_load_print_meta: n_embd           = 2048
0.00.050.346 I llm_load_print_meta: n_layer          = 24
0.00.050.348 I llm_load_print_meta: n_head           = 16
0.00.050.349 I llm_load_print_meta: n_head_kv        = 16
0.00.050.349 I llm_load_print_meta: n_rot            = 32
0.00.050.350 I llm_load_print_meta: n_swa            = 0
0.00.050.350 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.350 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.352 I llm_load_print_meta: n_gqa            = 1
0.00.050.352 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.355 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.355 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.356 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.356 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.356 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.356 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.358 I llm_load_print_meta: n_ff             = 8192
0.00.050.358 I llm_load_print_meta: n_expert         = 0
0.00.050.358 I llm_load_print_meta: n_expert_used    = 0
0.00.050.359 I llm_load_print_meta: causal attn      = 1
0.00.050.359 I llm_load_print_meta: pooling type     = 0
0.00.050.361 I llm_load_print_meta: rope type        = 2
0.00.050.361 I llm_load_print_meta: rope scaling     = linear
0.00.050.361 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.362 I llm_load_print_meta: freq_scale_train = 1
0.00.050.362 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.362 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.368 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.370 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.370 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.370 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.370 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.370 I llm_load_print_meta: model type       = 1.4B
0.00.050.371 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.371 I llm_load_print_meta: model params     = 1.41 B
0.00.050.372 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.372 I llm_load_print_meta: general.name     = 1.4B
0.00.050.372 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.372 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.372 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.372 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.373 I llm_load_print_meta: LF token         = 128 ''
0.00.050.373 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.373 I llm_load_print_meta: max token length = 1024
0.00.052.141 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.141 I llm_load_tensors: offloading output layer to GPU
0.00.052.141 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.151 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.152 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.999 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.000 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.000 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.000 I llama_new_context_with_model: n_batch       = 2048
0.00.053.000 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.000 I llama_new_context_with_model: flash_attn    = 0
0.00.053.001 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.001 I llama_new_context_with_model: freq_scale    = 1
0.00.053.001 I ggml_metal_init: allocating
0.00.053.006 I ggml_metal_init: found device: Apple M4
0.00.053.008 I ggml_metal_init: picking default device: Apple M4
0.00.053.619 I ggml_metal_init: using embedded metal library
0.00.055.956 I ggml_metal_init: GPU name:   Apple M4
0.00.055.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.959 I ggml_metal_init: simdgroup reduction   = true
0.00.055.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.959 I ggml_metal_init: has bfloat            = true
0.00.055.959 I ggml_metal_init: use bfloat            = true
0.00.055.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.396 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.994 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.999 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.019 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.014 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.016 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.016 I llama_new_context_with_model: graph nodes  = 967
0.00.085.016 I llama_new_context_with_model: graph splits = 2
0.00.085.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.789 I main: llama threadpool init, n_threads = 4
0.00.653.831 I 
0.00.653.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.881 I 
0.00.654.033 I sampler seed: 1234
0.00.654.038 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.654.048 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.654.048 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.654.048 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.406.562 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.406.562 I llama_perf_context_print:        load time =     644.75 ms
0.01.406.563 I llama_perf_context_print: prompt eval time =      47.48 ms /     7 tokens (    6.78 ms per token,   147.44 tokens per second)
0.01.406.565 I llama_perf_context_print:        eval time =     701.93 ms /    63 runs   (   11.14 ms per token,    89.75 tokens per second)
0.01.406.568 I llama_perf_context_print:       total time =     752.78 ms /    70 tokens
0.01.406.800 I ggml_metal_free: deallocating

real	0m1.423s
user	0m0.109s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.449 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.750 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.418 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.419 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.420 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.420 I llama_model_loader: - type  f32:  194 tensors
0.00.024.421 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.421 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.421 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.906 I llm_load_vocab: special tokens cache size = 25
0.00.049.623 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.626 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.626 I llm_load_print_meta: arch             = gptneox
0.00.049.626 I llm_load_print_meta: vocab type       = BPE
0.00.049.627 I llm_load_print_meta: n_vocab          = 50304
0.00.049.627 I llm_load_print_meta: n_merges         = 50009
0.00.049.627 I llm_load_print_meta: vocab_only       = 0
0.00.049.627 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.628 I llm_load_print_meta: n_embd           = 2048
0.00.049.628 I llm_load_print_meta: n_layer          = 24
0.00.049.630 I llm_load_print_meta: n_head           = 16
0.00.049.631 I llm_load_print_meta: n_head_kv        = 16
0.00.049.631 I llm_load_print_meta: n_rot            = 32
0.00.049.632 I llm_load_print_meta: n_swa            = 0
0.00.049.633 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.633 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.633 I llm_load_print_meta: n_gqa            = 1
0.00.049.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.635 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.638 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.639 I llm_load_print_meta: n_ff             = 8192
0.00.049.639 I llm_load_print_meta: n_expert         = 0
0.00.049.639 I llm_load_print_meta: n_expert_used    = 0
0.00.049.639 I llm_load_print_meta: causal attn      = 1
0.00.049.639 I llm_load_print_meta: pooling type     = 0
0.00.049.639 I llm_load_print_meta: rope type        = 2
0.00.049.640 I llm_load_print_meta: rope scaling     = linear
0.00.049.640 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.640 I llm_load_print_meta: freq_scale_train = 1
0.00.049.640 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.640 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.641 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.641 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.641 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.641 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.641 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.641 I llm_load_print_meta: model type       = 1.4B
0.00.049.642 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.642 I llm_load_print_meta: model params     = 1.41 B
0.00.049.642 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.647 I llm_load_print_meta: general.name     = 1.4B
0.00.049.648 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.648 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.649 I llm_load_print_meta: LF token         = 128 ''
0.00.049.650 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.651 I llm_load_print_meta: max token length = 1024
0.00.051.359 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.359 I llm_load_tensors: offloading output layer to GPU
0.00.051.359 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.369 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.370 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.188 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.189 I llama_new_context_with_model: n_ctx         = 128
0.00.052.189 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.189 I llama_new_context_with_model: n_batch       = 128
0.00.052.189 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.189 I llama_new_context_with_model: flash_attn    = 0
0.00.052.190 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.190 I llama_new_context_with_model: freq_scale    = 1
0.00.052.190 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.191 I ggml_metal_init: allocating
0.00.052.197 I ggml_metal_init: found device: Apple M4
0.00.052.200 I ggml_metal_init: picking default device: Apple M4
0.00.052.778 I ggml_metal_init: using embedded metal library
0.00.055.191 I ggml_metal_init: GPU name:   Apple M4
0.00.055.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.193 I ggml_metal_init: simdgroup reduction   = true
0.00.055.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.194 I ggml_metal_init: has bfloat            = true
0.00.055.194 I ggml_metal_init: use bfloat            = true
0.00.055.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.643 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.649 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.563 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.567 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.567 I llama_new_context_with_model: graph nodes  = 967
0.00.067.567 I llama_new_context_with_model: graph splits = 2
0.00.067.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.616 I 
0.00.614.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.655 I perplexity: tokenizing the input ..
0.00.622.248 I perplexity: tokenization took 7.591 ms
0.00.622.254 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.688 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.757.821 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.757.833 I llama_perf_context_print:        load time =     604.16 ms
0.00.757.835 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.69 tokens per second)
0.00.757.836 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.836 I llama_perf_context_print:       total time =     143.22 ms /   129 tokens
0.00.758.322 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.077s
sys	0m0.128s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.452 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.870 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.871 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.882 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.670 I llama_model_loader: - type  f32:  194 tensors
0.00.025.670 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.670 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.405 I llm_load_vocab: special tokens cache size = 25
0.00.051.120 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.123 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.124 I llm_load_print_meta: arch             = gptneox
0.00.051.124 I llm_load_print_meta: vocab type       = BPE
0.00.051.124 I llm_load_print_meta: n_vocab          = 50304
0.00.051.125 I llm_load_print_meta: n_merges         = 50009
0.00.051.125 I llm_load_print_meta: vocab_only       = 0
0.00.051.125 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.125 I llm_load_print_meta: n_embd           = 2048
0.00.051.125 I llm_load_print_meta: n_layer          = 24
0.00.051.128 I llm_load_print_meta: n_head           = 16
0.00.051.129 I llm_load_print_meta: n_head_kv        = 16
0.00.051.129 I llm_load_print_meta: n_rot            = 32
0.00.051.129 I llm_load_print_meta: n_swa            = 0
0.00.051.129 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.129 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.130 I llm_load_print_meta: n_gqa            = 1
0.00.051.131 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.131 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.132 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.132 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.132 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.133 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.133 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.133 I llm_load_print_meta: n_ff             = 8192
0.00.051.134 I llm_load_print_meta: n_expert         = 0
0.00.051.134 I llm_load_print_meta: n_expert_used    = 0
0.00.051.134 I llm_load_print_meta: causal attn      = 1
0.00.051.134 I llm_load_print_meta: pooling type     = 0
0.00.051.134 I llm_load_print_meta: rope type        = 2
0.00.051.134 I llm_load_print_meta: rope scaling     = linear
0.00.051.137 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.137 I llm_load_print_meta: freq_scale_train = 1
0.00.051.137 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.138 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.138 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.138 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.138 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.138 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.138 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.139 I llm_load_print_meta: model type       = 1.4B
0.00.051.139 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.139 I llm_load_print_meta: model params     = 1.41 B
0.00.051.140 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.140 I llm_load_print_meta: general.name     = 1.4B
0.00.051.140 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.141 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.141 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.141 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.141 I llm_load_print_meta: LF token         = 128 ''
0.00.051.142 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.142 I llm_load_print_meta: max token length = 1024
0.00.052.865 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.865 I llm_load_tensors: offloading output layer to GPU
0.00.052.865 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.876 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.877 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.723 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.723 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.724 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.724 I llama_new_context_with_model: n_batch       = 2048
0.00.053.724 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.724 I llama_new_context_with_model: flash_attn    = 0
0.00.053.724 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.725 I llama_new_context_with_model: freq_scale    = 1
0.00.053.725 I ggml_metal_init: allocating
0.00.053.728 I ggml_metal_init: found device: Apple M4
0.00.053.730 I ggml_metal_init: picking default device: Apple M4
0.00.054.287 I ggml_metal_init: using embedded metal library
0.00.056.620 I ggml_metal_init: GPU name:   Apple M4
0.00.056.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.623 I ggml_metal_init: simdgroup reduction   = true
0.00.056.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.623 I ggml_metal_init: has bfloat            = true
0.00.056.623 I ggml_metal_init: use bfloat            = true
0.00.056.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.940 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.468 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.475 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.493 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.504 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.505 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.505 I llama_new_context_with_model: graph nodes  = 967
0.00.086.506 I llama_new_context_with_model: graph splits = 2
0.00.086.509 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.650 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.656 I main: llama threadpool init, n_threads = 4
0.00.753.695 I 
0.00.753.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.719 I 
0.00.753.880 I sampler seed: 1234
0.00.753.886 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.912 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.914 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.598.162 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.598.163 I llama_perf_context_print:        load time =     743.20 ms
0.01.598.164 I llama_perf_context_print: prompt eval time =      52.00 ms /     7 tokens (    7.43 ms per token,   134.62 tokens per second)
0.01.598.164 I llama_perf_context_print:        eval time =     789.23 ms /    63 runs   (   12.53 ms per token,    79.82 tokens per second)
0.01.598.165 I llama_perf_context_print:       total time =     844.51 ms /    70 tokens
0.01.598.425 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.922 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.490 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.496 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.497 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.799 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.801 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.801 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.801 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.802 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.802 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.803 I llama_model_loader: - type  f32:  194 tensors
0.00.023.803 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.803 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.247 I llm_load_vocab: special tokens cache size = 25
0.00.049.961 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.963 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.963 I llm_load_print_meta: arch             = gptneox
0.00.049.964 I llm_load_print_meta: vocab type       = BPE
0.00.049.964 I llm_load_print_meta: n_vocab          = 50304
0.00.049.964 I llm_load_print_meta: n_merges         = 50009
0.00.049.964 I llm_load_print_meta: vocab_only       = 0
0.00.049.965 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.965 I llm_load_print_meta: n_embd           = 2048
0.00.049.965 I llm_load_print_meta: n_layer          = 24
0.00.049.967 I llm_load_print_meta: n_head           = 16
0.00.049.968 I llm_load_print_meta: n_head_kv        = 16
0.00.049.969 I llm_load_print_meta: n_rot            = 32
0.00.049.969 I llm_load_print_meta: n_swa            = 0
0.00.049.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.970 I llm_load_print_meta: n_gqa            = 1
0.00.049.971 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.973 I llm_load_print_meta: n_ff             = 8192
0.00.049.974 I llm_load_print_meta: n_expert         = 0
0.00.049.976 I llm_load_print_meta: n_expert_used    = 0
0.00.049.976 I llm_load_print_meta: causal attn      = 1
0.00.049.976 I llm_load_print_meta: pooling type     = 0
0.00.049.976 I llm_load_print_meta: rope type        = 2
0.00.049.977 I llm_load_print_meta: rope scaling     = linear
0.00.049.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.977 I llm_load_print_meta: freq_scale_train = 1
0.00.049.978 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.984 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.986 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.987 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.987 I llm_load_print_meta: model type       = 1.4B
0.00.049.987 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.988 I llm_load_print_meta: model params     = 1.41 B
0.00.049.988 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.988 I llm_load_print_meta: general.name     = 1.4B
0.00.049.989 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.989 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.989 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.989 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: LF token         = 128 ''
0.00.049.990 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.991 I llm_load_print_meta: max token length = 1024
0.00.051.705 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.705 I llm_load_tensors: offloading output layer to GPU
0.00.051.705 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.715 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.716 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.547 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.547 I llama_new_context_with_model: n_ctx         = 128
0.00.052.548 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.548 I llama_new_context_with_model: n_batch       = 128
0.00.052.548 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.548 I llama_new_context_with_model: flash_attn    = 0
0.00.052.548 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.549 I llama_new_context_with_model: freq_scale    = 1
0.00.052.549 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.549 I ggml_metal_init: allocating
0.00.052.552 I ggml_metal_init: found device: Apple M4
0.00.052.554 I ggml_metal_init: picking default device: Apple M4
0.00.053.158 I ggml_metal_init: using embedded metal library
0.00.055.505 I ggml_metal_init: GPU name:   Apple M4
0.00.055.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.508 I ggml_metal_init: simdgroup reduction   = true
0.00.055.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.508 I ggml_metal_init: has bfloat            = true
0.00.055.509 I ggml_metal_init: use bfloat            = true
0.00.055.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.900 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.301 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.306 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.319 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.158 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.159 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.159 I llama_new_context_with_model: graph nodes  = 967
0.00.067.159 I llama_new_context_with_model: graph splits = 2
0.00.067.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.617 I 
0.00.691.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.652 I perplexity: tokenizing the input ..
0.00.699.411 I perplexity: tokenization took 7.758 ms
0.00.699.417 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.932 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.841.044 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.841.060 I llama_perf_context_print:        load time =     681.69 ms
0.00.841.062 I llama_perf_context_print: prompt eval time =     140.30 ms /   128 tokens (    1.10 ms per token,   912.35 tokens per second)
0.00.841.063 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.064 I llama_perf_context_print:       total time =     149.44 ms /   129 tokens
0.00.841.428 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.078s
sys	0m0.144s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.523 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.661 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.665 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.668 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.668 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.671 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.198 I llama_model_loader: - type  f32:  194 tensors
0.00.023.199 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.003 I llm_load_vocab: special tokens cache size = 25
0.00.048.901 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.904 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.904 I llm_load_print_meta: arch             = gptneox
0.00.048.905 I llm_load_print_meta: vocab type       = BPE
0.00.048.905 I llm_load_print_meta: n_vocab          = 50304
0.00.048.905 I llm_load_print_meta: n_merges         = 50009
0.00.048.905 I llm_load_print_meta: vocab_only       = 0
0.00.048.906 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.906 I llm_load_print_meta: n_embd           = 2048
0.00.048.906 I llm_load_print_meta: n_layer          = 24
0.00.048.908 I llm_load_print_meta: n_head           = 16
0.00.048.909 I llm_load_print_meta: n_head_kv        = 16
0.00.048.909 I llm_load_print_meta: n_rot            = 32
0.00.048.909 I llm_load_print_meta: n_swa            = 0
0.00.048.909 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.910 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.910 I llm_load_print_meta: n_gqa            = 1
0.00.048.911 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.912 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.912 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.913 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.913 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.914 I llm_load_print_meta: n_ff             = 8192
0.00.048.914 I llm_load_print_meta: n_expert         = 0
0.00.048.914 I llm_load_print_meta: n_expert_used    = 0
0.00.048.914 I llm_load_print_meta: causal attn      = 1
0.00.048.916 I llm_load_print_meta: pooling type     = 0
0.00.048.918 I llm_load_print_meta: rope type        = 2
0.00.048.918 I llm_load_print_meta: rope scaling     = linear
0.00.048.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.919 I llm_load_print_meta: freq_scale_train = 1
0.00.048.919 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.919 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.920 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.920 I llm_load_print_meta: model type       = 1.4B
0.00.048.920 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.921 I llm_load_print_meta: model params     = 1.41 B
0.00.048.921 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.921 I llm_load_print_meta: general.name     = 1.4B
0.00.048.922 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.922 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.922 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.923 I llm_load_print_meta: LF token         = 128 ''
0.00.048.923 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.923 I llm_load_print_meta: max token length = 1024
0.00.050.682 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.682 I llm_load_tensors: offloading output layer to GPU
0.00.050.682 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.692 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.693 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.545 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.546 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.546 I llama_new_context_with_model: n_batch       = 2048
0.00.051.546 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.547 I llama_new_context_with_model: flash_attn    = 0
0.00.051.547 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.547 I llama_new_context_with_model: freq_scale    = 1
0.00.051.548 I ggml_metal_init: allocating
0.00.051.551 I ggml_metal_init: found device: Apple M4
0.00.051.553 I ggml_metal_init: picking default device: Apple M4
0.00.052.128 I ggml_metal_init: using embedded metal library
0.00.054.465 I ggml_metal_init: GPU name:   Apple M4
0.00.054.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.467 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.467 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.467 I ggml_metal_init: simdgroup reduction   = true
0.00.054.468 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.468 I ggml_metal_init: has bfloat            = true
0.00.054.468 I ggml_metal_init: use bfloat            = true
0.00.054.468 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.469 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.826 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.899 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.909 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.941 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.941 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.942 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.943 I llama_new_context_with_model: graph nodes  = 967
0.00.083.943 I llama_new_context_with_model: graph splits = 2
0.00.083.945 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.046 I main: llama threadpool init, n_threads = 4
0.00.812.089 I 
0.00.812.133 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.136 I 
0.00.812.288 I sampler seed: 1234
0.00.812.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.312 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.313 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.313 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.679.393 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.679.394 I llama_perf_context_print:        load time =     803.52 ms
0.01.679.395 I llama_perf_context_print: prompt eval time =      54.88 ms /     7 tokens (    7.84 ms per token,   127.54 tokens per second)
0.01.679.396 I llama_perf_context_print:        eval time =     809.14 ms /    63 runs   (   12.84 ms per token,    77.86 tokens per second)
0.01.679.396 I llama_perf_context_print:       total time =     867.35 ms /    70 tokens
0.01.679.637 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4410 (dc32e8f0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.483 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.067 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.556 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.558 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.559 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.559 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.559 I llama_model_loader: - type  f32:  194 tensors
0.00.022.559 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.073 I llm_load_vocab: special tokens cache size = 25
0.00.047.724 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.047.727 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.047.728 I llm_load_print_meta: arch             = gptneox
0.00.047.728 I llm_load_print_meta: vocab type       = BPE
0.00.047.728 I llm_load_print_meta: n_vocab          = 50304
0.00.047.728 I llm_load_print_meta: n_merges         = 50009
0.00.047.729 I llm_load_print_meta: vocab_only       = 0
0.00.047.729 I llm_load_print_meta: n_ctx_train      = 2048
0.00.047.729 I llm_load_print_meta: n_embd           = 2048
0.00.047.729 I llm_load_print_meta: n_layer          = 24
0.00.047.732 I llm_load_print_meta: n_head           = 16
0.00.047.733 I llm_load_print_meta: n_head_kv        = 16
0.00.047.733 I llm_load_print_meta: n_rot            = 32
0.00.047.733 I llm_load_print_meta: n_swa            = 0
0.00.047.733 I llm_load_print_meta: n_embd_head_k    = 128
0.00.047.734 I llm_load_print_meta: n_embd_head_v    = 128
0.00.047.734 I llm_load_print_meta: n_gqa            = 1
0.00.047.735 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.047.736 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.047.736 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.047.737 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.047.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.047.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.047.737 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.047.738 I llm_load_print_meta: n_ff             = 8192
0.00.047.738 I llm_load_print_meta: n_expert         = 0
0.00.047.738 I llm_load_print_meta: n_expert_used    = 0
0.00.047.738 I llm_load_print_meta: causal attn      = 1
0.00.047.740 I llm_load_print_meta: pooling type     = 0
0.00.047.740 I llm_load_print_meta: rope type        = 2
0.00.047.741 I llm_load_print_meta: rope scaling     = linear
0.00.047.741 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.047.741 I llm_load_print_meta: freq_scale_train = 1
0.00.047.742 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.047.742 I llm_load_print_meta: rope_finetuned   = unknown
0.00.047.743 I llm_load_print_meta: ssm_d_conv       = 0
0.00.047.744 I llm_load_print_meta: ssm_d_inner      = 0
0.00.047.744 I llm_load_print_meta: ssm_d_state      = 0
0.00.047.744 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.047.745 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.047.745 I llm_load_print_meta: model type       = 1.4B
0.00.047.745 I llm_load_print_meta: model ftype      = Q6_K
0.00.047.746 I llm_load_print_meta: model params     = 1.41 B
0.00.047.746 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.047.746 I llm_load_print_meta: general.name     = 1.4B
0.00.047.747 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.047.747 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.047.747 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.047.748 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.047.748 I llm_load_print_meta: LF token         = 128 ''
0.00.047.751 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.047.751 I llm_load_print_meta: max token length = 1024
0.00.049.444 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.049.444 I llm_load_tensors: offloading output layer to GPU
0.00.049.444 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.049.454 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.049.455 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.050.276 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.277 I llama_new_context_with_model: n_ctx         = 128
0.00.050.277 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.277 I llama_new_context_with_model: n_batch       = 128
0.00.050.277 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.278 I llama_new_context_with_model: flash_attn    = 0
0.00.050.278 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.278 I llama_new_context_with_model: freq_scale    = 1
0.00.050.278 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.279 I ggml_metal_init: allocating
0.00.050.282 I ggml_metal_init: found device: Apple M4
0.00.050.283 I ggml_metal_init: picking default device: Apple M4
0.00.050.843 I ggml_metal_init: using embedded metal library
0.00.053.183 I ggml_metal_init: GPU name:   Apple M4
0.00.053.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.185 I ggml_metal_init: simdgroup reduction   = true
0.00.053.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.186 I ggml_metal_init: has bfloat            = true
0.00.053.186 I ggml_metal_init: use bfloat            = true
0.00.053.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.605 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.892 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.894 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.907 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.820 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.821 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.821 I llama_new_context_with_model: graph nodes  = 967
0.00.064.822 I llama_new_context_with_model: graph splits = 2
0.00.064.822 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.335 I 
0.00.565.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.380 I perplexity: tokenizing the input ..
0.00.572.577 I perplexity: tokenization took 7.196 ms
0.00.572.581 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.712.491 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.713.627 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.713.645 I llama_perf_context_print:        load time =     556.85 ms
0.00.713.645 I llama_perf_context_print: prompt eval time =     139.69 ms /   128 tokens (    1.09 ms per token,   916.31 tokens per second)
0.00.713.646 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.646 I llama_perf_context_print:       total time =     148.31 ms /   129 tokens
0.00.714.131 I ggml_metal_free: deallocating

real	0m0.727s
user	0m0.077s
sys	0m0.137s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4410 (dc32e8f0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141c0a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141c0adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141c0b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141c0b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141c0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141c0c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141c0ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141c0d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141c0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141c0dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141c0dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141c0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141c0efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141c0f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141c0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141c106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141c10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141c11500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141c11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141c123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141c12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141c13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141c13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141c141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141c14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141c14bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141c151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141c15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141c16390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141c16650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141c16af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141c16db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141c17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141c17b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141c17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141c182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141c18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141c18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141c190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141c19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141c19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141c19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141c1a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141c1a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141c1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141c1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141c1b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141c1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141c1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141c1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141c1d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141c1d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141c1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141c1e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141c1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141c1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141c1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141c1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141c1fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141c20630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141c208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141c20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141c21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141c216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141c21b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141c22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141c224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141c22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141c22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141c23290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141c23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141c23bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141c24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141c245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141c24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141c25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141c255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141c25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141c26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141c265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141c26af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141c27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141c27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141c27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141c28030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141c28580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141c28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141c29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141c29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141c29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141c2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141c2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141c2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141c2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141c2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141c2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141c2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141c1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141c2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141c2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141c2d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141c2d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141c2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141c2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141c2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141c2ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141c2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141c2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141c2fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141c30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141c30680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141c30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141c31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141c315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141c31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141c31f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141c323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141c32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141c32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141c33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141c33620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141c33ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141c33f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141c34400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141c348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141c34d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141c351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141c35680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141c35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141c35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141c36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141c36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141c36da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141c37240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141c376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141c37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141c38020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141c384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141c38960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141c38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141c392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141c39740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141c39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141c3a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141c3a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141c3a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141c3ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141c3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141c3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141c3bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141c3c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141c3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141c3ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141c3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141c3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141c3d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141c3dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141c3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141c3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141c3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141c3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141c3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141c3f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141c3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141c401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141c40640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141c40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141c40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141c41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141c418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141c41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141c42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141c426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141c42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141c42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141c43480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141c43920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141c43dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141c44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141c44700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141c44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141c45040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141c454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141c45980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141c45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141c462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141c46760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141c46c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141c470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141c47540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141c479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141c47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141c48320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141c48870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141c48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141c49310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141c49860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141c49b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141c4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141c4a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141c4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141c4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141c4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141c4bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141c4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141c4c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141c4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141c4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141c4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141c4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141c4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141c4eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141c4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141c4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141c4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141c500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141c50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141c50b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141c510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141c51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141c51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141c520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141c52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141c52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141c530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141c535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141c53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141c54090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141c545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141c54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141c55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141c555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141c55b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141c56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141c565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141c56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141c57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141c575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141c57b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141c58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141c585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141c58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141c59040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141c59590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141c59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141c5a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141c5a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141c5aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141c5b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141c5b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141c5bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141c5c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141c5c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141c5cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141c5d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141c5d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141c5daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141c5dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141c5e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141c5ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141c5efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141c5f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141c5fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141c5ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141c60520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141c60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141c60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141c61460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141c61900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141c61da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141c62240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141c626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141c62b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141c63020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141c634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141c63960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141c63e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141c642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141c64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141c64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141c65080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141c65520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141c65a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141c66190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141c668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141c66fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141c676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141c679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141c681a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141c68460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141c68a70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.183.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.183.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141d04e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141d052a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141d05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141d05b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141d05ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141d06460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141d068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141d06d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141d071b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141d07620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141d07a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141d08150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141d08c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141d09420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141d09c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141d0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141d0aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141d0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141d0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141d0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141d0c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141d0cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141d0d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141d0dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141d0e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141d0e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141d0e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141d0ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141d0f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141d0f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141d0fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141d10090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141d10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141d107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141d10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141d110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141d11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141d11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141d11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141d12260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141d126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141d12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141d12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141d13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141d13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141d13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141d14170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141d145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141d14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141d14ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141d15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141d157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141d15c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141d16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141d164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141d16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141d16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141d173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141d17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141d17cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141d18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141d18590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141d18a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141d18e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141d192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141d19750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141d19bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141d1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141d1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141d1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141d1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141d1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141d1b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141d1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141d1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141d1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141d1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141d1cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141d1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141d1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141d1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141d1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141d1e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141d1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141d1eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141d1f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141d1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141d1f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141d1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141d201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141d20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141d20ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141d20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141d21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141d21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141d21c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141d220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141d22550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141d229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141d22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141d232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141d23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141d23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141d23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141d24460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141d248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141d24d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141d251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141d25620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141d25a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141d25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141d26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141d267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141d26c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141d270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141d27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141d279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141d27e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141d28280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141d286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141d28b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141d28fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141d29440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141d298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141d29d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141d2a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141d2a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141d2aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141d2aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141d2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141d2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141d2bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141d2c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141d2c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141d2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141d2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141d2d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141d2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141d2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141d2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141d2e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141d2e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141d2ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141d2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141d2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141d2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141d2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141d30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141d307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141d30c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141d31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141d314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141d31960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141d31dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141d32240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141d326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141d32b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141d32f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141d33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141d33870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141d33ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141d34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141d345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141d34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141d34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141d35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141d35f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141d36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141d364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141d36930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141d36da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141d37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141d37680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141d37af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141d37f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141d383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141d38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141d38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141d39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141d39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141d39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141d39e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141d3a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141d3a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141d3abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141d3b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141d3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141d3b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141d3bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141d3c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141d3c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141d3cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141d3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141d3d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141d3d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141d3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141d3e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141d3e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141d3e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141d3ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141d3f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141d3f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141d3fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141d401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141d40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141d40a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141d40ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141d41360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141d41880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141d41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141d42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141d42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141d43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141d43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141d43d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141d442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141d44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141d44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141d45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141d459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141d45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141d46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141d46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141d470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141d47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141d47c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141d48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141d487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141d48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141d49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141d49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141d49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141d4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141d4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141d4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141d4b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141d4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141d4c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141d4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141d4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141d4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141d4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141d4de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141d4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141d4e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141d4ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141d4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141d4fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141d50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141d50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141d50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141d511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141d51780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141d51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141d52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141d528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141d52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141d53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141d53a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141d53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141d54580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141d54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141d55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141d556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141d55c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141d56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141d56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141d56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141d572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141d577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141d57cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141d581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141d586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141d58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141d590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141d595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141d59ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141d59fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141d5a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141d5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141d5aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141d5b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141d5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141d5c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141d5c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141d5d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141d5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141d5daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141d5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141d5e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141d5ebb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141d5bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141d4c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141d4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141d484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141d45c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141d553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141d52b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141d50900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141d4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141d46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141d43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141d49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141d4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141d4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141d4c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141d54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141d46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141d4f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141d49bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141d42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141d4d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141d48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141d53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141d4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141d43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141d456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141d55f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141d4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141d53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141d49600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141d4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141d4fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141d4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141d47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141d51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141d46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141d54840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141d52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141d4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141d56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141d45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141d56500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141d44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141d54e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141d4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141d50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141d53cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141d525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141d4a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141d42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141d049d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141d5ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141d07d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141d5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141d5f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141d5f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141d5fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141d5fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141d60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141d60310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141d605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141d60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141d60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141d60e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141d610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141d61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141d61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141d61910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141d61bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141d61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141d62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141d62410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141d626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141d62990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141d62c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141d62f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141d631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141d63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141d63750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141d63a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141d63cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141d63f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141d64250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141d64510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141d647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141d64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141d64d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141d65010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141d652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141d65590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141d65850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141d65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141d65dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141d66090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141d66350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141d66610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141d668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141d66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141d66e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141d67110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141d673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141d67690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141d67950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141d67c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141d67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141d68190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141d68450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141d68710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141d689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141d68c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141d68f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141d69210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141d694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141d69790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141d69a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141d69d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141d69fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141d6a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141d6a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141d6a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141d6aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141d6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141d6b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141d6b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141d6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141d6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141d6bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141d6be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141d6c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141d6c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141d6c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141d6c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141d6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141d6ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141d6d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141d6d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141d6d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141d6d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141d6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141d6df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141d6e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141d6e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141d6e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141d6ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141d6ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141d6ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141d6f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141d6f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141d6f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141d6fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141d6fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141d70010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141d702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141d70590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141d70850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141d70b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141d70dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141d71090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141d71350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141d71610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141d718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141d71b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141d71e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141d72110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141d723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141d72690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141d72950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141d72c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141d72ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141d73190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141d73450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141d73710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141d739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141d73c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141d73f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141d74210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141d744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141d74790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141d74a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141d74d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141d74fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141d75290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141d75550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141d75810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141d75ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141d75d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141d76050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141d76310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141d765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141d76890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141d76b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141d76e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141d770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141d77390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141d77650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141d77910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141d77bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141d77e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141d78150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141d78410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141d786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141d78990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141d78c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141d78f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141d791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141d79490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141d79750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141d79a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141d79cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141d79f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141d7a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141d7a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141d7aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141d7ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141d7b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141d7b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141d7b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141d7b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141d7bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141d7c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141d7c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141d7cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141d7d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141d7d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141d7ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141d7e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141d7e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141d7edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141d7f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141d7f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141d7fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141d80300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141d80850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141d80da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141d812f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141d81840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141d81d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141d822e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141d82830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141d82d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141d832d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141d83820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141d83d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141d842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141d84810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141d84d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141d852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141d85800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141d85d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141d862a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141d867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141d86d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141d87290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141d877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141d87d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141d88280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141d887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141d88d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141d89270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141d897c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141d89d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141d8a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141d8a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141d8ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141d8b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141d8b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141d8bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141d8bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141d8c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141d8c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141d8cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141d8d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141d8d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141d8db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141d8e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141d8e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141d8ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141d8ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141d8f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141d8f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141d8fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141d90370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141d90870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141d91280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141d919a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141d920c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141d927e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141d92aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141d93290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141d93550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141d93b60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.873s
user	0m0.310s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4410 (dc32e8f0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14660a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14660a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14660aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14660b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14660ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14660bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14660c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14660cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14660d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14660d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14660daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14660dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14660eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14660f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14660fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1466101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146611f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146613480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1466168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146617170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1466176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1466182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146619090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1466199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14661a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14661a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14661abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14661b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14661bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14661c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14661c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14661cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14661d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14661d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14661df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14661e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14661ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14661f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14661f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14661f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146620160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1466208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146620d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1466216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146623260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1466240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1466250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1466260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1466270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146627610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1466280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1466290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1466295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146629b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14662a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14662a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14662ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14662b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14662b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14662bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14661b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14662bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14662c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14662cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14662d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14662d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14662dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14662e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14662e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14662ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14662f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14662f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14662fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1466301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1466310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1466335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1466343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1466351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1466368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1466376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14663a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14663a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14663a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14663ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14663b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14663b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14663bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14663c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14663c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14663c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14663ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14663d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14663d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14663dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14663e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14663e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14663ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14663eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14663f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14663f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14663fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1466413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1466421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1466438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1466446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146644b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1466454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146645950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146645df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1466479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1466483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1466488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14664a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14664a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14664b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14664b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14664b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14664bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14664c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14664cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14664d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14664d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14664d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14664e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14664e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14664ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14664f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14664f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14664fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146650150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1466506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146650bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146651be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146652130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146654bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1466560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146656b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1466570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1466580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146658620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146658b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1466590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146659610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14665a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14665a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14665ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14665b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14665b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14665bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14665c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14665c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14665cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14665d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14665d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14665db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14665e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14665e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14665eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14665f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14665f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14665fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146660050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1466605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1466618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146662210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1466626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146662b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146662ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146663930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146664270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146664710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1466655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146665cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1466663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1466674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1466685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.278 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156707e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1567082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156708760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156708bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156709040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1567094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156709920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156709d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15670a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15670a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15670ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15670b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15670bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15670c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15670cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15670d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15670db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15670e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15670e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15670f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15670f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15670ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156710690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156710db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1567114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156711790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156711a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156711ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156712330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1567127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156712c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156713140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1567135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156713870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156714150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1567145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156714a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156715310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156715780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156715bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156716060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1567164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156716940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156716db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156717220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156717690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156717b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156717f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1567183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156718850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156718cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156719130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1567195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156719a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15671a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15671a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15671ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15671b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15671b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15671bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15671bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15671c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15671c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15671cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15671d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15671d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15671d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15671de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15671e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15671e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15671eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15671eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15671f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15671f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15671fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1567201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156720a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156720f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156721370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1567217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156721c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1567220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156722530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1567229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156722e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156723280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1567236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156723fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1567248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156724d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156725190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156725600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156725a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156725ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156726350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1567267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1567270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156727510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156727980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156727df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156728260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1567286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156728b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156728fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156729420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156729890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156729d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15672a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15672a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15672aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15672aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15672b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15672b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15672bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15672c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15672c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15672c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15672cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15672d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15672d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15672db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15672df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15672e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15672e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15672ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15672f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15672f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15672fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15672fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156730310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156730780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156730bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156731060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1567314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156731940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156731db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156732220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156732690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156732b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156732f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1567333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156733850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156733cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156734130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1567345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156734a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156734e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1567352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156735760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156735bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156736040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1567364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156736920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156736d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156737200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156737670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156737f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1567383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156738ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1567392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156739570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1567399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156739e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15673a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15673a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15673aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15673b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15673b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15673b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15673bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15673c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15673c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15673cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15673cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15673d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15673d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15673dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15673e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15673e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15673e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15673ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15673f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15673f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15673fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15673fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156740460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1567408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156740d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1567411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156741620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156741a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156741f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156742370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1567427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156742d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156743250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1567436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156743b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156743fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156744410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156744930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156744e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1567459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156745c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156746230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1567467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156746db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156747370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156747930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156747ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1567484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156748a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156749030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1567495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156749bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15674a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15674a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15674acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15674b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15674b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15674be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15674c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15674c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15674cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15674d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15674daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15674e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15674e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15674ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15674f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15674f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15674fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156750330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1567508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156750eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156751470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156751a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156751ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1567525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156752b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156753130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1567536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156753cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156754270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156754830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156754df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1567553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156755970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156755f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1567564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156756ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156757070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1567581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156758770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156758d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1567592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1567598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15675a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15675a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15675ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15675b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15675b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15675bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15675c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15675c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15675cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15675d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15675d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15675da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15675df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15675e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15675e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15675f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15675faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1567601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1567608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156760ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156761650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156761c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1580044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1580056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1580063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158007820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158008340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158008af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158009300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158009a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15800a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15800a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15800af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15800b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15800be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15800c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15800ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15800d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15800daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15800ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15800e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15800e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15800e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15800edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15800f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15800f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15800fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15800fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158010300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158010770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158010be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158011050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1580114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158011930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158011da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158012210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158012680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158012af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158012f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1580133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158013840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158013cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158014120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158014590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158014a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158014e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1580152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158015750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158015bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158016030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1580165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158016aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158016f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158017380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1580177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158017c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1580180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158018540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1580189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158018e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158019290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158019b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158019fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15801a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15801a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15801ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15801b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15801b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15801ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15801bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15801c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15801c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15801cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15801d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15801d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15801d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15801de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15801e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15801e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15801eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15801efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15801f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15801f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15801fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158020180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1580205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158020a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158020ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158021340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1580217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158021c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158022090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158022500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158022970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158022de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158023250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158023ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158024210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158024680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158024f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1580253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158025840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158026a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1580272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158027750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158027bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158028030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1580284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158028910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1580291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158029660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158029ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15802a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15802a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15802ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15802b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15802b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15802b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15802be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15802c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15802c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15802cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15802d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15802d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15802d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15802dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15802e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15802e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15802eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15802ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15802f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15802f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15802fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1580300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158030550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1580309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1580312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158031710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158031ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1580328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158032d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1580331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158033620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158034370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1580347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158034c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1580350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158035530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1580359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158035e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158036280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1580366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158036b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158036fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158037440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1580378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158037d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158038190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158038600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158038a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158039350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1580397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158039c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15803a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15803a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15803a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15803adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15803b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15803b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15803bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15803bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15803c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15803c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15803cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15803d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15803d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15803da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15803dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15803e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15803e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15803ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15803f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15803f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15803f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15803fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158040240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1580406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158040b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158040f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158041b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158041dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158042090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158042500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158042970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158042de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158043250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1580436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158043b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158043fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158044410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158044880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158044cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158045160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1580455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158045a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158045eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158046320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158046790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158046c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158047070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1580474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158047950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158047dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158048230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1580486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158048b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158048f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1580493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158049860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158049cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15804a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15804a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15804aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15804ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15804b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15804b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15804bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15804c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15804c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15804c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15804cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15804d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15804d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15804daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15804df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15804e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15804e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15804ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15804f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15804f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15804fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15804fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1580502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158050750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158050bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158051030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1580514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158051910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158051d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1580521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158052660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158052ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158052f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1580533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158053820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158054100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158054570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1580549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158054e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1580552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158055730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1580561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1580568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158056fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158057700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1580579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158057e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158058430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158058a40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.896s
user	0m0.243s
sys	0m0.118s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.15 user         0.04 sys
```
