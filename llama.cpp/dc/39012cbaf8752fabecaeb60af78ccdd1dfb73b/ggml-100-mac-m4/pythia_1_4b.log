Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:43 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:106 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.305s
user	0m0.543s
sys	0m0.778s
++ nproc
+ make -j10
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  8%] Built target build_info
[  8%] Built target sha1
[  8%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  8%] Built target sha256
[  8%] Built target xxhash
[  9%] Linking CXX shared library libggml-base.dylib
[  9%] Built target ggml-base
[ 10%] Generate assembly for embedded Metal library
Embedding Metal library
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 13%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX shared library libllama.dylib
[ 23%] Built target llama-gguf
[ 23%] Built target llama
[ 23%] Built target llama-gguf-hash
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Built target llava
[ 32%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Built target test-c
[ 33%] Built target llama-simple
[ 33%] Built target llama-simple-chat
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llava_static
[ 33%] Built target common
[ 33%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-log
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Linking CXX executable ../bin/test-quantize-fns
[ 44%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-quantize-perf
[ 44%] Linking CXX executable ../bin/test-arg-parser
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-log
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Built target test-sampling
[ 46%] Built target test-quantize-perf
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 47%] Built target test-quantize-fns
[ 47%] Built target test-arg-parser
[ 47%] Built target test-chat-template
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Linking CXX executable ../bin/test-llama-grammar
[ 53%] Linking CXX executable ../bin/test-grammar-integration
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Linking CXX executable ../bin/test-barrier
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Built target test-grammar-parser
[ 57%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 57%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-rope
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Built target test-llama-grammar
[ 60%] Linking CXX executable ../../bin/llama-cvector-generator
[ 60%] Built target test-grammar-integration
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Built target test-barrier
[ 61%] Built target test-backend-ops
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 63%] Linking CXX executable ../bin/test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-rope
[ 65%] Built target llama-cvector-generator
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target test-model-load-cancel
[ 66%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 66%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target test-autorelease
[ 67%] Built target test-json-schema-to-grammar
[ 68%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-export-lora
[ 69%] Built target llama-batched
[ 69%] Built target llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Built target llama-convert-llama2c-to-ggml
[ 74%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 75%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 75%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 77%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 77%] Built target llama-gbnf-validator
[ 77%] Built target llama-gguf-split
[ 77%] Built target llama-export-lora
[ 77%] Built target llama-gritlm
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-llava-cli
[ 78%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 79%] Built target llama-imatrix
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 81%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 81%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 81%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 81%] Built target llama-bench
[ 81%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Built target llama-minicpmv-cli
[ 83%] Linking CXX executable ../../bin/llama-lookup-create
[ 84%] Linking CXX executable ../../bin/llama-lookup-stats
[ 84%] Built target llama-llava-cli
[ 84%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 84%] Built target llama-lookahead
[ 85%] Linking CXX executable ../../bin/llama-parallel
[ 86%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 87%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 88%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-passkey
[ 89%] Built target llama-lookup-merge
[ 89%] Built target llama-lookup
[ 89%] Built target llama-lookup-stats
[ 89%] Built target llama-cli
[ 89%] Built target llama-lookup-create
[ 89%] Linking CXX executable ../../bin/llama-perplexity
[ 89%] Linking CXX executable ../../bin/llama-quantize
[ 89%] Generating loading.html.hpp
[ 89%] Built target llama-parallel
[ 90%] Linking CXX executable ../../bin/llama-retrieval
[ 90%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 92%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 92%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 93%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 93%] Generating completion.js.hpp
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Built target llama-passkey
[ 95%] Linking CXX executable ../../bin/llama-speculative
[ 95%] Built target llama-perplexity
[ 96%] Generating deps_daisyui.min.css.hpp
[ 96%] Linking CXX executable ../../bin/llama-tokenize
[ 96%] Linking CXX executable ../../bin/llama-vdot
[ 96%] Built target llama-quantize
[ 97%] Linking CXX executable ../../bin/llama-save-load-state
[ 97%] Generating deps_markdown-it.js.hpp
[ 98%] Generating deps_tailwindcss.js.hpp
[ 99%] Generating deps_vue.esm-browser.js.hpp
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-retrieval
[ 99%] Generating index.html.hpp
[ 99%] Built target llama-tokenize
[ 99%] Built target llama-vdot
[ 99%] Built target llama-speculative
[ 99%] Built target llama-save-load-state
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.567s
user	0m5.762s
sys	0m8.921s

main: quantize time =  5894.47 ms
main:    total time =  5894.47 ms

main: quantize time =  1811.40 ms
main:    total time =  1811.40 ms

main: quantize time =  1835.99 ms
main:    total time =  1835.99 ms

main: quantize time =  1810.14 ms
main:    total time =  1810.14 ms

main: quantize time =  1934.12 ms
main:    total time =  1934.12 ms

main: quantize time =  5319.50 ms
main:    total time =  5319.50 ms

main: quantize time =  5845.43 ms
main:    total time =  5845.43 ms

main: quantize time =  7008.76 ms
main:    total time =  7008.76 ms

main: quantize time =  5981.56 ms
main:    total time =  5981.56 ms

main: quantize time =  4554.54 ms
main:    total time =  4554.54 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.081 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.179 I main: llama backend init
0.00.000.185 I main: load the model and apply lora adapter, if any
0.00.024.508 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.648 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.693 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.694 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.698 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.173 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.155 I llama_model_loader: - type  f32:  194 tensors
0.00.052.155 I llama_model_loader: - type  f16:   98 tensors
0.00.077.006 I llm_load_vocab: special tokens cache size = 25
0.00.083.219 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.223 I llm_load_print_meta: arch             = gptneox
0.00.083.224 I llm_load_print_meta: vocab type       = BPE
0.00.083.224 I llm_load_print_meta: n_vocab          = 50304
0.00.083.224 I llm_load_print_meta: n_merges         = 50009
0.00.083.224 I llm_load_print_meta: vocab_only       = 0
0.00.083.224 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.225 I llm_load_print_meta: n_embd           = 2048
0.00.083.225 I llm_load_print_meta: n_layer          = 24
0.00.083.228 I llm_load_print_meta: n_head           = 16
0.00.083.229 I llm_load_print_meta: n_head_kv        = 16
0.00.083.229 I llm_load_print_meta: n_rot            = 32
0.00.083.229 I llm_load_print_meta: n_swa            = 0
0.00.083.229 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.230 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.230 I llm_load_print_meta: n_gqa            = 1
0.00.083.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.232 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.232 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.232 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.232 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.232 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.233 I llm_load_print_meta: n_ff             = 8192
0.00.083.233 I llm_load_print_meta: n_expert         = 0
0.00.083.233 I llm_load_print_meta: n_expert_used    = 0
0.00.083.234 I llm_load_print_meta: causal attn      = 1
0.00.083.234 I llm_load_print_meta: pooling type     = 0
0.00.083.234 I llm_load_print_meta: rope type        = 2
0.00.083.234 I llm_load_print_meta: rope scaling     = linear
0.00.083.234 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.235 I llm_load_print_meta: freq_scale_train = 1
0.00.083.235 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.235 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.235 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.235 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.236 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.236 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.236 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.249 I llm_load_print_meta: model type       = 1.4B
0.00.083.249 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.250 I llm_load_print_meta: model params     = 1.41 B
0.00.083.250 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.250 I llm_load_print_meta: general.name     = 1.4B
0.00.083.253 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.253 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.253 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.254 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.254 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.083.254 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.254 I llm_load_print_meta: max token length = 1024
0.00.085.700 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.701 I llm_load_tensors: offloading output layer to GPU
0.00.085.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.720 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.721 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.661 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.662 I llama_new_context_with_model: n_ctx         = 2048
0.00.086.663 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.086.663 I llama_new_context_with_model: n_batch       = 2048
0.00.086.663 I llama_new_context_with_model: n_ubatch      = 512
0.00.086.663 I llama_new_context_with_model: flash_attn    = 0
0.00.086.664 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.664 I llama_new_context_with_model: freq_scale    = 1
0.00.086.664 I ggml_metal_init: allocating
0.00.086.668 I ggml_metal_init: found device: Apple M4
0.00.086.670 I ggml_metal_init: picking default device: Apple M4
0.00.087.296 I ggml_metal_init: using embedded metal library
0.00.126.839 I ggml_metal_init: GPU name:   Apple M4
0.00.126.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.126.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.126.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.126.846 I ggml_metal_init: simdgroup reduction   = true
0.00.126.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.126.847 I ggml_metal_init: has bfloat            = true
0.00.126.847 I ggml_metal_init: use bfloat            = true
0.00.126.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.126.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.226.741 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.226.748 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.226.773 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.228.258 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.228.260 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.228.260 I llama_new_context_with_model: graph nodes  = 967
0.00.228.261 I llama_new_context_with_model: graph splits = 2
0.00.228.286 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.379.553 I main: llama threadpool init, n_threads = 4
0.00.379.591 I 
0.00.379.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.379.625 I 
0.00.379.904 I sampler seed: 1234
0.00.379.911 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.379.935 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.379.936 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.379.936 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.236.928 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.02.236.928 I llama_perf_context_print:        load time =     355.03 ms
0.02.236.929 I llama_perf_context_print: prompt eval time =      38.04 ms /     7 tokens (    5.43 ms per token,   184.01 tokens per second)
0.02.236.930 I llama_perf_context_print:        eval time =    1815.99 ms /    63 runs   (   28.83 ms per token,    34.69 tokens per second)
0.02.236.930 I llama_perf_context_print:       total time =    1857.38 ms /    70 tokens
0.02.237.135 I ggml_metal_free: deallocating

real	0m2.586s
user	0m0.142s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.091 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.073 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.078 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.083 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.015 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.225 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.226 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.226 I llama_model_loader: - type  f32:  194 tensors
0.00.035.227 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.910 I llm_load_vocab: special tokens cache size = 25
0.00.065.121 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.125 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.125 I llm_load_print_meta: arch             = gptneox
0.00.065.126 I llm_load_print_meta: vocab type       = BPE
0.00.065.126 I llm_load_print_meta: n_vocab          = 50304
0.00.065.126 I llm_load_print_meta: n_merges         = 50009
0.00.065.126 I llm_load_print_meta: vocab_only       = 0
0.00.065.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.128 I llm_load_print_meta: n_embd           = 2048
0.00.065.129 I llm_load_print_meta: n_layer          = 24
0.00.065.133 I llm_load_print_meta: n_head           = 16
0.00.065.134 I llm_load_print_meta: n_head_kv        = 16
0.00.065.134 I llm_load_print_meta: n_rot            = 32
0.00.065.134 I llm_load_print_meta: n_swa            = 0
0.00.065.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.135 I llm_load_print_meta: n_gqa            = 1
0.00.065.136 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.137 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.138 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.138 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.139 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.139 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.139 I llm_load_print_meta: n_ff             = 8192
0.00.065.139 I llm_load_print_meta: n_expert         = 0
0.00.065.140 I llm_load_print_meta: n_expert_used    = 0
0.00.065.140 I llm_load_print_meta: causal attn      = 1
0.00.065.140 I llm_load_print_meta: pooling type     = 0
0.00.065.140 I llm_load_print_meta: rope type        = 2
0.00.065.140 I llm_load_print_meta: rope scaling     = linear
0.00.065.141 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.141 I llm_load_print_meta: freq_scale_train = 1
0.00.065.141 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.142 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.143 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.143 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.143 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.143 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.157 I llm_load_print_meta: model type       = 1.4B
0.00.065.157 I llm_load_print_meta: model ftype      = Q8_0
0.00.065.157 I llm_load_print_meta: model params     = 1.41 B
0.00.065.158 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.065.158 I llm_load_print_meta: general.name     = 1.4B
0.00.065.158 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.158 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.158 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.159 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.159 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.159 I llm_load_print_meta: max token length = 1024
0.00.067.629 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.629 I llm_load_tensors: offloading output layer to GPU
0.00.067.630 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.640 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.641 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.679 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.680 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.680 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.680 I llama_new_context_with_model: n_batch       = 2048
0.00.068.680 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.680 I llama_new_context_with_model: flash_attn    = 0
0.00.068.681 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.681 I llama_new_context_with_model: freq_scale    = 1
0.00.068.682 I ggml_metal_init: allocating
0.00.068.684 I ggml_metal_init: found device: Apple M4
0.00.068.686 I ggml_metal_init: picking default device: Apple M4
0.00.069.377 I ggml_metal_init: using embedded metal library
0.00.071.639 I ggml_metal_init: GPU name:   Apple M4
0.00.071.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.641 I ggml_metal_init: simdgroup reduction   = true
0.00.071.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.642 I ggml_metal_init: has bfloat            = true
0.00.071.642 I ggml_metal_init: use bfloat            = true
0.00.071.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.890 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.901 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.925 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.085 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.086 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.087 I llama_new_context_with_model: graph nodes  = 967
0.00.107.087 I llama_new_context_with_model: graph splits = 2
0.00.107.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.462.805 I main: llama threadpool init, n_threads = 4
0.01.462.842 I 
0.01.462.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.462.859 I 
0.01.463.096 I sampler seed: 1234
0.01.463.100 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.463.111 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.463.111 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.463.111 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.560.971 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.560.972 I llama_perf_context_print:        load time =    1452.71 ms
0.02.560.972 I llama_perf_context_print: prompt eval time =      33.48 ms /     7 tokens (    4.78 ms per token,   209.09 tokens per second)
0.02.560.973 I llama_perf_context_print:        eval time =    1061.36 ms /    63 runs   (   16.85 ms per token,    59.36 tokens per second)
0.02.560.973 I llama_perf_context_print:       total time =    1098.17 ms /    70 tokens
0.02.561.132 I ggml_metal_free: deallocating

real	0m2.578s
user	0m0.113s
sys	0m0.300s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.018.524 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.684 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.213 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.213 I llama_model_loader: - type  f32:  194 tensors
0.00.044.214 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.214 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.580 I llm_load_vocab: special tokens cache size = 25
0.00.079.033 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.038 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.038 I llm_load_print_meta: arch             = gptneox
0.00.079.039 I llm_load_print_meta: vocab type       = BPE
0.00.079.039 I llm_load_print_meta: n_vocab          = 50304
0.00.079.039 I llm_load_print_meta: n_merges         = 50009
0.00.079.039 I llm_load_print_meta: vocab_only       = 0
0.00.079.043 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.043 I llm_load_print_meta: n_embd           = 2048
0.00.079.043 I llm_load_print_meta: n_layer          = 24
0.00.079.047 I llm_load_print_meta: n_head           = 16
0.00.079.048 I llm_load_print_meta: n_head_kv        = 16
0.00.079.049 I llm_load_print_meta: n_rot            = 32
0.00.079.049 I llm_load_print_meta: n_swa            = 0
0.00.079.049 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.049 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.050 I llm_load_print_meta: n_gqa            = 1
0.00.079.051 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.052 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.053 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.053 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.053 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.053 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.054 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.055 I llm_load_print_meta: n_ff             = 8192
0.00.079.055 I llm_load_print_meta: n_expert         = 0
0.00.079.055 I llm_load_print_meta: n_expert_used    = 0
0.00.079.055 I llm_load_print_meta: causal attn      = 1
0.00.079.055 I llm_load_print_meta: pooling type     = 0
0.00.079.056 I llm_load_print_meta: rope type        = 2
0.00.079.056 I llm_load_print_meta: rope scaling     = linear
0.00.079.056 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.057 I llm_load_print_meta: freq_scale_train = 1
0.00.079.057 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.057 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.057 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.058 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.058 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.058 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.058 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.071 I llm_load_print_meta: model type       = 1.4B
0.00.079.071 I llm_load_print_meta: model ftype      = Q4_0
0.00.079.072 I llm_load_print_meta: model params     = 1.41 B
0.00.079.073 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.079.073 I llm_load_print_meta: general.name     = 1.4B
0.00.079.073 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.073 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.074 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.074 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.074 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.074 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.075 I llm_load_print_meta: max token length = 1024
0.00.081.880 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.880 I llm_load_tensors: offloading output layer to GPU
0.00.081.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.892 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.894 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.083.639 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.640 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.641 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.641 I llama_new_context_with_model: n_batch       = 2048
0.00.083.641 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.642 I llama_new_context_with_model: flash_attn    = 0
0.00.083.642 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.643 I llama_new_context_with_model: freq_scale    = 1
0.00.083.643 I ggml_metal_init: allocating
0.00.083.654 I ggml_metal_init: found device: Apple M4
0.00.083.658 I ggml_metal_init: picking default device: Apple M4
0.00.084.638 I ggml_metal_init: using embedded metal library
0.00.088.273 I ggml_metal_init: GPU name:   Apple M4
0.00.088.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.278 I ggml_metal_init: simdgroup reduction   = true
0.00.088.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.278 I ggml_metal_init: has bfloat            = true
0.00.088.278 I ggml_metal_init: use bfloat            = true
0.00.088.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.130.804 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.816 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.839 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.978 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.979 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.979 I llama_new_context_with_model: graph nodes  = 967
0.00.131.980 I llama_new_context_with_model: graph splits = 2
0.00.131.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.596 I main: llama threadpool init, n_threads = 4
0.00.797.649 I 
0.00.797.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.797.678 I 
0.00.797.944 I sampler seed: 1234
0.00.797.957 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.986 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.988 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.989 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.473.603 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48730.27 tokens per second)
0.01.473.604 I llama_perf_context_print:        load time =     779.07 ms
0.01.473.605 I llama_perf_context_print: prompt eval time =      32.68 ms /     7 tokens (    4.67 ms per token,   214.20 tokens per second)
0.01.473.606 I llama_perf_context_print:        eval time =     640.27 ms /    63 runs   (   10.16 ms per token,    98.40 tokens per second)
0.01.473.606 I llama_perf_context_print:       total time =     676.01 ms /    70 tokens
0.01.473.810 I ggml_metal_free: deallocating

real	0m1.491s
user	0m0.125s
sys	0m0.203s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.403 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.254 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.258 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.865 I llama_model_loader: - type  f32:  194 tensors
0.00.029.865 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.561 I llm_load_vocab: special tokens cache size = 25
0.00.056.431 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.434 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.434 I llm_load_print_meta: arch             = gptneox
0.00.056.435 I llm_load_print_meta: vocab type       = BPE
0.00.056.435 I llm_load_print_meta: n_vocab          = 50304
0.00.056.435 I llm_load_print_meta: n_merges         = 50009
0.00.056.435 I llm_load_print_meta: vocab_only       = 0
0.00.056.436 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.436 I llm_load_print_meta: n_embd           = 2048
0.00.056.436 I llm_load_print_meta: n_layer          = 24
0.00.056.438 I llm_load_print_meta: n_head           = 16
0.00.056.445 I llm_load_print_meta: n_head_kv        = 16
0.00.056.447 I llm_load_print_meta: n_rot            = 32
0.00.056.447 I llm_load_print_meta: n_swa            = 0
0.00.056.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.452 I llm_load_print_meta: n_gqa            = 1
0.00.056.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.455 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.455 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.456 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.456 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.457 I llm_load_print_meta: n_ff             = 8192
0.00.056.457 I llm_load_print_meta: n_expert         = 0
0.00.056.457 I llm_load_print_meta: n_expert_used    = 0
0.00.056.457 I llm_load_print_meta: causal attn      = 1
0.00.056.457 I llm_load_print_meta: pooling type     = 0
0.00.056.457 I llm_load_print_meta: rope type        = 2
0.00.056.458 I llm_load_print_meta: rope scaling     = linear
0.00.056.458 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.458 I llm_load_print_meta: freq_scale_train = 1
0.00.056.458 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.459 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.459 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.459 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.459 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.460 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.460 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.471 I llm_load_print_meta: model type       = 1.4B
0.00.056.472 I llm_load_print_meta: model ftype      = Q4_1
0.00.056.472 I llm_load_print_meta: model params     = 1.41 B
0.00.056.473 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.056.473 I llm_load_print_meta: general.name     = 1.4B
0.00.056.474 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.474 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.474 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.475 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.475 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.475 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.476 I llm_load_print_meta: max token length = 1024
0.00.057.995 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.996 I llm_load_tensors: offloading output layer to GPU
0.00.057.996 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.005 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.006 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.058.873 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.874 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.874 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.874 I llama_new_context_with_model: n_batch       = 2048
0.00.058.874 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.874 I llama_new_context_with_model: flash_attn    = 0
0.00.058.875 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.875 I llama_new_context_with_model: freq_scale    = 1
0.00.058.876 I ggml_metal_init: allocating
0.00.058.879 I ggml_metal_init: found device: Apple M4
0.00.058.881 I ggml_metal_init: picking default device: Apple M4
0.00.059.417 I ggml_metal_init: using embedded metal library
0.00.061.332 I ggml_metal_init: GPU name:   Apple M4
0.00.061.334 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.335 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.335 I ggml_metal_init: simdgroup reduction   = true
0.00.061.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.335 I ggml_metal_init: has bfloat            = true
0.00.061.335 I ggml_metal_init: use bfloat            = true
0.00.061.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.337 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.660 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.668 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.690 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.772 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.773 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.774 I llama_new_context_with_model: graph nodes  = 967
0.00.089.774 I llama_new_context_with_model: graph splits = 2
0.00.089.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.527 I main: llama threadpool init, n_threads = 4
0.00.770.557 I 
0.00.770.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.770.586 I 
0.00.770.809 I sampler seed: 1234
0.00.770.815 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.863 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.864 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.865 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.497.265 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66293.18 tokens per second)
0.01.497.266 I llama_perf_context_print:        load time =     762.12 ms
0.01.497.266 I llama_perf_context_print: prompt eval time =      32.91 ms /     7 tokens (    4.70 ms per token,   212.69 tokens per second)
0.01.497.267 I llama_perf_context_print:        eval time =     690.66 ms /    63 runs   (   10.96 ms per token,    91.22 tokens per second)
0.01.497.270 I llama_perf_context_print:       total time =     726.74 ms /    70 tokens
0.01.497.477 I ggml_metal_free: deallocating

real	0m1.515s
user	0m0.108s
sys	0m0.191s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.020.281 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.030.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.326 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.329 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.329 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.329 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.330 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.331 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.331 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.333 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.333 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.333 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.041.829 I llama_model_loader: - type  f32:  194 tensors
0.00.041.830 I llama_model_loader: - type q5_0:   97 tensors
0.00.041.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.076.832 I llm_load_vocab: special tokens cache size = 25
0.00.086.982 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.986 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.987 I llm_load_print_meta: arch             = gptneox
0.00.086.987 I llm_load_print_meta: vocab type       = BPE
0.00.086.987 I llm_load_print_meta: n_vocab          = 50304
0.00.086.988 I llm_load_print_meta: n_merges         = 50009
0.00.086.988 I llm_load_print_meta: vocab_only       = 0
0.00.086.988 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.988 I llm_load_print_meta: n_embd           = 2048
0.00.086.988 I llm_load_print_meta: n_layer          = 24
0.00.086.992 I llm_load_print_meta: n_head           = 16
0.00.086.993 I llm_load_print_meta: n_head_kv        = 16
0.00.086.993 I llm_load_print_meta: n_rot            = 32
0.00.086.995 I llm_load_print_meta: n_swa            = 0
0.00.086.996 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.996 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.997 I llm_load_print_meta: n_gqa            = 1
0.00.086.998 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.999 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.000 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.000 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.000 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.000 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.001 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.001 I llm_load_print_meta: n_ff             = 8192
0.00.087.002 I llm_load_print_meta: n_expert         = 0
0.00.087.002 I llm_load_print_meta: n_expert_used    = 0
0.00.087.002 I llm_load_print_meta: causal attn      = 1
0.00.087.002 I llm_load_print_meta: pooling type     = 0
0.00.087.002 I llm_load_print_meta: rope type        = 2
0.00.087.003 I llm_load_print_meta: rope scaling     = linear
0.00.087.003 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.003 I llm_load_print_meta: freq_scale_train = 1
0.00.087.004 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.004 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.004 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.004 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.004 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.005 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.005 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.017 I llm_load_print_meta: model type       = 1.4B
0.00.087.017 I llm_load_print_meta: model ftype      = Q5_0
0.00.087.018 I llm_load_print_meta: model params     = 1.41 B
0.00.087.019 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.087.019 I llm_load_print_meta: general.name     = 1.4B
0.00.087.019 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.020 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.020 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.020 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.021 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.021 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.021 I llm_load_print_meta: max token length = 1024
0.00.089.757 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.757 I llm_load_tensors: offloading output layer to GPU
0.00.089.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.769 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.089.771 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.091.204 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.206 I llama_new_context_with_model: n_ctx         = 2048
0.00.091.206 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.091.206 I llama_new_context_with_model: n_batch       = 2048
0.00.091.206 I llama_new_context_with_model: n_ubatch      = 512
0.00.091.207 I llama_new_context_with_model: flash_attn    = 0
0.00.091.207 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.208 I llama_new_context_with_model: freq_scale    = 1
0.00.091.208 I ggml_metal_init: allocating
0.00.091.212 I ggml_metal_init: found device: Apple M4
0.00.091.214 I ggml_metal_init: picking default device: Apple M4
0.00.091.935 I ggml_metal_init: using embedded metal library
0.00.094.833 I ggml_metal_init: GPU name:   Apple M4
0.00.094.835 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.835 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.836 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.836 I ggml_metal_init: simdgroup reduction   = true
0.00.094.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.836 I ggml_metal_init: has bfloat            = true
0.00.094.836 I ggml_metal_init: use bfloat            = true
0.00.094.837 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.125.733 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.745 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.769 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.726 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.727 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.728 I llama_new_context_with_model: graph nodes  = 967
0.00.126.728 I llama_new_context_with_model: graph splits = 2
0.00.126.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.307.717 I main: llama threadpool init, n_threads = 4
0.01.307.770 I 
0.01.307.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.307.800 I 
0.01.308.049 I sampler seed: 1234
0.01.308.053 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.308.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.308.076 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.308.076 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.099.289 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.02.099.290 I llama_perf_context_print:        load time =    1287.43 ms
0.02.099.291 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.46 tokens per second)
0.02.099.294 I llama_perf_context_print:        eval time =     751.58 ms /    63 runs   (   11.93 ms per token,    83.82 tokens per second)
0.02.099.294 I llama_perf_context_print:       total time =     791.58 ms /    70 tokens
0.02.099.492 I ggml_metal_free: deallocating

real	0m2.133s
user	0m0.137s
sys	0m0.232s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.588 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.321 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.329 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.332 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.333 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.333 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.335 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.335 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.166 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.269 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.270 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.271 I llama_model_loader: - type  f32:  194 tensors
0.00.035.271 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.455 I llm_load_vocab: special tokens cache size = 25
0.00.063.470 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.472 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.473 I llm_load_print_meta: arch             = gptneox
0.00.063.473 I llm_load_print_meta: vocab type       = BPE
0.00.063.473 I llm_load_print_meta: n_vocab          = 50304
0.00.063.473 I llm_load_print_meta: n_merges         = 50009
0.00.063.473 I llm_load_print_meta: vocab_only       = 0
0.00.063.473 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.474 I llm_load_print_meta: n_embd           = 2048
0.00.063.474 I llm_load_print_meta: n_layer          = 24
0.00.063.476 I llm_load_print_meta: n_head           = 16
0.00.063.477 I llm_load_print_meta: n_head_kv        = 16
0.00.063.477 I llm_load_print_meta: n_rot            = 32
0.00.063.477 I llm_load_print_meta: n_swa            = 0
0.00.063.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.478 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.478 I llm_load_print_meta: n_gqa            = 1
0.00.063.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.481 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.481 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.481 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.481 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.482 I llm_load_print_meta: n_ff             = 8192
0.00.063.482 I llm_load_print_meta: n_expert         = 0
0.00.063.482 I llm_load_print_meta: n_expert_used    = 0
0.00.063.482 I llm_load_print_meta: causal attn      = 1
0.00.063.483 I llm_load_print_meta: pooling type     = 0
0.00.063.483 I llm_load_print_meta: rope type        = 2
0.00.063.483 I llm_load_print_meta: rope scaling     = linear
0.00.063.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.485 I llm_load_print_meta: freq_scale_train = 1
0.00.063.486 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.486 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.499 I llm_load_print_meta: model type       = 1.4B
0.00.063.499 I llm_load_print_meta: model ftype      = Q5_1
0.00.063.499 I llm_load_print_meta: model params     = 1.41 B
0.00.063.501 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.063.501 I llm_load_print_meta: general.name     = 1.4B
0.00.063.501 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.502 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.502 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.502 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.503 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.503 I llm_load_print_meta: max token length = 1024
0.00.065.594 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.594 I llm_load_tensors: offloading output layer to GPU
0.00.065.594 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.605 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.065.606 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.066.656 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.656 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.657 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.657 I llama_new_context_with_model: n_batch       = 2048
0.00.066.657 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.657 I llama_new_context_with_model: flash_attn    = 0
0.00.066.658 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.658 I llama_new_context_with_model: freq_scale    = 1
0.00.066.658 I ggml_metal_init: allocating
0.00.066.666 I ggml_metal_init: found device: Apple M4
0.00.066.668 I ggml_metal_init: picking default device: Apple M4
0.00.067.257 I ggml_metal_init: using embedded metal library
0.00.069.370 I ggml_metal_init: GPU name:   Apple M4
0.00.069.372 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.373 I ggml_metal_init: simdgroup reduction   = true
0.00.069.373 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.373 I ggml_metal_init: has bfloat            = true
0.00.069.373 I ggml_metal_init: use bfloat            = true
0.00.069.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.899 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.907 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.926 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.953 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.954 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.954 I llama_new_context_with_model: graph nodes  = 967
0.00.098.955 I llama_new_context_with_model: graph splits = 2
0.00.098.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.492 I main: llama threadpool init, n_threads = 4
0.00.874.521 I 
0.00.874.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.874.542 I 
0.00.874.698 I sampler seed: 1234
0.00.874.703 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.734 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.755 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.756 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.718.379 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.718.380 I llama_perf_context_print:        load time =     865.90 ms
0.01.718.381 I llama_perf_context_print: prompt eval time =      36.58 ms /     7 tokens (    5.23 ms per token,   191.36 tokens per second)
0.01.718.381 I llama_perf_context_print:        eval time =     804.05 ms /    63 runs   (   12.76 ms per token,    78.35 tokens per second)
0.01.718.382 I llama_perf_context_print:       total time =     843.89 ms /    70 tokens
0.01.718.559 I ggml_metal_free: deallocating

real	0m1.733s
user	0m0.110s
sys	0m0.195s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.019.222 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.437 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.026.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.611 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.548 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.551 I llama_model_loader: - type  f32:  194 tensors
0.00.036.552 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.552 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.824 I llm_load_vocab: special tokens cache size = 25
0.00.075.205 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.209 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.209 I llm_load_print_meta: arch             = gptneox
0.00.075.210 I llm_load_print_meta: vocab type       = BPE
0.00.075.210 I llm_load_print_meta: n_vocab          = 50304
0.00.075.210 I llm_load_print_meta: n_merges         = 50009
0.00.075.210 I llm_load_print_meta: vocab_only       = 0
0.00.075.211 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.211 I llm_load_print_meta: n_embd           = 2048
0.00.075.211 I llm_load_print_meta: n_layer          = 24
0.00.075.214 I llm_load_print_meta: n_head           = 16
0.00.075.215 I llm_load_print_meta: n_head_kv        = 16
0.00.075.217 I llm_load_print_meta: n_rot            = 32
0.00.075.217 I llm_load_print_meta: n_swa            = 0
0.00.075.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.219 I llm_load_print_meta: n_gqa            = 1
0.00.075.219 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.220 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.221 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.221 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.222 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.222 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.222 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.223 I llm_load_print_meta: n_ff             = 8192
0.00.075.223 I llm_load_print_meta: n_expert         = 0
0.00.075.223 I llm_load_print_meta: n_expert_used    = 0
0.00.075.223 I llm_load_print_meta: causal attn      = 1
0.00.075.223 I llm_load_print_meta: pooling type     = 0
0.00.075.226 I llm_load_print_meta: rope type        = 2
0.00.075.226 I llm_load_print_meta: rope scaling     = linear
0.00.075.227 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.227 I llm_load_print_meta: freq_scale_train = 1
0.00.075.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.228 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.228 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.228 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.228 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.240 I llm_load_print_meta: model type       = 1.4B
0.00.075.241 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.075.241 I llm_load_print_meta: model params     = 1.41 B
0.00.075.242 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.075.242 I llm_load_print_meta: general.name     = 1.4B
0.00.075.243 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.243 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.243 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.244 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.244 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.075.245 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.245 I llm_load_print_meta: max token length = 1024
0.00.077.701 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.701 I llm_load_tensors: offloading output layer to GPU
0.00.077.706 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.718 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.077.719 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.079.265 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.267 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.267 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.267 I llama_new_context_with_model: n_batch       = 2048
0.00.079.268 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.268 I llama_new_context_with_model: flash_attn    = 0
0.00.079.269 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.269 I llama_new_context_with_model: freq_scale    = 1
0.00.079.269 I ggml_metal_init: allocating
0.00.079.275 I ggml_metal_init: found device: Apple M4
0.00.079.278 I ggml_metal_init: picking default device: Apple M4
0.00.080.093 I ggml_metal_init: using embedded metal library
0.00.083.241 I ggml_metal_init: GPU name:   Apple M4
0.00.083.244 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.245 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.245 I ggml_metal_init: simdgroup reduction   = true
0.00.083.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.245 I ggml_metal_init: has bfloat            = true
0.00.083.246 I ggml_metal_init: use bfloat            = true
0.00.083.246 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.733 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.740 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.756 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.667 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.668 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.669 I llama_new_context_with_model: graph nodes  = 967
0.00.115.669 I llama_new_context_with_model: graph splits = 2
0.00.115.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.459 I main: llama threadpool init, n_threads = 4
0.00.652.497 I 
0.00.652.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.652.515 I 
0.00.652.678 I sampler seed: 1234
0.00.652.684 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.652.717 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.652.718 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.652.718 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.328.565 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.328.566 I llama_perf_context_print:        load time =     633.23 ms
0.01.328.567 I llama_perf_context_print: prompt eval time =      35.90 ms /     7 tokens (    5.13 ms per token,   194.96 tokens per second)
0.01.328.568 I llama_perf_context_print:        eval time =     637.36 ms /    63 runs   (   10.12 ms per token,    98.85 tokens per second)
0.01.328.569 I llama_perf_context_print:       total time =     676.11 ms /    70 tokens
0.01.328.760 I ggml_metal_free: deallocating

real	0m1.353s
user	0m0.126s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.272 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.777 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.780 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.783 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.478 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.480 I llama_model_loader: - type  f32:  194 tensors
0.00.024.480 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.481 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.481 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.450 I llm_load_vocab: special tokens cache size = 25
0.00.051.462 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.464 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.465 I llm_load_print_meta: arch             = gptneox
0.00.051.465 I llm_load_print_meta: vocab type       = BPE
0.00.051.465 I llm_load_print_meta: n_vocab          = 50304
0.00.051.465 I llm_load_print_meta: n_merges         = 50009
0.00.051.466 I llm_load_print_meta: vocab_only       = 0
0.00.051.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.466 I llm_load_print_meta: n_embd           = 2048
0.00.051.466 I llm_load_print_meta: n_layer          = 24
0.00.051.469 I llm_load_print_meta: n_head           = 16
0.00.051.470 I llm_load_print_meta: n_head_kv        = 16
0.00.051.470 I llm_load_print_meta: n_rot            = 32
0.00.051.470 I llm_load_print_meta: n_swa            = 0
0.00.051.470 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.470 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.471 I llm_load_print_meta: n_gqa            = 1
0.00.051.472 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.472 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.473 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.474 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.474 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.474 I llm_load_print_meta: n_ff             = 8192
0.00.051.475 I llm_load_print_meta: n_expert         = 0
0.00.051.478 I llm_load_print_meta: n_expert_used    = 0
0.00.051.478 I llm_load_print_meta: causal attn      = 1
0.00.051.478 I llm_load_print_meta: pooling type     = 0
0.00.051.478 I llm_load_print_meta: rope type        = 2
0.00.051.478 I llm_load_print_meta: rope scaling     = linear
0.00.051.479 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.480 I llm_load_print_meta: freq_scale_train = 1
0.00.051.480 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.481 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.481 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.481 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.481 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.481 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.481 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.493 I llm_load_print_meta: model type       = 1.4B
0.00.051.493 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.493 I llm_load_print_meta: model params     = 1.41 B
0.00.051.494 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.494 I llm_load_print_meta: general.name     = 1.4B
0.00.051.494 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.494 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.494 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.495 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: max token length = 1024
0.00.053.043 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.044 I llm_load_tensors: offloading output layer to GPU
0.00.053.044 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.053 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.054 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.939 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.939 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.939 I llama_new_context_with_model: n_batch       = 2048
0.00.053.940 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.940 I llama_new_context_with_model: flash_attn    = 0
0.00.053.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.941 I llama_new_context_with_model: freq_scale    = 1
0.00.053.941 I ggml_metal_init: allocating
0.00.053.944 I ggml_metal_init: found device: Apple M4
0.00.053.946 I ggml_metal_init: picking default device: Apple M4
0.00.054.468 I ggml_metal_init: using embedded metal library
0.00.056.390 I ggml_metal_init: GPU name:   Apple M4
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.393 I ggml_metal_init: simdgroup reduction   = true
0.00.056.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.393 I ggml_metal_init: has bfloat            = true
0.00.056.394 I ggml_metal_init: use bfloat            = true
0.00.056.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.991 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.013 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.091 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.092 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.092 I llama_new_context_with_model: graph nodes  = 967
0.00.085.093 I llama_new_context_with_model: graph splits = 2
0.00.085.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.048 I main: llama threadpool init, n_threads = 4
0.00.610.084 I 
0.00.610.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.610.103 I 
0.00.610.343 I sampler seed: 1234
0.00.610.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.610.368 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.610.368 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.610.368 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.355.354 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.355.354 I llama_perf_context_print:        load time =     600.77 ms
0.01.355.355 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.31 tokens per second)
0.01.355.356 I llama_perf_context_print:        eval time =     706.17 ms /    63 runs   (   11.21 ms per token,    89.21 tokens per second)
0.01.355.356 I llama_perf_context_print:       total time =     745.31 ms /    70 tokens
0.01.355.532 I ggml_metal_free: deallocating

real	0m1.373s
user	0m0.108s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.625 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.171 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.971 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.972 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.972 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.973 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.973 I llama_model_loader: - type  f32:  194 tensors
0.00.023.974 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.974 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.974 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.497 I llm_load_vocab: special tokens cache size = 25
0.00.050.517 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.519 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.520 I llm_load_print_meta: arch             = gptneox
0.00.050.520 I llm_load_print_meta: vocab type       = BPE
0.00.050.520 I llm_load_print_meta: n_vocab          = 50304
0.00.050.521 I llm_load_print_meta: n_merges         = 50009
0.00.050.521 I llm_load_print_meta: vocab_only       = 0
0.00.050.521 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.521 I llm_load_print_meta: n_embd           = 2048
0.00.050.521 I llm_load_print_meta: n_layer          = 24
0.00.050.524 I llm_load_print_meta: n_head           = 16
0.00.050.525 I llm_load_print_meta: n_head_kv        = 16
0.00.050.525 I llm_load_print_meta: n_rot            = 32
0.00.050.525 I llm_load_print_meta: n_swa            = 0
0.00.050.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.526 I llm_load_print_meta: n_gqa            = 1
0.00.050.527 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.528 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.528 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.529 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.529 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.529 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.529 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.530 I llm_load_print_meta: n_ff             = 8192
0.00.050.530 I llm_load_print_meta: n_expert         = 0
0.00.050.532 I llm_load_print_meta: n_expert_used    = 0
0.00.050.533 I llm_load_print_meta: causal attn      = 1
0.00.050.533 I llm_load_print_meta: pooling type     = 0
0.00.050.534 I llm_load_print_meta: rope type        = 2
0.00.050.534 I llm_load_print_meta: rope scaling     = linear
0.00.050.534 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.534 I llm_load_print_meta: freq_scale_train = 1
0.00.050.535 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.535 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.535 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.535 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.535 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.535 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.536 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.547 I llm_load_print_meta: model type       = 1.4B
0.00.050.548 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.548 I llm_load_print_meta: model params     = 1.41 B
0.00.050.549 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.550 I llm_load_print_meta: general.name     = 1.4B
0.00.050.550 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.550 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.550 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.550 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.551 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.552 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.552 I llm_load_print_meta: max token length = 1024
0.00.052.562 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.562 I llm_load_tensors: offloading output layer to GPU
0.00.052.562 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.572 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.573 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.545 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.547 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.547 I llama_new_context_with_model: n_batch       = 2048
0.00.053.547 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.547 I llama_new_context_with_model: flash_attn    = 0
0.00.053.548 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.548 I llama_new_context_with_model: freq_scale    = 1
0.00.053.548 I ggml_metal_init: allocating
0.00.053.554 I ggml_metal_init: found device: Apple M4
0.00.053.556 I ggml_metal_init: picking default device: Apple M4
0.00.054.105 I ggml_metal_init: using embedded metal library
0.00.056.052 I ggml_metal_init: GPU name:   Apple M4
0.00.056.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.055 I ggml_metal_init: simdgroup reduction   = true
0.00.056.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.056 I ggml_metal_init: has bfloat            = true
0.00.056.057 I ggml_metal_init: use bfloat            = true
0.00.056.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.333 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.263 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.264 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.264 I llama_new_context_with_model: graph nodes  = 967
0.00.085.264 I llama_new_context_with_model: graph splits = 2
0.00.085.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.897 I main: llama threadpool init, n_threads = 4
0.00.676.932 I 
0.00.676.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.676.953 I 
0.00.677.110 I sampler seed: 1234
0.00.677.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.677.123 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.677.125 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.677.125 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.430.417 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.430.417 I llama_perf_context_print:        load time =     668.27 ms
0.01.430.418 I llama_perf_context_print: prompt eval time =      36.25 ms /     7 tokens (    5.18 ms per token,   193.09 tokens per second)
0.01.430.419 I llama_perf_context_print:        eval time =     713.90 ms /    63 runs   (   11.33 ms per token,    88.25 tokens per second)
0.01.430.419 I llama_perf_context_print:       total time =     753.52 ms /    70 tokens
0.01.430.634 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.106s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.012.157 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.538 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.544 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.545 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.546 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.551 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.552 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.367 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.165 I llama_model_loader: - type  f32:  194 tensors
0.00.027.165 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.166 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.599 I llm_load_vocab: special tokens cache size = 25
0.00.053.673 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.676 I llm_load_print_meta: arch             = gptneox
0.00.053.676 I llm_load_print_meta: vocab type       = BPE
0.00.053.676 I llm_load_print_meta: n_vocab          = 50304
0.00.053.677 I llm_load_print_meta: n_merges         = 50009
0.00.053.677 I llm_load_print_meta: vocab_only       = 0
0.00.053.677 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.677 I llm_load_print_meta: n_embd           = 2048
0.00.053.677 I llm_load_print_meta: n_layer          = 24
0.00.053.681 I llm_load_print_meta: n_head           = 16
0.00.053.682 I llm_load_print_meta: n_head_kv        = 16
0.00.053.682 I llm_load_print_meta: n_rot            = 32
0.00.053.682 I llm_load_print_meta: n_swa            = 0
0.00.053.682 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.682 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.683 I llm_load_print_meta: n_gqa            = 1
0.00.053.684 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.684 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.685 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.685 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.686 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.686 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.686 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.687 I llm_load_print_meta: n_ff             = 8192
0.00.053.687 I llm_load_print_meta: n_expert         = 0
0.00.053.687 I llm_load_print_meta: n_expert_used    = 0
0.00.053.689 I llm_load_print_meta: causal attn      = 1
0.00.053.690 I llm_load_print_meta: pooling type     = 0
0.00.053.690 I llm_load_print_meta: rope type        = 2
0.00.053.691 I llm_load_print_meta: rope scaling     = linear
0.00.053.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.691 I llm_load_print_meta: freq_scale_train = 1
0.00.053.692 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.692 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.692 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.692 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.692 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.692 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.699 I llm_load_print_meta: model type       = 1.4B
0.00.053.699 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.700 I llm_load_print_meta: model params     = 1.41 B
0.00.053.700 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.700 I llm_load_print_meta: general.name     = 1.4B
0.00.053.701 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.702 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.702 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.703 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.703 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.703 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.703 I llm_load_print_meta: max token length = 1024
0.00.055.498 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.499 I llm_load_tensors: offloading output layer to GPU
0.00.055.499 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.504 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.505 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.525 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.526 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.526 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.526 I llama_new_context_with_model: n_batch       = 2048
0.00.056.526 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.527 I llama_new_context_with_model: flash_attn    = 0
0.00.056.527 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.527 I llama_new_context_with_model: freq_scale    = 1
0.00.056.528 I ggml_metal_init: allocating
0.00.056.533 I ggml_metal_init: found device: Apple M4
0.00.056.535 I ggml_metal_init: picking default device: Apple M4
0.00.057.099 I ggml_metal_init: using embedded metal library
0.00.059.021 I ggml_metal_init: GPU name:   Apple M4
0.00.059.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.025 I ggml_metal_init: simdgroup reduction   = true
0.00.059.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.028 I ggml_metal_init: has bfloat            = true
0.00.059.028 I ggml_metal_init: use bfloat            = true
0.00.059.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.605 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.625 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.692 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.694 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.694 I llama_new_context_with_model: graph nodes  = 967
0.00.088.694 I llama_new_context_with_model: graph splits = 2
0.00.088.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.732 I main: llama threadpool init, n_threads = 4
0.00.760.762 I 
0.00.760.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.760.781 I 
0.00.760.992 I sampler seed: 1234
0.00.760.998 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.009 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.011 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.011 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.382 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.602.382 I llama_perf_context_print:        load time =     748.57 ms
0.01.602.383 I llama_perf_context_print: prompt eval time =      38.60 ms /     7 tokens (    5.51 ms per token,   181.37 tokens per second)
0.01.602.384 I llama_perf_context_print:        eval time =     799.88 ms /    63 runs   (   12.70 ms per token,    78.76 tokens per second)
0.01.602.385 I llama_perf_context_print:       total time =     841.65 ms /    70 tokens
0.01.602.560 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.107s
sys	0m0.196s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.268 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.910 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.911 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.912 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.912 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.913 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.914 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.916 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.556 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.557 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.558 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.558 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.559 I llama_model_loader: - type  f32:  194 tensors
0.00.024.559 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.854 I llm_load_vocab: special tokens cache size = 25
0.00.051.660 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.663 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.663 I llm_load_print_meta: arch             = gptneox
0.00.051.663 I llm_load_print_meta: vocab type       = BPE
0.00.051.664 I llm_load_print_meta: n_vocab          = 50304
0.00.051.664 I llm_load_print_meta: n_merges         = 50009
0.00.051.664 I llm_load_print_meta: vocab_only       = 0
0.00.051.664 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.664 I llm_load_print_meta: n_embd           = 2048
0.00.051.665 I llm_load_print_meta: n_layer          = 24
0.00.051.668 I llm_load_print_meta: n_head           = 16
0.00.051.669 I llm_load_print_meta: n_head_kv        = 16
0.00.051.669 I llm_load_print_meta: n_rot            = 32
0.00.051.669 I llm_load_print_meta: n_swa            = 0
0.00.051.669 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.669 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.670 I llm_load_print_meta: n_gqa            = 1
0.00.051.671 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.672 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.672 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.672 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.672 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.673 I llm_load_print_meta: n_ff             = 8192
0.00.051.674 I llm_load_print_meta: n_expert         = 0
0.00.051.674 I llm_load_print_meta: n_expert_used    = 0
0.00.051.676 I llm_load_print_meta: causal attn      = 1
0.00.051.676 I llm_load_print_meta: pooling type     = 0
0.00.051.676 I llm_load_print_meta: rope type        = 2
0.00.051.676 I llm_load_print_meta: rope scaling     = linear
0.00.051.677 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.679 I llm_load_print_meta: freq_scale_train = 1
0.00.051.679 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.679 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.680 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.680 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.691 I llm_load_print_meta: model type       = 1.4B
0.00.051.692 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.692 I llm_load_print_meta: model params     = 1.41 B
0.00.051.692 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.693 I llm_load_print_meta: general.name     = 1.4B
0.00.051.693 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.695 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: max token length = 1024
0.00.053.775 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.776 I llm_load_tensors: offloading output layer to GPU
0.00.053.776 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.786 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.788 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.754 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.754 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.754 I llama_new_context_with_model: n_batch       = 2048
0.00.054.755 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.755 I llama_new_context_with_model: flash_attn    = 0
0.00.054.755 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.756 I llama_new_context_with_model: freq_scale    = 1
0.00.054.756 I ggml_metal_init: allocating
0.00.054.759 I ggml_metal_init: found device: Apple M4
0.00.054.761 I ggml_metal_init: picking default device: Apple M4
0.00.055.319 I ggml_metal_init: using embedded metal library
0.00.057.270 I ggml_metal_init: GPU name:   Apple M4
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.272 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.272 I ggml_metal_init: simdgroup reduction   = true
0.00.057.272 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.273 I ggml_metal_init: has bfloat            = true
0.00.057.273 I ggml_metal_init: use bfloat            = true
0.00.057.273 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.274 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.106 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.114 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.133 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.151 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.152 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.153 I llama_new_context_with_model: graph nodes  = 967
0.00.086.153 I llama_new_context_with_model: graph splits = 2
0.00.086.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.709 I main: llama threadpool init, n_threads = 4
0.00.808.747 I 
0.00.808.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.808.767 I 
0.00.808.995 I sampler seed: 1234
0.00.809.002 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.013 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.014 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.014 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.682.209 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.682.210 I llama_perf_context_print:        load time =     799.44 ms
0.01.682.211 I llama_perf_context_print: prompt eval time =      38.45 ms /     7 tokens (    5.49 ms per token,   182.04 tokens per second)
0.01.682.212 I llama_perf_context_print:        eval time =     831.71 ms /    63 runs   (   13.20 ms per token,    75.75 tokens per second)
0.01.682.212 I llama_perf_context_print:       total time =     873.50 ms /    70 tokens
0.01.682.383 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.525 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.130 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.771 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.790 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.808 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.809 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.809 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.522 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.503 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.506 I llama_model_loader: - type  f32:  194 tensors
0.00.055.506 I llama_model_loader: - type  f16:   98 tensors
0.00.088.634 I llm_load_vocab: special tokens cache size = 25
0.00.095.858 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.861 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.861 I llm_load_print_meta: arch             = gptneox
0.00.095.862 I llm_load_print_meta: vocab type       = BPE
0.00.095.862 I llm_load_print_meta: n_vocab          = 50304
0.00.095.862 I llm_load_print_meta: n_merges         = 50009
0.00.095.862 I llm_load_print_meta: vocab_only       = 0
0.00.095.863 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.863 I llm_load_print_meta: n_embd           = 2048
0.00.095.863 I llm_load_print_meta: n_layer          = 24
0.00.095.865 I llm_load_print_meta: n_head           = 16
0.00.095.866 I llm_load_print_meta: n_head_kv        = 16
0.00.095.866 I llm_load_print_meta: n_rot            = 32
0.00.095.866 I llm_load_print_meta: n_swa            = 0
0.00.095.867 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.867 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.867 I llm_load_print_meta: n_gqa            = 1
0.00.095.871 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.871 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.872 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.872 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.873 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.873 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.873 I llm_load_print_meta: n_ff             = 8192
0.00.095.874 I llm_load_print_meta: n_expert         = 0
0.00.095.874 I llm_load_print_meta: n_expert_used    = 0
0.00.095.874 I llm_load_print_meta: causal attn      = 1
0.00.095.874 I llm_load_print_meta: pooling type     = 0
0.00.095.874 I llm_load_print_meta: rope type        = 2
0.00.095.874 I llm_load_print_meta: rope scaling     = linear
0.00.095.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.875 I llm_load_print_meta: freq_scale_train = 1
0.00.095.875 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.875 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.884 I llm_load_print_meta: model type       = 1.4B
0.00.095.884 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.885 I llm_load_print_meta: model params     = 1.41 B
0.00.095.885 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.886 I llm_load_print_meta: general.name     = 1.4B
0.00.095.886 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.886 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.886 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.886 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.887 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.095.888 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.888 I llm_load_print_meta: max token length = 1024
0.00.097.810 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.811 I llm_load_tensors: offloading output layer to GPU
0.00.097.811 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.816 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.817 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.867 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.868 I llama_new_context_with_model: n_ctx         = 128
0.00.098.868 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.868 I llama_new_context_with_model: n_batch       = 128
0.00.098.868 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.868 I llama_new_context_with_model: flash_attn    = 0
0.00.098.869 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.869 I llama_new_context_with_model: freq_scale    = 1
0.00.098.870 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.870 I ggml_metal_init: allocating
0.00.098.873 I ggml_metal_init: found device: Apple M4
0.00.098.875 I ggml_metal_init: picking default device: Apple M4
0.00.099.484 I ggml_metal_init: using embedded metal library
0.00.101.753 I ggml_metal_init: GPU name:   Apple M4
0.00.101.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.755 I ggml_metal_init: simdgroup reduction   = true
0.00.101.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.756 I ggml_metal_init: has bfloat            = true
0.00.101.756 I ggml_metal_init: use bfloat            = true
0.00.101.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.565 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.568 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.599 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.483 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.484 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.484 I llama_new_context_with_model: graph nodes  = 967
0.00.111.484 I llama_new_context_with_model: graph splits = 2
0.00.111.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.478.025 I 
0.01.478.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.478.144 I perplexity: tokenizing the input ..
0.01.489.021 I perplexity: tokenization took 10.875 ms
0.01.489.025 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.607.337 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.608.643 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.608.657 I llama_perf_context_print:        load time =    1452.88 ms
0.01.608.658 I llama_perf_context_print: prompt eval time =     118.08 ms /   128 tokens (    0.92 ms per token,  1083.99 tokens per second)
0.01.608.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.608.659 I llama_perf_context_print:       total time =     130.63 ms /   129 tokens
0.01.609.079 I ggml_metal_free: deallocating

real	0m1.797s
user	0m0.118s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.065 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.811 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.815 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.822 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.824 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.824 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.824 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.654 I llama_model_loader: - type  f32:  194 tensors
0.00.024.654 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.084 I llm_load_vocab: special tokens cache size = 25
0.00.052.005 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.008 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.009 I llm_load_print_meta: arch             = gptneox
0.00.052.009 I llm_load_print_meta: vocab type       = BPE
0.00.052.010 I llm_load_print_meta: n_vocab          = 50304
0.00.052.010 I llm_load_print_meta: n_merges         = 50009
0.00.052.012 I llm_load_print_meta: vocab_only       = 0
0.00.052.012 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.012 I llm_load_print_meta: n_embd           = 2048
0.00.052.012 I llm_load_print_meta: n_layer          = 24
0.00.052.016 I llm_load_print_meta: n_head           = 16
0.00.052.017 I llm_load_print_meta: n_head_kv        = 16
0.00.052.017 I llm_load_print_meta: n_rot            = 32
0.00.052.018 I llm_load_print_meta: n_swa            = 0
0.00.052.018 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.018 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.019 I llm_load_print_meta: n_gqa            = 1
0.00.052.019 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.020 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.021 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.021 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.021 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.021 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.022 I llm_load_print_meta: n_ff             = 8192
0.00.052.022 I llm_load_print_meta: n_expert         = 0
0.00.052.022 I llm_load_print_meta: n_expert_used    = 0
0.00.052.023 I llm_load_print_meta: causal attn      = 1
0.00.052.023 I llm_load_print_meta: pooling type     = 0
0.00.052.023 I llm_load_print_meta: rope type        = 2
0.00.052.023 I llm_load_print_meta: rope scaling     = linear
0.00.052.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.026 I llm_load_print_meta: freq_scale_train = 1
0.00.052.027 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.028 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.028 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.028 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.028 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.028 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.028 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.041 I llm_load_print_meta: model type       = 1.4B
0.00.052.041 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.041 I llm_load_print_meta: model params     = 1.41 B
0.00.052.042 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.042 I llm_load_print_meta: general.name     = 1.4B
0.00.052.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.042 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.046 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.046 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.046 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.047 I llm_load_print_meta: max token length = 1024
0.00.054.153 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.153 I llm_load_tensors: offloading output layer to GPU
0.00.054.153 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.163 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.165 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.149 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.149 I llama_new_context_with_model: n_ctx         = 128
0.00.055.150 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.150 I llama_new_context_with_model: n_batch       = 128
0.00.055.150 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.150 I llama_new_context_with_model: flash_attn    = 0
0.00.055.151 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.151 I llama_new_context_with_model: freq_scale    = 1
0.00.055.151 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.151 I ggml_metal_init: allocating
0.00.055.155 I ggml_metal_init: found device: Apple M4
0.00.055.157 I ggml_metal_init: picking default device: Apple M4
0.00.055.703 I ggml_metal_init: using embedded metal library
0.00.057.772 I ggml_metal_init: GPU name:   Apple M4
0.00.057.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.774 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.784 I ggml_metal_init: simdgroup reduction   = true
0.00.057.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.784 I ggml_metal_init: has bfloat            = true
0.00.057.784 I ggml_metal_init: use bfloat            = true
0.00.057.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.142 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.151 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.173 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.131 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.132 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.133 I llama_new_context_with_model: graph nodes  = 967
0.00.068.133 I llama_new_context_with_model: graph splits = 2
0.00.068.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.938.564 I 
0.00.938.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.938.653 I perplexity: tokenizing the input ..
0.00.946.755 I perplexity: tokenization took 8.101 ms
0.00.946.758 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.068.597 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.840 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.851 I llama_perf_context_print:        load time =     929.49 ms
0.01.069.851 I llama_perf_context_print: prompt eval time =     121.61 ms /   128 tokens (    0.95 ms per token,  1052.55 tokens per second)
0.01.069.852 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.853 I llama_perf_context_print:       total time =     131.29 ms /   129 tokens
0.01.070.189 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.077s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.897 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.641 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.641 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.231 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.234 I llama_model_loader: - type  f32:  194 tensors
0.00.024.235 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.235 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.417 I llm_load_vocab: special tokens cache size = 25
0.00.050.343 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.345 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.345 I llm_load_print_meta: arch             = gptneox
0.00.050.346 I llm_load_print_meta: vocab type       = BPE
0.00.050.346 I llm_load_print_meta: n_vocab          = 50304
0.00.050.346 I llm_load_print_meta: n_merges         = 50009
0.00.050.346 I llm_load_print_meta: vocab_only       = 0
0.00.050.347 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.347 I llm_load_print_meta: n_embd           = 2048
0.00.050.347 I llm_load_print_meta: n_layer          = 24
0.00.050.349 I llm_load_print_meta: n_head           = 16
0.00.050.350 I llm_load_print_meta: n_head_kv        = 16
0.00.050.350 I llm_load_print_meta: n_rot            = 32
0.00.050.350 I llm_load_print_meta: n_swa            = 0
0.00.050.351 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.353 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.353 I llm_load_print_meta: n_gqa            = 1
0.00.050.354 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.355 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.355 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.356 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.356 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.356 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.356 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.357 I llm_load_print_meta: n_ff             = 8192
0.00.050.357 I llm_load_print_meta: n_expert         = 0
0.00.050.357 I llm_load_print_meta: n_expert_used    = 0
0.00.050.357 I llm_load_print_meta: causal attn      = 1
0.00.050.358 I llm_load_print_meta: pooling type     = 0
0.00.050.358 I llm_load_print_meta: rope type        = 2
0.00.050.358 I llm_load_print_meta: rope scaling     = linear
0.00.050.358 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.359 I llm_load_print_meta: freq_scale_train = 1
0.00.050.359 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.359 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.359 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.359 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.360 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.360 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.360 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.371 I llm_load_print_meta: model type       = 1.4B
0.00.050.371 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.371 I llm_load_print_meta: model params     = 1.41 B
0.00.050.372 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.372 I llm_load_print_meta: general.name     = 1.4B
0.00.050.373 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.374 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.374 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.374 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.375 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.375 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.375 I llm_load_print_meta: max token length = 1024
0.00.051.893 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.894 I llm_load_tensors: offloading output layer to GPU
0.00.051.894 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.903 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.905 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.809 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.810 I llama_new_context_with_model: n_ctx         = 128
0.00.052.810 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.810 I llama_new_context_with_model: n_batch       = 128
0.00.052.810 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.811 I llama_new_context_with_model: flash_attn    = 0
0.00.052.811 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.811 I llama_new_context_with_model: freq_scale    = 1
0.00.052.811 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.812 I ggml_metal_init: allocating
0.00.052.817 I ggml_metal_init: found device: Apple M4
0.00.052.819 I ggml_metal_init: picking default device: Apple M4
0.00.053.339 I ggml_metal_init: using embedded metal library
0.00.055.245 I ggml_metal_init: GPU name:   Apple M4
0.00.055.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.247 I ggml_metal_init: simdgroup reduction   = true
0.00.055.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.248 I ggml_metal_init: has bfloat            = true
0.00.055.248 I ggml_metal_init: use bfloat            = true
0.00.055.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.737 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.739 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.763 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.646 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.648 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.648 I llama_new_context_with_model: graph nodes  = 967
0.00.065.648 I llama_new_context_with_model: graph splits = 2
0.00.065.660 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.022 I 
0.00.659.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.659.049 I perplexity: tokenizing the input ..
0.00.666.420 I perplexity: tokenization took 7.371 ms
0.00.666.423 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.305 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.790.535 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.790.561 I llama_perf_context_print:        load time =     649.12 ms
0.00.790.562 I llama_perf_context_print: prompt eval time =     122.66 ms /   128 tokens (    0.96 ms per token,  1043.57 tokens per second)
0.00.790.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.563 I llama_perf_context_print:       total time =     131.54 ms /   129 tokens
0.00.790.981 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.075s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.748 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.509 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.319 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.319 I llama_model_loader: - type  f32:  194 tensors
0.00.023.319 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.320 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.426 I llm_load_vocab: special tokens cache size = 25
0.00.050.349 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.352 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.353 I llm_load_print_meta: arch             = gptneox
0.00.050.353 I llm_load_print_meta: vocab type       = BPE
0.00.050.353 I llm_load_print_meta: n_vocab          = 50304
0.00.050.353 I llm_load_print_meta: n_merges         = 50009
0.00.050.354 I llm_load_print_meta: vocab_only       = 0
0.00.050.354 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.354 I llm_load_print_meta: n_embd           = 2048
0.00.050.354 I llm_load_print_meta: n_layer          = 24
0.00.050.358 I llm_load_print_meta: n_head           = 16
0.00.050.359 I llm_load_print_meta: n_head_kv        = 16
0.00.050.359 I llm_load_print_meta: n_rot            = 32
0.00.050.359 I llm_load_print_meta: n_swa            = 0
0.00.050.359 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.359 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.360 I llm_load_print_meta: n_gqa            = 1
0.00.050.361 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.361 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.362 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.362 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.365 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.365 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.365 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.365 I llm_load_print_meta: n_ff             = 8192
0.00.050.366 I llm_load_print_meta: n_expert         = 0
0.00.050.366 I llm_load_print_meta: n_expert_used    = 0
0.00.050.366 I llm_load_print_meta: causal attn      = 1
0.00.050.366 I llm_load_print_meta: pooling type     = 0
0.00.050.366 I llm_load_print_meta: rope type        = 2
0.00.050.366 I llm_load_print_meta: rope scaling     = linear
0.00.050.367 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.367 I llm_load_print_meta: freq_scale_train = 1
0.00.050.367 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.370 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.370 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.371 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.371 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.371 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.371 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.381 I llm_load_print_meta: model type       = 1.4B
0.00.050.381 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.382 I llm_load_print_meta: model params     = 1.41 B
0.00.050.382 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.382 I llm_load_print_meta: general.name     = 1.4B
0.00.050.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.383 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.383 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.383 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.383 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.384 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: max token length = 1024
0.00.052.336 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.336 I llm_load_tensors: offloading output layer to GPU
0.00.052.337 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.347 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.348 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.319 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.319 I llama_new_context_with_model: n_ctx         = 128
0.00.053.320 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.320 I llama_new_context_with_model: n_batch       = 128
0.00.053.320 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.320 I llama_new_context_with_model: flash_attn    = 0
0.00.053.321 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.321 I llama_new_context_with_model: freq_scale    = 1
0.00.053.321 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.321 I ggml_metal_init: allocating
0.00.053.325 I ggml_metal_init: found device: Apple M4
0.00.053.327 I ggml_metal_init: picking default device: Apple M4
0.00.053.834 I ggml_metal_init: using embedded metal library
0.00.055.761 I ggml_metal_init: GPU name:   Apple M4
0.00.055.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.763 I ggml_metal_init: simdgroup reduction   = true
0.00.055.763 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.763 I ggml_metal_init: has bfloat            = true
0.00.055.764 I ggml_metal_init: use bfloat            = true
0.00.055.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.744 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.748 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.761 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.680 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.681 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.682 I llama_new_context_with_model: graph nodes  = 967
0.00.065.682 I llama_new_context_with_model: graph splits = 2
0.00.065.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.918 I 
0.00.681.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.681.995 I perplexity: tokenizing the input ..
0.00.689.848 I perplexity: tokenization took 7.851 ms
0.00.689.853 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.630 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.813.889 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.813.907 I llama_perf_context_print:        load time =     673.15 ms
0.00.813.908 I llama_perf_context_print: prompt eval time =     122.55 ms /   128 tokens (    0.96 ms per token,  1044.44 tokens per second)
0.00.813.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.910 I llama_perf_context_print:       total time =     132.00 ms /   129 tokens
0.00.814.319 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.076s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.087 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.106 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.591 I llama_model_loader: - type  f32:  194 tensors
0.00.024.592 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.592 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.876 I llm_load_vocab: special tokens cache size = 25
0.00.050.949 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.952 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.952 I llm_load_print_meta: arch             = gptneox
0.00.050.952 I llm_load_print_meta: vocab type       = BPE
0.00.050.952 I llm_load_print_meta: n_vocab          = 50304
0.00.050.953 I llm_load_print_meta: n_merges         = 50009
0.00.050.953 I llm_load_print_meta: vocab_only       = 0
0.00.050.953 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.953 I llm_load_print_meta: n_embd           = 2048
0.00.050.953 I llm_load_print_meta: n_layer          = 24
0.00.050.956 I llm_load_print_meta: n_head           = 16
0.00.050.957 I llm_load_print_meta: n_head_kv        = 16
0.00.050.957 I llm_load_print_meta: n_rot            = 32
0.00.050.957 I llm_load_print_meta: n_swa            = 0
0.00.050.957 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.957 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.958 I llm_load_print_meta: n_gqa            = 1
0.00.050.959 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.959 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.960 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.960 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.960 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.961 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.961 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.961 I llm_load_print_meta: n_ff             = 8192
0.00.050.962 I llm_load_print_meta: n_expert         = 0
0.00.050.962 I llm_load_print_meta: n_expert_used    = 0
0.00.050.962 I llm_load_print_meta: causal attn      = 1
0.00.050.962 I llm_load_print_meta: pooling type     = 0
0.00.050.962 I llm_load_print_meta: rope type        = 2
0.00.050.962 I llm_load_print_meta: rope scaling     = linear
0.00.050.966 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.967 I llm_load_print_meta: freq_scale_train = 1
0.00.050.968 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.968 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.968 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.980 I llm_load_print_meta: model type       = 1.4B
0.00.050.981 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.981 I llm_load_print_meta: model params     = 1.41 B
0.00.050.981 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.982 I llm_load_print_meta: general.name     = 1.4B
0.00.050.982 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.982 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.982 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.982 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.983 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.983 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.983 I llm_load_print_meta: max token length = 1024
0.00.052.981 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.982 I llm_load_tensors: offloading output layer to GPU
0.00.052.982 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.992 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.993 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.971 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.972 I llama_new_context_with_model: n_ctx         = 128
0.00.053.972 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.972 I llama_new_context_with_model: n_batch       = 128
0.00.053.972 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.973 I llama_new_context_with_model: flash_attn    = 0
0.00.053.973 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.973 I llama_new_context_with_model: freq_scale    = 1
0.00.053.974 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.974 I ggml_metal_init: allocating
0.00.053.977 I ggml_metal_init: found device: Apple M4
0.00.053.979 I ggml_metal_init: picking default device: Apple M4
0.00.054.516 I ggml_metal_init: using embedded metal library
0.00.056.428 I ggml_metal_init: GPU name:   Apple M4
0.00.056.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.431 I ggml_metal_init: simdgroup reduction   = true
0.00.056.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.431 I ggml_metal_init: has bfloat            = true
0.00.056.431 I ggml_metal_init: use bfloat            = true
0.00.056.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.576 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.590 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.524 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.525 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.525 I llama_new_context_with_model: graph nodes  = 967
0.00.066.525 I llama_new_context_with_model: graph splits = 2
0.00.066.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.733 I 
0.00.780.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.780.766 I perplexity: tokenizing the input ..
0.00.787.951 I perplexity: tokenization took 7.184 ms
0.00.787.954 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.610 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.923.859 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.923.879 I llama_perf_context_print:        load time =     770.42 ms
0.00.923.879 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.16 tokens per second)
0.00.923.885 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.886 I llama_perf_context_print:       total time =     143.15 ms /   129 tokens
0.00.924.215 I ggml_metal_free: deallocating

real	0m0.938s
user	0m0.075s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.306 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.016 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.805 I llama_model_loader: - type  f32:  194 tensors
0.00.022.805 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.806 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.057 I llm_load_vocab: special tokens cache size = 25
0.00.049.153 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.156 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.156 I llm_load_print_meta: arch             = gptneox
0.00.049.156 I llm_load_print_meta: vocab type       = BPE
0.00.049.157 I llm_load_print_meta: n_vocab          = 50304
0.00.049.157 I llm_load_print_meta: n_merges         = 50009
0.00.049.157 I llm_load_print_meta: vocab_only       = 0
0.00.049.157 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.157 I llm_load_print_meta: n_embd           = 2048
0.00.049.158 I llm_load_print_meta: n_layer          = 24
0.00.049.160 I llm_load_print_meta: n_head           = 16
0.00.049.161 I llm_load_print_meta: n_head_kv        = 16
0.00.049.161 I llm_load_print_meta: n_rot            = 32
0.00.049.161 I llm_load_print_meta: n_swa            = 0
0.00.049.162 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.162 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.162 I llm_load_print_meta: n_gqa            = 1
0.00.049.163 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.164 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.164 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.165 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.165 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.165 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.165 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.166 I llm_load_print_meta: n_ff             = 8192
0.00.049.166 I llm_load_print_meta: n_expert         = 0
0.00.049.166 I llm_load_print_meta: n_expert_used    = 0
0.00.049.166 I llm_load_print_meta: causal attn      = 1
0.00.049.166 I llm_load_print_meta: pooling type     = 0
0.00.049.167 I llm_load_print_meta: rope type        = 2
0.00.049.167 I llm_load_print_meta: rope scaling     = linear
0.00.049.167 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.168 I llm_load_print_meta: freq_scale_train = 1
0.00.049.168 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.168 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.168 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.168 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.169 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.169 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.169 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.180 I llm_load_print_meta: model type       = 1.4B
0.00.049.181 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.181 I llm_load_print_meta: model params     = 1.41 B
0.00.049.182 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.182 I llm_load_print_meta: general.name     = 1.4B
0.00.049.182 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.182 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.182 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.182 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.183 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.183 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.183 I llm_load_print_meta: max token length = 1024
0.00.051.170 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.170 I llm_load_tensors: offloading output layer to GPU
0.00.051.170 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.180 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.181 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.104 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.105 I llama_new_context_with_model: n_ctx         = 128
0.00.052.105 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.105 I llama_new_context_with_model: n_batch       = 128
0.00.052.105 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.106 I llama_new_context_with_model: flash_attn    = 0
0.00.052.106 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.106 I llama_new_context_with_model: freq_scale    = 1
0.00.052.106 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.107 I ggml_metal_init: allocating
0.00.052.110 I ggml_metal_init: found device: Apple M4
0.00.052.112 I ggml_metal_init: picking default device: Apple M4
0.00.052.690 I ggml_metal_init: using embedded metal library
0.00.054.647 I ggml_metal_init: GPU name:   Apple M4
0.00.054.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.649 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.649 I ggml_metal_init: simdgroup reduction   = true
0.00.054.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.650 I ggml_metal_init: has bfloat            = true
0.00.054.650 I ggml_metal_init: use bfloat            = true
0.00.054.650 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.651 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.317 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.320 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.277 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.278 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.279 I llama_new_context_with_model: graph nodes  = 967
0.00.065.279 I llama_new_context_with_model: graph splits = 2
0.00.065.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.068 I 
0.00.748.101 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.748.132 I perplexity: tokenizing the input ..
0.00.755.407 I perplexity: tokenization took 7.274 ms
0.00.755.410 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.890.074 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.891.245 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.891.281 I llama_perf_context_print:        load time =     739.32 ms
0.00.891.282 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.30 tokens per second)
0.00.891.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.891.283 I llama_perf_context_print:       total time =     143.22 ms /   129 tokens
0.00.891.591 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.076s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.352 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.900 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.905 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.906 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.908 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.445 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.447 I llama_model_loader: - type  f32:  194 tensors
0.00.024.447 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.448 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.448 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.449 I llm_load_vocab: special tokens cache size = 25
0.00.050.302 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.305 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.305 I llm_load_print_meta: arch             = gptneox
0.00.050.305 I llm_load_print_meta: vocab type       = BPE
0.00.050.306 I llm_load_print_meta: n_vocab          = 50304
0.00.050.306 I llm_load_print_meta: n_merges         = 50009
0.00.050.306 I llm_load_print_meta: vocab_only       = 0
0.00.050.306 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.306 I llm_load_print_meta: n_embd           = 2048
0.00.050.307 I llm_load_print_meta: n_layer          = 24
0.00.050.309 I llm_load_print_meta: n_head           = 16
0.00.050.310 I llm_load_print_meta: n_head_kv        = 16
0.00.050.310 I llm_load_print_meta: n_rot            = 32
0.00.050.310 I llm_load_print_meta: n_swa            = 0
0.00.050.311 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.311 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.312 I llm_load_print_meta: n_gqa            = 1
0.00.050.313 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.314 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.314 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.315 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.316 I llm_load_print_meta: n_ff             = 8192
0.00.050.318 I llm_load_print_meta: n_expert         = 0
0.00.050.318 I llm_load_print_meta: n_expert_used    = 0
0.00.050.318 I llm_load_print_meta: causal attn      = 1
0.00.050.318 I llm_load_print_meta: pooling type     = 0
0.00.050.318 I llm_load_print_meta: rope type        = 2
0.00.050.319 I llm_load_print_meta: rope scaling     = linear
0.00.050.319 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.319 I llm_load_print_meta: freq_scale_train = 1
0.00.050.319 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.320 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.320 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.320 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.320 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.320 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.320 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.331 I llm_load_print_meta: model type       = 1.4B
0.00.050.332 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.333 I llm_load_print_meta: model params     = 1.41 B
0.00.050.333 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.333 I llm_load_print_meta: general.name     = 1.4B
0.00.050.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.335 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.335 I llm_load_print_meta: max token length = 1024
0.00.051.904 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.904 I llm_load_tensors: offloading output layer to GPU
0.00.051.904 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.914 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.915 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.793 I llama_new_context_with_model: n_ctx         = 128
0.00.052.793 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.793 I llama_new_context_with_model: n_batch       = 128
0.00.052.793 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.793 I llama_new_context_with_model: flash_attn    = 0
0.00.052.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.794 I llama_new_context_with_model: freq_scale    = 1
0.00.052.794 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.795 I ggml_metal_init: allocating
0.00.052.798 I ggml_metal_init: found device: Apple M4
0.00.052.800 I ggml_metal_init: picking default device: Apple M4
0.00.053.341 I ggml_metal_init: using embedded metal library
0.00.055.290 I ggml_metal_init: GPU name:   Apple M4
0.00.055.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.292 I ggml_metal_init: simdgroup reduction   = true
0.00.055.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.292 I ggml_metal_init: has bfloat            = true
0.00.055.293 I ggml_metal_init: use bfloat            = true
0.00.055.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.435 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.441 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.455 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.347 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.348 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.349 I llama_new_context_with_model: graph nodes  = 967
0.00.065.349 I llama_new_context_with_model: graph splits = 2
0.00.065.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.430 I 
0.00.454.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.454.459 I perplexity: tokenizing the input ..
0.00.462.013 I perplexity: tokenization took 7.555 ms
0.00.462.017 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.594.542 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.595.742 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.595.764 I llama_perf_context_print:        load time =     444.07 ms
0.00.595.765 I llama_perf_context_print: prompt eval time =     132.30 ms /   128 tokens (    1.03 ms per token,   967.51 tokens per second)
0.00.595.766 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.595.767 I llama_perf_context_print:       total time =     141.33 ms /   129 tokens
0.00.596.213 I ggml_metal_free: deallocating

real	0m0.610s
user	0m0.075s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.641 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.238 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.035 I llama_model_loader: - type  f32:  194 tensors
0.00.023.036 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.036 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.036 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.036 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.081 I llm_load_vocab: special tokens cache size = 25
0.00.050.092 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.095 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.095 I llm_load_print_meta: arch             = gptneox
0.00.050.095 I llm_load_print_meta: vocab type       = BPE
0.00.050.095 I llm_load_print_meta: n_vocab          = 50304
0.00.050.096 I llm_load_print_meta: n_merges         = 50009
0.00.050.096 I llm_load_print_meta: vocab_only       = 0
0.00.050.096 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.096 I llm_load_print_meta: n_embd           = 2048
0.00.050.096 I llm_load_print_meta: n_layer          = 24
0.00.050.099 I llm_load_print_meta: n_head           = 16
0.00.050.100 I llm_load_print_meta: n_head_kv        = 16
0.00.050.100 I llm_load_print_meta: n_rot            = 32
0.00.050.100 I llm_load_print_meta: n_swa            = 0
0.00.050.102 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.103 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.103 I llm_load_print_meta: n_gqa            = 1
0.00.050.104 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.105 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.106 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.106 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.106 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.106 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.106 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.107 I llm_load_print_meta: n_ff             = 8192
0.00.050.107 I llm_load_print_meta: n_expert         = 0
0.00.050.107 I llm_load_print_meta: n_expert_used    = 0
0.00.050.108 I llm_load_print_meta: causal attn      = 1
0.00.050.108 I llm_load_print_meta: pooling type     = 0
0.00.050.108 I llm_load_print_meta: rope type        = 2
0.00.050.110 I llm_load_print_meta: rope scaling     = linear
0.00.050.110 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.111 I llm_load_print_meta: freq_scale_train = 1
0.00.050.111 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.111 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.111 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.111 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.111 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.111 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.112 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.123 I llm_load_print_meta: model type       = 1.4B
0.00.050.124 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.124 I llm_load_print_meta: model params     = 1.41 B
0.00.050.125 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.125 I llm_load_print_meta: general.name     = 1.4B
0.00.050.125 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.126 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.127 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.127 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.128 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.128 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.128 I llm_load_print_meta: max token length = 1024
0.00.052.157 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.157 I llm_load_tensors: offloading output layer to GPU
0.00.052.158 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.168 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.169 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.135 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.136 I llama_new_context_with_model: n_ctx         = 128
0.00.053.136 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.136 I llama_new_context_with_model: n_batch       = 128
0.00.053.136 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.137 I llama_new_context_with_model: flash_attn    = 0
0.00.053.137 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.137 I llama_new_context_with_model: freq_scale    = 1
0.00.053.138 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.138 I ggml_metal_init: allocating
0.00.053.142 I ggml_metal_init: found device: Apple M4
0.00.053.144 I ggml_metal_init: picking default device: Apple M4
0.00.053.729 I ggml_metal_init: using embedded metal library
0.00.055.703 I ggml_metal_init: GPU name:   Apple M4
0.00.055.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.706 I ggml_metal_init: simdgroup reduction   = true
0.00.055.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.706 I ggml_metal_init: has bfloat            = true
0.00.055.707 I ggml_metal_init: use bfloat            = true
0.00.055.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.169 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.172 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.186 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.098 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.099 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.100 I llama_new_context_with_model: graph nodes  = 967
0.00.066.100 I llama_new_context_with_model: graph splits = 2
0.00.066.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.528.600 I 
0.00.528.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.528.670 I perplexity: tokenizing the input ..
0.00.536.816 I perplexity: tokenization took 8.145 ms
0.00.536.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.668.776 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.670.033 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.670.054 I llama_perf_context_print:        load time =     519.96 ms
0.00.670.056 I llama_perf_context_print: prompt eval time =     131.73 ms /   128 tokens (    1.03 ms per token,   971.71 tokens per second)
0.00.670.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.057 I llama_perf_context_print:       total time =     141.45 ms /   129 tokens
0.00.670.505 I ggml_metal_free: deallocating

real	0m0.682s
user	0m0.076s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.751 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.540 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.543 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.545 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.546 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.241 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.011 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.014 I llama_model_loader: - type  f32:  194 tensors
0.00.023.014 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.015 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.015 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.431 I llm_load_vocab: special tokens cache size = 25
0.00.049.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.194 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.195 I llm_load_print_meta: arch             = gptneox
0.00.049.195 I llm_load_print_meta: vocab type       = BPE
0.00.049.195 I llm_load_print_meta: n_vocab          = 50304
0.00.049.195 I llm_load_print_meta: n_merges         = 50009
0.00.049.195 I llm_load_print_meta: vocab_only       = 0
0.00.049.196 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.196 I llm_load_print_meta: n_embd           = 2048
0.00.049.196 I llm_load_print_meta: n_layer          = 24
0.00.049.199 I llm_load_print_meta: n_head           = 16
0.00.049.199 I llm_load_print_meta: n_head_kv        = 16
0.00.049.200 I llm_load_print_meta: n_rot            = 32
0.00.049.200 I llm_load_print_meta: n_swa            = 0
0.00.049.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.200 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.204 I llm_load_print_meta: n_gqa            = 1
0.00.049.205 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.205 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.206 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.206 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.206 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.207 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.207 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.207 I llm_load_print_meta: n_ff             = 8192
0.00.049.207 I llm_load_print_meta: n_expert         = 0
0.00.049.208 I llm_load_print_meta: n_expert_used    = 0
0.00.049.208 I llm_load_print_meta: causal attn      = 1
0.00.049.208 I llm_load_print_meta: pooling type     = 0
0.00.049.208 I llm_load_print_meta: rope type        = 2
0.00.049.208 I llm_load_print_meta: rope scaling     = linear
0.00.049.210 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.210 I llm_load_print_meta: freq_scale_train = 1
0.00.049.211 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.211 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.211 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.211 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.211 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.223 I llm_load_print_meta: model type       = 1.4B
0.00.049.224 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.224 I llm_load_print_meta: model params     = 1.41 B
0.00.049.224 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.225 I llm_load_print_meta: general.name     = 1.4B
0.00.049.225 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.225 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.225 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.225 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.225 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.226 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.226 I llm_load_print_meta: max token length = 1024
0.00.051.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.198 I llm_load_tensors: offloading output layer to GPU
0.00.051.199 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.209 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.210 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.151 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.152 I llama_new_context_with_model: n_ctx         = 128
0.00.052.152 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.153 I llama_new_context_with_model: n_batch       = 128
0.00.052.153 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.153 I llama_new_context_with_model: flash_attn    = 0
0.00.052.153 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.154 I llama_new_context_with_model: freq_scale    = 1
0.00.052.154 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.154 I ggml_metal_init: allocating
0.00.052.157 I ggml_metal_init: found device: Apple M4
0.00.052.159 I ggml_metal_init: picking default device: Apple M4
0.00.052.688 I ggml_metal_init: using embedded metal library
0.00.054.590 I ggml_metal_init: GPU name:   Apple M4
0.00.054.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.592 I ggml_metal_init: simdgroup reduction   = true
0.00.054.592 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.592 I ggml_metal_init: has bfloat            = true
0.00.054.592 I ggml_metal_init: use bfloat            = true
0.00.054.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.900 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.905 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.919 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.858 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.859 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.859 I llama_new_context_with_model: graph nodes  = 967
0.00.064.860 I llama_new_context_with_model: graph splits = 2
0.00.064.872 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.236 I 
0.00.600.275 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.600.304 I perplexity: tokenizing the input ..
0.00.607.835 I perplexity: tokenization took 7.531 ms
0.00.607.841 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.596 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.743.138 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.743.153 I llama_perf_context_print:        load time =     591.48 ms
0.00.743.154 I llama_perf_context_print: prompt eval time =     133.50 ms /   128 tokens (    1.04 ms per token,   958.77 tokens per second)
0.00.743.155 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.156 I llama_perf_context_print:       total time =     142.92 ms /   129 tokens
0.00.743.445 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.074s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.400 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.129 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.135 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.136 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.137 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.143 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.144 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.820 I llama_model_loader: - type  f32:  194 tensors
0.00.024.820 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.820 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.103 I llm_load_vocab: special tokens cache size = 25
0.00.052.162 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.167 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.168 I llm_load_print_meta: arch             = gptneox
0.00.052.168 I llm_load_print_meta: vocab type       = BPE
0.00.052.168 I llm_load_print_meta: n_vocab          = 50304
0.00.052.173 I llm_load_print_meta: n_merges         = 50009
0.00.052.175 I llm_load_print_meta: vocab_only       = 0
0.00.052.175 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.175 I llm_load_print_meta: n_embd           = 2048
0.00.052.176 I llm_load_print_meta: n_layer          = 24
0.00.052.179 I llm_load_print_meta: n_head           = 16
0.00.052.180 I llm_load_print_meta: n_head_kv        = 16
0.00.052.180 I llm_load_print_meta: n_rot            = 32
0.00.052.180 I llm_load_print_meta: n_swa            = 0
0.00.052.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.181 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.181 I llm_load_print_meta: n_gqa            = 1
0.00.052.182 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.182 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.183 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.183 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.185 I llm_load_print_meta: n_ff             = 8192
0.00.052.187 I llm_load_print_meta: n_expert         = 0
0.00.052.187 I llm_load_print_meta: n_expert_used    = 0
0.00.052.187 I llm_load_print_meta: causal attn      = 1
0.00.052.187 I llm_load_print_meta: pooling type     = 0
0.00.052.187 I llm_load_print_meta: rope type        = 2
0.00.052.187 I llm_load_print_meta: rope scaling     = linear
0.00.052.188 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.188 I llm_load_print_meta: freq_scale_train = 1
0.00.052.188 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.188 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.188 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.188 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.188 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.189 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.189 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.202 I llm_load_print_meta: model type       = 1.4B
0.00.052.202 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.203 I llm_load_print_meta: model params     = 1.41 B
0.00.052.203 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.203 I llm_load_print_meta: general.name     = 1.4B
0.00.052.203 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.204 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.204 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.205 I llm_load_print_meta: max token length = 1024
0.00.054.280 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.281 I llm_load_tensors: offloading output layer to GPU
0.00.054.281 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.296 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.298 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.300 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.301 I llama_new_context_with_model: n_ctx         = 128
0.00.055.301 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.301 I llama_new_context_with_model: n_batch       = 128
0.00.055.301 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.301 I llama_new_context_with_model: flash_attn    = 0
0.00.055.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.302 I llama_new_context_with_model: freq_scale    = 1
0.00.055.302 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.303 I ggml_metal_init: allocating
0.00.055.314 I ggml_metal_init: found device: Apple M4
0.00.055.321 I ggml_metal_init: picking default device: Apple M4
0.00.055.922 I ggml_metal_init: using embedded metal library
0.00.058.140 I ggml_metal_init: GPU name:   Apple M4
0.00.058.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.143 I ggml_metal_init: simdgroup reduction   = true
0.00.058.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.144 I ggml_metal_init: has bfloat            = true
0.00.058.144 I ggml_metal_init: use bfloat            = true
0.00.058.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.971 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.975 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.990 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.964 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.965 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.966 I llama_new_context_with_model: graph nodes  = 967
0.00.067.966 I llama_new_context_with_model: graph splits = 2
0.00.067.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.007 I 
0.00.703.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.703.106 I perplexity: tokenizing the input ..
0.00.710.769 I perplexity: tokenization took 7.662 ms
0.00.710.774 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.686 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.852.969 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.852.987 I llama_perf_context_print:        load time =     692.60 ms
0.00.852.988 I llama_perf_context_print: prompt eval time =     140.66 ms /   128 tokens (    1.10 ms per token,   910.02 tokens per second)
0.00.852.989 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.990 I llama_perf_context_print:       total time =     149.99 ms /   129 tokens
0.00.853.495 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.076s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.693 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.255 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.774 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.774 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.775 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.775 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.776 I llama_model_loader: - type  f32:  194 tensors
0.00.022.776 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.892 I llm_load_vocab: special tokens cache size = 25
0.00.048.834 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.837 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.837 I llm_load_print_meta: arch             = gptneox
0.00.048.837 I llm_load_print_meta: vocab type       = BPE
0.00.048.838 I llm_load_print_meta: n_vocab          = 50304
0.00.048.838 I llm_load_print_meta: n_merges         = 50009
0.00.048.838 I llm_load_print_meta: vocab_only       = 0
0.00.048.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.838 I llm_load_print_meta: n_embd           = 2048
0.00.048.839 I llm_load_print_meta: n_layer          = 24
0.00.048.842 I llm_load_print_meta: n_head           = 16
0.00.048.842 I llm_load_print_meta: n_head_kv        = 16
0.00.048.842 I llm_load_print_meta: n_rot            = 32
0.00.048.843 I llm_load_print_meta: n_swa            = 0
0.00.048.843 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.843 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.844 I llm_load_print_meta: n_gqa            = 1
0.00.048.844 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.845 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.846 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.846 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.846 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.848 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.848 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.849 I llm_load_print_meta: n_ff             = 8192
0.00.048.849 I llm_load_print_meta: n_expert         = 0
0.00.048.849 I llm_load_print_meta: n_expert_used    = 0
0.00.048.849 I llm_load_print_meta: causal attn      = 1
0.00.048.851 I llm_load_print_meta: pooling type     = 0
0.00.048.851 I llm_load_print_meta: rope type        = 2
0.00.048.852 I llm_load_print_meta: rope scaling     = linear
0.00.048.852 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.852 I llm_load_print_meta: freq_scale_train = 1
0.00.048.852 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.853 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.853 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.853 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.853 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.853 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.853 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.865 I llm_load_print_meta: model type       = 1.4B
0.00.048.865 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.866 I llm_load_print_meta: model params     = 1.41 B
0.00.048.866 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.866 I llm_load_print_meta: general.name     = 1.4B
0.00.048.866 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.866 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.867 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.867 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.867 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.867 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.867 I llm_load_print_meta: max token length = 1024
0.00.050.867 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.868 I llm_load_tensors: offloading output layer to GPU
0.00.050.868 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.878 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.879 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.876 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.877 I llama_new_context_with_model: n_ctx         = 128
0.00.051.877 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.877 I llama_new_context_with_model: n_batch       = 128
0.00.051.877 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.877 I llama_new_context_with_model: flash_attn    = 0
0.00.051.878 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.878 I llama_new_context_with_model: freq_scale    = 1
0.00.051.878 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.879 I ggml_metal_init: allocating
0.00.051.883 I ggml_metal_init: found device: Apple M4
0.00.051.885 I ggml_metal_init: picking default device: Apple M4
0.00.052.425 I ggml_metal_init: using embedded metal library
0.00.054.326 I ggml_metal_init: GPU name:   Apple M4
0.00.054.327 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.328 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.328 I ggml_metal_init: simdgroup reduction   = true
0.00.054.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.329 I ggml_metal_init: has bfloat            = true
0.00.054.329 I ggml_metal_init: use bfloat            = true
0.00.054.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.330 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.416 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.418 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.440 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.346 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.347 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.347 I llama_new_context_with_model: graph nodes  = 967
0.00.064.347 I llama_new_context_with_model: graph splits = 2
0.00.064.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.805 I 
0.00.686.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.686.882 I perplexity: tokenizing the input ..
0.00.694.929 I perplexity: tokenization took 8.054 ms
0.00.694.933 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.585 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.835.840 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.835.848 I llama_perf_context_print:        load time =     678.10 ms
0.00.835.850 I llama_perf_context_print: prompt eval time =     139.43 ms /   128 tokens (    1.09 ms per token,   918.02 tokens per second)
0.00.835.851 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.851 I llama_perf_context_print:       total time =     149.05 ms /   129 tokens
0.00.836.208 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.075s
sys	0m0.129s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.380 I build: 4157 (dc39012c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.520 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.422 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.439 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.444 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.445 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.451 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.253 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.384 I llama_model_loader: - type  f32:  194 tensors
0.00.054.384 I llama_model_loader: - type  f16:   98 tensors
0.00.085.670 I llm_load_vocab: special tokens cache size = 25
0.00.092.700 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.703 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.703 I llm_load_print_meta: arch             = gptneox
0.00.092.703 I llm_load_print_meta: vocab type       = BPE
0.00.092.704 I llm_load_print_meta: n_vocab          = 50304
0.00.092.704 I llm_load_print_meta: n_merges         = 50009
0.00.092.704 I llm_load_print_meta: vocab_only       = 0
0.00.092.704 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.704 I llm_load_print_meta: n_embd           = 2048
0.00.092.704 I llm_load_print_meta: n_layer          = 24
0.00.092.707 I llm_load_print_meta: n_head           = 16
0.00.092.709 I llm_load_print_meta: n_head_kv        = 16
0.00.092.709 I llm_load_print_meta: n_rot            = 32
0.00.092.709 I llm_load_print_meta: n_swa            = 0
0.00.092.709 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.709 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.710 I llm_load_print_meta: n_gqa            = 1
0.00.092.711 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.711 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.712 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.712 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.712 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.712 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.712 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.713 I llm_load_print_meta: n_ff             = 8192
0.00.092.713 I llm_load_print_meta: n_expert         = 0
0.00.092.714 I llm_load_print_meta: n_expert_used    = 0
0.00.092.714 I llm_load_print_meta: causal attn      = 1
0.00.092.714 I llm_load_print_meta: pooling type     = 0
0.00.092.714 I llm_load_print_meta: rope type        = 2
0.00.092.714 I llm_load_print_meta: rope scaling     = linear
0.00.092.715 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.715 I llm_load_print_meta: freq_scale_train = 1
0.00.092.715 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.715 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.716 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.716 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.716 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.716 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.716 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.728 I llm_load_print_meta: model type       = 1.4B
0.00.092.728 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.729 I llm_load_print_meta: model params     = 1.41 B
0.00.092.729 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.730 I llm_load_print_meta: general.name     = 1.4B
0.00.092.730 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.730 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.730 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.730 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.732 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.732 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.733 I llm_load_print_meta: max token length = 1024
0.00.095.425 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.425 I llm_load_tensors: offloading output layer to GPU
0.00.095.426 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.436 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.437 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.424 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.425 I llama_new_context_with_model: n_ctx         = 128
0.00.096.425 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.426 I llama_new_context_with_model: n_batch       = 128
0.00.096.426 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.426 I llama_new_context_with_model: flash_attn    = 0
0.00.096.426 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.427 I llama_new_context_with_model: freq_scale    = 1
0.00.096.427 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.427 I ggml_metal_init: allocating
0.00.096.431 I ggml_metal_init: found device: Apple M4
0.00.096.433 I ggml_metal_init: picking default device: Apple M4
0.00.097.047 I ggml_metal_init: using embedded metal library
0.00.099.274 I ggml_metal_init: GPU name:   Apple M4
0.00.099.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.277 I ggml_metal_init: simdgroup reduction   = true
0.00.099.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.277 I ggml_metal_init: has bfloat            = true
0.00.099.277 I ggml_metal_init: use bfloat            = true
0.00.099.278 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.242 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.246 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.154 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.155 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.155 I llama_new_context_with_model: graph nodes  = 967
0.00.109.155 I llama_new_context_with_model: graph splits = 2
0.00.109.168 I 
0.00.109.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.109.190 I compute_imatrix: tokenizing the input ..
0.00.115.871 I compute_imatrix: tokenization took 6.68 ms
0.00.115.872 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.620.399 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.794 I llama_perf_context_print:        load time =    1595.87 ms
0.01.622.794 I llama_perf_context_print: prompt eval time =    1503.89 ms /   128 tokens (   11.75 ms per token,    85.11 tokens per second)
0.01.622.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.795 I llama_perf_context_print:       total time =    1598.26 ms /   129 tokens
0.01.623.272 I ggml_metal_free: deallocating

real	0m1.807s
user	0m0.161s
sys	0m0.253s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4157 (dc39012c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143e0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143e0a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143e0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143e0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143e0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143e0be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143e0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143e0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143e0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143e0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143e0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143e0de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143e0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143e0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143e0f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143e10070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143e10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143e10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143e115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143e11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143e124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143e12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143e13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143e13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143e142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143e14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143e14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143e15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143e15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143e16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143e164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143e16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143e16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143e17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143e177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143e17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143e18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143e185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143e18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143e18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143e193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143e19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143e19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143e1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143e1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143e1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143e1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143e1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143e1bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143e1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143e1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143e1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143e1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143e1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143e1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143e1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143e1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143e1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143e1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143e1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143e202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143e20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143e20be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143e21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143e21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143e219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143e21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143e22300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143e227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143e22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143e23a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143e23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143e24360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143e24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143e24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143e25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143e255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143e25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143e25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143e263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143e26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143e26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143e271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143e27640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143e27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143e27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143e28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143e288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143e28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143e29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143e296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143e29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143e29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143e2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143e2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143e1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143e2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143e2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143e2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143e2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143e2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143e2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143e2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143e2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143e2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143e2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143e2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143e2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143e2e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143e2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143e2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143e2f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143e2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143e2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143e302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143e30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143e30bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143e31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143e31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143e319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143e31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143e32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143e327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143e32c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143e330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143e33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143e33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143e33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143e34370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143e34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143e34cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143e35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143e355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143e35a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143e35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143e363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143e36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143e371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143e37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143e37af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143e37f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143e38430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143e388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143e38d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143e39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143e396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143e39b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143e39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143e3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143e3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143e3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143e3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143e3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143e3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143e3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143e3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143e3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143e3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143e3d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143e3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143e3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143e3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143e3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143e3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143e3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143e40250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143e407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143e40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143e41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143e41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143e41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143e42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143e42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143e43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143e43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143e43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143e44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143e44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143e45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143e45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143e45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143e461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143e46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143e471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143e47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143e47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143e481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143e48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143e48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143e491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143e49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143e4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143e4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143e4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143e4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143e4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143e4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143e4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143e4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143e4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143e4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143e4d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143e4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143e4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143e4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143e4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143e4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143e4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143e50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143e506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143e50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143e51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143e52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143e52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143e52b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143e52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143e53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143e53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143e53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143e54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143e546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143e54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143e55020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143e554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143e55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143e55e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143e562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143e567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143e56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143e57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143e57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143e58470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143e58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143e58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143e59350 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.165.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137404ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137404f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1374053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137405830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137405ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137406110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137406580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1374069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137406e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137407360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1374077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137407e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137408970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137409120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137409930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13740a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13740a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13740ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13740b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13740bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13740c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13740cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13740d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13740da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13740e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13740e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13740e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13740eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13740ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13740f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13740f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13740fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137410200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1374104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137410930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137410da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137411210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137411680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137411af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137411f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1374123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137412840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137412cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137413120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137413590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137413a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137413e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1374142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137414750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137414bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137415030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1374154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137415910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137415d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1374161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137416660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137416bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1374170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137417540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1374179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137417e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137418290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137418700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137418b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137418fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137419450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1374198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137419d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13741a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13741a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13741aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13741aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13741b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13741b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13741bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13741c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13741c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13741c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13741ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13741d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13741d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13741db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13741dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13741e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13741e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13741ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13741f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13741f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13741fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13741fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137420340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1374207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137420c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137421090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137421500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137421970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137421de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137422250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1374226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137422b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137422fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137423410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137423880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137423cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137424160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1374245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137424a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137424eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137425320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137425790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137425c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137426070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1374264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137426950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137426dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137427230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1374276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137427b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137427f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1374283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137428860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137428cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137429140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1374295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137429a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137429e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13742a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13742a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13742abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13742b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13742b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13742b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13742bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13742c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13742c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13742caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13742cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13742d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13742d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13742dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13742e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13742e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13742ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13742ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13742f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13742f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13742fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137430030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1374304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137430910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137430d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1374311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137431660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137431ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137431f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1374323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137432820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137432c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137433100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137433570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1374339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137433e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1374342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137434730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137434ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137435010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137435480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137436010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1374362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137436590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137436a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137436e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1374372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137437750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137437bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137438030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1374384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137438910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137438d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1374391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137439660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137439ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137439f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13743a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13743a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13743ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13743b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13743b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13743b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13743be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13743c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13743c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13743cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13743d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13743d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13743d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13743dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13743e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13743e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13743eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13743ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13743f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13743f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13743fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1374400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137440550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1374409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137440e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1374412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137441710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137441b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137441ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137442460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1374428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137442d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1374431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137443620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137443a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137443f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137444370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1374447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137444c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1374450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137445530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1374459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137445e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137446280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1374466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137446b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137446fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137447440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1374478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137447d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137448190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137448600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137448a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137448ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137449350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137449e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13744a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13744acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13744b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13744b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13744b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13744bde0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1373046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137304b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137304fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137305430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1373058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137305d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137306180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1373065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137306a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137306ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137307340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137307a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137308580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137308d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137309540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137309c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13730a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13730aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13730b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13730b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13730c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13730c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13730ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13730d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13730dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13730df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13730e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13730e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13730eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13730ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13730f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13730f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13730fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137310030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1373104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137310910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137310d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1373111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137311660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137311ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137311f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1373123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137312820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137312c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137313100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137313570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1373139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137313e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1373142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137314730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137314ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137315010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137315480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1373158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137315d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1373161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137316740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137316c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1373170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137317520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137317990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137317e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137318270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1373186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137318b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137318fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137319430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1373198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137319d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13731a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13731a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13731aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13731aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13731b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13731b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13731bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13731c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13731c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13731c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13731cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13731d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13731d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13731db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13731dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13731e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13731e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13731ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13731f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13731f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13731fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13731feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137320320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137320790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137320c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137321070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1373214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137321950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137321dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137322230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1373226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137322b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137322f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1373233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137323860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137323cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137324140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1373245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137324a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137324e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137325300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137325770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137325be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137326050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1373264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137326930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137326da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137327210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137327680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137327af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137327f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1373283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137328840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137328cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137329120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137329590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137329a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137329e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13732a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13732a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13732abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13732b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13732b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13732b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13732bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13732c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13732c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13732cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13732cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13732d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13732d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13732dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13732e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13732e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13732e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13732ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13732f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13732f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13732fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137330010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137330480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1373308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137330d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1373311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137331640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137331ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137331f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137332390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137332800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137332c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1373330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137333550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1373339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137333e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1373342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137334710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137334b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137334ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137335b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137335e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137336100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137336570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1373369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137336e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1373372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137337730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137337ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137338010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137338480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1373388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137338d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1373391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137339640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137339ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137339f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13733a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13733a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13733ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13733b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13733b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13733b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13733be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13733c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13733c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13733cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13733cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13733d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13733d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13733dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13733e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13733e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13733ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13733ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13733f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13733f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13733fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1373400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137340530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1373409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137340e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137341280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1373416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137341b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137341fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137342440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1373428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137342d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137343190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137343600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137343a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137343ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137344350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1373447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137344c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1373450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137345510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137345980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137345df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137346260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1373466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137346b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137346fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137347420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137347890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137347d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137348170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1373485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137348a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137348ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137349a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13734a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13734a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13734af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13734b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13734b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13734b950 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.860s
user	0m0.284s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4157 (dc39012c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153e0b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153e0b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153e0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153e0c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153e0ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153e0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153e0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153e0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153e0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153e0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153e0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153e0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153e0fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153e102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153e10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153e11200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153e11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153e12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153e12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153e12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153e13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153e14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153e14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153e15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153e15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153e15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153e16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153e16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153e17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153e178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153e18180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153e186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153e18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153e18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153e192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153e19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153e1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153e1a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153e1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153e1ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153e1b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153e1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153e1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153e1c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153e1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153e1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153e1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153e1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153e1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153e1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153e1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153e1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153e1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153e200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153e20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153e20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153e21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153e21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153e218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153e21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153e22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153e226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153e22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153e22ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153e23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153e23930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153e23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153e24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153e24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153e24bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153e25050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153e254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153e25990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153e25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153e26770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153e26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153e270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153e27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153e279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153e27e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153e28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153e287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153e28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153e29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153e295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153e29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153e29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153e2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153e2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153e2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153e2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153e2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153e2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153e1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153e2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153e2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153e2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153e2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153e2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153e2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153e2dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153e2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153e2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153e2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153e2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153e2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153e2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153e301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153e30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153e30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153e30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153e31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153e318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153e31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153e32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153e326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153e32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153e33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153e334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153e33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153e33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153e34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153e34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153e34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153e35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153e35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153e359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153e35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153e362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153e36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153e36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153e370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153e37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153e37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153e37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153e387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153e38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153e39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153e395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153e39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153e39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153e3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153e3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153e3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153e3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153e3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153e3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153e3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153e3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153e3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153e3d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153e3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153e3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153e3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153e3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153e3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153e3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153e3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153e40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153e406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153e40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153e413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153e41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153e41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153e423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153e42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153e42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153e433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153e43910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153e43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153e443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153e44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153e44e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153e453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153e458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153e45e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153e46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153e468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153e46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153e47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153e478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153e47e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153e48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153e488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153e48e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153e49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153e498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153e49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153e4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153e4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153e4adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153e4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153e4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153e4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153e4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153e4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153e4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153e4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153e4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153e4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153e4e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153e4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153e4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153e4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153e4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153e4fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153e502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153e50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153e50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153e512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153e51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153e51d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153e522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153e52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153e52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153e532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153e53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153e53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153e54150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153e545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153e54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153e54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153e55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153e55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153e561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153e56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153e56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153e56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153e57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153e57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153e580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153e587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153e58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153e59600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153e598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153e59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153e5a4e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155004c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1550050e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155005550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1550059c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155005e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1550062a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155006710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155006b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155006ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155007460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1550078d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155007f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155008ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155009260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155009a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15500a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15500a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15500afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15500b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15500bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15500c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15500cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15500d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15500db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15500e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15500e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15500e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15500ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15500f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15500f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15500fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15500ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1550103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155010670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155010ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155010f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1550114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1550119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155011eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1550123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1550128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155012db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1550132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1550137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155013cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155014120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155014590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155014a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155014e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1550152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155015750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155015bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155016030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1550164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155016910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1550170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155017580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155017840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155017e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155018640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155018ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155018f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1550198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155019d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15501a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15501a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15501ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15501afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15501b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15501b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15501bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15501c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15501c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15501cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15501d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15501d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15501d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15501de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15501e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15501e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15501ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15501f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15501f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15501f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15501fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155020320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1550207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155020c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155021100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1550215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155021a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155021ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155022380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155022820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155022cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155023160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155023600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155023aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155023f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1550243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155024880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155024d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1550251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155025660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155025b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155025fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155026440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1550268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155026d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155027220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1550276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155027b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155028000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1550284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155028940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155028de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155029280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155029720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155029bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15502a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15502a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15502a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15502ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15502b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15502b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15502bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15502c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15502c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15502ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15502cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15502d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15502d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15502dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15502e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15502e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15502ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15502ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15502f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15502f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15502fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155030180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155030620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155030ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155030f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155031400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1550318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155031d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1550321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155032680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155032b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155032fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155033460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1550339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155034450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1550349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155034c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155035270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155035880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155035e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1550364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155036ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1550372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155037740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155037be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155038080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155038830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155038d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1550392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155039820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155039d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15503a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15503a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15503ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15503b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15503b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15503bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15503c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15503c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15503cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15503d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15503d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15503dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15503e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15503e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15503ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15503f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15503f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15503fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155040260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1550407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155040d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155041250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1550417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155041cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155042240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155042790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155042ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155043230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155043780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155043cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155044220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153f0a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153f0a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153f0a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153f0ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153f0b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153f0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153f0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153f0bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153f0c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153f0c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153f0ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153f0d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153f0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153f0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153f0de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153f0e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153f0e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153f0ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153f0f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153f0f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153f0f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153f0fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153f101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153f10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153f10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153f10f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153f113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153f11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153f11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153f12100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153f12570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153f129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153f12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153f132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153f13730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153f14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153f14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153f15040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153f15760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153f15a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153f15ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153f16150 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1552046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155204b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155204fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155205430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1552058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155205d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155206180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1552065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155206a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155206ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155207340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155207a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155208580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155208d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155209540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155209c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15520a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15520aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15520b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15520b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15520c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15520c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15520ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15520d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15520dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15520df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15520e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15520e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15520eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15520ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15520f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15520f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15520fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155210030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1552104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155210910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155210d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1552111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155211660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155211ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155211f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1552123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155212820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155212c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155213100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155213570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1552139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155213e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1552142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155214730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155214ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155215010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155215480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1552158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155215d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1552161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155216740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155216c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1552170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155217520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155217990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155217e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155218270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1552186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155218b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155218fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155219430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1552198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155219d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15521a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15521a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15521aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15521aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15521b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15521b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15521bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15521c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15521c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15521c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15521cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15521d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15521d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15521db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15521dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15521e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15521e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15521ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15521f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15521f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15521fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15521feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155220320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155220790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155220c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155221070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1552214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155221950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155221dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155222230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1552226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155222b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155222f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1552233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155223860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155223cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155224140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1552245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155224a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155224e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155225300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155225770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155225be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155226050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1552264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155226930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155226da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155227210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155227680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155227af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155227f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1552283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155228840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155228cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155229120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155229590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155229a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155229e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15522a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15522a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15522abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15522b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15522b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15522b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15522bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15522c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15522c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15522cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15522cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15522d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15522d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15522dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15522e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15522e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15522e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15522ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15522f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15522f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15522fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155230010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155230480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1552308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155230d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1552311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155231640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155231ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155231f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155232390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155232800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155232c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1552330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155233550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1552339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155233e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1552342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155234710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155234b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155234ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155235b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155235e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155236100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155236570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1552369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155236e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1552372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155237730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155237ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155238010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155238480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1552388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155238d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1552391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155239640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155239ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155239f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15523a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15523a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15523ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15523b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15523b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15523b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15523be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15523c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15523c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15523cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15523cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15523d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15523d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15523dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15523e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15523e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15523ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15523ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15523f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15523f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15523fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1552400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155240530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1552409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155240e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155241280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1552416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155241b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155241fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155242440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1552428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155242d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155243190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155243600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155243a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155243ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155244350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1552447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155244c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1552450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155245510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155245980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155245df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155246260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1552466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155246b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155246fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155247420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155247890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155247d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155248170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1552485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155248a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155248ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155249a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15524a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15524a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15524af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15524b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15524b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15524b950 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.940s
user	0m0.238s
sys	0m0.126s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
