### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.29 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.18 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.47 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.87 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.90 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  189.58 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.86 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.85 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 250.25 sec*proc (29 tests)

Total Test time (real) = 250.26 sec

real	4m10.323s
user	8m24.881s
sys	0m7.086s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.88 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.95 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.23 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.75 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.74 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.36 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.07 sec*proc (29 tests)

Total Test time (real) =  55.08 sec

real	0m55.089s
user	1m17.088s
sys	0m6.235s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.209 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.307 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.981 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.991 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.992 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.993 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.994 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.995 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.996 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.996 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.997 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.001 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.004 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.005 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.006 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.006 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.007 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.008 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.008 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.107 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.109 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.110 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.110 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.110 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.111 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.031.112 I llama_model_loader: - type  f32:  124 tensors
0.00.031.112 I llama_model_loader: - type  f16:   73 tensors
0.00.031.113 I print_info: file format = GGUF V3 (latest)
0.00.031.114 I print_info: file type   = F16
0.00.031.115 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.035.418 I load: special tokens cache size = 5
0.00.037.621 I load: token to piece cache size = 0.2032 MB
0.00.037.626 I print_info: arch             = bert
0.00.037.626 I print_info: vocab_only       = 0
0.00.037.626 I print_info: n_ctx_train      = 512
0.00.037.627 I print_info: n_embd           = 384
0.00.037.627 I print_info: n_layer          = 12
0.00.037.630 I print_info: n_head           = 12
0.00.037.631 I print_info: n_head_kv        = 12
0.00.037.631 I print_info: n_rot            = 32
0.00.037.631 I print_info: n_swa            = 0
0.00.037.633 I print_info: n_embd_head_k    = 32
0.00.037.633 I print_info: n_embd_head_v    = 32
0.00.037.634 I print_info: n_gqa            = 1
0.00.037.635 I print_info: n_embd_k_gqa     = 384
0.00.037.636 I print_info: n_embd_v_gqa     = 384
0.00.037.636 I print_info: f_norm_eps       = 1.0e-12
0.00.037.643 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.643 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.643 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.643 I print_info: f_logit_scale    = 0.0e+00
0.00.037.644 I print_info: n_ff             = 1536
0.00.037.645 I print_info: n_expert         = 0
0.00.037.645 I print_info: n_expert_used    = 0
0.00.037.645 I print_info: causal attn      = 0
0.00.037.646 I print_info: pooling type     = 2
0.00.037.646 I print_info: rope type        = 2
0.00.037.646 I print_info: rope scaling     = linear
0.00.037.647 I print_info: freq_base_train  = 10000.0
0.00.037.647 I print_info: freq_scale_train = 1
0.00.037.647 I print_info: n_ctx_orig_yarn  = 512
0.00.037.648 I print_info: rope_finetuned   = unknown
0.00.037.648 I print_info: ssm_d_conv       = 0
0.00.037.648 I print_info: ssm_d_inner      = 0
0.00.037.648 I print_info: ssm_d_state      = 0
0.00.037.649 I print_info: ssm_dt_rank      = 0
0.00.037.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.649 I print_info: model type       = 33M
0.00.037.650 I print_info: model params     = 33.21 M
0.00.037.650 I print_info: general.name     = Bge Small
0.00.037.651 I print_info: vocab type       = WPM
0.00.037.651 I print_info: n_vocab          = 30522
0.00.037.653 I print_info: n_merges         = 0
0.00.037.653 I print_info: BOS token        = 101 '[CLS]'
0.00.037.654 I print_info: UNK token        = 100 '[UNK]'
0.00.037.654 I print_info: SEP token        = 102 '[SEP]'
0.00.037.654 I print_info: PAD token        = 0 '[PAD]'
0.00.037.655 I print_info: MASK token       = 103 '[MASK]'
0.00.037.655 I print_info: LF token         = 0 '[PAD]'
0.00.037.655 I print_info: max token length = 21
0.00.037.656 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.040.807 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.808 I load_tensors: offloading output layer to GPU
0.00.040.809 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.833 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.835 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.131 I llama_init_from_model: n_seq_max     = 1
0.00.041.133 I llama_init_from_model: n_ctx         = 512
0.00.041.133 I llama_init_from_model: n_ctx_per_seq = 512
0.00.041.134 I llama_init_from_model: n_batch       = 2048
0.00.041.134 I llama_init_from_model: n_ubatch      = 2048
0.00.041.134 I llama_init_from_model: flash_attn    = 0
0.00.041.135 I llama_init_from_model: freq_base     = 10000.0
0.00.041.135 I llama_init_from_model: freq_scale    = 1
0.00.041.136 I ggml_metal_init: allocating
0.00.041.141 I ggml_metal_init: found device: Apple M4
0.00.041.146 I ggml_metal_init: picking default device: Apple M4
0.00.041.862 I ggml_metal_init: using embedded metal library
0.00.045.945 I ggml_metal_init: GPU name:   Apple M4
0.00.045.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.949 I ggml_metal_init: simdgroup reduction   = true
0.00.045.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.950 I ggml_metal_init: has residency sets    = true
0.00.045.950 I ggml_metal_init: has bfloat            = true
0.00.045.950 I ggml_metal_init: use bfloat            = true
0.00.045.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.520 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.197 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.199 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.221 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.387 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.388 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.389 I llama_init_from_model: graph nodes  = 429
0.00.060.389 I llama_init_from_model: graph splits = 2
0.00.060.390 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.256 I 
0.00.066.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.928 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.077 I llama_perf_context_print:        load time =      46.93 ms
0.00.072.080 I llama_perf_context_print: prompt eval time =       5.01 ms /     9 tokens (    0.56 ms per token,  1794.62 tokens per second)
0.00.072.080 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.081 I llama_perf_context_print:       total time =       5.82 ms /    10 tokens
0.00.072.226 I ggml_metal_free: deallocating

real	0m0.253s
user	0m0.050s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.387 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.069 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.076 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.077 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.077 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.083 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.087 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.088 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.089 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.089 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.089 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.092 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.092 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.094 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.094 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.094 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.095 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.578 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.278 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.279 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.280 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.280 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.280 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.281 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.281 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.282 I llama_model_loader: - type  f32:  124 tensors
0.00.015.282 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.282 I print_info: file format = GGUF V3 (latest)
0.00.015.283 I print_info: file type   = Q8_0
0.00.015.284 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.889 I load: special tokens cache size = 5
0.00.019.214 I load: token to piece cache size = 0.2032 MB
0.00.019.218 I print_info: arch             = bert
0.00.019.218 I print_info: vocab_only       = 0
0.00.019.218 I print_info: n_ctx_train      = 512
0.00.019.218 I print_info: n_embd           = 384
0.00.019.218 I print_info: n_layer          = 12
0.00.019.221 I print_info: n_head           = 12
0.00.019.222 I print_info: n_head_kv        = 12
0.00.019.222 I print_info: n_rot            = 32
0.00.019.222 I print_info: n_swa            = 0
0.00.019.225 I print_info: n_embd_head_k    = 32
0.00.019.225 I print_info: n_embd_head_v    = 32
0.00.019.226 I print_info: n_gqa            = 1
0.00.019.226 I print_info: n_embd_k_gqa     = 384
0.00.019.227 I print_info: n_embd_v_gqa     = 384
0.00.019.227 I print_info: f_norm_eps       = 1.0e-12
0.00.019.228 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.229 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.229 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.229 I print_info: f_logit_scale    = 0.0e+00
0.00.019.230 I print_info: n_ff             = 1536
0.00.019.230 I print_info: n_expert         = 0
0.00.019.230 I print_info: n_expert_used    = 0
0.00.019.230 I print_info: causal attn      = 0
0.00.019.230 I print_info: pooling type     = 2
0.00.019.230 I print_info: rope type        = 2
0.00.019.231 I print_info: rope scaling     = linear
0.00.019.231 I print_info: freq_base_train  = 10000.0
0.00.019.231 I print_info: freq_scale_train = 1
0.00.019.231 I print_info: n_ctx_orig_yarn  = 512
0.00.019.231 I print_info: rope_finetuned   = unknown
0.00.019.231 I print_info: ssm_d_conv       = 0
0.00.019.232 I print_info: ssm_d_inner      = 0
0.00.019.232 I print_info: ssm_d_state      = 0
0.00.019.232 I print_info: ssm_dt_rank      = 0
0.00.019.232 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.232 I print_info: model type       = 33M
0.00.019.233 I print_info: model params     = 33.21 M
0.00.019.233 I print_info: general.name     = Bge Small
0.00.019.237 I print_info: vocab type       = WPM
0.00.019.237 I print_info: n_vocab          = 30522
0.00.019.237 I print_info: n_merges         = 0
0.00.019.237 I print_info: BOS token        = 101 '[CLS]'
0.00.019.238 I print_info: UNK token        = 100 '[UNK]'
0.00.019.238 I print_info: SEP token        = 102 '[SEP]'
0.00.019.238 I print_info: PAD token        = 0 '[PAD]'
0.00.019.238 I print_info: MASK token       = 103 '[MASK]'
0.00.019.238 I print_info: LF token         = 0 '[PAD]'
0.00.019.238 I print_info: max token length = 21
0.00.019.239 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.969 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.970 I load_tensors: offloading output layer to GPU
0.00.020.970 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.976 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.977 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.143 I llama_init_from_model: n_seq_max     = 1
0.00.021.144 I llama_init_from_model: n_ctx         = 512
0.00.021.144 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.144 I llama_init_from_model: n_batch       = 2048
0.00.021.144 I llama_init_from_model: n_ubatch      = 2048
0.00.021.144 I llama_init_from_model: flash_attn    = 0
0.00.021.145 I llama_init_from_model: freq_base     = 10000.0
0.00.021.145 I llama_init_from_model: freq_scale    = 1
0.00.021.145 I ggml_metal_init: allocating
0.00.021.149 I ggml_metal_init: found device: Apple M4
0.00.021.154 I ggml_metal_init: picking default device: Apple M4
0.00.021.685 I ggml_metal_init: using embedded metal library
0.00.024.250 I ggml_metal_init: GPU name:   Apple M4
0.00.024.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.254 I ggml_metal_init: simdgroup reduction   = true
0.00.024.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.254 I ggml_metal_init: has residency sets    = true
0.00.024.254 I ggml_metal_init: has bfloat            = true
0.00.024.254 I ggml_metal_init: use bfloat            = true
0.00.024.255 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.256 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.634 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.240 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.242 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.255 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.253 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.255 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.255 I llama_init_from_model: graph nodes  = 429
0.00.036.255 I llama_init_from_model: graph splits = 2
0.00.036.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.382 I 
0.00.040.399 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.903 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.383 I llama_perf_context_print:        load time =      30.99 ms
0.00.045.384 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2072.30 tokens per second)
0.00.045.385 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.385 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.045.605 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.288 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.905 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.856 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.864 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.867 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.868 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.869 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.870 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.871 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.877 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.878 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.878 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.882 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.882 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.885 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.342 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.963 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.964 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.965 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.965 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.965 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.966 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.966 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.966 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.967 I llama_model_loader: - type  f32:   40 tensors
0.00.050.967 I llama_model_loader: - type  f16:   30 tensors
0.00.050.968 I print_info: file format = GGUF V3 (latest)
0.00.050.969 I print_info: file type   = F16
0.00.050.970 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.055.168 W load: empty token at index 5
0.00.060.146 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.566 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.600 I load: special tokens cache size = 5
0.00.322.053 I load: token to piece cache size = 1.5060 MB
0.00.322.059 I print_info: arch             = jina-bert-v2
0.00.322.059 I print_info: vocab_only       = 0
0.00.322.059 I print_info: n_ctx_train      = 8192
0.00.322.060 I print_info: n_embd           = 384
0.00.322.060 I print_info: n_layer          = 4
0.00.322.067 I print_info: n_head           = 12
0.00.322.067 I print_info: n_head_kv        = 12
0.00.322.067 I print_info: n_rot            = 32
0.00.322.068 I print_info: n_swa            = 0
0.00.322.068 I print_info: n_embd_head_k    = 32
0.00.322.068 I print_info: n_embd_head_v    = 32
0.00.322.069 I print_info: n_gqa            = 1
0.00.322.069 I print_info: n_embd_k_gqa     = 384
0.00.322.070 I print_info: n_embd_v_gqa     = 384
0.00.322.075 I print_info: f_norm_eps       = 1.0e-12
0.00.322.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.075 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.076 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.076 I print_info: f_logit_scale    = 0.0e+00
0.00.322.076 I print_info: n_ff             = 1536
0.00.322.077 I print_info: n_expert         = 0
0.00.322.077 I print_info: n_expert_used    = 0
0.00.322.077 I print_info: causal attn      = 0
0.00.322.077 I print_info: pooling type     = -1
0.00.322.079 I print_info: rope type        = -1
0.00.322.080 I print_info: rope scaling     = linear
0.00.322.080 I print_info: freq_base_train  = 10000.0
0.00.322.081 I print_info: freq_scale_train = 1
0.00.322.081 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.081 I print_info: rope_finetuned   = unknown
0.00.322.081 I print_info: ssm_d_conv       = 0
0.00.322.083 I print_info: ssm_d_inner      = 0
0.00.322.083 I print_info: ssm_d_state      = 0
0.00.322.083 I print_info: ssm_dt_rank      = 0
0.00.322.083 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.083 I print_info: model type       = 33M
0.00.322.084 I print_info: model params     = 32.90 M
0.00.322.084 I print_info: general.name     = Jina Bert Implementation
0.00.322.086 I print_info: vocab type       = BPE
0.00.322.087 I print_info: n_vocab          = 61056
0.00.322.087 I print_info: n_merges         = 39382
0.00.322.087 I print_info: BOS token        = 0 '<s>'
0.00.322.087 I print_info: EOS token        = 2 '</s>'
0.00.322.087 I print_info: UNK token        = 3 '<unk>'
0.00.322.092 I print_info: SEP token        = 2 '</s>'
0.00.322.093 I print_info: PAD token        = 1 '<pad>'
0.00.322.093 I print_info: MASK token       = 4 '<mask>'
0.00.322.093 I print_info: EOG token        = 2 '</s>'
0.00.322.093 I print_info: max token length = 45
0.00.322.094 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.346 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.347 I load_tensors: offloading output layer to GPU
0.00.324.347 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.371 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.372 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.595 I llama_init_from_model: n_seq_max     = 1
0.00.324.596 I llama_init_from_model: n_ctx         = 8192
0.00.324.596 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.596 I llama_init_from_model: n_batch       = 2048
0.00.324.596 I llama_init_from_model: n_ubatch      = 2048
0.00.324.596 I llama_init_from_model: flash_attn    = 0
0.00.324.596 I llama_init_from_model: freq_base     = 10000.0
0.00.324.597 I llama_init_from_model: freq_scale    = 1
0.00.324.597 I ggml_metal_init: allocating
0.00.324.600 I ggml_metal_init: found device: Apple M4
0.00.324.604 I ggml_metal_init: picking default device: Apple M4
0.00.325.278 I ggml_metal_init: using embedded metal library
0.00.328.172 I ggml_metal_init: GPU name:   Apple M4
0.00.328.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.175 I ggml_metal_init: simdgroup reduction   = true
0.00.328.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.175 I ggml_metal_init: has residency sets    = true
0.00.328.175 I ggml_metal_init: has bfloat            = true
0.00.328.175 I ggml_metal_init: use bfloat            = true
0.00.328.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.672 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.780 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.782 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.802 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.380 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.381 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.382 I llama_init_from_model: graph nodes  = 154
0.00.347.382 I llama_init_from_model: graph splits = 2
0.00.347.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.828 I 
0.00.354.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.945 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.354.946 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.354.949 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.354.949 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.354.958 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.354.958 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.355.475 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.359.229 I llama_perf_context_print:        load time =     330.89 ms
0.00.359.230 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16546.57 tokens per second)
0.00.359.231 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.359.231 I llama_perf_context_print:       total time =       4.40 ms /    63 tokens
0.00.359.462 I ggml_metal_free: deallocating

real	0m1.064s
user	0m0.328s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.112 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.283 I main: llama backend init
0.00.000.290 I main: load the model and apply lora adapter, if any
0.00.042.225 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.054.554 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.054.570 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.054.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.054.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.054.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.054.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.054.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.054.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.054.577 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.054.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.054.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.054.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.054.580 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.054.581 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.054.583 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.054.584 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.054.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.061.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.063.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.070.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.070.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.070.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.070.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.070.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.070.098 I llama_model_loader: - type  f32:  194 tensors
0.00.070.098 I llama_model_loader: - type  f16:   98 tensors
0.00.070.099 I print_info: file format = GGUF V3 (latest)
0.00.070.099 I print_info: file type   = all F32 (guessed)
0.00.070.101 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.034 I load: special tokens cache size = 25
0.00.084.124 I load: token to piece cache size = 0.2984 MB
0.00.084.129 I print_info: arch             = gptneox
0.00.084.130 I print_info: vocab_only       = 0
0.00.084.130 I print_info: n_ctx_train      = 2048
0.00.084.130 I print_info: n_embd           = 2048
0.00.084.130 I print_info: n_layer          = 24
0.00.084.135 I print_info: n_head           = 16
0.00.084.136 I print_info: n_head_kv        = 16
0.00.084.136 I print_info: n_rot            = 32
0.00.084.136 I print_info: n_swa            = 0
0.00.084.136 I print_info: n_embd_head_k    = 128
0.00.084.137 I print_info: n_embd_head_v    = 128
0.00.084.137 I print_info: n_gqa            = 1
0.00.084.138 I print_info: n_embd_k_gqa     = 2048
0.00.084.139 I print_info: n_embd_v_gqa     = 2048
0.00.084.139 I print_info: f_norm_eps       = 1.0e-05
0.00.084.140 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.140 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.140 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.140 I print_info: f_logit_scale    = 0.0e+00
0.00.084.141 I print_info: n_ff             = 8192
0.00.084.141 I print_info: n_expert         = 0
0.00.084.141 I print_info: n_expert_used    = 0
0.00.084.141 I print_info: causal attn      = 1
0.00.084.141 I print_info: pooling type     = 0
0.00.084.142 I print_info: rope type        = 2
0.00.084.142 I print_info: rope scaling     = linear
0.00.084.142 I print_info: freq_base_train  = 10000.0
0.00.084.142 I print_info: freq_scale_train = 1
0.00.084.143 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.143 I print_info: rope_finetuned   = unknown
0.00.084.143 I print_info: ssm_d_conv       = 0
0.00.084.143 I print_info: ssm_d_inner      = 0
0.00.084.143 I print_info: ssm_d_state      = 0
0.00.084.143 I print_info: ssm_dt_rank      = 0
0.00.084.143 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.144 I print_info: model type       = 1.4B
0.00.084.144 I print_info: model params     = 1.41 B
0.00.084.144 I print_info: general.name     = 1.4B
0.00.084.145 I print_info: vocab type       = BPE
0.00.084.145 I print_info: n_vocab          = 50304
0.00.084.145 I print_info: n_merges         = 50009
0.00.084.145 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.146 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.146 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.146 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.146 I print_info: LF token         = 187 ''
0.00.084.147 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.147 I print_info: max token length = 1024
0.00.084.147 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.129.693 I load_tensors: offloading 24 repeating layers to GPU
0.00.129.695 I load_tensors: offloading output layer to GPU
0.00.129.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.129.721 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.129.723 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.130.255 I llama_init_from_model: n_seq_max     = 1
0.00.130.256 I llama_init_from_model: n_ctx         = 2048
0.00.130.257 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.130.257 I llama_init_from_model: n_batch       = 2048
0.00.130.257 I llama_init_from_model: n_ubatch      = 512
0.00.130.257 I llama_init_from_model: flash_attn    = 0
0.00.130.258 I llama_init_from_model: freq_base     = 10000.0
0.00.130.258 I llama_init_from_model: freq_scale    = 1
0.00.130.260 I ggml_metal_init: allocating
0.00.130.297 I ggml_metal_init: found device: Apple M4
0.00.130.303 I ggml_metal_init: picking default device: Apple M4
0.00.130.961 I ggml_metal_init: using embedded metal library
0.00.173.182 I ggml_metal_init: GPU name:   Apple M4
0.00.173.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.173.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.173.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.173.189 I ggml_metal_init: simdgroup reduction   = true
0.00.173.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.173.189 I ggml_metal_init: has residency sets    = true
0.00.173.189 I ggml_metal_init: has bfloat            = true
0.00.173.189 I ggml_metal_init: use bfloat            = true
0.00.173.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.173.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.225.318 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.255.129 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.255.134 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.255.179 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.258.617 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.258.619 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.258.619 I llama_init_from_model: graph nodes  = 967
0.00.258.620 I llama_init_from_model: graph splits = 2
0.00.258.626 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.258.756 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.258.756 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.325.442 I main: llama threadpool init, n_threads = 4
0.00.325.484 I 
0.00.325.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.325.499 I 
0.00.325.682 I sampler seed: 1234
0.00.325.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.325.711 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.325.713 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.325.713 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.163.306 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.02.163.306 I llama_perf_context_print:        load time =     282.39 ms
0.02.163.307 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.54 tokens per second)
0.02.163.308 I llama_perf_context_print:        eval time =    1791.27 ms /    63 runs   (   28.43 ms per token,    35.17 tokens per second)
0.02.163.308 I llama_perf_context_print:       total time =    1838.68 ms /    70 tokens
0.02.163.515 I ggml_metal_free: deallocating

real	0m2.466s
user	0m0.119s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.785 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.692 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.465 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.482 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.490 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.491 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.726 I llama_model_loader: - type  f32:  194 tensors
0.00.056.726 I llama_model_loader: - type  f16:   98 tensors
0.00.056.727 I print_info: file format = GGUF V3 (latest)
0.00.056.728 I print_info: file type   = all F32 (guessed)
0.00.056.729 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.060 I load: special tokens cache size = 25
0.00.078.270 I load: token to piece cache size = 0.2984 MB
0.00.078.273 I print_info: arch             = gptneox
0.00.078.273 I print_info: vocab_only       = 0
0.00.078.274 I print_info: n_ctx_train      = 2048
0.00.078.274 I print_info: n_embd           = 2048
0.00.078.274 I print_info: n_layer          = 24
0.00.078.277 I print_info: n_head           = 16
0.00.078.278 I print_info: n_head_kv        = 16
0.00.078.278 I print_info: n_rot            = 32
0.00.078.279 I print_info: n_swa            = 0
0.00.078.279 I print_info: n_embd_head_k    = 128
0.00.078.279 I print_info: n_embd_head_v    = 128
0.00.078.280 I print_info: n_gqa            = 1
0.00.078.281 I print_info: n_embd_k_gqa     = 2048
0.00.078.281 I print_info: n_embd_v_gqa     = 2048
0.00.078.282 I print_info: f_norm_eps       = 1.0e-05
0.00.078.282 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.282 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.282 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.283 I print_info: f_logit_scale    = 0.0e+00
0.00.078.283 I print_info: n_ff             = 8192
0.00.078.284 I print_info: n_expert         = 0
0.00.078.284 I print_info: n_expert_used    = 0
0.00.078.284 I print_info: causal attn      = 1
0.00.078.284 I print_info: pooling type     = 0
0.00.078.284 I print_info: rope type        = 2
0.00.078.287 I print_info: rope scaling     = linear
0.00.078.287 I print_info: freq_base_train  = 10000.0
0.00.078.288 I print_info: freq_scale_train = 1
0.00.078.288 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.288 I print_info: rope_finetuned   = unknown
0.00.078.288 I print_info: ssm_d_conv       = 0
0.00.078.288 I print_info: ssm_d_inner      = 0
0.00.078.289 I print_info: ssm_d_state      = 0
0.00.078.289 I print_info: ssm_dt_rank      = 0
0.00.078.289 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.289 I print_info: model type       = 1.4B
0.00.078.289 I print_info: model params     = 1.41 B
0.00.078.290 I print_info: general.name     = 1.4B
0.00.078.290 I print_info: vocab type       = BPE
0.00.078.290 I print_info: n_vocab          = 50304
0.00.078.290 I print_info: n_merges         = 50009
0.00.078.295 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.295 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.295 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.295 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.295 I print_info: LF token         = 187 ''
0.00.078.296 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.296 I print_info: max token length = 1024
0.00.078.296 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.488.134 I load_tensors: offloading 24 repeating layers to GPU
0.01.488.138 I load_tensors: offloading output layer to GPU
0.01.488.138 I load_tensors: offloaded 25/25 layers to GPU
0.01.488.164 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.488.165 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.489.368 I llama_init_from_model: n_seq_max     = 1
0.01.489.369 I llama_init_from_model: n_ctx         = 128
0.01.489.369 I llama_init_from_model: n_ctx_per_seq = 128
0.01.489.369 I llama_init_from_model: n_batch       = 128
0.01.489.369 I llama_init_from_model: n_ubatch      = 128
0.01.489.370 I llama_init_from_model: flash_attn    = 0
0.01.489.370 I llama_init_from_model: freq_base     = 10000.0
0.01.489.371 I llama_init_from_model: freq_scale    = 1
0.01.489.371 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.489.372 I ggml_metal_init: allocating
0.01.489.440 I ggml_metal_init: found device: Apple M4
0.01.489.447 I ggml_metal_init: picking default device: Apple M4
0.01.490.597 I ggml_metal_init: using embedded metal library
0.01.494.487 I ggml_metal_init: GPU name:   Apple M4
0.01.494.489 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.494.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.494.490 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.494.490 I ggml_metal_init: simdgroup reduction   = true
0.01.494.490 I ggml_metal_init: simdgroup matrix mul. = true
0.01.494.491 I ggml_metal_init: has residency sets    = true
0.01.494.491 I ggml_metal_init: has bfloat            = true
0.01.494.491 I ggml_metal_init: use bfloat            = true
0.01.494.492 I ggml_metal_init: hasUnifiedMemory      = true
0.01.494.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.505.210 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.506.930 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.506.932 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.506.957 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.508.574 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.508.575 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.508.576 I llama_init_from_model: graph nodes  = 967
0.01.508.576 I llama_init_from_model: graph splits = 2
0.01.508.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.508.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.543.727 I 
0.01.543.755 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.543.759 I perplexity: tokenizing the input ..
0.01.548.717 I perplexity: tokenization took 4.956 ms
0.01.548.721 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.667.103 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.668.430 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.668.462 I llama_perf_context_print:        load time =    1519.01 ms
0.01.668.463 I llama_perf_context_print: prompt eval time =     118.08 ms /   128 tokens (    0.92 ms per token,  1084.05 tokens per second)
0.01.668.464 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.668.464 I llama_perf_context_print:       total time =     124.74 ms /   129 tokens
0.01.668.846 I ggml_metal_free: deallocating

real	0m1.857s
user	0m0.099s
sys	0m0.274s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.913 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.914 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.914 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.918 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.919 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.919 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.920 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.921 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.922 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.922 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.041 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.043 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.045 I llama_model_loader: - type  f32:  194 tensors
0.00.034.045 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.046 I print_info: file format = GGUF V3 (latest)
0.00.034.047 I print_info: file type   = Q8_0
0.00.034.048 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.072 I load: special tokens cache size = 25
0.00.048.228 I load: token to piece cache size = 0.2984 MB
0.00.048.233 I print_info: arch             = gptneox
0.00.048.233 I print_info: vocab_only       = 0
0.00.048.234 I print_info: n_ctx_train      = 2048
0.00.048.237 I print_info: n_embd           = 2048
0.00.048.237 I print_info: n_layer          = 24
0.00.048.244 I print_info: n_head           = 16
0.00.048.244 I print_info: n_head_kv        = 16
0.00.048.249 I print_info: n_rot            = 32
0.00.048.249 I print_info: n_swa            = 0
0.00.048.249 I print_info: n_embd_head_k    = 128
0.00.048.250 I print_info: n_embd_head_v    = 128
0.00.048.250 I print_info: n_gqa            = 1
0.00.048.251 I print_info: n_embd_k_gqa     = 2048
0.00.048.252 I print_info: n_embd_v_gqa     = 2048
0.00.048.253 I print_info: f_norm_eps       = 1.0e-05
0.00.048.253 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.254 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.255 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.255 I print_info: f_logit_scale    = 0.0e+00
0.00.048.255 I print_info: n_ff             = 8192
0.00.048.256 I print_info: n_expert         = 0
0.00.048.256 I print_info: n_expert_used    = 0
0.00.048.256 I print_info: causal attn      = 1
0.00.048.256 I print_info: pooling type     = 0
0.00.048.256 I print_info: rope type        = 2
0.00.048.259 I print_info: rope scaling     = linear
0.00.048.260 I print_info: freq_base_train  = 10000.0
0.00.048.260 I print_info: freq_scale_train = 1
0.00.048.261 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.261 I print_info: rope_finetuned   = unknown
0.00.048.261 I print_info: ssm_d_conv       = 0
0.00.048.261 I print_info: ssm_d_inner      = 0
0.00.048.261 I print_info: ssm_d_state      = 0
0.00.048.261 I print_info: ssm_dt_rank      = 0
0.00.048.262 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.262 I print_info: model type       = 1.4B
0.00.048.265 I print_info: model params     = 1.41 B
0.00.048.265 I print_info: general.name     = 1.4B
0.00.048.266 I print_info: vocab type       = BPE
0.00.048.266 I print_info: n_vocab          = 50304
0.00.048.266 I print_info: n_merges         = 50009
0.00.048.266 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.266 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.268 I print_info: LF token         = 187 ''
0.00.048.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.268 I print_info: max token length = 1024
0.00.048.268 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.119.768 I load_tensors: offloading 24 repeating layers to GPU
0.01.119.774 I load_tensors: offloading output layer to GPU
0.01.119.775 I load_tensors: offloaded 25/25 layers to GPU
0.01.119.797 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.119.799 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.120.799 I llama_init_from_model: n_seq_max     = 1
0.01.120.801 I llama_init_from_model: n_ctx         = 2048
0.01.120.802 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.120.802 I llama_init_from_model: n_batch       = 2048
0.01.120.803 I llama_init_from_model: n_ubatch      = 512
0.01.120.803 I llama_init_from_model: flash_attn    = 0
0.01.120.804 I llama_init_from_model: freq_base     = 10000.0
0.01.120.804 I llama_init_from_model: freq_scale    = 1
0.01.120.805 I ggml_metal_init: allocating
0.01.120.825 I ggml_metal_init: found device: Apple M4
0.01.120.835 I ggml_metal_init: picking default device: Apple M4
0.01.122.236 I ggml_metal_init: using embedded metal library
0.01.127.849 I ggml_metal_init: GPU name:   Apple M4
0.01.127.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.127.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.127.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.127.854 I ggml_metal_init: simdgroup reduction   = true
0.01.127.855 I ggml_metal_init: simdgroup matrix mul. = true
0.01.127.855 I ggml_metal_init: has residency sets    = true
0.01.127.855 I ggml_metal_init: has bfloat            = true
0.01.127.855 I ggml_metal_init: use bfloat            = true
0.01.127.856 I ggml_metal_init: hasUnifiedMemory      = true
0.01.127.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.143.532 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.195.192 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.195.200 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.195.240 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.199.660 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.199.661 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.199.662 I llama_init_from_model: graph nodes  = 967
0.01.199.662 I llama_init_from_model: graph splits = 2
0.01.199.668 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.199.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.199.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.257.392 I main: llama threadpool init, n_threads = 4
0.01.257.431 I 
0.01.257.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.257.446 I 
0.01.257.598 I sampler seed: 1234
0.01.257.602 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.257.638 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.257.641 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.257.641 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.339.322 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.02.339.323 I llama_perf_context_print:        load time =    1246.68 ms
0.02.339.324 I llama_perf_context_print: prompt eval time =      45.32 ms /     7 tokens (    6.47 ms per token,   154.45 tokens per second)
0.02.339.324 I llama_perf_context_print:        eval time =    1033.55 ms /    63 runs   (   16.41 ms per token,    60.96 tokens per second)
0.02.339.325 I llama_perf_context_print:       total time =    1082.61 ms /    70 tokens
0.02.339.577 I ggml_metal_free: deallocating

real	0m2.358s
user	0m0.108s
sys	0m0.276s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.171 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.635 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.636 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.636 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.636 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.407 I llama_model_loader: - type  f32:  194 tensors
0.00.025.408 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.408 I print_info: file format = GGUF V3 (latest)
0.00.025.409 I print_info: file type   = Q8_0
0.00.025.410 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.440 I load: special tokens cache size = 25
0.00.039.506 I load: token to piece cache size = 0.2984 MB
0.00.039.511 I print_info: arch             = gptneox
0.00.039.511 I print_info: vocab_only       = 0
0.00.039.511 I print_info: n_ctx_train      = 2048
0.00.039.511 I print_info: n_embd           = 2048
0.00.039.512 I print_info: n_layer          = 24
0.00.039.516 I print_info: n_head           = 16
0.00.039.517 I print_info: n_head_kv        = 16
0.00.039.517 I print_info: n_rot            = 32
0.00.039.517 I print_info: n_swa            = 0
0.00.039.517 I print_info: n_embd_head_k    = 128
0.00.039.518 I print_info: n_embd_head_v    = 128
0.00.039.521 I print_info: n_gqa            = 1
0.00.039.522 I print_info: n_embd_k_gqa     = 2048
0.00.039.523 I print_info: n_embd_v_gqa     = 2048
0.00.039.523 I print_info: f_norm_eps       = 1.0e-05
0.00.039.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.524 I print_info: f_logit_scale    = 0.0e+00
0.00.039.525 I print_info: n_ff             = 8192
0.00.039.525 I print_info: n_expert         = 0
0.00.039.525 I print_info: n_expert_used    = 0
0.00.039.525 I print_info: causal attn      = 1
0.00.039.526 I print_info: pooling type     = 0
0.00.039.535 I print_info: rope type        = 2
0.00.039.537 I print_info: rope scaling     = linear
0.00.039.538 I print_info: freq_base_train  = 10000.0
0.00.039.540 I print_info: freq_scale_train = 1
0.00.039.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.540 I print_info: rope_finetuned   = unknown
0.00.039.541 I print_info: ssm_d_conv       = 0
0.00.039.541 I print_info: ssm_d_inner      = 0
0.00.039.541 I print_info: ssm_d_state      = 0
0.00.039.541 I print_info: ssm_dt_rank      = 0
0.00.039.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.542 I print_info: model type       = 1.4B
0.00.039.542 I print_info: model params     = 1.41 B
0.00.039.542 I print_info: general.name     = 1.4B
0.00.039.543 I print_info: vocab type       = BPE
0.00.039.543 I print_info: n_vocab          = 50304
0.00.039.543 I print_info: n_merges         = 50009
0.00.039.543 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: LF token         = 187 ''
0.00.039.545 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.545 I print_info: max token length = 1024
0.00.039.549 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.892.015 I load_tensors: offloading 24 repeating layers to GPU
0.00.892.021 I load_tensors: offloading output layer to GPU
0.00.892.022 I load_tensors: offloaded 25/25 layers to GPU
0.00.892.051 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.892.053 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.893.558 I llama_init_from_model: n_seq_max     = 1
0.00.893.560 I llama_init_from_model: n_ctx         = 128
0.00.893.561 I llama_init_from_model: n_ctx_per_seq = 128
0.00.893.561 I llama_init_from_model: n_batch       = 128
0.00.893.561 I llama_init_from_model: n_ubatch      = 128
0.00.893.562 I llama_init_from_model: flash_attn    = 0
0.00.893.563 I llama_init_from_model: freq_base     = 10000.0
0.00.893.563 I llama_init_from_model: freq_scale    = 1
0.00.893.564 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.893.565 I ggml_metal_init: allocating
0.00.893.654 I ggml_metal_init: found device: Apple M4
0.00.893.666 I ggml_metal_init: picking default device: Apple M4
0.00.895.073 I ggml_metal_init: using embedded metal library
0.00.900.314 I ggml_metal_init: GPU name:   Apple M4
0.00.900.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.900.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.900.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.900.319 I ggml_metal_init: simdgroup reduction   = true
0.00.900.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.900.320 I ggml_metal_init: has residency sets    = true
0.00.900.320 I ggml_metal_init: has bfloat            = true
0.00.900.320 I ggml_metal_init: use bfloat            = true
0.00.900.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.900.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.915.324 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.918.713 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.918.716 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.918.754 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.922.145 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.922.147 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.922.147 I llama_init_from_model: graph nodes  = 967
0.00.922.147 I llama_init_from_model: graph splits = 2
0.00.922.151 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.948.626 I 
0.00.948.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.948.741 I perplexity: tokenizing the input ..
0.00.955.737 I perplexity: tokenization took 6.992 ms
0.00.955.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.092.738 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.094.079 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.094.105 I llama_perf_context_print:        load time =     939.44 ms
0.01.094.106 I llama_perf_context_print: prompt eval time =     136.38 ms /   128 tokens (    1.07 ms per token,   938.56 tokens per second)
0.01.094.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.094.107 I llama_perf_context_print:       total time =     145.48 ms /   129 tokens
0.01.094.484 I ggml_metal_free: deallocating

real	0m1.110s
user	0m0.076s
sys	0m0.162s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.015.590 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.777 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.779 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.779 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.780 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.780 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.780 I llama_model_loader: - type  f32:  194 tensors
0.00.046.781 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.781 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.782 I print_info: file format = GGUF V3 (latest)
0.00.046.782 I print_info: file type   = Q4_0
0.00.046.783 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.453 I load: special tokens cache size = 25
0.00.065.662 I load: token to piece cache size = 0.2984 MB
0.00.065.666 I print_info: arch             = gptneox
0.00.065.666 I print_info: vocab_only       = 0
0.00.065.667 I print_info: n_ctx_train      = 2048
0.00.065.667 I print_info: n_embd           = 2048
0.00.065.667 I print_info: n_layer          = 24
0.00.065.672 I print_info: n_head           = 16
0.00.065.673 I print_info: n_head_kv        = 16
0.00.065.673 I print_info: n_rot            = 32
0.00.065.674 I print_info: n_swa            = 0
0.00.065.674 I print_info: n_embd_head_k    = 128
0.00.065.674 I print_info: n_embd_head_v    = 128
0.00.065.675 I print_info: n_gqa            = 1
0.00.065.676 I print_info: n_embd_k_gqa     = 2048
0.00.065.676 I print_info: n_embd_v_gqa     = 2048
0.00.065.677 I print_info: f_norm_eps       = 1.0e-05
0.00.065.678 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.678 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.678 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.678 I print_info: f_logit_scale    = 0.0e+00
0.00.065.679 I print_info: n_ff             = 8192
0.00.065.679 I print_info: n_expert         = 0
0.00.065.679 I print_info: n_expert_used    = 0
0.00.065.680 I print_info: causal attn      = 1
0.00.065.680 I print_info: pooling type     = 0
0.00.065.680 I print_info: rope type        = 2
0.00.065.680 I print_info: rope scaling     = linear
0.00.065.684 I print_info: freq_base_train  = 10000.0
0.00.065.684 I print_info: freq_scale_train = 1
0.00.065.684 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.685 I print_info: rope_finetuned   = unknown
0.00.065.685 I print_info: ssm_d_conv       = 0
0.00.065.685 I print_info: ssm_d_inner      = 0
0.00.065.685 I print_info: ssm_d_state      = 0
0.00.065.685 I print_info: ssm_dt_rank      = 0
0.00.065.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.686 I print_info: model type       = 1.4B
0.00.065.686 I print_info: model params     = 1.41 B
0.00.065.686 I print_info: general.name     = 1.4B
0.00.065.687 I print_info: vocab type       = BPE
0.00.065.687 I print_info: n_vocab          = 50304
0.00.065.688 I print_info: n_merges         = 50009
0.00.065.688 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.688 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.689 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.689 I print_info: LF token         = 187 ''
0.00.065.689 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.689 I print_info: max token length = 1024
0.00.065.695 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.702.897 I load_tensors: offloading 24 repeating layers to GPU
0.00.702.913 I load_tensors: offloading output layer to GPU
0.00.702.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.702.949 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.702.950 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.704.406 I llama_init_from_model: n_seq_max     = 1
0.00.704.408 I llama_init_from_model: n_ctx         = 2048
0.00.704.409 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.704.409 I llama_init_from_model: n_batch       = 2048
0.00.704.410 I llama_init_from_model: n_ubatch      = 512
0.00.704.410 I llama_init_from_model: flash_attn    = 0
0.00.704.413 I llama_init_from_model: freq_base     = 10000.0
0.00.704.414 I llama_init_from_model: freq_scale    = 1
0.00.704.416 I ggml_metal_init: allocating
0.00.704.542 I ggml_metal_init: found device: Apple M4
0.00.704.556 I ggml_metal_init: picking default device: Apple M4
0.00.706.588 I ggml_metal_init: using embedded metal library
0.00.713.319 I ggml_metal_init: GPU name:   Apple M4
0.00.713.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.713.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.713.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.713.325 I ggml_metal_init: simdgroup reduction   = true
0.00.713.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.713.326 I ggml_metal_init: has residency sets    = true
0.00.713.326 I ggml_metal_init: has bfloat            = true
0.00.713.326 I ggml_metal_init: use bfloat            = true
0.00.713.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.713.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.349 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.792.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.792.245 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.792.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.796.932 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.796.934 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.796.934 I llama_init_from_model: graph nodes  = 967
0.00.796.934 I llama_init_from_model: graph splits = 2
0.00.796.941 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.797.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.797.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.629 I main: llama threadpool init, n_threads = 4
0.00.855.669 I 
0.00.855.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.684 I 
0.00.855.857 I sampler seed: 1234
0.00.855.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.908 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.912 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.912 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.533.768 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.533.769 I llama_perf_context_print:        load time =     839.30 ms
0.01.533.770 I llama_perf_context_print: prompt eval time =      45.79 ms /     7 tokens (    6.54 ms per token,   152.87 tokens per second)
0.01.533.770 I llama_perf_context_print:        eval time =     629.23 ms /    63 runs   (    9.99 ms per token,   100.12 tokens per second)
0.01.533.770 I llama_perf_context_print:       total time =     678.87 ms /    70 tokens
0.01.534.035 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.119s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.566 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.877 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.896 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.832 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.725 I llama_model_loader: - type  f32:  194 tensors
0.00.025.725 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.726 I print_info: file format = GGUF V3 (latest)
0.00.025.727 I print_info: file type   = Q4_0
0.00.025.732 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.970 I load: special tokens cache size = 25
0.00.040.090 I load: token to piece cache size = 0.2984 MB
0.00.040.094 I print_info: arch             = gptneox
0.00.040.095 I print_info: vocab_only       = 0
0.00.040.095 I print_info: n_ctx_train      = 2048
0.00.040.095 I print_info: n_embd           = 2048
0.00.040.095 I print_info: n_layer          = 24
0.00.040.100 I print_info: n_head           = 16
0.00.040.100 I print_info: n_head_kv        = 16
0.00.040.101 I print_info: n_rot            = 32
0.00.040.101 I print_info: n_swa            = 0
0.00.040.101 I print_info: n_embd_head_k    = 128
0.00.040.101 I print_info: n_embd_head_v    = 128
0.00.040.104 I print_info: n_gqa            = 1
0.00.040.105 I print_info: n_embd_k_gqa     = 2048
0.00.040.106 I print_info: n_embd_v_gqa     = 2048
0.00.040.106 I print_info: f_norm_eps       = 1.0e-05
0.00.040.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.107 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.107 I print_info: f_logit_scale    = 0.0e+00
0.00.040.108 I print_info: n_ff             = 8192
0.00.040.108 I print_info: n_expert         = 0
0.00.040.108 I print_info: n_expert_used    = 0
0.00.040.108 I print_info: causal attn      = 1
0.00.040.108 I print_info: pooling type     = 0
0.00.040.108 I print_info: rope type        = 2
0.00.040.109 I print_info: rope scaling     = linear
0.00.040.109 I print_info: freq_base_train  = 10000.0
0.00.040.109 I print_info: freq_scale_train = 1
0.00.040.109 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.110 I print_info: rope_finetuned   = unknown
0.00.040.110 I print_info: ssm_d_conv       = 0
0.00.040.110 I print_info: ssm_d_inner      = 0
0.00.040.110 I print_info: ssm_d_state      = 0
0.00.040.110 I print_info: ssm_dt_rank      = 0
0.00.040.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.111 I print_info: model type       = 1.4B
0.00.040.112 I print_info: model params     = 1.41 B
0.00.040.112 I print_info: general.name     = 1.4B
0.00.040.113 I print_info: vocab type       = BPE
0.00.040.113 I print_info: n_vocab          = 50304
0.00.040.113 I print_info: n_merges         = 50009
0.00.040.113 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.113 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.114 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.114 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.114 I print_info: LF token         = 187 ''
0.00.040.114 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.114 I print_info: max token length = 1024
0.00.040.115 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.192 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.207 I load_tensors: offloading output layer to GPU
0.00.622.207 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.244 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.622.245 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.623.924 I llama_init_from_model: n_seq_max     = 1
0.00.623.927 I llama_init_from_model: n_ctx         = 128
0.00.623.927 I llama_init_from_model: n_ctx_per_seq = 128
0.00.623.928 I llama_init_from_model: n_batch       = 128
0.00.623.928 I llama_init_from_model: n_ubatch      = 128
0.00.623.929 I llama_init_from_model: flash_attn    = 0
0.00.623.930 I llama_init_from_model: freq_base     = 10000.0
0.00.623.931 I llama_init_from_model: freq_scale    = 1
0.00.623.932 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.623.934 I ggml_metal_init: allocating
0.00.624.006 I ggml_metal_init: found device: Apple M4
0.00.624.018 I ggml_metal_init: picking default device: Apple M4
0.00.625.828 I ggml_metal_init: using embedded metal library
0.00.631.247 I ggml_metal_init: GPU name:   Apple M4
0.00.631.265 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.265 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.266 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.267 I ggml_metal_init: simdgroup reduction   = true
0.00.631.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.267 I ggml_metal_init: has residency sets    = true
0.00.631.268 I ggml_metal_init: has bfloat            = true
0.00.631.268 I ggml_metal_init: use bfloat            = true
0.00.631.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.274 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.609 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.257 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.655.264 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.655.332 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.664 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.665 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.666 I llama_init_from_model: graph nodes  = 967
0.00.658.666 I llama_init_from_model: graph splits = 2
0.00.658.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.688 I 
0.00.685.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.750 I perplexity: tokenizing the input ..
0.00.693.170 I perplexity: tokenization took 7.417 ms
0.00.693.177 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.177 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.830.589 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.830.609 I llama_perf_context_print:        load time =     676.10 ms
0.00.830.610 I llama_perf_context_print: prompt eval time =     135.06 ms /   128 tokens (    1.06 ms per token,   947.73 tokens per second)
0.00.830.610 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.611 I llama_perf_context_print:       total time =     144.93 ms /   129 tokens
0.00.830.948 I ggml_metal_free: deallocating

real	0m0.847s
user	0m0.081s
sys	0m0.144s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.740 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.757 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.757 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.630 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.631 I llama_model_loader: - type  f32:  194 tensors
0.00.034.632 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.632 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.632 I print_info: file format = GGUF V3 (latest)
0.00.034.633 I print_info: file type   = Q4_1
0.00.034.634 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.343 I load: special tokens cache size = 25
0.00.050.008 I load: token to piece cache size = 0.2984 MB
0.00.050.011 I print_info: arch             = gptneox
0.00.050.012 I print_info: vocab_only       = 0
0.00.050.012 I print_info: n_ctx_train      = 2048
0.00.050.012 I print_info: n_embd           = 2048
0.00.050.012 I print_info: n_layer          = 24
0.00.050.016 I print_info: n_head           = 16
0.00.050.016 I print_info: n_head_kv        = 16
0.00.050.016 I print_info: n_rot            = 32
0.00.050.018 I print_info: n_swa            = 0
0.00.050.018 I print_info: n_embd_head_k    = 128
0.00.050.019 I print_info: n_embd_head_v    = 128
0.00.050.019 I print_info: n_gqa            = 1
0.00.050.020 I print_info: n_embd_k_gqa     = 2048
0.00.050.025 I print_info: n_embd_v_gqa     = 2048
0.00.050.026 I print_info: f_norm_eps       = 1.0e-05
0.00.050.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.027 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.027 I print_info: f_logit_scale    = 0.0e+00
0.00.050.027 I print_info: n_ff             = 8192
0.00.050.028 I print_info: n_expert         = 0
0.00.050.028 I print_info: n_expert_used    = 0
0.00.050.028 I print_info: causal attn      = 1
0.00.050.028 I print_info: pooling type     = 0
0.00.050.028 I print_info: rope type        = 2
0.00.050.028 I print_info: rope scaling     = linear
0.00.050.029 I print_info: freq_base_train  = 10000.0
0.00.050.029 I print_info: freq_scale_train = 1
0.00.050.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.029 I print_info: rope_finetuned   = unknown
0.00.050.029 I print_info: ssm_d_conv       = 0
0.00.050.030 I print_info: ssm_d_inner      = 0
0.00.050.030 I print_info: ssm_d_state      = 0
0.00.050.030 I print_info: ssm_dt_rank      = 0
0.00.050.030 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.030 I print_info: model type       = 1.4B
0.00.050.030 I print_info: model params     = 1.41 B
0.00.050.030 I print_info: general.name     = 1.4B
0.00.050.031 I print_info: vocab type       = BPE
0.00.050.031 I print_info: n_vocab          = 50304
0.00.050.031 I print_info: n_merges         = 50009
0.00.050.032 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.032 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.032 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.032 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.032 I print_info: LF token         = 187 ''
0.00.050.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.033 I print_info: max token length = 1024
0.00.050.033 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.208 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.220 I load_tensors: offloading output layer to GPU
0.00.669.221 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.250 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.669.253 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.670.652 I llama_init_from_model: n_seq_max     = 1
0.00.670.657 I llama_init_from_model: n_ctx         = 2048
0.00.670.658 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.659 I llama_init_from_model: n_batch       = 2048
0.00.670.659 I llama_init_from_model: n_ubatch      = 512
0.00.670.660 I llama_init_from_model: flash_attn    = 0
0.00.670.661 I llama_init_from_model: freq_base     = 10000.0
0.00.670.661 I llama_init_from_model: freq_scale    = 1
0.00.670.664 I ggml_metal_init: allocating
0.00.670.710 I ggml_metal_init: found device: Apple M4
0.00.670.724 I ggml_metal_init: picking default device: Apple M4
0.00.672.480 I ggml_metal_init: using embedded metal library
0.00.679.241 I ggml_metal_init: GPU name:   Apple M4
0.00.679.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.249 I ggml_metal_init: simdgroup reduction   = true
0.00.679.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.249 I ggml_metal_init: has residency sets    = true
0.00.679.250 I ggml_metal_init: has bfloat            = true
0.00.679.250 I ggml_metal_init: use bfloat            = true
0.00.679.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.739 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.267 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.758.276 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.758.313 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.762.578 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.762.580 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.762.581 I llama_init_from_model: graph nodes  = 967
0.00.762.581 I llama_init_from_model: graph splits = 2
0.00.762.586 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.762.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.762.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.066 I main: llama threadpool init, n_threads = 4
0.00.816.113 I 
0.00.816.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.129 I 
0.00.816.281 I sampler seed: 1234
0.00.816.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.297 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.297 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.299 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.533.521 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.533.522 I llama_perf_context_print:        load time =     806.56 ms
0.01.533.526 I llama_perf_context_print: prompt eval time =      39.29 ms /     7 tokens (    5.61 ms per token,   178.15 tokens per second)
0.01.533.526 I llama_perf_context_print:        eval time =     675.24 ms /    63 runs   (   10.72 ms per token,    93.30 tokens per second)
0.01.533.527 I llama_perf_context_print:       total time =     718.13 ms /    70 tokens
0.01.533.818 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.112s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.467 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.477 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.384 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.386 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.387 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.387 I llama_model_loader: - type  f32:  194 tensors
0.00.026.388 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.388 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.389 I print_info: file format = GGUF V3 (latest)
0.00.026.389 I print_info: file type   = Q4_1
0.00.026.390 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.511 I load: special tokens cache size = 25
0.00.040.607 I load: token to piece cache size = 0.2984 MB
0.00.040.611 I print_info: arch             = gptneox
0.00.040.611 I print_info: vocab_only       = 0
0.00.040.611 I print_info: n_ctx_train      = 2048
0.00.040.611 I print_info: n_embd           = 2048
0.00.040.612 I print_info: n_layer          = 24
0.00.040.615 I print_info: n_head           = 16
0.00.040.616 I print_info: n_head_kv        = 16
0.00.040.616 I print_info: n_rot            = 32
0.00.040.616 I print_info: n_swa            = 0
0.00.040.616 I print_info: n_embd_head_k    = 128
0.00.040.616 I print_info: n_embd_head_v    = 128
0.00.040.617 I print_info: n_gqa            = 1
0.00.040.618 I print_info: n_embd_k_gqa     = 2048
0.00.040.618 I print_info: n_embd_v_gqa     = 2048
0.00.040.618 I print_info: f_norm_eps       = 1.0e-05
0.00.040.619 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.619 I print_info: f_logit_scale    = 0.0e+00
0.00.040.620 I print_info: n_ff             = 8192
0.00.040.620 I print_info: n_expert         = 0
0.00.040.620 I print_info: n_expert_used    = 0
0.00.040.620 I print_info: causal attn      = 1
0.00.040.620 I print_info: pooling type     = 0
0.00.040.620 I print_info: rope type        = 2
0.00.040.620 I print_info: rope scaling     = linear
0.00.040.621 I print_info: freq_base_train  = 10000.0
0.00.040.621 I print_info: freq_scale_train = 1
0.00.040.621 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.622 I print_info: rope_finetuned   = unknown
0.00.040.622 I print_info: ssm_d_conv       = 0
0.00.040.622 I print_info: ssm_d_inner      = 0
0.00.040.622 I print_info: ssm_d_state      = 0
0.00.040.622 I print_info: ssm_dt_rank      = 0
0.00.040.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.632 I print_info: model type       = 1.4B
0.00.040.632 I print_info: model params     = 1.41 B
0.00.040.632 I print_info: general.name     = 1.4B
0.00.040.633 I print_info: vocab type       = BPE
0.00.040.633 I print_info: n_vocab          = 50304
0.00.040.633 I print_info: n_merges         = 50009
0.00.040.633 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.633 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: LF token         = 187 ''
0.00.040.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: max token length = 1024
0.00.040.637 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.070 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.073 I load_tensors: offloading output layer to GPU
0.00.602.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.093 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.602.094 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.602.940 I llama_init_from_model: n_seq_max     = 1
0.00.602.943 I llama_init_from_model: n_ctx         = 128
0.00.602.944 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.944 I llama_init_from_model: n_batch       = 128
0.00.602.944 I llama_init_from_model: n_ubatch      = 128
0.00.602.945 I llama_init_from_model: flash_attn    = 0
0.00.602.946 I llama_init_from_model: freq_base     = 10000.0
0.00.602.947 I llama_init_from_model: freq_scale    = 1
0.00.602.947 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.948 I ggml_metal_init: allocating
0.00.602.999 I ggml_metal_init: found device: Apple M4
0.00.603.009 I ggml_metal_init: picking default device: Apple M4
0.00.604.044 I ggml_metal_init: using embedded metal library
0.00.608.148 I ggml_metal_init: GPU name:   Apple M4
0.00.608.157 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.159 I ggml_metal_init: simdgroup reduction   = true
0.00.608.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.160 I ggml_metal_init: has residency sets    = true
0.00.608.160 I ggml_metal_init: has bfloat            = true
0.00.608.160 I ggml_metal_init: use bfloat            = true
0.00.608.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.676 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.256 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.258 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.284 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.834 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.835 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.836 I llama_init_from_model: graph nodes  = 967
0.00.624.836 I llama_init_from_model: graph splits = 2
0.00.624.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.220 I 
0.00.650.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.249 I perplexity: tokenizing the input ..
0.00.654.094 I perplexity: tokenization took 3.843 ms
0.00.654.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.630 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.788.130 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.788.148 I llama_perf_context_print:        load time =     641.37 ms
0.00.788.150 I llama_perf_context_print: prompt eval time =     132.31 ms /   128 tokens (    1.03 ms per token,   967.46 tokens per second)
0.00.788.150 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.151 I llama_perf_context_print:       total time =     137.93 ms /   129 tokens
0.00.788.471 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.068s
sys	0m0.094s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.404 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.027.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.036.281 I llama_model_loader: - type  f32:  194 tensors
0.00.036.281 I llama_model_loader: - type q5_0:   97 tensors
0.00.036.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.282 I print_info: file format = GGUF V3 (latest)
0.00.036.282 I print_info: file type   = Q5_0
0.00.036.283 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.544 I load: special tokens cache size = 25
0.00.051.508 I load: token to piece cache size = 0.2984 MB
0.00.051.511 I print_info: arch             = gptneox
0.00.051.512 I print_info: vocab_only       = 0
0.00.051.512 I print_info: n_ctx_train      = 2048
0.00.051.512 I print_info: n_embd           = 2048
0.00.051.512 I print_info: n_layer          = 24
0.00.051.515 I print_info: n_head           = 16
0.00.051.516 I print_info: n_head_kv        = 16
0.00.051.517 I print_info: n_rot            = 32
0.00.051.517 I print_info: n_swa            = 0
0.00.051.517 I print_info: n_embd_head_k    = 128
0.00.051.517 I print_info: n_embd_head_v    = 128
0.00.051.518 I print_info: n_gqa            = 1
0.00.051.519 I print_info: n_embd_k_gqa     = 2048
0.00.051.519 I print_info: n_embd_v_gqa     = 2048
0.00.051.520 I print_info: f_norm_eps       = 1.0e-05
0.00.051.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.521 I print_info: f_logit_scale    = 0.0e+00
0.00.051.521 I print_info: n_ff             = 8192
0.00.051.522 I print_info: n_expert         = 0
0.00.051.522 I print_info: n_expert_used    = 0
0.00.051.522 I print_info: causal attn      = 1
0.00.051.522 I print_info: pooling type     = 0
0.00.051.524 I print_info: rope type        = 2
0.00.051.525 I print_info: rope scaling     = linear
0.00.051.526 I print_info: freq_base_train  = 10000.0
0.00.051.526 I print_info: freq_scale_train = 1
0.00.051.526 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.526 I print_info: rope_finetuned   = unknown
0.00.051.527 I print_info: ssm_d_conv       = 0
0.00.051.527 I print_info: ssm_d_inner      = 0
0.00.051.527 I print_info: ssm_d_state      = 0
0.00.051.527 I print_info: ssm_dt_rank      = 0
0.00.051.527 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.527 I print_info: model type       = 1.4B
0.00.051.528 I print_info: model params     = 1.41 B
0.00.051.528 I print_info: general.name     = 1.4B
0.00.051.528 I print_info: vocab type       = BPE
0.00.051.528 I print_info: n_vocab          = 50304
0.00.051.529 I print_info: n_merges         = 50009
0.00.051.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.534 I print_info: LF token         = 187 ''
0.00.051.534 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.535 I print_info: max token length = 1024
0.00.051.535 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.667.990 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.997 I load_tensors: offloading output layer to GPU
0.00.667.998 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.015 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.668.016 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.668.980 I llama_init_from_model: n_seq_max     = 1
0.00.668.984 I llama_init_from_model: n_ctx         = 2048
0.00.668.985 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.985 I llama_init_from_model: n_batch       = 2048
0.00.668.985 I llama_init_from_model: n_ubatch      = 512
0.00.668.986 I llama_init_from_model: flash_attn    = 0
0.00.668.987 I llama_init_from_model: freq_base     = 10000.0
0.00.668.988 I llama_init_from_model: freq_scale    = 1
0.00.668.996 I ggml_metal_init: allocating
0.00.669.052 I ggml_metal_init: found device: Apple M4
0.00.669.065 I ggml_metal_init: picking default device: Apple M4
0.00.670.213 I ggml_metal_init: using embedded metal library
0.00.674.374 I ggml_metal_init: GPU name:   Apple M4
0.00.674.383 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.383 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.384 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.384 I ggml_metal_init: simdgroup reduction   = true
0.00.674.385 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.385 I ggml_metal_init: has residency sets    = true
0.00.674.385 I ggml_metal_init: has bfloat            = true
0.00.674.385 I ggml_metal_init: use bfloat            = true
0.00.674.387 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.628 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.720.571 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.438 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.440 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.440 I llama_init_from_model: graph nodes  = 967
0.00.725.441 I llama_init_from_model: graph splits = 2
0.00.725.446 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.328 I main: llama threadpool init, n_threads = 4
0.00.773.370 I 
0.00.773.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.387 I 
0.00.773.495 I sampler seed: 1234
0.00.773.499 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.510 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.510 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.510 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.562.251 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.562.251 I llama_perf_context_print:        load time =     763.87 ms
0.01.562.252 I llama_perf_context_print: prompt eval time =      42.89 ms /     7 tokens (    6.13 ms per token,   163.20 tokens per second)
0.01.562.253 I llama_perf_context_print:        eval time =     742.82 ms /    63 runs   (   11.79 ms per token,    84.81 tokens per second)
0.01.562.253 I llama_perf_context_print:       total time =     789.60 ms /    70 tokens
0.01.562.442 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.105s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.672 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.408 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.426 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.376 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.285 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.286 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.286 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.287 I llama_model_loader: - type  f32:  194 tensors
0.00.025.287 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.288 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.288 I print_info: file format = GGUF V3 (latest)
0.00.025.289 I print_info: file type   = Q5_0
0.00.025.290 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.637 I load: special tokens cache size = 25
0.00.039.904 I load: token to piece cache size = 0.2984 MB
0.00.039.908 I print_info: arch             = gptneox
0.00.039.909 I print_info: vocab_only       = 0
0.00.039.909 I print_info: n_ctx_train      = 2048
0.00.039.909 I print_info: n_embd           = 2048
0.00.039.909 I print_info: n_layer          = 24
0.00.039.913 I print_info: n_head           = 16
0.00.039.914 I print_info: n_head_kv        = 16
0.00.039.914 I print_info: n_rot            = 32
0.00.039.914 I print_info: n_swa            = 0
0.00.039.915 I print_info: n_embd_head_k    = 128
0.00.039.917 I print_info: n_embd_head_v    = 128
0.00.039.918 I print_info: n_gqa            = 1
0.00.039.919 I print_info: n_embd_k_gqa     = 2048
0.00.039.919 I print_info: n_embd_v_gqa     = 2048
0.00.039.920 I print_info: f_norm_eps       = 1.0e-05
0.00.039.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.921 I print_info: f_logit_scale    = 0.0e+00
0.00.039.921 I print_info: n_ff             = 8192
0.00.039.921 I print_info: n_expert         = 0
0.00.039.921 I print_info: n_expert_used    = 0
0.00.039.922 I print_info: causal attn      = 1
0.00.039.922 I print_info: pooling type     = 0
0.00.039.922 I print_info: rope type        = 2
0.00.039.923 I print_info: rope scaling     = linear
0.00.039.923 I print_info: freq_base_train  = 10000.0
0.00.039.923 I print_info: freq_scale_train = 1
0.00.039.923 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.924 I print_info: rope_finetuned   = unknown
0.00.039.924 I print_info: ssm_d_conv       = 0
0.00.039.924 I print_info: ssm_d_inner      = 0
0.00.039.924 I print_info: ssm_d_state      = 0
0.00.039.924 I print_info: ssm_dt_rank      = 0
0.00.039.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.925 I print_info: model type       = 1.4B
0.00.039.925 I print_info: model params     = 1.41 B
0.00.039.925 I print_info: general.name     = 1.4B
0.00.039.926 I print_info: vocab type       = BPE
0.00.039.926 I print_info: n_vocab          = 50304
0.00.039.926 I print_info: n_merges         = 50009
0.00.039.926 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.927 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.927 I print_info: LF token         = 187 ''
0.00.039.927 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.927 I print_info: max token length = 1024
0.00.039.927 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.596 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.609 I load_tensors: offloading output layer to GPU
0.00.650.610 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.641 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.650.643 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.652.141 I llama_init_from_model: n_seq_max     = 1
0.00.652.146 I llama_init_from_model: n_ctx         = 128
0.00.652.147 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.147 I llama_init_from_model: n_batch       = 128
0.00.652.147 I llama_init_from_model: n_ubatch      = 128
0.00.652.148 I llama_init_from_model: flash_attn    = 0
0.00.652.149 I llama_init_from_model: freq_base     = 10000.0
0.00.652.150 I llama_init_from_model: freq_scale    = 1
0.00.652.151 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.153 I ggml_metal_init: allocating
0.00.652.200 I ggml_metal_init: found device: Apple M4
0.00.652.211 I ggml_metal_init: picking default device: Apple M4
0.00.653.658 I ggml_metal_init: using embedded metal library
0.00.660.168 I ggml_metal_init: GPU name:   Apple M4
0.00.660.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.175 I ggml_metal_init: simdgroup reduction   = true
0.00.660.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.175 I ggml_metal_init: has residency sets    = true
0.00.660.176 I ggml_metal_init: has bfloat            = true
0.00.660.176 I ggml_metal_init: use bfloat            = true
0.00.660.177 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.525 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.963 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.681.966 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.682.008 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.565 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.685.567 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.685.567 I llama_init_from_model: graph nodes  = 967
0.00.685.568 I llama_init_from_model: graph splits = 2
0.00.685.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.500 I 
0.00.719.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.570 I perplexity: tokenizing the input ..
0.00.724.745 I perplexity: tokenization took 5.173 ms
0.00.724.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.436 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.872.799 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.872.824 I llama_perf_context_print:        load time =     710.81 ms
0.00.872.824 I llama_perf_context_print: prompt eval time =     146.46 ms /   128 tokens (    1.14 ms per token,   873.98 tokens per second)
0.00.872.827 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.827 I llama_perf_context_print:       total time =     153.33 ms /   129 tokens
0.00.873.197 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.078s
sys	0m0.151s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.354 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.355 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.356 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.357 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.357 I llama_model_loader: - type  f32:  194 tensors
0.00.026.357 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.358 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.358 I print_info: file format = GGUF V3 (latest)
0.00.026.359 I print_info: file type   = Q5_1
0.00.026.360 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.463 I load: special tokens cache size = 25
0.00.040.414 I load: token to piece cache size = 0.2984 MB
0.00.040.419 I print_info: arch             = gptneox
0.00.040.419 I print_info: vocab_only       = 0
0.00.040.420 I print_info: n_ctx_train      = 2048
0.00.040.424 I print_info: n_embd           = 2048
0.00.040.424 I print_info: n_layer          = 24
0.00.040.428 I print_info: n_head           = 16
0.00.040.429 I print_info: n_head_kv        = 16
0.00.040.429 I print_info: n_rot            = 32
0.00.040.430 I print_info: n_swa            = 0
0.00.040.430 I print_info: n_embd_head_k    = 128
0.00.040.430 I print_info: n_embd_head_v    = 128
0.00.040.431 I print_info: n_gqa            = 1
0.00.040.431 I print_info: n_embd_k_gqa     = 2048
0.00.040.432 I print_info: n_embd_v_gqa     = 2048
0.00.040.433 I print_info: f_norm_eps       = 1.0e-05
0.00.040.433 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.435 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.435 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.435 I print_info: f_logit_scale    = 0.0e+00
0.00.040.436 I print_info: n_ff             = 8192
0.00.040.436 I print_info: n_expert         = 0
0.00.040.436 I print_info: n_expert_used    = 0
0.00.040.436 I print_info: causal attn      = 1
0.00.040.437 I print_info: pooling type     = 0
0.00.040.437 I print_info: rope type        = 2
0.00.040.437 I print_info: rope scaling     = linear
0.00.040.437 I print_info: freq_base_train  = 10000.0
0.00.040.438 I print_info: freq_scale_train = 1
0.00.040.438 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.438 I print_info: rope_finetuned   = unknown
0.00.040.441 I print_info: ssm_d_conv       = 0
0.00.040.441 I print_info: ssm_d_inner      = 0
0.00.040.441 I print_info: ssm_d_state      = 0
0.00.040.441 I print_info: ssm_dt_rank      = 0
0.00.040.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.442 I print_info: model type       = 1.4B
0.00.040.442 I print_info: model params     = 1.41 B
0.00.040.443 I print_info: general.name     = 1.4B
0.00.040.443 I print_info: vocab type       = BPE
0.00.040.443 I print_info: n_vocab          = 50304
0.00.040.443 I print_info: n_merges         = 50009
0.00.040.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.444 I print_info: LF token         = 187 ''
0.00.040.445 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: max token length = 1024
0.00.040.446 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.910 I load_tensors: offloading output layer to GPU
0.00.632.911 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.945 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.632.952 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.634.458 I llama_init_from_model: n_seq_max     = 1
0.00.634.462 I llama_init_from_model: n_ctx         = 2048
0.00.634.462 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.463 I llama_init_from_model: n_batch       = 2048
0.00.634.463 I llama_init_from_model: n_ubatch      = 512
0.00.634.464 I llama_init_from_model: flash_attn    = 0
0.00.634.466 I llama_init_from_model: freq_base     = 10000.0
0.00.634.466 I llama_init_from_model: freq_scale    = 1
0.00.634.480 I ggml_metal_init: allocating
0.00.634.530 I ggml_metal_init: found device: Apple M4
0.00.634.554 I ggml_metal_init: picking default device: Apple M4
0.00.636.379 I ggml_metal_init: using embedded metal library
0.00.642.878 I ggml_metal_init: GPU name:   Apple M4
0.00.642.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.883 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.883 I ggml_metal_init: simdgroup reduction   = true
0.00.642.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.884 I ggml_metal_init: has residency sets    = true
0.00.642.884 I ggml_metal_init: has bfloat            = true
0.00.642.885 I ggml_metal_init: use bfloat            = true
0.00.642.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.627 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.920 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.714.927 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.714.962 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.314 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.316 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.317 I llama_init_from_model: graph nodes  = 967
0.00.720.317 I llama_init_from_model: graph splits = 2
0.00.720.323 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.897 I main: llama threadpool init, n_threads = 4
0.00.778.940 I 
0.00.778.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.956 I 
0.00.779.125 I sampler seed: 1234
0.00.779.130 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.150 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.151 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.633.147 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49859.55 tokens per second)
0.01.633.147 I llama_perf_context_print:        load time =     768.29 ms
0.01.633.148 I llama_perf_context_print: prompt eval time =      51.94 ms /     7 tokens (    7.42 ms per token,   134.78 tokens per second)
0.01.633.148 I llama_perf_context_print:        eval time =     799.34 ms /    63 runs   (   12.69 ms per token,    78.82 tokens per second)
0.01.633.150 I llama_perf_context_print:       total time =     854.94 ms /    70 tokens
0.01.633.420 I ggml_metal_free: deallocating

real	0m1.651s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.571 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.726 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.730 I llama_model_loader: - type  f32:  194 tensors
0.00.026.730 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.730 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.731 I print_info: file format = GGUF V3 (latest)
0.00.026.731 I print_info: file type   = Q5_1
0.00.026.732 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.651 I load: special tokens cache size = 25
0.00.040.722 I load: token to piece cache size = 0.2984 MB
0.00.040.725 I print_info: arch             = gptneox
0.00.040.726 I print_info: vocab_only       = 0
0.00.040.726 I print_info: n_ctx_train      = 2048
0.00.040.726 I print_info: n_embd           = 2048
0.00.040.726 I print_info: n_layer          = 24
0.00.040.729 I print_info: n_head           = 16
0.00.040.730 I print_info: n_head_kv        = 16
0.00.040.730 I print_info: n_rot            = 32
0.00.040.730 I print_info: n_swa            = 0
0.00.040.731 I print_info: n_embd_head_k    = 128
0.00.040.731 I print_info: n_embd_head_v    = 128
0.00.040.733 I print_info: n_gqa            = 1
0.00.040.734 I print_info: n_embd_k_gqa     = 2048
0.00.040.734 I print_info: n_embd_v_gqa     = 2048
0.00.040.735 I print_info: f_norm_eps       = 1.0e-05
0.00.040.735 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.735 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.736 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.736 I print_info: f_logit_scale    = 0.0e+00
0.00.040.736 I print_info: n_ff             = 8192
0.00.040.736 I print_info: n_expert         = 0
0.00.040.737 I print_info: n_expert_used    = 0
0.00.040.737 I print_info: causal attn      = 1
0.00.040.737 I print_info: pooling type     = 0
0.00.040.740 I print_info: rope type        = 2
0.00.040.740 I print_info: rope scaling     = linear
0.00.040.741 I print_info: freq_base_train  = 10000.0
0.00.040.742 I print_info: freq_scale_train = 1
0.00.040.742 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.742 I print_info: rope_finetuned   = unknown
0.00.040.743 I print_info: ssm_d_conv       = 0
0.00.040.743 I print_info: ssm_d_inner      = 0
0.00.040.743 I print_info: ssm_d_state      = 0
0.00.040.743 I print_info: ssm_dt_rank      = 0
0.00.040.743 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.743 I print_info: model type       = 1.4B
0.00.040.744 I print_info: model params     = 1.41 B
0.00.040.744 I print_info: general.name     = 1.4B
0.00.040.744 I print_info: vocab type       = BPE
0.00.040.744 I print_info: n_vocab          = 50304
0.00.040.745 I print_info: n_merges         = 50009
0.00.040.745 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.746 I print_info: LF token         = 187 ''
0.00.040.746 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.746 I print_info: max token length = 1024
0.00.040.746 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.734 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.741 I load_tensors: offloading output layer to GPU
0.00.642.742 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.771 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.642.774 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.643.968 I llama_init_from_model: n_seq_max     = 1
0.00.643.973 I llama_init_from_model: n_ctx         = 128
0.00.643.973 I llama_init_from_model: n_ctx_per_seq = 128
0.00.643.974 I llama_init_from_model: n_batch       = 128
0.00.643.974 I llama_init_from_model: n_ubatch      = 128
0.00.643.974 I llama_init_from_model: flash_attn    = 0
0.00.643.976 I llama_init_from_model: freq_base     = 10000.0
0.00.643.976 I llama_init_from_model: freq_scale    = 1
0.00.643.977 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.643.982 I ggml_metal_init: allocating
0.00.644.059 I ggml_metal_init: found device: Apple M4
0.00.644.073 I ggml_metal_init: picking default device: Apple M4
0.00.645.777 I ggml_metal_init: using embedded metal library
0.00.652.219 I ggml_metal_init: GPU name:   Apple M4
0.00.652.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.229 I ggml_metal_init: simdgroup reduction   = true
0.00.652.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.230 I ggml_metal_init: has residency sets    = true
0.00.652.230 I ggml_metal_init: has bfloat            = true
0.00.652.230 I ggml_metal_init: use bfloat            = true
0.00.652.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.601 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.084 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.088 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.134 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.348 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.350 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.351 I llama_init_from_model: graph nodes  = 967
0.00.676.351 I llama_init_from_model: graph splits = 2
0.00.676.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.475 I 
0.00.707.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.539 I perplexity: tokenizing the input ..
0.00.712.954 I perplexity: tokenization took 5.414 ms
0.00.712.961 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.860.676 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.861.993 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.862.016 I llama_perf_context_print:        load time =     696.89 ms
0.00.862.016 I llama_perf_context_print: prompt eval time =     147.43 ms /   128 tokens (    1.15 ms per token,   868.21 tokens per second)
0.00.862.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.018 I llama_perf_context_print:       total time =     154.55 ms /   129 tokens
0.00.862.411 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.076s
sys	0m0.130s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.015 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.621 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.623 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.624 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.619 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.623 I llama_model_loader: - type  f32:  194 tensors
0.00.024.624 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.624 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.624 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.625 I print_info: file format = GGUF V3 (latest)
0.00.024.625 I print_info: file type   = Q2_K - Medium
0.00.024.630 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.146 I load: special tokens cache size = 25
0.00.039.517 I load: token to piece cache size = 0.2984 MB
0.00.039.521 I print_info: arch             = gptneox
0.00.039.522 I print_info: vocab_only       = 0
0.00.039.522 I print_info: n_ctx_train      = 2048
0.00.039.522 I print_info: n_embd           = 2048
0.00.039.522 I print_info: n_layer          = 24
0.00.039.526 I print_info: n_head           = 16
0.00.039.526 I print_info: n_head_kv        = 16
0.00.039.527 I print_info: n_rot            = 32
0.00.039.528 I print_info: n_swa            = 0
0.00.039.528 I print_info: n_embd_head_k    = 128
0.00.039.528 I print_info: n_embd_head_v    = 128
0.00.039.529 I print_info: n_gqa            = 1
0.00.039.530 I print_info: n_embd_k_gqa     = 2048
0.00.039.530 I print_info: n_embd_v_gqa     = 2048
0.00.039.531 I print_info: f_norm_eps       = 1.0e-05
0.00.039.531 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.531 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.531 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.531 I print_info: f_logit_scale    = 0.0e+00
0.00.039.532 I print_info: n_ff             = 8192
0.00.039.532 I print_info: n_expert         = 0
0.00.039.532 I print_info: n_expert_used    = 0
0.00.039.533 I print_info: causal attn      = 1
0.00.039.533 I print_info: pooling type     = 0
0.00.039.534 I print_info: rope type        = 2
0.00.039.535 I print_info: rope scaling     = linear
0.00.039.536 I print_info: freq_base_train  = 10000.0
0.00.039.536 I print_info: freq_scale_train = 1
0.00.039.536 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.536 I print_info: rope_finetuned   = unknown
0.00.039.536 I print_info: ssm_d_conv       = 0
0.00.039.536 I print_info: ssm_d_inner      = 0
0.00.039.538 I print_info: ssm_d_state      = 0
0.00.039.538 I print_info: ssm_dt_rank      = 0
0.00.039.538 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.538 I print_info: model type       = 1.4B
0.00.039.543 I print_info: model params     = 1.41 B
0.00.039.545 I print_info: general.name     = 1.4B
0.00.039.548 I print_info: vocab type       = BPE
0.00.039.548 I print_info: n_vocab          = 50304
0.00.039.548 I print_info: n_merges         = 50009
0.00.039.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.550 I print_info: LF token         = 187 ''
0.00.039.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.550 I print_info: max token length = 1024
0.00.039.551 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.343.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.343.370 I load_tensors: offloading output layer to GPU
0.00.343.371 I load_tensors: offloaded 25/25 layers to GPU
0.00.343.404 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.343.406 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.344.556 I llama_init_from_model: n_seq_max     = 1
0.00.344.559 I llama_init_from_model: n_ctx         = 2048
0.00.344.560 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.344.560 I llama_init_from_model: n_batch       = 2048
0.00.344.560 I llama_init_from_model: n_ubatch      = 512
0.00.344.561 I llama_init_from_model: flash_attn    = 0
0.00.344.563 I llama_init_from_model: freq_base     = 10000.0
0.00.344.563 I llama_init_from_model: freq_scale    = 1
0.00.344.573 I ggml_metal_init: allocating
0.00.344.701 I ggml_metal_init: found device: Apple M4
0.00.344.715 I ggml_metal_init: picking default device: Apple M4
0.00.346.578 I ggml_metal_init: using embedded metal library
0.00.351.347 I ggml_metal_init: GPU name:   Apple M4
0.00.351.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.358 I ggml_metal_init: simdgroup reduction   = true
0.00.351.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.359 I ggml_metal_init: has residency sets    = true
0.00.351.359 I ggml_metal_init: has bfloat            = true
0.00.351.360 I ggml_metal_init: use bfloat            = true
0.00.351.361 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.370.199 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.424.441 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.424.448 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.424.482 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.429.334 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.429.337 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.429.337 I llama_init_from_model: graph nodes  = 967
0.00.429.337 I llama_init_from_model: graph splits = 2
0.00.429.342 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.429.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.429.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.794 I main: llama threadpool init, n_threads = 4
0.00.490.834 I 
0.00.490.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.850 I 
0.00.491.028 I sampler seed: 1234
0.00.491.032 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.491.043 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.491.043 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.491.043 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.178.319 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.178.320 I llama_perf_context_print:        load time =     481.04 ms
0.01.178.320 I llama_perf_context_print: prompt eval time =      44.18 ms /     7 tokens (    6.31 ms per token,   158.44 tokens per second)
0.01.178.322 I llama_perf_context_print:        eval time =     640.35 ms /    63 runs   (   10.16 ms per token,    98.38 tokens per second)
0.01.178.327 I llama_perf_context_print:       total time =     688.26 ms /    70 tokens
0.01.178.617 I ggml_metal_free: deallocating

real	0m1.198s
user	0m0.112s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.990 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.039.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.999 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.000 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.000 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.005 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.007 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.008 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.886 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.048.865 I llama_model_loader: - type  f32:  194 tensors
0.00.048.865 I llama_model_loader: - type q2_K:   49 tensors
0.00.048.865 I llama_model_loader: - type q3_K:   48 tensors
0.00.048.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.866 I print_info: file format = GGUF V3 (latest)
0.00.048.871 I print_info: file type   = Q2_K - Medium
0.00.048.872 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.057.386 I load: special tokens cache size = 25
0.00.063.377 I load: token to piece cache size = 0.2984 MB
0.00.063.381 I print_info: arch             = gptneox
0.00.063.381 I print_info: vocab_only       = 0
0.00.063.382 I print_info: n_ctx_train      = 2048
0.00.063.382 I print_info: n_embd           = 2048
0.00.063.382 I print_info: n_layer          = 24
0.00.063.387 I print_info: n_head           = 16
0.00.063.387 I print_info: n_head_kv        = 16
0.00.063.388 I print_info: n_rot            = 32
0.00.063.392 I print_info: n_swa            = 0
0.00.063.392 I print_info: n_embd_head_k    = 128
0.00.063.392 I print_info: n_embd_head_v    = 128
0.00.063.393 I print_info: n_gqa            = 1
0.00.063.393 I print_info: n_embd_k_gqa     = 2048
0.00.063.394 I print_info: n_embd_v_gqa     = 2048
0.00.063.394 I print_info: f_norm_eps       = 1.0e-05
0.00.063.395 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.395 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.395 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.395 I print_info: f_logit_scale    = 0.0e+00
0.00.063.396 I print_info: n_ff             = 8192
0.00.063.396 I print_info: n_expert         = 0
0.00.063.396 I print_info: n_expert_used    = 0
0.00.063.397 I print_info: causal attn      = 1
0.00.063.397 I print_info: pooling type     = 0
0.00.063.397 I print_info: rope type        = 2
0.00.063.402 I print_info: rope scaling     = linear
0.00.063.404 I print_info: freq_base_train  = 10000.0
0.00.063.404 I print_info: freq_scale_train = 1
0.00.063.404 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.404 I print_info: rope_finetuned   = unknown
0.00.063.405 I print_info: ssm_d_conv       = 0
0.00.063.405 I print_info: ssm_d_inner      = 0
0.00.063.405 I print_info: ssm_d_state      = 0
0.00.063.405 I print_info: ssm_dt_rank      = 0
0.00.063.405 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.405 I print_info: model type       = 1.4B
0.00.063.406 I print_info: model params     = 1.41 B
0.00.063.406 I print_info: general.name     = 1.4B
0.00.063.406 I print_info: vocab type       = BPE
0.00.063.407 I print_info: n_vocab          = 50304
0.00.063.407 I print_info: n_merges         = 50009
0.00.063.407 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.407 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.407 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.408 I print_info: LF token         = 187 ''
0.00.063.408 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.408 I print_info: max token length = 1024
0.00.063.409 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.394.293 I load_tensors: offloading 24 repeating layers to GPU
0.00.394.305 I load_tensors: offloading output layer to GPU
0.00.394.305 I load_tensors: offloaded 25/25 layers to GPU
0.00.394.337 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.394.338 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.395.551 I llama_init_from_model: n_seq_max     = 1
0.00.395.555 I llama_init_from_model: n_ctx         = 128
0.00.395.556 I llama_init_from_model: n_ctx_per_seq = 128
0.00.395.556 I llama_init_from_model: n_batch       = 128
0.00.395.556 I llama_init_from_model: n_ubatch      = 128
0.00.395.557 I llama_init_from_model: flash_attn    = 0
0.00.395.558 I llama_init_from_model: freq_base     = 10000.0
0.00.395.558 I llama_init_from_model: freq_scale    = 1
0.00.395.559 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.395.561 I ggml_metal_init: allocating
0.00.395.633 I ggml_metal_init: found device: Apple M4
0.00.395.663 I ggml_metal_init: picking default device: Apple M4
0.00.397.412 I ggml_metal_init: using embedded metal library
0.00.402.710 I ggml_metal_init: GPU name:   Apple M4
0.00.402.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.402.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.402.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.402.738 I ggml_metal_init: simdgroup reduction   = true
0.00.402.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.402.738 I ggml_metal_init: has residency sets    = true
0.00.402.739 I ggml_metal_init: has bfloat            = true
0.00.402.739 I ggml_metal_init: use bfloat            = true
0.00.402.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.402.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.423.722 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.427.444 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.427.451 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.427.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.430.916 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.430.918 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.430.918 I llama_init_from_model: graph nodes  = 967
0.00.430.919 I llama_init_from_model: graph splits = 2
0.00.430.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.430.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.463.931 I 
0.00.463.991 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.464.000 I perplexity: tokenizing the input ..
0.00.470.186 I perplexity: tokenization took 6.184 ms
0.00.470.190 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.747 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.617.078 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.617.101 I llama_perf_context_print:        load time =     454.86 ms
0.00.617.102 I llama_perf_context_print: prompt eval time =     145.17 ms /   128 tokens (    1.13 ms per token,   881.70 tokens per second)
0.00.617.103 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.103 I llama_perf_context_print:       total time =     153.17 ms /   129 tokens
0.00.617.479 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.080s
sys	0m0.098s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.442 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.043 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.044 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.732 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.735 I llama_model_loader: - type  f32:  194 tensors
0.00.025.736 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.736 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.736 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.736 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.737 I print_info: file format = GGUF V3 (latest)
0.00.025.737 I print_info: file type   = Q3_K - Medium
0.00.025.738 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.576 I load: special tokens cache size = 25
0.00.039.584 I load: token to piece cache size = 0.2984 MB
0.00.039.587 I print_info: arch             = gptneox
0.00.039.587 I print_info: vocab_only       = 0
0.00.039.588 I print_info: n_ctx_train      = 2048
0.00.039.588 I print_info: n_embd           = 2048
0.00.039.588 I print_info: n_layer          = 24
0.00.039.591 I print_info: n_head           = 16
0.00.039.592 I print_info: n_head_kv        = 16
0.00.039.592 I print_info: n_rot            = 32
0.00.039.592 I print_info: n_swa            = 0
0.00.039.592 I print_info: n_embd_head_k    = 128
0.00.039.593 I print_info: n_embd_head_v    = 128
0.00.039.596 I print_info: n_gqa            = 1
0.00.039.596 I print_info: n_embd_k_gqa     = 2048
0.00.039.597 I print_info: n_embd_v_gqa     = 2048
0.00.039.598 I print_info: f_norm_eps       = 1.0e-05
0.00.039.598 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.599 I print_info: f_logit_scale    = 0.0e+00
0.00.039.600 I print_info: n_ff             = 8192
0.00.039.600 I print_info: n_expert         = 0
0.00.039.600 I print_info: n_expert_used    = 0
0.00.039.602 I print_info: causal attn      = 1
0.00.039.603 I print_info: pooling type     = 0
0.00.039.603 I print_info: rope type        = 2
0.00.039.603 I print_info: rope scaling     = linear
0.00.039.603 I print_info: freq_base_train  = 10000.0
0.00.039.604 I print_info: freq_scale_train = 1
0.00.039.604 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.604 I print_info: rope_finetuned   = unknown
0.00.039.604 I print_info: ssm_d_conv       = 0
0.00.039.605 I print_info: ssm_d_inner      = 0
0.00.039.605 I print_info: ssm_d_state      = 0
0.00.039.609 I print_info: ssm_dt_rank      = 0
0.00.039.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.609 I print_info: model type       = 1.4B
0.00.039.609 I print_info: model params     = 1.41 B
0.00.039.609 I print_info: general.name     = 1.4B
0.00.039.610 I print_info: vocab type       = BPE
0.00.039.610 I print_info: n_vocab          = 50304
0.00.039.610 I print_info: n_merges         = 50009
0.00.039.611 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: LF token         = 187 ''
0.00.039.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: max token length = 1024
0.00.039.612 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.450.639 I load_tensors: offloading 24 repeating layers to GPU
0.00.450.657 I load_tensors: offloading output layer to GPU
0.00.450.657 I load_tensors: offloaded 25/25 layers to GPU
0.00.450.693 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.450.704 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.452.447 I llama_init_from_model: n_seq_max     = 1
0.00.452.450 I llama_init_from_model: n_ctx         = 2048
0.00.452.451 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.452.452 I llama_init_from_model: n_batch       = 2048
0.00.452.452 I llama_init_from_model: n_ubatch      = 512
0.00.452.452 I llama_init_from_model: flash_attn    = 0
0.00.452.454 I llama_init_from_model: freq_base     = 10000.0
0.00.452.455 I llama_init_from_model: freq_scale    = 1
0.00.452.457 I ggml_metal_init: allocating
0.00.452.529 I ggml_metal_init: found device: Apple M4
0.00.452.542 I ggml_metal_init: picking default device: Apple M4
0.00.454.515 I ggml_metal_init: using embedded metal library
0.00.460.120 I ggml_metal_init: GPU name:   Apple M4
0.00.460.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.140 I ggml_metal_init: simdgroup reduction   = true
0.00.460.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.141 I ggml_metal_init: has residency sets    = true
0.00.460.141 I ggml_metal_init: has bfloat            = true
0.00.460.141 I ggml_metal_init: use bfloat            = true
0.00.460.143 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.480.741 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.539.428 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.539.436 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.539.483 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.544.844 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.544.847 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.544.847 I llama_init_from_model: graph nodes  = 967
0.00.544.847 I llama_init_from_model: graph splits = 2
0.00.544.852 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.544.977 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.544.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.393 I main: llama threadpool init, n_threads = 4
0.00.599.444 I 
0.00.599.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.460 I 
0.00.599.632 I sampler seed: 1234
0.00.599.637 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.599.681 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.599.685 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.599.685 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.079 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.334.080 I llama_perf_context_print:        load time =     589.26 ms
0.01.334.081 I llama_perf_context_print: prompt eval time =      40.43 ms /     7 tokens (    5.78 ms per token,   173.16 tokens per second)
0.01.334.082 I llama_perf_context_print:        eval time =     691.15 ms /    63 runs   (   10.97 ms per token,    91.15 tokens per second)
0.01.334.083 I llama_perf_context_print:       total time =     735.37 ms /    70 tokens
0.01.334.335 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.111s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.024.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.516 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.518 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.483 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.033.458 I llama_model_loader: - type  f32:  194 tensors
0.00.033.459 I llama_model_loader: - type q3_K:   25 tensors
0.00.033.459 I llama_model_loader: - type q4_K:   71 tensors
0.00.033.459 I llama_model_loader: - type q5_K:    1 tensors
0.00.033.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.460 I print_info: file format = GGUF V3 (latest)
0.00.033.460 I print_info: file type   = Q3_K - Medium
0.00.033.461 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.041.796 I load: special tokens cache size = 25
0.00.048.251 I load: token to piece cache size = 0.2984 MB
0.00.048.254 I print_info: arch             = gptneox
0.00.048.254 I print_info: vocab_only       = 0
0.00.048.254 I print_info: n_ctx_train      = 2048
0.00.048.254 I print_info: n_embd           = 2048
0.00.048.255 I print_info: n_layer          = 24
0.00.048.258 I print_info: n_head           = 16
0.00.048.259 I print_info: n_head_kv        = 16
0.00.048.259 I print_info: n_rot            = 32
0.00.048.259 I print_info: n_swa            = 0
0.00.048.259 I print_info: n_embd_head_k    = 128
0.00.048.259 I print_info: n_embd_head_v    = 128
0.00.048.260 I print_info: n_gqa            = 1
0.00.048.261 I print_info: n_embd_k_gqa     = 2048
0.00.048.261 I print_info: n_embd_v_gqa     = 2048
0.00.048.262 I print_info: f_norm_eps       = 1.0e-05
0.00.048.262 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.263 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.263 I print_info: f_logit_scale    = 0.0e+00
0.00.048.264 I print_info: n_ff             = 8192
0.00.048.265 I print_info: n_expert         = 0
0.00.048.265 I print_info: n_expert_used    = 0
0.00.048.265 I print_info: causal attn      = 1
0.00.048.265 I print_info: pooling type     = 0
0.00.048.265 I print_info: rope type        = 2
0.00.048.266 I print_info: rope scaling     = linear
0.00.048.268 I print_info: freq_base_train  = 10000.0
0.00.048.268 I print_info: freq_scale_train = 1
0.00.048.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.269 I print_info: rope_finetuned   = unknown
0.00.048.269 I print_info: ssm_d_conv       = 0
0.00.048.269 I print_info: ssm_d_inner      = 0
0.00.048.269 I print_info: ssm_d_state      = 0
0.00.048.269 I print_info: ssm_dt_rank      = 0
0.00.048.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.270 I print_info: model type       = 1.4B
0.00.048.270 I print_info: model params     = 1.41 B
0.00.048.270 I print_info: general.name     = 1.4B
0.00.048.274 I print_info: vocab type       = BPE
0.00.048.274 I print_info: n_vocab          = 50304
0.00.048.274 I print_info: n_merges         = 50009
0.00.048.274 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.277 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.277 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.278 I print_info: LF token         = 187 ''
0.00.048.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.278 I print_info: max token length = 1024
0.00.048.278 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.491.597 I load_tensors: offloading 24 repeating layers to GPU
0.00.491.610 I load_tensors: offloading output layer to GPU
0.00.491.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.491.644 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.491.645 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.493.277 I llama_init_from_model: n_seq_max     = 1
0.00.493.279 I llama_init_from_model: n_ctx         = 128
0.00.493.280 I llama_init_from_model: n_ctx_per_seq = 128
0.00.493.280 I llama_init_from_model: n_batch       = 128
0.00.493.281 I llama_init_from_model: n_ubatch      = 128
0.00.493.281 I llama_init_from_model: flash_attn    = 0
0.00.493.284 I llama_init_from_model: freq_base     = 10000.0
0.00.493.285 I llama_init_from_model: freq_scale    = 1
0.00.493.285 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.493.288 I ggml_metal_init: allocating
0.00.493.360 I ggml_metal_init: found device: Apple M4
0.00.493.375 I ggml_metal_init: picking default device: Apple M4
0.00.495.142 I ggml_metal_init: using embedded metal library
0.00.500.501 I ggml_metal_init: GPU name:   Apple M4
0.00.500.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.500.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.500.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.500.509 I ggml_metal_init: simdgroup reduction   = true
0.00.500.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.500.509 I ggml_metal_init: has residency sets    = true
0.00.500.510 I ggml_metal_init: has bfloat            = true
0.00.500.510 I ggml_metal_init: use bfloat            = true
0.00.500.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.500.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.520.407 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.052 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.524.061 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.524.113 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.527.445 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.527.447 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.527.448 I llama_init_from_model: graph nodes  = 967
0.00.527.448 I llama_init_from_model: graph splits = 2
0.00.527.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.527.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.553.119 I 
0.00.553.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.553.195 I perplexity: tokenizing the input ..
0.00.560.785 I perplexity: tokenization took 7.588 ms
0.00.560.792 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.997 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.695.332 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.695.355 I llama_perf_context_print:        load time =     544.18 ms
0.00.695.356 I llama_perf_context_print: prompt eval time =     132.24 ms /   128 tokens (    1.03 ms per token,   967.94 tokens per second)
0.00.695.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.695.357 I llama_perf_context_print:       total time =     142.24 ms /   129 tokens
0.00.695.732 I ggml_metal_free: deallocating

real	0m0.711s
user	0m0.082s
sys	0m0.122s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.112 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.551 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.360 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.286 I llama_model_loader: - type  f32:  194 tensors
0.00.027.286 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.286 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.286 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.287 I print_info: file format = GGUF V3 (latest)
0.00.027.288 I print_info: file type   = Q4_K - Medium
0.00.027.288 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.136 I load: special tokens cache size = 25
0.00.040.910 I load: token to piece cache size = 0.2984 MB
0.00.040.913 I print_info: arch             = gptneox
0.00.040.913 I print_info: vocab_only       = 0
0.00.040.914 I print_info: n_ctx_train      = 2048
0.00.040.914 I print_info: n_embd           = 2048
0.00.040.914 I print_info: n_layer          = 24
0.00.040.917 I print_info: n_head           = 16
0.00.040.917 I print_info: n_head_kv        = 16
0.00.040.918 I print_info: n_rot            = 32
0.00.040.918 I print_info: n_swa            = 0
0.00.040.918 I print_info: n_embd_head_k    = 128
0.00.040.920 I print_info: n_embd_head_v    = 128
0.00.040.921 I print_info: n_gqa            = 1
0.00.040.922 I print_info: n_embd_k_gqa     = 2048
0.00.040.922 I print_info: n_embd_v_gqa     = 2048
0.00.040.923 I print_info: f_norm_eps       = 1.0e-05
0.00.040.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.927 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.928 I print_info: f_logit_scale    = 0.0e+00
0.00.040.930 I print_info: n_ff             = 8192
0.00.040.930 I print_info: n_expert         = 0
0.00.040.930 I print_info: n_expert_used    = 0
0.00.040.931 I print_info: causal attn      = 1
0.00.040.931 I print_info: pooling type     = 0
0.00.040.931 I print_info: rope type        = 2
0.00.040.931 I print_info: rope scaling     = linear
0.00.040.932 I print_info: freq_base_train  = 10000.0
0.00.040.932 I print_info: freq_scale_train = 1
0.00.040.932 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.932 I print_info: rope_finetuned   = unknown
0.00.040.932 I print_info: ssm_d_conv       = 0
0.00.040.933 I print_info: ssm_d_inner      = 0
0.00.040.933 I print_info: ssm_d_state      = 0
0.00.040.933 I print_info: ssm_dt_rank      = 0
0.00.040.934 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.936 I print_info: model type       = 1.4B
0.00.040.937 I print_info: model params     = 1.41 B
0.00.040.937 I print_info: general.name     = 1.4B
0.00.040.937 I print_info: vocab type       = BPE
0.00.040.939 I print_info: n_vocab          = 50304
0.00.040.939 I print_info: n_merges         = 50009
0.00.040.939 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.939 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.939 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.939 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.940 I print_info: LF token         = 187 ''
0.00.040.940 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.940 I print_info: max token length = 1024
0.00.040.940 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.764 I load_tensors: offloading output layer to GPU
0.00.533.765 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.801 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.803 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.535.163 I llama_init_from_model: n_seq_max     = 1
0.00.535.166 I llama_init_from_model: n_ctx         = 2048
0.00.535.166 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.535.167 I llama_init_from_model: n_batch       = 2048
0.00.535.167 I llama_init_from_model: n_ubatch      = 512
0.00.535.168 I llama_init_from_model: flash_attn    = 0
0.00.535.169 I llama_init_from_model: freq_base     = 10000.0
0.00.535.170 I llama_init_from_model: freq_scale    = 1
0.00.535.174 I ggml_metal_init: allocating
0.00.535.284 I ggml_metal_init: found device: Apple M4
0.00.535.298 I ggml_metal_init: picking default device: Apple M4
0.00.537.159 I ggml_metal_init: using embedded metal library
0.00.543.720 I ggml_metal_init: GPU name:   Apple M4
0.00.543.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.728 I ggml_metal_init: simdgroup reduction   = true
0.00.543.728 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.729 I ggml_metal_init: has residency sets    = true
0.00.543.729 I ggml_metal_init: has bfloat            = true
0.00.543.729 I ggml_metal_init: use bfloat            = true
0.00.543.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.739 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.699 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.615.634 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.615.670 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.956 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.958 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.958 I llama_init_from_model: graph nodes  = 967
0.00.619.958 I llama_init_from_model: graph splits = 2
0.00.619.965 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.620.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.620.099 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.558 I main: llama threadpool init, n_threads = 4
0.00.676.601 I 
0.00.676.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.616 I 
0.00.676.793 I sampler seed: 1234
0.00.676.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.809 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.810 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.810 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.427.334 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.01.427.335 I llama_perf_context_print:        load time =     663.75 ms
0.01.427.335 I llama_perf_context_print: prompt eval time =      46.76 ms /     7 tokens (    6.68 ms per token,   149.71 tokens per second)
0.01.427.336 I llama_perf_context_print:        eval time =     700.84 ms /    63 runs   (   11.12 ms per token,    89.89 tokens per second)
0.01.427.336 I llama_perf_context_print:       total time =     751.46 ms /    70 tokens
0.01.427.561 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.596 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.024.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.800 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.801 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.801 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.802 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.803 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.803 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.804 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.807 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.223 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.224 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.033.225 I llama_model_loader: - type  f32:  194 tensors
0.00.033.226 I llama_model_loader: - type q4_K:   61 tensors
0.00.033.226 I llama_model_loader: - type q5_K:   24 tensors
0.00.033.226 I llama_model_loader: - type q6_K:   13 tensors
0.00.033.227 I print_info: file format = GGUF V3 (latest)
0.00.033.227 I print_info: file type   = Q4_K - Medium
0.00.033.229 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.041.478 I load: special tokens cache size = 25
0.00.047.556 I load: token to piece cache size = 0.2984 MB
0.00.047.561 I print_info: arch             = gptneox
0.00.047.561 I print_info: vocab_only       = 0
0.00.047.561 I print_info: n_ctx_train      = 2048
0.00.047.562 I print_info: n_embd           = 2048
0.00.047.562 I print_info: n_layer          = 24
0.00.047.566 I print_info: n_head           = 16
0.00.047.567 I print_info: n_head_kv        = 16
0.00.047.567 I print_info: n_rot            = 32
0.00.047.567 I print_info: n_swa            = 0
0.00.047.570 I print_info: n_embd_head_k    = 128
0.00.047.571 I print_info: n_embd_head_v    = 128
0.00.047.571 I print_info: n_gqa            = 1
0.00.047.572 I print_info: n_embd_k_gqa     = 2048
0.00.047.573 I print_info: n_embd_v_gqa     = 2048
0.00.047.573 I print_info: f_norm_eps       = 1.0e-05
0.00.047.574 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.574 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.575 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.575 I print_info: f_logit_scale    = 0.0e+00
0.00.047.575 I print_info: n_ff             = 8192
0.00.047.575 I print_info: n_expert         = 0
0.00.047.576 I print_info: n_expert_used    = 0
0.00.047.576 I print_info: causal attn      = 1
0.00.047.576 I print_info: pooling type     = 0
0.00.047.576 I print_info: rope type        = 2
0.00.047.576 I print_info: rope scaling     = linear
0.00.047.577 I print_info: freq_base_train  = 10000.0
0.00.047.578 I print_info: freq_scale_train = 1
0.00.047.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.578 I print_info: rope_finetuned   = unknown
0.00.047.579 I print_info: ssm_d_conv       = 0
0.00.047.579 I print_info: ssm_d_inner      = 0
0.00.047.580 I print_info: ssm_d_state      = 0
0.00.047.580 I print_info: ssm_dt_rank      = 0
0.00.047.580 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.580 I print_info: model type       = 1.4B
0.00.047.580 I print_info: model params     = 1.41 B
0.00.047.580 I print_info: general.name     = 1.4B
0.00.047.581 I print_info: vocab type       = BPE
0.00.047.581 I print_info: n_vocab          = 50304
0.00.047.581 I print_info: n_merges         = 50009
0.00.047.581 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.581 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.582 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.582 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.582 I print_info: LF token         = 187 ''
0.00.047.582 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.582 I print_info: max token length = 1024
0.00.047.583 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.556.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.556.702 I load_tensors: offloading output layer to GPU
0.00.556.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.556.739 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.556.740 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.558.453 I llama_init_from_model: n_seq_max     = 1
0.00.558.456 I llama_init_from_model: n_ctx         = 128
0.00.558.457 I llama_init_from_model: n_ctx_per_seq = 128
0.00.558.457 I llama_init_from_model: n_batch       = 128
0.00.558.457 I llama_init_from_model: n_ubatch      = 128
0.00.558.458 I llama_init_from_model: flash_attn    = 0
0.00.558.460 I llama_init_from_model: freq_base     = 10000.0
0.00.558.461 I llama_init_from_model: freq_scale    = 1
0.00.558.461 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.558.464 I ggml_metal_init: allocating
0.00.558.573 I ggml_metal_init: found device: Apple M4
0.00.558.588 I ggml_metal_init: picking default device: Apple M4
0.00.560.449 I ggml_metal_init: using embedded metal library
0.00.567.548 I ggml_metal_init: GPU name:   Apple M4
0.00.567.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.567.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.567.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.567.558 I ggml_metal_init: simdgroup reduction   = true
0.00.567.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.567.558 I ggml_metal_init: has residency sets    = true
0.00.567.559 I ggml_metal_init: has bfloat            = true
0.00.567.560 I ggml_metal_init: use bfloat            = true
0.00.567.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.567.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.586.314 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.589.803 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.589.807 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.589.850 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.592.939 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.592.941 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.592.941 I llama_init_from_model: graph nodes  = 967
0.00.592.942 I llama_init_from_model: graph splits = 2
0.00.592.945 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.592.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.080 I 
0.00.620.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.106 I perplexity: tokenizing the input ..
0.00.627.132 I perplexity: tokenization took 7.023 ms
0.00.627.141 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.124 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.764.637 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.764.663 I llama_perf_context_print:        load time =     605.48 ms
0.00.764.664 I llama_perf_context_print: prompt eval time =     135.02 ms /   128 tokens (    1.05 ms per token,   948.00 tokens per second)
0.00.764.665 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.665 I llama_perf_context_print:       total time =     144.58 ms /   129 tokens
0.00.764.979 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.080s
sys	0m0.124s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.404 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.230 I llama_model_loader: - type  f32:  194 tensors
0.00.024.231 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.231 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.231 I print_info: file format = GGUF V3 (latest)
0.00.024.232 I print_info: file type   = Q5_K - Medium
0.00.024.233 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.352 I load: special tokens cache size = 25
0.00.038.477 I load: token to piece cache size = 0.2984 MB
0.00.038.480 I print_info: arch             = gptneox
0.00.038.481 I print_info: vocab_only       = 0
0.00.038.481 I print_info: n_ctx_train      = 2048
0.00.038.481 I print_info: n_embd           = 2048
0.00.038.481 I print_info: n_layer          = 24
0.00.038.484 I print_info: n_head           = 16
0.00.038.485 I print_info: n_head_kv        = 16
0.00.038.485 I print_info: n_rot            = 32
0.00.038.485 I print_info: n_swa            = 0
0.00.038.485 I print_info: n_embd_head_k    = 128
0.00.038.486 I print_info: n_embd_head_v    = 128
0.00.038.486 I print_info: n_gqa            = 1
0.00.038.487 I print_info: n_embd_k_gqa     = 2048
0.00.038.488 I print_info: n_embd_v_gqa     = 2048
0.00.038.488 I print_info: f_norm_eps       = 1.0e-05
0.00.038.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.489 I print_info: f_logit_scale    = 0.0e+00
0.00.038.490 I print_info: n_ff             = 8192
0.00.038.490 I print_info: n_expert         = 0
0.00.038.490 I print_info: n_expert_used    = 0
0.00.038.490 I print_info: causal attn      = 1
0.00.038.490 I print_info: pooling type     = 0
0.00.038.491 I print_info: rope type        = 2
0.00.038.491 I print_info: rope scaling     = linear
0.00.038.491 I print_info: freq_base_train  = 10000.0
0.00.038.492 I print_info: freq_scale_train = 1
0.00.038.492 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.492 I print_info: rope_finetuned   = unknown
0.00.038.492 I print_info: ssm_d_conv       = 0
0.00.038.492 I print_info: ssm_d_inner      = 0
0.00.038.493 I print_info: ssm_d_state      = 0
0.00.038.493 I print_info: ssm_dt_rank      = 0
0.00.038.493 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.493 I print_info: model type       = 1.4B
0.00.038.494 I print_info: model params     = 1.41 B
0.00.038.494 I print_info: general.name     = 1.4B
0.00.038.494 I print_info: vocab type       = BPE
0.00.038.495 I print_info: n_vocab          = 50304
0.00.038.495 I print_info: n_merges         = 50009
0.00.038.495 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.498 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.498 I print_info: LF token         = 187 ''
0.00.038.498 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.498 I print_info: max token length = 1024
0.00.038.499 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.677 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.693 I load_tensors: offloading output layer to GPU
0.00.616.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.730 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.616.731 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.618.440 I llama_init_from_model: n_seq_max     = 1
0.00.618.444 I llama_init_from_model: n_ctx         = 2048
0.00.618.444 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.618.444 I llama_init_from_model: n_batch       = 2048
0.00.618.445 I llama_init_from_model: n_ubatch      = 512
0.00.618.445 I llama_init_from_model: flash_attn    = 0
0.00.618.447 I llama_init_from_model: freq_base     = 10000.0
0.00.618.447 I llama_init_from_model: freq_scale    = 1
0.00.618.450 I ggml_metal_init: allocating
0.00.618.499 I ggml_metal_init: found device: Apple M4
0.00.618.511 I ggml_metal_init: picking default device: Apple M4
0.00.620.042 I ggml_metal_init: using embedded metal library
0.00.626.351 I ggml_metal_init: GPU name:   Apple M4
0.00.626.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.357 I ggml_metal_init: simdgroup reduction   = true
0.00.626.358 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.358 I ggml_metal_init: has residency sets    = true
0.00.626.358 I ggml_metal_init: has bfloat            = true
0.00.626.358 I ggml_metal_init: use bfloat            = true
0.00.626.359 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.188 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.979 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.701.987 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.702.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.706.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.706.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.706.320 I llama_init_from_model: graph nodes  = 967
0.00.706.320 I llama_init_from_model: graph splits = 2
0.00.706.326 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.706.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.706.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.265 I main: llama threadpool init, n_threads = 4
0.00.769.307 I 
0.00.769.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.321 I 
0.00.769.483 I sampler seed: 1234
0.00.769.488 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.541 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.545 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.608.766 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.608.766 I llama_perf_context_print:        load time =     759.73 ms
0.01.608.767 I llama_perf_context_print: prompt eval time =      51.44 ms /     7 tokens (    7.35 ms per token,   136.08 tokens per second)
0.01.608.768 I llama_perf_context_print:        eval time =     784.89 ms /    63 runs   (   12.46 ms per token,    80.27 tokens per second)
0.01.608.768 I llama_perf_context_print:       total time =     840.27 ms /    70 tokens
0.01.609.020 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.108s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.007 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.332 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.332 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.333 I llama_model_loader: - type  f32:  194 tensors
0.00.027.333 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.333 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.334 I print_info: file format = GGUF V3 (latest)
0.00.027.335 I print_info: file type   = Q5_K - Medium
0.00.027.336 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.744 I load: special tokens cache size = 25
0.00.041.704 I load: token to piece cache size = 0.2984 MB
0.00.041.708 I print_info: arch             = gptneox
0.00.041.708 I print_info: vocab_only       = 0
0.00.041.709 I print_info: n_ctx_train      = 2048
0.00.041.709 I print_info: n_embd           = 2048
0.00.041.709 I print_info: n_layer          = 24
0.00.041.714 I print_info: n_head           = 16
0.00.041.715 I print_info: n_head_kv        = 16
0.00.041.715 I print_info: n_rot            = 32
0.00.041.715 I print_info: n_swa            = 0
0.00.041.715 I print_info: n_embd_head_k    = 128
0.00.041.715 I print_info: n_embd_head_v    = 128
0.00.041.719 I print_info: n_gqa            = 1
0.00.041.720 I print_info: n_embd_k_gqa     = 2048
0.00.041.720 I print_info: n_embd_v_gqa     = 2048
0.00.041.721 I print_info: f_norm_eps       = 1.0e-05
0.00.041.721 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.722 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.722 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.722 I print_info: f_logit_scale    = 0.0e+00
0.00.041.723 I print_info: n_ff             = 8192
0.00.041.723 I print_info: n_expert         = 0
0.00.041.724 I print_info: n_expert_used    = 0
0.00.041.725 I print_info: causal attn      = 1
0.00.041.725 I print_info: pooling type     = 0
0.00.041.725 I print_info: rope type        = 2
0.00.041.725 I print_info: rope scaling     = linear
0.00.041.725 I print_info: freq_base_train  = 10000.0
0.00.041.729 I print_info: freq_scale_train = 1
0.00.041.729 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.729 I print_info: rope_finetuned   = unknown
0.00.041.729 I print_info: ssm_d_conv       = 0
0.00.041.730 I print_info: ssm_d_inner      = 0
0.00.041.730 I print_info: ssm_d_state      = 0
0.00.041.730 I print_info: ssm_dt_rank      = 0
0.00.041.730 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.730 I print_info: model type       = 1.4B
0.00.041.731 I print_info: model params     = 1.41 B
0.00.041.731 I print_info: general.name     = 1.4B
0.00.041.731 I print_info: vocab type       = BPE
0.00.041.731 I print_info: n_vocab          = 50304
0.00.041.732 I print_info: n_merges         = 50009
0.00.041.733 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.733 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.733 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.733 I print_info: LF token         = 187 ''
0.00.041.733 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.734 I print_info: max token length = 1024
0.00.041.735 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.032 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.051 I load_tensors: offloading output layer to GPU
0.00.645.051 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.087 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.645.091 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.646.576 I llama_init_from_model: n_seq_max     = 1
0.00.646.578 I llama_init_from_model: n_ctx         = 128
0.00.646.578 I llama_init_from_model: n_ctx_per_seq = 128
0.00.646.579 I llama_init_from_model: n_batch       = 128
0.00.646.579 I llama_init_from_model: n_ubatch      = 128
0.00.646.580 I llama_init_from_model: flash_attn    = 0
0.00.646.582 I llama_init_from_model: freq_base     = 10000.0
0.00.646.582 I llama_init_from_model: freq_scale    = 1
0.00.646.583 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.646.585 I ggml_metal_init: allocating
0.00.646.664 I ggml_metal_init: found device: Apple M4
0.00.646.677 I ggml_metal_init: picking default device: Apple M4
0.00.648.551 I ggml_metal_init: using embedded metal library
0.00.655.948 I ggml_metal_init: GPU name:   Apple M4
0.00.655.955 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.956 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.956 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.960 I ggml_metal_init: simdgroup reduction   = true
0.00.655.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.961 I ggml_metal_init: has residency sets    = true
0.00.655.961 I ggml_metal_init: has bfloat            = true
0.00.655.961 I ggml_metal_init: use bfloat            = true
0.00.655.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.125 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.636 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.677.640 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.693 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.940 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.680.942 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.680.942 I llama_init_from_model: graph nodes  = 967
0.00.680.943 I llama_init_from_model: graph splits = 2
0.00.680.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.946 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.528 I 
0.00.718.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.594 I perplexity: tokenizing the input ..
0.00.725.296 I perplexity: tokenization took 6.699 ms
0.00.725.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.517 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.880.846 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.880.873 I llama_perf_context_print:        load time =     709.51 ms
0.00.880.875 I llama_perf_context_print: prompt eval time =     153.30 ms /   128 tokens (    1.20 ms per token,   834.99 tokens per second)
0.00.880.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.880.876 I llama_perf_context_print:       total time =     162.35 ms /   129 tokens
0.00.881.234 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.080s
sys	0m0.142s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.820 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.356 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.363 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.363 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.364 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.365 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.368 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.368 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.369 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.886 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.887 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.888 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.889 I llama_model_loader: - type  f32:  194 tensors
0.00.023.889 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.890 I print_info: file format = GGUF V3 (latest)
0.00.023.890 I print_info: file type   = Q6_K
0.00.023.891 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.638 I load: special tokens cache size = 25
0.00.037.733 I load: token to piece cache size = 0.2984 MB
0.00.037.736 I print_info: arch             = gptneox
0.00.037.736 I print_info: vocab_only       = 0
0.00.037.737 I print_info: n_ctx_train      = 2048
0.00.037.737 I print_info: n_embd           = 2048
0.00.037.737 I print_info: n_layer          = 24
0.00.037.740 I print_info: n_head           = 16
0.00.037.740 I print_info: n_head_kv        = 16
0.00.037.741 I print_info: n_rot            = 32
0.00.037.741 I print_info: n_swa            = 0
0.00.037.742 I print_info: n_embd_head_k    = 128
0.00.037.742 I print_info: n_embd_head_v    = 128
0.00.037.744 I print_info: n_gqa            = 1
0.00.037.745 I print_info: n_embd_k_gqa     = 2048
0.00.037.745 I print_info: n_embd_v_gqa     = 2048
0.00.037.746 I print_info: f_norm_eps       = 1.0e-05
0.00.037.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.747 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.747 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.747 I print_info: f_logit_scale    = 0.0e+00
0.00.037.748 I print_info: n_ff             = 8192
0.00.037.748 I print_info: n_expert         = 0
0.00.037.748 I print_info: n_expert_used    = 0
0.00.037.748 I print_info: causal attn      = 1
0.00.037.748 I print_info: pooling type     = 0
0.00.037.748 I print_info: rope type        = 2
0.00.037.749 I print_info: rope scaling     = linear
0.00.037.749 I print_info: freq_base_train  = 10000.0
0.00.037.753 I print_info: freq_scale_train = 1
0.00.037.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.754 I print_info: rope_finetuned   = unknown
0.00.037.754 I print_info: ssm_d_conv       = 0
0.00.037.754 I print_info: ssm_d_inner      = 0
0.00.037.754 I print_info: ssm_d_state      = 0
0.00.037.754 I print_info: ssm_dt_rank      = 0
0.00.037.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.755 I print_info: model type       = 1.4B
0.00.037.755 I print_info: model params     = 1.41 B
0.00.037.755 I print_info: general.name     = 1.4B
0.00.037.756 I print_info: vocab type       = BPE
0.00.037.756 I print_info: n_vocab          = 50304
0.00.037.756 I print_info: n_merges         = 50009
0.00.037.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.757 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.757 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.758 I print_info: LF token         = 187 ''
0.00.037.759 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.759 I print_info: max token length = 1024
0.00.037.760 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.658.996 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.011 I load_tensors: offloading output layer to GPU
0.00.659.012 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.042 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.659.043 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.660.645 I llama_init_from_model: n_seq_max     = 1
0.00.660.647 I llama_init_from_model: n_ctx         = 2048
0.00.660.648 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.648 I llama_init_from_model: n_batch       = 2048
0.00.660.648 I llama_init_from_model: n_ubatch      = 512
0.00.660.649 I llama_init_from_model: flash_attn    = 0
0.00.660.650 I llama_init_from_model: freq_base     = 10000.0
0.00.660.650 I llama_init_from_model: freq_scale    = 1
0.00.660.652 I ggml_metal_init: allocating
0.00.660.687 I ggml_metal_init: found device: Apple M4
0.00.660.696 I ggml_metal_init: picking default device: Apple M4
0.00.662.169 I ggml_metal_init: using embedded metal library
0.00.668.360 I ggml_metal_init: GPU name:   Apple M4
0.00.668.364 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.365 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.366 I ggml_metal_init: simdgroup reduction   = true
0.00.668.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.367 I ggml_metal_init: has residency sets    = true
0.00.668.367 I ggml_metal_init: has bfloat            = true
0.00.668.367 I ggml_metal_init: use bfloat            = true
0.00.668.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.338 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.988 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.738.022 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.171 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.173 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.173 I llama_init_from_model: graph nodes  = 967
0.00.742.173 I llama_init_from_model: graph splits = 2
0.00.742.179 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.005 I main: llama threadpool init, n_threads = 4
0.00.810.038 I 
0.00.810.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.050 I 
0.00.810.219 I sampler seed: 1234
0.00.810.224 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.257 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.259 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.259 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.679.591 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.679.591 I llama_perf_context_print:        load time =     800.41 ms
0.01.679.592 I llama_perf_context_print: prompt eval time =      54.17 ms /     7 tokens (    7.74 ms per token,   129.23 tokens per second)
0.01.679.593 I llama_perf_context_print:        eval time =     812.27 ms /    63 runs   (   12.89 ms per token,    77.56 tokens per second)
0.01.679.593 I llama_perf_context_print:       total time =     870.36 ms /    70 tokens
0.01.679.879 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.107s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4720 (fc1b0d09) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.681 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.368 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.368 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.370 I llama_model_loader: - type  f32:  194 tensors
0.00.025.370 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.371 I print_info: file format = GGUF V3 (latest)
0.00.025.372 I print_info: file type   = Q6_K
0.00.025.373 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.672 I load: special tokens cache size = 25
0.00.040.082 I load: token to piece cache size = 0.2984 MB
0.00.040.085 I print_info: arch             = gptneox
0.00.040.086 I print_info: vocab_only       = 0
0.00.040.086 I print_info: n_ctx_train      = 2048
0.00.040.086 I print_info: n_embd           = 2048
0.00.040.086 I print_info: n_layer          = 24
0.00.040.090 I print_info: n_head           = 16
0.00.040.091 I print_info: n_head_kv        = 16
0.00.040.091 I print_info: n_rot            = 32
0.00.040.091 I print_info: n_swa            = 0
0.00.040.092 I print_info: n_embd_head_k    = 128
0.00.040.095 I print_info: n_embd_head_v    = 128
0.00.040.095 I print_info: n_gqa            = 1
0.00.040.096 I print_info: n_embd_k_gqa     = 2048
0.00.040.097 I print_info: n_embd_v_gqa     = 2048
0.00.040.098 I print_info: f_norm_eps       = 1.0e-05
0.00.040.098 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.099 I print_info: f_logit_scale    = 0.0e+00
0.00.040.099 I print_info: n_ff             = 8192
0.00.040.099 I print_info: n_expert         = 0
0.00.040.100 I print_info: n_expert_used    = 0
0.00.040.100 I print_info: causal attn      = 1
0.00.040.100 I print_info: pooling type     = 0
0.00.040.100 I print_info: rope type        = 2
0.00.040.100 I print_info: rope scaling     = linear
0.00.040.101 I print_info: freq_base_train  = 10000.0
0.00.040.101 I print_info: freq_scale_train = 1
0.00.040.101 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.101 I print_info: rope_finetuned   = unknown
0.00.040.102 I print_info: ssm_d_conv       = 0
0.00.040.102 I print_info: ssm_d_inner      = 0
0.00.040.102 I print_info: ssm_d_state      = 0
0.00.040.102 I print_info: ssm_dt_rank      = 0
0.00.040.102 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.102 I print_info: model type       = 1.4B
0.00.040.103 I print_info: model params     = 1.41 B
0.00.040.103 I print_info: general.name     = 1.4B
0.00.040.105 I print_info: vocab type       = BPE
0.00.040.105 I print_info: n_vocab          = 50304
0.00.040.105 I print_info: n_merges         = 50009
0.00.040.105 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.105 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.105 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.106 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.106 I print_info: LF token         = 187 ''
0.00.040.106 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.106 I print_info: max token length = 1024
0.00.040.107 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.617 I load_tensors: offloading output layer to GPU
0.00.647.618 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.643 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.647.646 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.649.205 I llama_init_from_model: n_seq_max     = 1
0.00.649.208 I llama_init_from_model: n_ctx         = 128
0.00.649.208 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.208 I llama_init_from_model: n_batch       = 128
0.00.649.209 I llama_init_from_model: n_ubatch      = 128
0.00.649.209 I llama_init_from_model: flash_attn    = 0
0.00.649.210 I llama_init_from_model: freq_base     = 10000.0
0.00.649.211 I llama_init_from_model: freq_scale    = 1
0.00.649.212 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.213 I ggml_metal_init: allocating
0.00.649.236 I ggml_metal_init: found device: Apple M4
0.00.649.245 I ggml_metal_init: picking default device: Apple M4
0.00.650.610 I ggml_metal_init: using embedded metal library
0.00.656.322 I ggml_metal_init: GPU name:   Apple M4
0.00.656.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.328 I ggml_metal_init: simdgroup reduction   = true
0.00.656.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.328 I ggml_metal_init: has residency sets    = true
0.00.656.329 I ggml_metal_init: has bfloat            = true
0.00.656.329 I ggml_metal_init: use bfloat            = true
0.00.656.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.472 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.931 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.675.935 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.192 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.193 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.194 I llama_init_from_model: graph nodes  = 967
0.00.679.194 I llama_init_from_model: graph splits = 2
0.00.679.197 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.856 I 
0.00.712.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.921 I perplexity: tokenizing the input ..
0.00.720.248 I perplexity: tokenization took 7.323 ms
0.00.720.255 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.403 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.862.757 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.862.781 I llama_perf_context_print:        load time =     703.90 ms
0.00.862.781 I llama_perf_context_print: prompt eval time =     140.21 ms /   128 tokens (    1.10 ms per token,   912.91 tokens per second)
0.00.862.782 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.782 I llama_perf_context_print:       total time =     149.93 ms /   129 tokens
0.00.863.137 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.078s
sys	0m0.140s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4720 (fc1b0d09)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154105940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154105fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154106420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154109110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154109580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1541099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154109fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15410a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15410ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15410b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15410b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15410ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15410c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15410ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15410d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15410dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15410e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15410ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15410f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15410f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154110050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154110770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154110e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154111730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154111e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154112110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154112720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154113390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1541138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154113b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154114030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1541142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154114b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1541150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154115380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154115820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154115cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154116160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154116600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154116aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154116f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1541173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154117880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154117d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154117fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1541185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154118c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154119520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154119b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15411a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15411a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15411ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15411b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15411b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15411c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15411c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15411cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15411cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15411d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15411db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15411de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15411e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15411e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15411ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15411f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15411f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15411f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15411fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154120330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1541207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154120c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154121110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1541215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154121b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154122050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1541225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154122af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154123040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154123590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154123ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154124030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154124580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154124ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154125020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154125570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154125ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154126010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154126560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154126ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154127000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154127550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154127aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154127ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154128540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154128a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154128fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154129530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154119210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1541299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15412a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15412a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15412abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15412b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15412b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15412bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15412c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15412c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15412cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15412d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15412d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15412dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15412e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15412e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15412eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15412efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15412f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15412f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15412fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154130220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1541306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154130b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154131000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1541314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154131940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154131de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154132280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154132720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154132bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154133060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154133500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1541339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154133e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1541342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154134780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154134c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1541350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154135560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154135a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154135ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154136340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1541367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154136c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154137120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1541375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154137a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154137f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1541383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154138840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154138ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154139180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154139620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154139ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154139f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15413a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15413a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15413ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15413b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15413b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15413bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15413bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15413c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15413c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15413cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15413d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15413d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15413db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15413e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15413e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15413e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15413ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15413f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15413f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15413fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154140080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154140520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1541409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154140e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154141300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1541417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154141c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1541420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154142580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154142a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154142ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154143360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154143800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154143ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154144140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1541445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154144a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154144f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1541453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154145860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154145db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154146300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154146850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154146da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154147060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154147670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154147c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154148290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154148a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154148f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1541491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1541497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154149e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15414a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15414aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15414af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15414b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15414bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15414c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15414c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15414cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15414d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15414d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15414db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15414e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15414e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15414eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15414f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15414f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15414fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154150090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1541505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154150b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154151080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1541515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154151b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154152070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1541525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154152b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154153060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1541535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154153b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154154050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1541545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154154af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154155040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154155590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154155ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154156030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154156580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154156ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154157020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154157570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154157ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154158010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154158560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154158ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154159000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154159550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154159aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154159ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15415a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15415aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15415afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15415b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15415ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15415bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15415c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15415ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15415cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15415d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15415da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15415dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15415e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15415e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15415ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15415f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15415f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15415fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1541600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154160560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154160a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154160ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154161340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1541617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154161c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154162120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1541625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154162a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154162fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1541636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154163df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154164510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154164c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154164ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1541656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1541659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154165fb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.724.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.256 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154165c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154147930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154147320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154147f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15411b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15411aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15411d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1541123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154118ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1541197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154119df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1541188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15411b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1541113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15411bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15411d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154129c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1541651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1541145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154114870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154148550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1541129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154112ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154112f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154166410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1541666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154166990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154166c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154166f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1541671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154167490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154167750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154167a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154167cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154167f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154168250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154168510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1541687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154168a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154168d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154169010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1541692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154169590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154169850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154169b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154169dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15416a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15416a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15416a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15416a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15416ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15416ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15416b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15416b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15416b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15416b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15416bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15416bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15416c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15416c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15416c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15416c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15416cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15416cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15416d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15416d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15416d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15416da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15416dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15416dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15416e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15416e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15416e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15416ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15416ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15416f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15416f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15416f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15416f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15416fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15416fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1541700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154170390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154170650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154170910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154170bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154170e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154171150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154171410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1541716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154171990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154171c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154171f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1541721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154172490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154172750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154172a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154172cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154172f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154173250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154173510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1541737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154173a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154173d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154174010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1541742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154174590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154174850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154174b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154174dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154175090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154175350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154175610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1541758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154175b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154175e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154176110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1541763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154176690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154176950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154176c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154176ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154177190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154177450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154177710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1541779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154177c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154177f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154178210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1541784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154178790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154178a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154178d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154178fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154179290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154179550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154179810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154179ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154179d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15417a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15417a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15417a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15417a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15417ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15417ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15417b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15417b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15417b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15417b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15417bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15417be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15417c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15417c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15417c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15417c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15417cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15417cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15417d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15417d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15417d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15417da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15417dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15417df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15417e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15417e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15417e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15417ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15417ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15417f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15417f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15417f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15417f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15417fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15417fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154180090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154180350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154180610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1541808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154180b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154180e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154181110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1541813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154181690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154181950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154181c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154181ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154182190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154182450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154182710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1541829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154182c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154182f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154183210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1541834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154183790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154183a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154183d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154183fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154184290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154184550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154184810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154184ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154184d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1541852d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154185810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154185d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154186010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154186410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1541868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154186d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154187500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1541877c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154187a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154187ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154188360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1541887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154188c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1541890b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154189520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154189990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154189e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15418a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15418a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15418ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15418afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15418b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15418b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15418bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15418c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15418c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15418ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15418ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15418d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15418d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15418dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15418e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15418e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15418e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15418ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15418f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15418f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15418fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15418ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154190410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154190880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154190cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154191160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1541915d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154191a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154191eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154192320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154192790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154192c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154193070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1541934e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154193950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154193dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154194230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1541946a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154194b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154194f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1541953f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154195860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154195cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154196140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1541965b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154196a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154196e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154197300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154197770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154197be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154198050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1541984c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154198930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154198da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154199210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154199680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154199af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154199f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15419a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15419a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15419acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15419b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15419bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15419c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15419c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15419d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15419d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15419dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15419de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15419e470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154304760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154304bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154305040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1543054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154305920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154305d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154306200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154306670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154306ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154306f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1543073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154307a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154308580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154308d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154309540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154309c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15430a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15430aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15430b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15430b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15430c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15430c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15430cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15430d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15430dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15430dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15430e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15430e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15430eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15430f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15430f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15430f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15430fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1543100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154310540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1543109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154310e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154311290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154311700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154311b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154311fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154312450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1543128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154312d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1543131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154313610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154313a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154313ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154314360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1543147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154314c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1543150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154315520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154315990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154315e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154316270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1543167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154316ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154317150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1543175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154317a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154317ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154318310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154318780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154318bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154319060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1543194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154319940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154319db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15431a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15431a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15431ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15431af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15431b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15431b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15431bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15431c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15431c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15431ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15431ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15431d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15431d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15431dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15431e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15431e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15431e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15431ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15431f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15431f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15431fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15431ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1543203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154320830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154320ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154321110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154321580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1543219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154321e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1543222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154322740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154322bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154323020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154323490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154323d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154323fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154324450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1543248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154324d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1543251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154325610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154325a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154325ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154326360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1543267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154326c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1543270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154327520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154327990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154327e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154328270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1543286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154328b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154328fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154329430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1543298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154329d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15432a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15432a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15432aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15432aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15432b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15432b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15432bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15432c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15432c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15432c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15432cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15432d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15432d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15432db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15432dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15432e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15432e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15432ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15432f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15432f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15432fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15432feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154330320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154330790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154330c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154331070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1543314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154331950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154331dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154332230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1543326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154332b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154332f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1543333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154333860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154333cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154334140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1543345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154334a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154334e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154335300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154335770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x154335be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154336050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1543364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154336930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154336da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154337210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154337680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154337af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154337f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1543383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154338840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154338cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154339120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154339590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154339a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154339e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15433a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15433a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15433abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15433b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15433b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15433b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15433bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15433c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15433c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15433cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15433cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15433d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15433d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15433dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15433e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15433e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15433e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15433ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15433f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15433f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15433fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154340010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154340480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1543408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154340d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1543411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154341d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154342010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1543422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154342740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154342bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154343020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154343490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154343900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154343d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1543441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154344650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154344ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154344f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1543453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154345810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154345c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1543460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154346560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1543469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154346e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1543472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154347720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154347b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154348000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154348470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1543488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154348d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1543491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154349630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154349aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154349f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15434a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15434a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15434ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15434b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15434b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15434b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15434be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15434c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15434c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15434cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15434cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15434d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15434d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15434dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15434e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15434e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15434ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15434eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15434f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15434f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15434fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1543500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154350520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154350990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154350e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154351270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1543516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154351b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154351fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154352430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1543528a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154352d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154353180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1543535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154353a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154353ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154354340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1543547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154354c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154355090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154355500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154355970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1543563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154356b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154357220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154357940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154357c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154358070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154358670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154358c80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.278s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4720 (fc1b0d09)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12af0e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12af0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12af0f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12af0fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12af10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12af105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12af10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12af11140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12af116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12af11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12af120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12af125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12af13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12af138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12af140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12af147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12af14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12af15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12af15d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12af16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12af16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12af17360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12af17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12af18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12af18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12af18d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12af19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12af19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12af1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12af1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12af1ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12af1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12af1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12af1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12af1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12af1c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12af1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12af1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12af1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12af1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12af1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12af1dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12af1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12af1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12af1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12af1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12af1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12af20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12af20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12af20d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12af21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12af21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12af21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12af22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12af22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12af23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12af236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12af23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12af23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12af24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12af24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12af24ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12af25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12af25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12af25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12af26140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12af265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12af26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12af26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12af273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12af27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12af27d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12af281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12af286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12af28c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12af29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12af296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12af29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12af2a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12af2a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12af2ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12af2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12af2b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12af2bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12af2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12af2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12af2cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12af2d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12af2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12af2dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12af2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12af2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12af2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12af2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12af2f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12af2fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12af30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12af1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12af30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12af30d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12af31290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12af317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12af31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12af32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12af327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12af32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12af33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12af337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12af33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12af34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12af347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12af34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12af35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12af356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12af35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12af36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12af364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12af36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12af36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12af372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12af37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12af37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12af38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12af38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12af389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12af38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12af39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12af397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12af39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12af3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12af3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12af3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12af3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12af3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12af3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12af3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12af3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12af3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12af3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12af3cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12af3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12af3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12af3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12af3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12af3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12af3eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12af3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12af3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12af3f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12af3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12af40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12af406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12af40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12af40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12af41490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12af41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12af41dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12af42270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12af42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12af42bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12af43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12af434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12af43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12af43e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12af442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12af44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12af44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12af450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12af45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12af459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12af45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12af46330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12af467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12af46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12af47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12af475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12af47a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12af47ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12af48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12af48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12af48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12af49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12af49610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12af49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12af49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12af4a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12af4a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12af4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12af4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12af4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12af4bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12af4bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12af4c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12af4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12af4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12af4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12af4d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12af4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12af4e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12af4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12af4ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12af4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12af4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12af4fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12af503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12af509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12af511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12af51680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12af51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12af51fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12af52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12af52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12af53210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12af53760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12af53cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12af54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12af54750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12af54ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12af551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12af55740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12af55c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12af561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12af56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12af56c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12af571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12af57720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12af57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12af581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12af58710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12af58c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12af591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12af59700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12af59c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12af5a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12af5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12af5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12af5b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12af5b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12af5bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12af5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12af5c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12af5cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12af5d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12af5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12af5dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12af5e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12af5e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12af5ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12af5f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12af5f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12af5fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12af60140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12af60690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12af60be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12af61130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12af61680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12af61bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12af62120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12af62670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12af62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12af63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12af63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12af63bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12af64100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12af64650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12af64ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12af650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12af65590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12af65a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12af65ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12af66370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12af66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12af66cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12af67150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12af675f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12af67a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12af67f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12af683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12af68870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12af68d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12af691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12af69650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12af69ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12af6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12af6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12af6b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12af6b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12af6bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12af6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12af6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12af6cba0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ae07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ae07bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ae08020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ae08490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ae08900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ae08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ae091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ae09650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ae09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ae09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ae0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ae0aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ae0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ae0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ae0c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ae0cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ae0d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ae0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ae0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ae0ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ae0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ae0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ae0ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ae10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ae10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ae11070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ae11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ae117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ae11c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ae12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ae12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ae12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ae12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ae131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ae13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ae13aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ae14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ae14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ae14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ae14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ae15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ae15900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ae15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ae16300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ae16800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ae16c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ae170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ae17550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ae179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ae17e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ae182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ae18710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ae18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ae18ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ae19460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ae19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ae1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ae1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ae1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ae1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ae1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ae1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ae1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ae1c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ae1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ae1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ae1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ae1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ae1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ae1dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ae1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ae1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ae1edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ae1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ae1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ae1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ae202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ae20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ae20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ae212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ae21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ae21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ae222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ae22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ae22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ae232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ae23810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ae23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ae242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ae24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ae24d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ae252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ae257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ae25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ae26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ae267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ae26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ae27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ae277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ae27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ae28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ae287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ae28d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ae29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ae297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ae29d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ae2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ae2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ae2acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ae2b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ae2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ae2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ae2c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ae2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ae2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ae2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ae2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ae2d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ae2ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ae2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ae2e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ae2ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ae2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ae2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ae2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ae2fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ae302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ae30790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ae30c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ae310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ae31570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ae31a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ae31eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ae32350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ae327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ae32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ae33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ae335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ae33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ae33f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ae343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ae34850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ae34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ae35190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ae35630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ae35ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ae35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ae36410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ae368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ae36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ae371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ae37690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ae37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ae37fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ae38470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ae38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ae38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ae39250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ae396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ae39b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ae3a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ae3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ae3a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ae3ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ae3b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ae3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ae3bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ae3c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ae3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ae3c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ae3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ae3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ae3d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ae3dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ae3e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ae3e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ae3ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ae3eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ae3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ae3f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ae3fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ae40150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ae405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ae40a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ae40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ae413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ae41870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ae41d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ae421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ae42650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ae42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ae42f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ae43430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ae43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ae43ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ae44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ae44970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ae44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ae45240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ae45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ae45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ae46650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ae46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ae46db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ae473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ae479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ae481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ae48660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ae48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ae48fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ae49750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ae49ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ae4a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ae4a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ae4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ae4b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ae4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ae4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ae4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ae4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ae4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ae4d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ae4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ae4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ae4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ae4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ae4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ae4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ae4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ae4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ae50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ae506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ae50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ae51180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ae516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ae51c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ae52170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ae526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ae52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ae53160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ae536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ae53c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ae54150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ae546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ae54bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ae55140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ae55690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ae55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ae56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ae56680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ae56bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ae57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ae57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ae57bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ae58110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ae58660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ae58bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ae59100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ae59650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ae59ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ae5a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ae5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ae5ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ae5b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ae5b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ae5bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ae5c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ae5c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ae5ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ae5ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ae5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ae5d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ae5dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ae5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ae5e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ae5ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ae5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ae5f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ae5f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ae5fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ae60190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ae60630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ae60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ae612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ae619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ae620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ae62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ae62ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ae632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ae63570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ae63b80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe45790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fe4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fe4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fe4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fe4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fe4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fe4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fe4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fe4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fe4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fe4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fe4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fe4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fe50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fe504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fe50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fe50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fe511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fe51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fe51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fe51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fe523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fe52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fe52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fe53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fe53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fe539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fe53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fe542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fe54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fe54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fe55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fe55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fe558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fe56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fe56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fe571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fe578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fe57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fe57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fe585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fe58c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.230s
sys	0m0.194s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.18 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.62 sec*proc (2 tests)

Total Test time (real) =   1.63 sec
        1.65 real         0.51 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
