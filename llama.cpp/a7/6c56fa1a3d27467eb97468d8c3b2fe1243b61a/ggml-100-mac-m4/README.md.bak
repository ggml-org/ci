### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.38 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.41 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.37 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.01 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  178.12 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.88 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   25.84 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 220.37 sec*proc (27 tests)

Total Test time (real) = 220.38 sec

real	3m40.486s
user	7m35.668s
sys	0m6.052s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.24 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.37 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.39 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.04 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.21 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.07 sec*proc (27 tests)

Total Test time (real) =  51.08 sec

real	0m51.097s
user	1m11.132s
sys	0m5.832s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.124 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.306 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.469 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.480 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.482 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.483 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.484 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.485 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.486 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.487 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.488 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.492 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.497 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.497 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.498 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.499 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.500 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.500 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.501 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.034.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.035.512 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.514 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.035.515 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.035.515 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.035.516 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.035.516 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.035.517 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.035.518 I llama_model_loader: - type  f32:  124 tensors
0.00.035.518 I llama_model_loader: - type  f16:   73 tensors
0.00.040.651 I llm_load_vocab: special tokens cache size = 5
0.00.043.180 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.043.184 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.043.185 I llm_load_print_meta: arch             = bert
0.00.043.185 I llm_load_print_meta: vocab type       = WPM
0.00.043.186 I llm_load_print_meta: n_vocab          = 30522
0.00.043.186 I llm_load_print_meta: n_merges         = 0
0.00.043.186 I llm_load_print_meta: vocab_only       = 0
0.00.043.186 I llm_load_print_meta: n_ctx_train      = 512
0.00.043.187 I llm_load_print_meta: n_embd           = 384
0.00.043.187 I llm_load_print_meta: n_layer          = 12
0.00.043.221 I llm_load_print_meta: n_head           = 12
0.00.043.223 I llm_load_print_meta: n_head_kv        = 12
0.00.043.223 I llm_load_print_meta: n_rot            = 32
0.00.043.223 I llm_load_print_meta: n_swa            = 0
0.00.043.223 I llm_load_print_meta: n_embd_head_k    = 32
0.00.043.224 I llm_load_print_meta: n_embd_head_v    = 32
0.00.043.225 I llm_load_print_meta: n_gqa            = 1
0.00.043.226 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.043.228 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.043.229 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.043.230 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.043.230 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.043.230 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.043.230 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.043.231 I llm_load_print_meta: n_ff             = 1536
0.00.043.232 I llm_load_print_meta: n_expert         = 0
0.00.043.232 I llm_load_print_meta: n_expert_used    = 0
0.00.043.233 I llm_load_print_meta: causal attn      = 0
0.00.043.233 I llm_load_print_meta: pooling type     = 2
0.00.043.234 I llm_load_print_meta: rope type        = 2
0.00.043.234 I llm_load_print_meta: rope scaling     = linear
0.00.043.235 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.043.235 I llm_load_print_meta: freq_scale_train = 1
0.00.043.235 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.043.236 I llm_load_print_meta: rope_finetuned   = unknown
0.00.043.236 I llm_load_print_meta: ssm_d_conv       = 0
0.00.043.236 I llm_load_print_meta: ssm_d_inner      = 0
0.00.043.237 I llm_load_print_meta: ssm_d_state      = 0
0.00.043.237 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.043.237 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.043.249 I llm_load_print_meta: model type       = 33M
0.00.043.249 I llm_load_print_meta: model ftype      = F16
0.00.043.250 I llm_load_print_meta: model params     = 33.21 M
0.00.043.251 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.043.251 I llm_load_print_meta: general.name     = Bge Small
0.00.043.256 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.043.256 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.043.256 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.043.257 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.043.257 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.043.257 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.043.258 I llm_load_print_meta: max token length = 21
0.00.045.572 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.045.574 I llm_load_tensors: offloading output layer to GPU
0.00.045.574 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.045.604 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.045.606 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.046.293 I llama_new_context_with_model: n_seq_max     = 1
0.00.046.295 I llama_new_context_with_model: n_ctx         = 512
0.00.046.295 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.046.295 I llama_new_context_with_model: n_batch       = 2048
0.00.046.296 I llama_new_context_with_model: n_ubatch      = 2048
0.00.046.296 I llama_new_context_with_model: flash_attn    = 0
0.00.046.297 I llama_new_context_with_model: freq_base     = 10000.0
0.00.046.297 I llama_new_context_with_model: freq_scale    = 1
0.00.046.298 I ggml_metal_init: allocating
0.00.046.311 I ggml_metal_init: found device: Apple M4
0.00.046.318 I ggml_metal_init: picking default device: Apple M4
0.00.047.343 I ggml_metal_init: using embedded metal library
0.00.052.012 I ggml_metal_init: GPU name:   Apple M4
0.00.052.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.016 I ggml_metal_init: simdgroup reduction   = true
0.00.052.016 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.017 I ggml_metal_init: has bfloat            = true
0.00.052.017 I ggml_metal_init: use bfloat            = true
0.00.052.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.681 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.066.684 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.066.685 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.067.861 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.067.863 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.067.863 I llama_new_context_with_model: graph nodes  = 429
0.00.067.864 I llama_new_context_with_model: graph splits = 2
0.00.067.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.077.318 I 
0.00.077.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.078.236 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.083.370 I llama_perf_context_print:        load time =      54.00 ms
0.00.083.371 I llama_perf_context_print: prompt eval time =       4.96 ms /     9 tokens (    0.55 ms per token,  1813.42 tokens per second)
0.00.083.372 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.083.372 I llama_perf_context_print:       total time =       6.05 ms /    10 tokens
0.00.083.604 I ggml_metal_free: deallocating

real	0m0.293s
user	0m0.056s
sys	0m0.038s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.032 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.001 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.047 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.052 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.053 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.053 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.054 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.054 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.055 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.055 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.055 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.056 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.058 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.058 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.059 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.059 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.059 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.059 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.060 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.113 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.114 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.115 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.115 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.115 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.116 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.116 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.117 I llama_model_loader: - type  f32:  124 tensors
0.00.014.117 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.470 I llm_load_vocab: special tokens cache size = 5
0.00.017.803 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.807 I llm_load_print_meta: arch             = bert
0.00.017.807 I llm_load_print_meta: vocab type       = WPM
0.00.017.807 I llm_load_print_meta: n_vocab          = 30522
0.00.017.808 I llm_load_print_meta: n_merges         = 0
0.00.017.808 I llm_load_print_meta: vocab_only       = 0
0.00.017.808 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.808 I llm_load_print_meta: n_embd           = 384
0.00.017.808 I llm_load_print_meta: n_layer          = 12
0.00.017.819 I llm_load_print_meta: n_head           = 12
0.00.017.819 I llm_load_print_meta: n_head_kv        = 12
0.00.017.819 I llm_load_print_meta: n_rot            = 32
0.00.017.820 I llm_load_print_meta: n_swa            = 0
0.00.017.820 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.820 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.820 I llm_load_print_meta: n_gqa            = 1
0.00.017.821 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.821 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.822 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.822 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.822 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.822 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.823 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.823 I llm_load_print_meta: n_ff             = 1536
0.00.017.823 I llm_load_print_meta: n_expert         = 0
0.00.017.824 I llm_load_print_meta: n_expert_used    = 0
0.00.017.824 I llm_load_print_meta: causal attn      = 0
0.00.017.824 I llm_load_print_meta: pooling type     = 2
0.00.017.824 I llm_load_print_meta: rope type        = 2
0.00.017.824 I llm_load_print_meta: rope scaling     = linear
0.00.017.825 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.825 I llm_load_print_meta: freq_scale_train = 1
0.00.017.825 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.825 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.825 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.826 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.826 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.826 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.826 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.830 I llm_load_print_meta: model type       = 33M
0.00.017.831 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.831 I llm_load_print_meta: model params     = 33.21 M
0.00.017.832 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.832 I llm_load_print_meta: general.name     = Bge Small
0.00.017.832 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.832 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.832 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.833 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.833 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.833 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.833 I llm_load_print_meta: max token length = 21
0.00.019.050 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.050 I llm_load_tensors: offloading output layer to GPU
0.00.019.051 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.059 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.060 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.409 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.410 I llama_new_context_with_model: n_ctx         = 512
0.00.019.410 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.411 I llama_new_context_with_model: n_batch       = 2048
0.00.019.411 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.411 I llama_new_context_with_model: flash_attn    = 0
0.00.019.411 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.412 I llama_new_context_with_model: freq_scale    = 1
0.00.019.412 I ggml_metal_init: allocating
0.00.019.415 I ggml_metal_init: found device: Apple M4
0.00.019.417 I ggml_metal_init: picking default device: Apple M4
0.00.020.017 I ggml_metal_init: using embedded metal library
0.00.022.297 I ggml_metal_init: GPU name:   Apple M4
0.00.022.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.299 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.300 I ggml_metal_init: simdgroup reduction   = true
0.00.022.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.300 I ggml_metal_init: has bfloat            = true
0.00.022.300 I ggml_metal_init: use bfloat            = true
0.00.022.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.903 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.905 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.907 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.482 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.483 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.483 I llama_new_context_with_model: graph nodes  = 429
0.00.033.483 I llama_new_context_with_model: graph splits = 2
0.00.033.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.105 I 
0.00.038.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.642 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.810 I llama_perf_context_print:        load time =      29.10 ms
0.00.041.811 I llama_perf_context_print: prompt eval time =       3.04 ms /     9 tokens (    0.34 ms per token,  2956.64 tokens per second)
0.00.041.811 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.812 I llama_perf_context_print:       total time =       3.71 ms /    10 tokens
0.00.041.992 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.162 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.987 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.992 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.994 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.996 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.997 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.998 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.999 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.000 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.001 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.001 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.002 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.005 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.006 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.007 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.703 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.704 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.704 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.705 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.705 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.705 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.706 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.706 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.707 I llama_model_loader: - type  f32:   41 tensors
0.00.047.707 I llama_model_loader: - type  f16:   29 tensors
0.00.065.386 W llm_load_vocab: empty token at index 5
0.00.069.901 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.188 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.219 I llm_load_vocab: special tokens cache size = 5
0.00.331.343 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.350 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.351 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.355 I llm_load_print_meta: vocab type       = BPE
0.00.331.355 I llm_load_print_meta: n_vocab          = 61056
0.00.331.355 I llm_load_print_meta: n_merges         = 39382
0.00.331.356 I llm_load_print_meta: vocab_only       = 0
0.00.331.356 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.356 I llm_load_print_meta: n_embd           = 384
0.00.331.356 I llm_load_print_meta: n_layer          = 4
0.00.331.393 I llm_load_print_meta: n_head           = 12
0.00.331.394 I llm_load_print_meta: n_head_kv        = 12
0.00.331.394 I llm_load_print_meta: n_rot            = 32
0.00.331.395 I llm_load_print_meta: n_swa            = 0
0.00.331.395 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.395 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.396 I llm_load_print_meta: n_gqa            = 1
0.00.331.396 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.397 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.397 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.400 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.400 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.400 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.400 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.401 I llm_load_print_meta: n_ff             = 1536
0.00.331.401 I llm_load_print_meta: n_expert         = 0
0.00.331.401 I llm_load_print_meta: n_expert_used    = 0
0.00.331.401 I llm_load_print_meta: causal attn      = 0
0.00.331.402 I llm_load_print_meta: pooling type     = -1
0.00.331.402 I llm_load_print_meta: rope type        = -1
0.00.331.402 I llm_load_print_meta: rope scaling     = linear
0.00.331.402 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.402 I llm_load_print_meta: freq_scale_train = 1
0.00.331.403 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.403 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.403 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.403 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.404 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.404 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.404 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.426 I llm_load_print_meta: model type       = 33M
0.00.331.426 I llm_load_print_meta: model ftype      = F16
0.00.331.427 I llm_load_print_meta: model params     = 32.90 M
0.00.331.427 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.427 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.427 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.428 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.428 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.429 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.429 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.429 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.429 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.429 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.429 I llm_load_print_meta: max token length = 45
0.00.332.535 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.535 I llm_load_tensors: offloading output layer to GPU
0.00.332.536 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.561 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.563 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.488 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.489 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.489 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.489 I llama_new_context_with_model: n_batch       = 2048
0.00.333.490 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.490 I llama_new_context_with_model: flash_attn    = 0
0.00.333.491 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.491 I llama_new_context_with_model: freq_scale    = 1
0.00.333.492 I ggml_metal_init: allocating
0.00.333.495 I ggml_metal_init: found device: Apple M4
0.00.333.497 I ggml_metal_init: picking default device: Apple M4
0.00.334.512 I ggml_metal_init: using embedded metal library
0.00.337.449 I ggml_metal_init: GPU name:   Apple M4
0.00.337.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.451 I ggml_metal_init: simdgroup reduction   = true
0.00.337.452 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.452 I ggml_metal_init: has bfloat            = true
0.00.337.452 I ggml_metal_init: use bfloat            = true
0.00.337.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.366 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.368 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.370 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.349.999 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.001 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.001 I llama_new_context_with_model: graph nodes  = 154
0.00.350.001 I llama_new_context_with_model: graph splits = 2
0.00.350.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.590 I 
0.00.359.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.774 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.775 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.777 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.777 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.781 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.781 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.324 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.016 I llama_perf_context_print:        load time =     336.88 ms
0.00.364.017 I llama_perf_context_print: prompt eval time =       3.68 ms /    62 tokens (    0.06 ms per token, 16829.53 tokens per second)
0.00.364.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.018 I llama_perf_context_print:       total time =       4.43 ms /    63 tokens
0.00.364.251 I ggml_metal_free: deallocating

real	0m1.055s
user	0m0.338s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.221 I main: llama backend init
0.00.000.227 I main: load the model and apply lora adapter, if any
0.00.059.280 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.070.244 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.266 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.070.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.070.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.070.270 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.070.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.070.280 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.070.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.070.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.070.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.070.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.070.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.077.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.086.314 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.315 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.318 I llama_model_loader: - type  f32:  194 tensors
0.00.086.318 I llama_model_loader: - type  f16:   98 tensors
0.00.125.027 I llm_load_vocab: special tokens cache size = 25
0.00.132.760 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.132.763 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.132.764 I llm_load_print_meta: arch             = gptneox
0.00.132.764 I llm_load_print_meta: vocab type       = BPE
0.00.132.764 I llm_load_print_meta: n_vocab          = 50304
0.00.132.764 I llm_load_print_meta: n_merges         = 50009
0.00.132.764 I llm_load_print_meta: vocab_only       = 0
0.00.132.765 I llm_load_print_meta: n_ctx_train      = 2048
0.00.132.765 I llm_load_print_meta: n_embd           = 2048
0.00.132.765 I llm_load_print_meta: n_layer          = 24
0.00.132.790 I llm_load_print_meta: n_head           = 16
0.00.132.791 I llm_load_print_meta: n_head_kv        = 16
0.00.132.791 I llm_load_print_meta: n_rot            = 32
0.00.132.791 I llm_load_print_meta: n_swa            = 0
0.00.132.791 I llm_load_print_meta: n_embd_head_k    = 128
0.00.132.792 I llm_load_print_meta: n_embd_head_v    = 128
0.00.132.792 I llm_load_print_meta: n_gqa            = 1
0.00.132.793 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.132.794 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.132.794 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.132.795 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.132.795 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.132.795 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.132.795 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.132.796 I llm_load_print_meta: n_ff             = 8192
0.00.132.796 I llm_load_print_meta: n_expert         = 0
0.00.132.796 I llm_load_print_meta: n_expert_used    = 0
0.00.132.796 I llm_load_print_meta: causal attn      = 1
0.00.132.797 I llm_load_print_meta: pooling type     = 0
0.00.132.798 I llm_load_print_meta: rope type        = 2
0.00.132.798 I llm_load_print_meta: rope scaling     = linear
0.00.132.800 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.132.800 I llm_load_print_meta: freq_scale_train = 1
0.00.132.801 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.132.801 I llm_load_print_meta: rope_finetuned   = unknown
0.00.132.801 I llm_load_print_meta: ssm_d_conv       = 0
0.00.132.801 I llm_load_print_meta: ssm_d_inner      = 0
0.00.132.801 I llm_load_print_meta: ssm_d_state      = 0
0.00.132.801 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.132.802 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.132.811 I llm_load_print_meta: model type       = 1.4B
0.00.132.812 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.132.812 I llm_load_print_meta: model params     = 1.41 B
0.00.132.814 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.132.814 I llm_load_print_meta: general.name     = 1.4B
0.00.132.814 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.132.814 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.132.814 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.132.814 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.132.815 I llm_load_print_meta: LF token         = 128 ''
0.00.132.815 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.132.815 I llm_load_print_meta: max token length = 1024
0.00.135.491 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.135.491 I llm_load_tensors: offloading output layer to GPU
0.00.135.491 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.135.510 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.135.511 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.136.543 I llama_new_context_with_model: n_seq_max     = 1
0.00.136.543 I llama_new_context_with_model: n_ctx         = 2048
0.00.136.544 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.136.544 I llama_new_context_with_model: n_batch       = 2048
0.00.136.544 I llama_new_context_with_model: n_ubatch      = 512
0.00.136.544 I llama_new_context_with_model: flash_attn    = 0
0.00.136.545 I llama_new_context_with_model: freq_base     = 10000.0
0.00.136.545 I llama_new_context_with_model: freq_scale    = 1
0.00.136.545 I ggml_metal_init: allocating
0.00.136.548 I ggml_metal_init: found device: Apple M4
0.00.136.550 I ggml_metal_init: picking default device: Apple M4
0.00.137.222 I ggml_metal_init: using embedded metal library
0.00.149.041 I ggml_metal_init: GPU name:   Apple M4
0.00.149.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.149.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.149.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.149.045 I ggml_metal_init: simdgroup reduction   = true
0.00.149.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.149.045 I ggml_metal_init: has bfloat            = true
0.00.149.045 I ggml_metal_init: use bfloat            = true
0.00.149.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.149.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.196.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.196.142 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.196.161 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.197.181 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.197.183 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.197.184 I llama_new_context_with_model: graph nodes  = 967
0.00.197.184 I llama_new_context_with_model: graph splits = 2
0.00.197.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.278.780 I main: llama threadpool init, n_threads = 4
0.00.278.816 I 
0.00.278.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.278.856 I 
0.00.278.942 I sampler seed: 1234
0.00.278.946 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.278.970 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.278.973 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.278.973 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.129.661 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.02.129.662 I llama_perf_context_print:        load time =     219.49 ms
0.02.129.663 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.72 tokens per second)
0.02.129.663 I llama_perf_context_print:        eval time =    1803.95 ms /    63 runs   (   28.63 ms per token,    34.92 tokens per second)
0.02.129.664 I llama_perf_context_print:       total time =    1850.88 ms /    70 tokens
0.02.129.864 I ggml_metal_free: deallocating

real	0m2.416s
user	0m0.150s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.588 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.302 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.364 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.377 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.402 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.403 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.409 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.113 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.115 I llama_model_loader: - type  f32:  194 tensors
0.00.053.115 I llama_model_loader: - type  f16:   98 tensors
0.00.083.456 I llm_load_vocab: special tokens cache size = 25
0.00.090.202 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.205 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.206 I llm_load_print_meta: arch             = gptneox
0.00.090.206 I llm_load_print_meta: vocab type       = BPE
0.00.090.206 I llm_load_print_meta: n_vocab          = 50304
0.00.090.206 I llm_load_print_meta: n_merges         = 50009
0.00.090.206 I llm_load_print_meta: vocab_only       = 0
0.00.090.207 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.207 I llm_load_print_meta: n_embd           = 2048
0.00.090.207 I llm_load_print_meta: n_layer          = 24
0.00.090.222 I llm_load_print_meta: n_head           = 16
0.00.090.223 I llm_load_print_meta: n_head_kv        = 16
0.00.090.223 I llm_load_print_meta: n_rot            = 32
0.00.090.224 I llm_load_print_meta: n_swa            = 0
0.00.090.224 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.224 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.225 I llm_load_print_meta: n_gqa            = 1
0.00.090.225 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.226 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.226 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.227 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.230 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.230 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.230 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.231 I llm_load_print_meta: n_ff             = 8192
0.00.090.231 I llm_load_print_meta: n_expert         = 0
0.00.090.231 I llm_load_print_meta: n_expert_used    = 0
0.00.090.231 I llm_load_print_meta: causal attn      = 1
0.00.090.231 I llm_load_print_meta: pooling type     = 0
0.00.090.231 I llm_load_print_meta: rope type        = 2
0.00.090.232 I llm_load_print_meta: rope scaling     = linear
0.00.090.233 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.233 I llm_load_print_meta: freq_scale_train = 1
0.00.090.233 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.234 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.234 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.234 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.234 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.234 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.244 I llm_load_print_meta: model type       = 1.4B
0.00.090.245 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.245 I llm_load_print_meta: model params     = 1.41 B
0.00.090.246 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.246 I llm_load_print_meta: general.name     = 1.4B
0.00.090.246 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.247 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: LF token         = 128 ''
0.00.090.248 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: max token length = 1024
0.00.092.819 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.819 I llm_load_tensors: offloading output layer to GPU
0.00.092.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.830 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.831 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.784 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.785 I llama_new_context_with_model: n_ctx         = 128
0.00.093.785 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.785 I llama_new_context_with_model: n_batch       = 128
0.00.093.785 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.785 I llama_new_context_with_model: flash_attn    = 0
0.00.093.786 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.786 I llama_new_context_with_model: freq_scale    = 1
0.00.093.787 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.787 I ggml_metal_init: allocating
0.00.093.797 I ggml_metal_init: found device: Apple M4
0.00.093.799 I ggml_metal_init: picking default device: Apple M4
0.00.094.425 I ggml_metal_init: using embedded metal library
0.00.097.003 I ggml_metal_init: GPU name:   Apple M4
0.00.097.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.006 I ggml_metal_init: simdgroup reduction   = true
0.00.097.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.006 I ggml_metal_init: has bfloat            = true
0.00.097.006 I ggml_metal_init: use bfloat            = true
0.00.097.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.544 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.547 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.429 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.430 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.431 I llama_new_context_with_model: graph nodes  = 967
0.00.108.431 I llama_new_context_with_model: graph splits = 2
0.00.108.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.053.627 I 
0.01.053.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.053.717 I perplexity: tokenizing the input ..
0.01.065.665 I perplexity: tokenization took 11.944 ms
0.01.065.696 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.186.275 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.188.065 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.188.098 I llama_perf_context_print:        load time =    1031.31 ms
0.01.188.100 I llama_perf_context_print: prompt eval time =     120.19 ms /   128 tokens (    0.94 ms per token,  1064.99 tokens per second)
0.01.188.101 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.188.102 I llama_perf_context_print:       total time =     134.47 ms /   129 tokens
0.01.188.663 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.124s
sys	0m0.200s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.132 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.143 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.145 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.363 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.559 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.559 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.560 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.560 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.561 I llama_model_loader: - type  f32:  194 tensors
0.00.038.561 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.670 I llm_load_vocab: special tokens cache size = 25
0.00.071.441 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.444 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.445 I llm_load_print_meta: arch             = gptneox
0.00.071.445 I llm_load_print_meta: vocab type       = BPE
0.00.071.446 I llm_load_print_meta: n_vocab          = 50304
0.00.071.446 I llm_load_print_meta: n_merges         = 50009
0.00.071.449 I llm_load_print_meta: vocab_only       = 0
0.00.071.449 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.449 I llm_load_print_meta: n_embd           = 2048
0.00.071.449 I llm_load_print_meta: n_layer          = 24
0.00.071.467 I llm_load_print_meta: n_head           = 16
0.00.071.469 I llm_load_print_meta: n_head_kv        = 16
0.00.071.469 I llm_load_print_meta: n_rot            = 32
0.00.071.469 I llm_load_print_meta: n_swa            = 0
0.00.071.470 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.470 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.471 I llm_load_print_meta: n_gqa            = 1
0.00.071.471 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.472 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.473 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.473 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.473 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.474 I llm_load_print_meta: n_ff             = 8192
0.00.071.474 I llm_load_print_meta: n_expert         = 0
0.00.071.475 I llm_load_print_meta: n_expert_used    = 0
0.00.071.475 I llm_load_print_meta: causal attn      = 1
0.00.071.475 I llm_load_print_meta: pooling type     = 0
0.00.071.476 I llm_load_print_meta: rope type        = 2
0.00.071.476 I llm_load_print_meta: rope scaling     = linear
0.00.071.477 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.477 I llm_load_print_meta: freq_scale_train = 1
0.00.071.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.478 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.488 I llm_load_print_meta: model type       = 1.4B
0.00.071.489 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.489 I llm_load_print_meta: model params     = 1.41 B
0.00.071.490 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.490 I llm_load_print_meta: general.name     = 1.4B
0.00.071.490 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.490 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.490 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.490 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.491 I llm_load_print_meta: LF token         = 128 ''
0.00.071.491 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.491 I llm_load_print_meta: max token length = 1024
0.00.074.111 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.111 I llm_load_tensors: offloading output layer to GPU
0.00.074.111 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.123 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.124 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.223 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.225 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.225 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.225 I llama_new_context_with_model: n_batch       = 2048
0.00.075.225 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.225 I llama_new_context_with_model: flash_attn    = 0
0.00.075.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.226 I llama_new_context_with_model: freq_scale    = 1
0.00.075.227 I ggml_metal_init: allocating
0.00.075.234 I ggml_metal_init: found device: Apple M4
0.00.075.237 I ggml_metal_init: picking default device: Apple M4
0.00.076.034 I ggml_metal_init: using embedded metal library
0.00.079.146 I ggml_metal_init: GPU name:   Apple M4
0.00.079.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.149 I ggml_metal_init: simdgroup reduction   = true
0.00.079.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.150 I ggml_metal_init: has bfloat            = true
0.00.079.150 I ggml_metal_init: use bfloat            = true
0.00.079.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.306 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.332 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.441 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.443 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.443 I llama_new_context_with_model: graph nodes  = 967
0.00.118.443 I llama_new_context_with_model: graph splits = 2
0.00.118.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.407.010 I main: llama threadpool init, n_threads = 4
0.01.407.051 I 
0.01.407.086 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.407.087 I 
0.01.407.314 I sampler seed: 1234
0.01.407.318 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.407.329 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.407.329 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.407.329 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.496.323 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 63055.06 tokens per second)
0.02.496.324 I llama_perf_context_print:        load time =    1397.34 ms
0.02.496.324 I llama_perf_context_print: prompt eval time =      39.94 ms /     7 tokens (    5.71 ms per token,   175.27 tokens per second)
0.02.496.325 I llama_perf_context_print:        eval time =    1046.15 ms /    63 runs   (   16.61 ms per token,    60.22 tokens per second)
0.02.496.325 I llama_perf_context_print:       total time =    1089.32 ms /    70 tokens
0.02.496.520 I ggml_metal_free: deallocating

real	0m2.514s
user	0m0.121s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.127 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.207 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.184 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.646 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.647 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.648 I llama_model_loader: - type  f32:  194 tensors
0.00.035.649 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.779 I llm_load_vocab: special tokens cache size = 25
0.00.067.911 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.915 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.915 I llm_load_print_meta: arch             = gptneox
0.00.067.916 I llm_load_print_meta: vocab type       = BPE
0.00.067.916 I llm_load_print_meta: n_vocab          = 50304
0.00.067.916 I llm_load_print_meta: n_merges         = 50009
0.00.067.916 I llm_load_print_meta: vocab_only       = 0
0.00.067.916 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.917 I llm_load_print_meta: n_embd           = 2048
0.00.067.917 I llm_load_print_meta: n_layer          = 24
0.00.067.934 I llm_load_print_meta: n_head           = 16
0.00.067.935 I llm_load_print_meta: n_head_kv        = 16
0.00.067.935 I llm_load_print_meta: n_rot            = 32
0.00.067.935 I llm_load_print_meta: n_swa            = 0
0.00.067.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.936 I llm_load_print_meta: n_gqa            = 1
0.00.067.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.938 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.939 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.939 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.939 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.941 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.941 I llm_load_print_meta: n_ff             = 8192
0.00.067.941 I llm_load_print_meta: n_expert         = 0
0.00.067.941 I llm_load_print_meta: n_expert_used    = 0
0.00.067.942 I llm_load_print_meta: causal attn      = 1
0.00.067.942 I llm_load_print_meta: pooling type     = 0
0.00.067.942 I llm_load_print_meta: rope type        = 2
0.00.067.942 I llm_load_print_meta: rope scaling     = linear
0.00.067.943 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.943 I llm_load_print_meta: freq_scale_train = 1
0.00.067.943 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.943 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.943 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.943 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.945 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.945 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.945 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.955 I llm_load_print_meta: model type       = 1.4B
0.00.067.956 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.956 I llm_load_print_meta: model params     = 1.41 B
0.00.067.958 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.958 I llm_load_print_meta: general.name     = 1.4B
0.00.067.958 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.958 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.958 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.958 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.959 I llm_load_print_meta: LF token         = 128 ''
0.00.067.959 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.959 I llm_load_print_meta: max token length = 1024
0.00.070.396 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.397 I llm_load_tensors: offloading output layer to GPU
0.00.070.397 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.409 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.410 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.393 I llama_new_context_with_model: n_ctx         = 128
0.00.071.393 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.393 I llama_new_context_with_model: n_batch       = 128
0.00.071.393 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.393 I llama_new_context_with_model: flash_attn    = 0
0.00.071.394 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.394 I llama_new_context_with_model: freq_scale    = 1
0.00.071.394 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.395 I ggml_metal_init: allocating
0.00.071.397 I ggml_metal_init: found device: Apple M4
0.00.071.399 I ggml_metal_init: picking default device: Apple M4
0.00.072.113 I ggml_metal_init: using embedded metal library
0.00.074.689 I ggml_metal_init: GPU name:   Apple M4
0.00.074.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.692 I ggml_metal_init: simdgroup reduction   = true
0.00.074.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.692 I ggml_metal_init: has bfloat            = true
0.00.074.692 I ggml_metal_init: use bfloat            = true
0.00.074.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.550 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.568 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.561 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.087.562 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.087.562 I llama_new_context_with_model: graph nodes  = 967
0.00.087.562 I llama_new_context_with_model: graph splits = 2
0.00.087.576 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.951.893 I 
0.00.951.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.951.956 I perplexity: tokenizing the input ..
0.00.959.432 I perplexity: tokenization took 7.475 ms
0.00.959.443 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.083.716 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.084.894 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.084.910 I llama_perf_context_print:        load time =     938.68 ms
0.01.084.912 I llama_perf_context_print: prompt eval time =     124.05 ms /   128 tokens (    0.97 ms per token,  1031.86 tokens per second)
0.01.084.913 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.084.913 I llama_perf_context_print:       total time =     133.02 ms /   129 tokens
0.01.085.385 I ggml_metal_free: deallocating

real	0m1.106s
user	0m0.096s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.013.690 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.089 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.089 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.090 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.091 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.091 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.091 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.092 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.092 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.095 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.096 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.096 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.365 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.365 I llama_model_loader: - type  f32:  194 tensors
0.00.030.366 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.026 I llm_load_vocab: special tokens cache size = 25
0.00.057.035 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.038 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.038 I llm_load_print_meta: arch             = gptneox
0.00.057.038 I llm_load_print_meta: vocab type       = BPE
0.00.057.039 I llm_load_print_meta: n_vocab          = 50304
0.00.057.039 I llm_load_print_meta: n_merges         = 50009
0.00.057.039 I llm_load_print_meta: vocab_only       = 0
0.00.057.039 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.039 I llm_load_print_meta: n_embd           = 2048
0.00.057.039 I llm_load_print_meta: n_layer          = 24
0.00.057.056 I llm_load_print_meta: n_head           = 16
0.00.057.057 I llm_load_print_meta: n_head_kv        = 16
0.00.057.057 I llm_load_print_meta: n_rot            = 32
0.00.057.057 I llm_load_print_meta: n_swa            = 0
0.00.057.057 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.057 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.058 I llm_load_print_meta: n_gqa            = 1
0.00.057.059 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.060 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.060 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.060 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.061 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.061 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.061 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.062 I llm_load_print_meta: n_ff             = 8192
0.00.057.064 I llm_load_print_meta: n_expert         = 0
0.00.057.064 I llm_load_print_meta: n_expert_used    = 0
0.00.057.064 I llm_load_print_meta: causal attn      = 1
0.00.057.064 I llm_load_print_meta: pooling type     = 0
0.00.057.065 I llm_load_print_meta: rope type        = 2
0.00.057.065 I llm_load_print_meta: rope scaling     = linear
0.00.057.065 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.065 I llm_load_print_meta: freq_scale_train = 1
0.00.057.065 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.066 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.066 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.066 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.066 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.066 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.066 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.076 I llm_load_print_meta: model type       = 1.4B
0.00.057.077 I llm_load_print_meta: model ftype      = Q4_0
0.00.057.078 I llm_load_print_meta: model params     = 1.41 B
0.00.057.078 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.057.078 I llm_load_print_meta: general.name     = 1.4B
0.00.057.079 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.079 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.079 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.079 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.079 I llm_load_print_meta: LF token         = 128 ''
0.00.057.080 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.080 I llm_load_print_meta: max token length = 1024
0.00.059.271 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.272 I llm_load_tensors: offloading output layer to GPU
0.00.059.272 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.283 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.059.285 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.060.282 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.283 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.283 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.283 I llama_new_context_with_model: n_batch       = 2048
0.00.060.283 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.284 I llama_new_context_with_model: flash_attn    = 0
0.00.060.284 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.284 I llama_new_context_with_model: freq_scale    = 1
0.00.060.285 I ggml_metal_init: allocating
0.00.060.289 I ggml_metal_init: found device: Apple M4
0.00.060.291 I ggml_metal_init: picking default device: Apple M4
0.00.060.993 I ggml_metal_init: using embedded metal library
0.00.063.450 I ggml_metal_init: GPU name:   Apple M4
0.00.063.451 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.452 I ggml_metal_init: simdgroup reduction   = true
0.00.063.452 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.452 I ggml_metal_init: has bfloat            = true
0.00.063.453 I ggml_metal_init: use bfloat            = true
0.00.063.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.172 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.183 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.209 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.291 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.294 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.294 I llama_new_context_with_model: graph nodes  = 967
0.00.099.294 I llama_new_context_with_model: graph splits = 2
0.00.099.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.522 I main: llama threadpool init, n_threads = 4
0.00.769.571 I 
0.00.769.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.604 I 
0.00.769.845 I sampler seed: 1234
0.00.769.850 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.889 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.891 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.891 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.443.001 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47523.43 tokens per second)
0.01.443.001 I llama_perf_context_print:        load time =     755.83 ms
0.01.443.003 I llama_perf_context_print: prompt eval time =      39.91 ms /     7 tokens (    5.70 ms per token,   175.39 tokens per second)
0.01.443.004 I llama_perf_context_print:        eval time =     630.72 ms /    63 runs   (   10.01 ms per token,    99.89 tokens per second)
0.01.443.006 I llama_perf_context_print:       total time =     673.48 ms /    70 tokens
0.01.443.234 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.112s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.340 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.896 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.897 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.897 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.899 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.608 I llama_model_loader: - type  f32:  194 tensors
0.00.023.609 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.609 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.606 I llm_load_vocab: special tokens cache size = 25
0.00.049.657 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.659 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.659 I llm_load_print_meta: arch             = gptneox
0.00.049.660 I llm_load_print_meta: vocab type       = BPE
0.00.049.660 I llm_load_print_meta: n_vocab          = 50304
0.00.049.660 I llm_load_print_meta: n_merges         = 50009
0.00.049.660 I llm_load_print_meta: vocab_only       = 0
0.00.049.661 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.661 I llm_load_print_meta: n_embd           = 2048
0.00.049.661 I llm_load_print_meta: n_layer          = 24
0.00.049.675 I llm_load_print_meta: n_head           = 16
0.00.049.676 I llm_load_print_meta: n_head_kv        = 16
0.00.049.676 I llm_load_print_meta: n_rot            = 32
0.00.049.676 I llm_load_print_meta: n_swa            = 0
0.00.049.677 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.677 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.677 I llm_load_print_meta: n_gqa            = 1
0.00.049.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.682 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.682 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.683 I llm_load_print_meta: n_ff             = 8192
0.00.049.684 I llm_load_print_meta: n_expert         = 0
0.00.049.684 I llm_load_print_meta: n_expert_used    = 0
0.00.049.684 I llm_load_print_meta: causal attn      = 1
0.00.049.684 I llm_load_print_meta: pooling type     = 0
0.00.049.684 I llm_load_print_meta: rope type        = 2
0.00.049.684 I llm_load_print_meta: rope scaling     = linear
0.00.049.685 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.685 I llm_load_print_meta: freq_scale_train = 1
0.00.049.685 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.685 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.686 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.686 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.686 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.686 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.686 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.696 I llm_load_print_meta: model type       = 1.4B
0.00.049.696 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.696 I llm_load_print_meta: model params     = 1.41 B
0.00.049.697 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.697 I llm_load_print_meta: general.name     = 1.4B
0.00.049.697 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.698 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.698 I llm_load_print_meta: LF token         = 128 ''
0.00.049.698 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.698 I llm_load_print_meta: max token length = 1024
0.00.051.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.597 I llm_load_tensors: offloading output layer to GPU
0.00.051.597 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.607 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.608 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.489 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.490 I llama_new_context_with_model: n_ctx         = 128
0.00.052.490 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.490 I llama_new_context_with_model: n_batch       = 128
0.00.052.490 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.490 I llama_new_context_with_model: flash_attn    = 0
0.00.052.491 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.491 I llama_new_context_with_model: freq_scale    = 1
0.00.052.492 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.492 I ggml_metal_init: allocating
0.00.052.498 I ggml_metal_init: found device: Apple M4
0.00.052.500 I ggml_metal_init: picking default device: Apple M4
0.00.053.048 I ggml_metal_init: using embedded metal library
0.00.055.388 I ggml_metal_init: GPU name:   Apple M4
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.391 I ggml_metal_init: simdgroup reduction   = true
0.00.055.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.391 I ggml_metal_init: has bfloat            = true
0.00.055.391 I ggml_metal_init: use bfloat            = true
0.00.055.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.467 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.480 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.314 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.315 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.315 I llama_new_context_with_model: graph nodes  = 967
0.00.067.316 I llama_new_context_with_model: graph splits = 2
0.00.067.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.979 I 
0.00.632.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.022 I perplexity: tokenizing the input ..
0.00.639.719 I perplexity: tokenization took 7.695 ms
0.00.639.733 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.332 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.763.469 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.763.479 I llama_perf_context_print:        load time =     622.63 ms
0.00.763.479 I llama_perf_context_print: prompt eval time =     122.37 ms /   128 tokens (    0.96 ms per token,  1046.00 tokens per second)
0.00.763.480 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.481 I llama_perf_context_print:       total time =     131.50 ms /   129 tokens
0.00.763.795 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.077s
sys	0m0.112s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.577 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.584 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.584 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.585 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.587 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.110 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.112 I llama_model_loader: - type  f32:  194 tensors
0.00.024.112 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.219 I llm_load_vocab: special tokens cache size = 25
0.00.051.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.316 I llm_load_print_meta: arch             = gptneox
0.00.051.316 I llm_load_print_meta: vocab type       = BPE
0.00.051.316 I llm_load_print_meta: n_vocab          = 50304
0.00.051.317 I llm_load_print_meta: n_merges         = 50009
0.00.051.317 I llm_load_print_meta: vocab_only       = 0
0.00.051.317 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.317 I llm_load_print_meta: n_embd           = 2048
0.00.051.317 I llm_load_print_meta: n_layer          = 24
0.00.051.334 I llm_load_print_meta: n_head           = 16
0.00.051.336 I llm_load_print_meta: n_head_kv        = 16
0.00.051.336 I llm_load_print_meta: n_rot            = 32
0.00.051.336 I llm_load_print_meta: n_swa            = 0
0.00.051.336 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.336 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.337 I llm_load_print_meta: n_gqa            = 1
0.00.051.341 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.342 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.342 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.344 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.345 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.345 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.345 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.346 I llm_load_print_meta: n_ff             = 8192
0.00.051.346 I llm_load_print_meta: n_expert         = 0
0.00.051.346 I llm_load_print_meta: n_expert_used    = 0
0.00.051.346 I llm_load_print_meta: causal attn      = 1
0.00.051.347 I llm_load_print_meta: pooling type     = 0
0.00.051.347 I llm_load_print_meta: rope type        = 2
0.00.051.347 I llm_load_print_meta: rope scaling     = linear
0.00.051.347 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.347 I llm_load_print_meta: freq_scale_train = 1
0.00.051.348 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.348 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.348 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.348 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.348 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.348 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.348 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.358 I llm_load_print_meta: model type       = 1.4B
0.00.051.358 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.359 I llm_load_print_meta: model params     = 1.41 B
0.00.051.360 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.360 I llm_load_print_meta: general.name     = 1.4B
0.00.051.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: LF token         = 128 ''
0.00.051.362 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: max token length = 1024
0.00.053.391 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.391 I llm_load_tensors: offloading output layer to GPU
0.00.053.391 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.402 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.403 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.304 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.304 I llama_new_context_with_model: n_batch       = 2048
0.00.054.304 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.304 I llama_new_context_with_model: flash_attn    = 0
0.00.054.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.305 I llama_new_context_with_model: freq_scale    = 1
0.00.054.306 I ggml_metal_init: allocating
0.00.054.310 I ggml_metal_init: found device: Apple M4
0.00.054.312 I ggml_metal_init: picking default device: Apple M4
0.00.054.916 I ggml_metal_init: using embedded metal library
0.00.057.216 I ggml_metal_init: GPU name:   Apple M4
0.00.057.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.219 I ggml_metal_init: simdgroup reduction   = true
0.00.057.219 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.220 I ggml_metal_init: has bfloat            = true
0.00.057.220 I ggml_metal_init: use bfloat            = true
0.00.057.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.695 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.701 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.720 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.709 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.710 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.711 I llama_new_context_with_model: graph nodes  = 967
0.00.089.711 I llama_new_context_with_model: graph splits = 2
0.00.089.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.269 I main: llama threadpool init, n_threads = 4
0.00.730.305 I 
0.00.730.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.337 I 
0.00.730.569 I sampler seed: 1234
0.00.730.573 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.617 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.621 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.621 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.465.297 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65862.71 tokens per second)
0.01.465.298 I llama_perf_context_print:        load time =     721.41 ms
0.01.465.299 I llama_perf_context_print: prompt eval time =      46.25 ms /     7 tokens (    6.61 ms per token,   151.36 tokens per second)
0.01.465.299 I llama_perf_context_print:        eval time =     685.60 ms /    63 runs   (   10.88 ms per token,    91.89 tokens per second)
0.01.465.300 I llama_perf_context_print:       total time =     735.03 ms /    70 tokens
0.01.465.502 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.754 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.722 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.727 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.727 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.728 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.686 I llama_model_loader: - type  f32:  194 tensors
0.00.023.686 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.687 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.614 I llm_load_vocab: special tokens cache size = 25
0.00.050.620 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.622 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.623 I llm_load_print_meta: arch             = gptneox
0.00.050.623 I llm_load_print_meta: vocab type       = BPE
0.00.050.623 I llm_load_print_meta: n_vocab          = 50304
0.00.050.623 I llm_load_print_meta: n_merges         = 50009
0.00.050.624 I llm_load_print_meta: vocab_only       = 0
0.00.050.624 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.624 I llm_load_print_meta: n_embd           = 2048
0.00.050.624 I llm_load_print_meta: n_layer          = 24
0.00.050.639 I llm_load_print_meta: n_head           = 16
0.00.050.639 I llm_load_print_meta: n_head_kv        = 16
0.00.050.640 I llm_load_print_meta: n_rot            = 32
0.00.050.640 I llm_load_print_meta: n_swa            = 0
0.00.050.640 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.640 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.641 I llm_load_print_meta: n_gqa            = 1
0.00.050.642 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.642 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.643 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.643 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.643 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.643 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.646 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.646 I llm_load_print_meta: n_ff             = 8192
0.00.050.646 I llm_load_print_meta: n_expert         = 0
0.00.050.647 I llm_load_print_meta: n_expert_used    = 0
0.00.050.647 I llm_load_print_meta: causal attn      = 1
0.00.050.648 I llm_load_print_meta: pooling type     = 0
0.00.050.648 I llm_load_print_meta: rope type        = 2
0.00.050.648 I llm_load_print_meta: rope scaling     = linear
0.00.050.649 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.649 I llm_load_print_meta: freq_scale_train = 1
0.00.050.649 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.649 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.649 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.650 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.650 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.651 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.660 I llm_load_print_meta: model type       = 1.4B
0.00.050.660 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.661 I llm_load_print_meta: model params     = 1.41 B
0.00.050.661 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.661 I llm_load_print_meta: general.name     = 1.4B
0.00.050.662 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.662 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.662 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.662 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.662 I llm_load_print_meta: LF token         = 128 ''
0.00.050.663 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.663 I llm_load_print_meta: max token length = 1024
0.00.052.640 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.641 I llm_load_tensors: offloading output layer to GPU
0.00.052.641 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.651 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.652 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.577 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.577 I llama_new_context_with_model: n_ctx         = 128
0.00.053.578 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.578 I llama_new_context_with_model: n_batch       = 128
0.00.053.578 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.578 I llama_new_context_with_model: flash_attn    = 0
0.00.053.578 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.579 I llama_new_context_with_model: freq_scale    = 1
0.00.053.579 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.579 I ggml_metal_init: allocating
0.00.053.585 I ggml_metal_init: found device: Apple M4
0.00.053.587 I ggml_metal_init: picking default device: Apple M4
0.00.054.141 I ggml_metal_init: using embedded metal library
0.00.056.506 I ggml_metal_init: GPU name:   Apple M4
0.00.056.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.508 I ggml_metal_init: simdgroup reduction   = true
0.00.056.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.508 I ggml_metal_init: has bfloat            = true
0.00.056.509 I ggml_metal_init: use bfloat            = true
0.00.056.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.335 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.341 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.354 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.248 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.249 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.249 I llama_new_context_with_model: graph nodes  = 967
0.00.068.250 I llama_new_context_with_model: graph splits = 2
0.00.068.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.780 I 
0.00.669.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.932 I perplexity: tokenizing the input ..
0.00.677.296 I perplexity: tokenization took 7.363 ms
0.00.677.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.292 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.801.526 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.801.549 I llama_perf_context_print:        load time =     661.02 ms
0.00.801.550 I llama_perf_context_print: prompt eval time =     122.76 ms /   128 tokens (    0.96 ms per token,  1042.68 tokens per second)
0.00.801.553 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.553 I llama_perf_context_print:       total time =     131.78 ms /   129 tokens
0.00.802.039 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.078s
sys	0m0.110s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.928 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.227 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.228 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.235 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.080 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.989 I llama_model_loader: - type  f32:  194 tensors
0.00.024.989 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.853 I llm_load_vocab: special tokens cache size = 25
0.00.051.963 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.966 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.966 I llm_load_print_meta: arch             = gptneox
0.00.051.967 I llm_load_print_meta: vocab type       = BPE
0.00.051.967 I llm_load_print_meta: n_vocab          = 50304
0.00.051.967 I llm_load_print_meta: n_merges         = 50009
0.00.051.967 I llm_load_print_meta: vocab_only       = 0
0.00.051.967 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.968 I llm_load_print_meta: n_embd           = 2048
0.00.051.968 I llm_load_print_meta: n_layer          = 24
0.00.051.982 I llm_load_print_meta: n_head           = 16
0.00.051.984 I llm_load_print_meta: n_head_kv        = 16
0.00.051.984 I llm_load_print_meta: n_rot            = 32
0.00.051.984 I llm_load_print_meta: n_swa            = 0
0.00.051.985 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.985 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.985 I llm_load_print_meta: n_gqa            = 1
0.00.051.986 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.987 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.988 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.988 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.988 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.989 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.989 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.990 I llm_load_print_meta: n_ff             = 8192
0.00.051.990 I llm_load_print_meta: n_expert         = 0
0.00.051.991 I llm_load_print_meta: n_expert_used    = 0
0.00.051.992 I llm_load_print_meta: causal attn      = 1
0.00.051.993 I llm_load_print_meta: pooling type     = 0
0.00.051.993 I llm_load_print_meta: rope type        = 2
0.00.051.993 I llm_load_print_meta: rope scaling     = linear
0.00.051.994 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.994 I llm_load_print_meta: freq_scale_train = 1
0.00.051.994 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.994 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.994 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.995 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.995 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.995 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.995 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.004 I llm_load_print_meta: model type       = 1.4B
0.00.052.005 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.005 I llm_load_print_meta: model params     = 1.41 B
0.00.052.006 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.006 I llm_load_print_meta: general.name     = 1.4B
0.00.052.006 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.006 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.006 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.006 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.007 I llm_load_print_meta: LF token         = 128 ''
0.00.052.007 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.007 I llm_load_print_meta: max token length = 1024
0.00.053.989 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.989 I llm_load_tensors: offloading output layer to GPU
0.00.053.989 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.999 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.001 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.901 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.902 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.902 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.902 I llama_new_context_with_model: n_batch       = 2048
0.00.054.902 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.902 I llama_new_context_with_model: flash_attn    = 0
0.00.054.903 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.903 I llama_new_context_with_model: freq_scale    = 1
0.00.054.903 I ggml_metal_init: allocating
0.00.054.907 I ggml_metal_init: found device: Apple M4
0.00.054.909 I ggml_metal_init: picking default device: Apple M4
0.00.055.508 I ggml_metal_init: using embedded metal library
0.00.057.852 I ggml_metal_init: GPU name:   Apple M4
0.00.057.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.854 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.854 I ggml_metal_init: simdgroup reduction   = true
0.00.057.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.854 I ggml_metal_init: has bfloat            = true
0.00.057.855 I ggml_metal_init: use bfloat            = true
0.00.057.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.688 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.693 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.710 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.757 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.758 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.758 I llama_new_context_with_model: graph nodes  = 967
0.00.088.758 I llama_new_context_with_model: graph splits = 2
0.00.088.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.112 I main: llama threadpool init, n_threads = 4
0.00.775.150 I 
0.00.775.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.180 I 
0.00.775.417 I sampler seed: 1234
0.00.775.421 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.462 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.463 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.463 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.566.634 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.566.635 I llama_perf_context_print:        load time =     765.18 ms
0.01.566.635 I llama_perf_context_print: prompt eval time =      46.17 ms /     7 tokens (    6.60 ms per token,   151.63 tokens per second)
0.01.566.636 I llama_perf_context_print:        eval time =     742.01 ms /    63 runs   (   11.78 ms per token,    84.90 tokens per second)
0.01.566.637 I llama_perf_context_print:       total time =     791.52 ms /    70 tokens
0.01.566.824 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.531 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.531 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.197 I llama_model_loader: - type  f32:  194 tensors
0.00.024.197 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.218 I llm_load_vocab: special tokens cache size = 25
0.00.050.229 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.233 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.234 I llm_load_print_meta: arch             = gptneox
0.00.050.234 I llm_load_print_meta: vocab type       = BPE
0.00.050.234 I llm_load_print_meta: n_vocab          = 50304
0.00.050.234 I llm_load_print_meta: n_merges         = 50009
0.00.050.235 I llm_load_print_meta: vocab_only       = 0
0.00.050.235 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.235 I llm_load_print_meta: n_embd           = 2048
0.00.050.235 I llm_load_print_meta: n_layer          = 24
0.00.050.249 I llm_load_print_meta: n_head           = 16
0.00.050.251 I llm_load_print_meta: n_head_kv        = 16
0.00.050.251 I llm_load_print_meta: n_rot            = 32
0.00.050.251 I llm_load_print_meta: n_swa            = 0
0.00.050.252 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.252 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.254 I llm_load_print_meta: n_gqa            = 1
0.00.050.255 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.256 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.256 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.257 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.258 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.258 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.258 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.259 I llm_load_print_meta: n_ff             = 8192
0.00.050.259 I llm_load_print_meta: n_expert         = 0
0.00.050.259 I llm_load_print_meta: n_expert_used    = 0
0.00.050.260 I llm_load_print_meta: causal attn      = 1
0.00.050.260 I llm_load_print_meta: pooling type     = 0
0.00.050.260 I llm_load_print_meta: rope type        = 2
0.00.050.260 I llm_load_print_meta: rope scaling     = linear
0.00.050.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.265 I llm_load_print_meta: freq_scale_train = 1
0.00.050.267 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.267 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.267 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.268 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.268 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.268 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.268 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.278 I llm_load_print_meta: model type       = 1.4B
0.00.050.278 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.279 I llm_load_print_meta: model params     = 1.41 B
0.00.050.279 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.279 I llm_load_print_meta: general.name     = 1.4B
0.00.050.280 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.280 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.280 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.280 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: LF token         = 128 ''
0.00.050.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.282 I llm_load_print_meta: max token length = 1024
0.00.052.264 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.264 I llm_load_tensors: offloading output layer to GPU
0.00.052.265 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.275 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.276 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.265 I llama_new_context_with_model: n_ctx         = 128
0.00.053.265 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.265 I llama_new_context_with_model: n_batch       = 128
0.00.053.266 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.266 I llama_new_context_with_model: flash_attn    = 0
0.00.053.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.266 I llama_new_context_with_model: freq_scale    = 1
0.00.053.267 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.267 I ggml_metal_init: allocating
0.00.053.270 I ggml_metal_init: found device: Apple M4
0.00.053.272 I ggml_metal_init: picking default device: Apple M4
0.00.053.830 I ggml_metal_init: using embedded metal library
0.00.056.115 I ggml_metal_init: GPU name:   Apple M4
0.00.056.116 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.117 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.117 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.117 I ggml_metal_init: simdgroup reduction   = true
0.00.056.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.117 I ggml_metal_init: has bfloat            = true
0.00.056.118 I ggml_metal_init: use bfloat            = true
0.00.056.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.792 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.794 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.807 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.721 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.722 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.722 I llama_new_context_with_model: graph nodes  = 967
0.00.067.722 I llama_new_context_with_model: graph splits = 2
0.00.067.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.849 I 
0.00.704.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.893 I perplexity: tokenizing the input ..
0.00.712.502 I perplexity: tokenization took 7.607 ms
0.00.712.513 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.585 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.848.839 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.848.858 I llama_perf_context_print:        load time =     695.18 ms
0.00.848.859 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.25 tokens per second)
0.00.848.860 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.860 I llama_perf_context_print:       total time =     144.01 ms /   129 tokens
0.00.849.376 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.077s
sys	0m0.109s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.717 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.609 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.611 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.612 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.612 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.613 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.613 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.617 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.356 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.357 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.359 I llama_model_loader: - type  f32:  194 tensors
0.00.025.359 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.359 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.444 I llm_load_vocab: special tokens cache size = 25
0.00.051.348 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.351 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.351 I llm_load_print_meta: arch             = gptneox
0.00.051.352 I llm_load_print_meta: vocab type       = BPE
0.00.051.352 I llm_load_print_meta: n_vocab          = 50304
0.00.051.352 I llm_load_print_meta: n_merges         = 50009
0.00.051.352 I llm_load_print_meta: vocab_only       = 0
0.00.051.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.353 I llm_load_print_meta: n_embd           = 2048
0.00.051.353 I llm_load_print_meta: n_layer          = 24
0.00.051.367 I llm_load_print_meta: n_head           = 16
0.00.051.369 I llm_load_print_meta: n_head_kv        = 16
0.00.051.369 I llm_load_print_meta: n_rot            = 32
0.00.051.369 I llm_load_print_meta: n_swa            = 0
0.00.051.369 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.369 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.370 I llm_load_print_meta: n_gqa            = 1
0.00.051.371 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.371 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.372 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.372 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.373 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.373 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.373 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.373 I llm_load_print_meta: n_ff             = 8192
0.00.051.374 I llm_load_print_meta: n_expert         = 0
0.00.051.374 I llm_load_print_meta: n_expert_used    = 0
0.00.051.375 I llm_load_print_meta: causal attn      = 1
0.00.051.377 I llm_load_print_meta: pooling type     = 0
0.00.051.377 I llm_load_print_meta: rope type        = 2
0.00.051.377 I llm_load_print_meta: rope scaling     = linear
0.00.051.377 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.377 I llm_load_print_meta: freq_scale_train = 1
0.00.051.378 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.378 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.379 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.379 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.379 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.379 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.379 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.389 I llm_load_print_meta: model type       = 1.4B
0.00.051.389 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.390 I llm_load_print_meta: model params     = 1.41 B
0.00.051.391 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.391 I llm_load_print_meta: general.name     = 1.4B
0.00.051.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.392 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.392 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.392 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.392 I llm_load_print_meta: LF token         = 128 ''
0.00.051.393 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.393 I llm_load_print_meta: max token length = 1024
0.00.053.383 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.383 I llm_load_tensors: offloading output layer to GPU
0.00.053.383 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.394 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.395 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.301 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.301 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.302 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.302 I llama_new_context_with_model: n_batch       = 2048
0.00.054.302 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.302 I llama_new_context_with_model: flash_attn    = 0
0.00.054.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.303 I llama_new_context_with_model: freq_scale    = 1
0.00.054.303 I ggml_metal_init: allocating
0.00.054.306 I ggml_metal_init: found device: Apple M4
0.00.054.308 I ggml_metal_init: picking default device: Apple M4
0.00.054.898 I ggml_metal_init: using embedded metal library
0.00.057.199 I ggml_metal_init: GPU name:   Apple M4
0.00.057.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.201 I ggml_metal_init: simdgroup reduction   = true
0.00.057.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.203 I ggml_metal_init: has bfloat            = true
0.00.057.203 I ggml_metal_init: use bfloat            = true
0.00.057.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.688 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.693 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.713 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.833 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.835 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.835 I llama_new_context_with_model: graph nodes  = 967
0.00.086.836 I llama_new_context_with_model: graph splits = 2
0.00.086.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.778 I main: llama threadpool init, n_threads = 4
0.00.719.821 I 
0.00.719.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.860 I 
0.00.720.103 I sampler seed: 1234
0.00.720.108 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.151 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.555.403 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.555.403 I llama_perf_context_print:        load time =     709.06 ms
0.01.555.404 I llama_perf_context_print: prompt eval time =      42.30 ms /     7 tokens (    6.04 ms per token,   165.47 tokens per second)
0.01.555.405 I llama_perf_context_print:        eval time =     789.89 ms /    63 runs   (   12.54 ms per token,    79.76 tokens per second)
0.01.555.408 I llama_perf_context_print:       total time =     835.63 ms /    70 tokens
0.01.555.596 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.108s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.673 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.264 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.264 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.265 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.265 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.266 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.266 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.266 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.267 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.267 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.268 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.270 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.270 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.090 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.223 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.224 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.224 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.225 I llama_model_loader: - type  f32:  194 tensors
0.00.023.225 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.225 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.129 I llm_load_vocab: special tokens cache size = 25
0.00.050.246 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.252 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.252 I llm_load_print_meta: arch             = gptneox
0.00.050.253 I llm_load_print_meta: vocab type       = BPE
0.00.050.253 I llm_load_print_meta: n_vocab          = 50304
0.00.050.253 I llm_load_print_meta: n_merges         = 50009
0.00.050.253 I llm_load_print_meta: vocab_only       = 0
0.00.050.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.254 I llm_load_print_meta: n_embd           = 2048
0.00.050.254 I llm_load_print_meta: n_layer          = 24
0.00.050.264 I llm_load_print_meta: n_head           = 16
0.00.050.265 I llm_load_print_meta: n_head_kv        = 16
0.00.050.265 I llm_load_print_meta: n_rot            = 32
0.00.050.265 I llm_load_print_meta: n_swa            = 0
0.00.050.268 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.268 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.269 I llm_load_print_meta: n_gqa            = 1
0.00.050.270 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.270 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.271 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.271 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.272 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.272 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.272 I llm_load_print_meta: n_ff             = 8192
0.00.050.272 I llm_load_print_meta: n_expert         = 0
0.00.050.273 I llm_load_print_meta: n_expert_used    = 0
0.00.050.273 I llm_load_print_meta: causal attn      = 1
0.00.050.273 I llm_load_print_meta: pooling type     = 0
0.00.050.273 I llm_load_print_meta: rope type        = 2
0.00.050.273 I llm_load_print_meta: rope scaling     = linear
0.00.050.273 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.274 I llm_load_print_meta: freq_scale_train = 1
0.00.050.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.274 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.274 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.274 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.274 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.274 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.275 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.279 I llm_load_print_meta: model type       = 1.4B
0.00.050.280 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.280 I llm_load_print_meta: model params     = 1.41 B
0.00.050.280 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.280 I llm_load_print_meta: general.name     = 1.4B
0.00.050.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: LF token         = 128 ''
0.00.050.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.282 I llm_load_print_meta: max token length = 1024
0.00.052.129 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.129 I llm_load_tensors: offloading output layer to GPU
0.00.052.129 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.135 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.136 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.084 I llama_new_context_with_model: n_ctx         = 128
0.00.053.084 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.085 I llama_new_context_with_model: n_batch       = 128
0.00.053.085 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.085 I llama_new_context_with_model: flash_attn    = 0
0.00.053.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.086 I llama_new_context_with_model: freq_scale    = 1
0.00.053.086 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.087 I ggml_metal_init: allocating
0.00.053.093 I ggml_metal_init: found device: Apple M4
0.00.053.095 I ggml_metal_init: picking default device: Apple M4
0.00.053.700 I ggml_metal_init: using embedded metal library
0.00.056.060 I ggml_metal_init: GPU name:   Apple M4
0.00.056.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.062 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.062 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.062 I ggml_metal_init: simdgroup reduction   = true
0.00.056.063 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.063 I ggml_metal_init: has bfloat            = true
0.00.056.063 I ggml_metal_init: use bfloat            = true
0.00.056.063 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.064 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.875 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.879 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.894 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.818 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.819 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.820 I llama_new_context_with_model: graph nodes  = 967
0.00.067.820 I llama_new_context_with_model: graph splits = 2
0.00.067.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.720 I 
0.00.659.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.765 I perplexity: tokenizing the input ..
0.00.667.006 I perplexity: tokenization took 7.24 ms
0.00.667.021 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.077 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.802.456 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.802.470 I llama_perf_context_print:        load time =     651.04 ms
0.00.802.472 I llama_perf_context_print: prompt eval time =     133.83 ms /   128 tokens (    1.05 ms per token,   956.43 tokens per second)
0.00.802.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.474 I llama_perf_context_print:       total time =     142.75 ms /   129 tokens
0.00.802.785 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.113s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.427 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.949 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.888 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.888 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.889 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.889 I llama_model_loader: - type  f32:  194 tensors
0.00.024.890 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.890 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.890 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.762 I llm_load_vocab: special tokens cache size = 25
0.00.051.865 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.868 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.869 I llm_load_print_meta: arch             = gptneox
0.00.051.869 I llm_load_print_meta: vocab type       = BPE
0.00.051.869 I llm_load_print_meta: n_vocab          = 50304
0.00.051.869 I llm_load_print_meta: n_merges         = 50009
0.00.051.870 I llm_load_print_meta: vocab_only       = 0
0.00.051.870 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.870 I llm_load_print_meta: n_embd           = 2048
0.00.051.870 I llm_load_print_meta: n_layer          = 24
0.00.051.880 I llm_load_print_meta: n_head           = 16
0.00.051.880 I llm_load_print_meta: n_head_kv        = 16
0.00.051.880 I llm_load_print_meta: n_rot            = 32
0.00.051.881 I llm_load_print_meta: n_swa            = 0
0.00.051.881 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.881 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.882 I llm_load_print_meta: n_gqa            = 1
0.00.051.882 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.883 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.884 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.884 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.884 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.884 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.885 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.885 I llm_load_print_meta: n_ff             = 8192
0.00.051.885 I llm_load_print_meta: n_expert         = 0
0.00.051.886 I llm_load_print_meta: n_expert_used    = 0
0.00.051.886 I llm_load_print_meta: causal attn      = 1
0.00.051.886 I llm_load_print_meta: pooling type     = 0
0.00.051.886 I llm_load_print_meta: rope type        = 2
0.00.051.886 I llm_load_print_meta: rope scaling     = linear
0.00.051.887 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.887 I llm_load_print_meta: freq_scale_train = 1
0.00.051.887 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.887 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.888 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.888 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.890 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.890 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.895 I llm_load_print_meta: model type       = 1.4B
0.00.051.895 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.896 I llm_load_print_meta: model params     = 1.41 B
0.00.051.896 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.896 I llm_load_print_meta: general.name     = 1.4B
0.00.051.897 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.897 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.897 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.898 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.898 I llm_load_print_meta: LF token         = 128 ''
0.00.051.899 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.899 I llm_load_print_meta: max token length = 1024
0.00.053.574 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.574 I llm_load_tensors: offloading output layer to GPU
0.00.053.574 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.579 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.580 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.466 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.466 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.466 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.467 I llama_new_context_with_model: n_batch       = 2048
0.00.054.467 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.467 I llama_new_context_with_model: flash_attn    = 0
0.00.054.468 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.468 I llama_new_context_with_model: freq_scale    = 1
0.00.054.468 I ggml_metal_init: allocating
0.00.054.474 I ggml_metal_init: found device: Apple M4
0.00.054.476 I ggml_metal_init: picking default device: Apple M4
0.00.055.030 I ggml_metal_init: using embedded metal library
0.00.057.517 I ggml_metal_init: GPU name:   Apple M4
0.00.057.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.519 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.519 I ggml_metal_init: simdgroup reduction   = true
0.00.057.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.520 I ggml_metal_init: has bfloat            = true
0.00.057.520 I ggml_metal_init: use bfloat            = true
0.00.057.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.428 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.436 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.454 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.524 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.526 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.526 I llama_new_context_with_model: graph nodes  = 967
0.00.087.526 I llama_new_context_with_model: graph splits = 2
0.00.087.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.033 I main: llama threadpool init, n_threads = 4
0.00.443.082 I 
0.00.443.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.443.112 I 
0.00.443.344 I sampler seed: 1234
0.00.443.350 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.394 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.394 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.118.121 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.118.124 I llama_perf_context_print:        load time =     432.60 ms
0.01.118.126 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.12 ms per token,   195.50 tokens per second)
0.01.118.127 I llama_perf_context_print:        eval time =     636.39 ms /    63 runs   (   10.10 ms per token,    99.00 tokens per second)
0.01.118.127 I llama_perf_context_print:       total time =     675.09 ms /    70 tokens
0.01.118.363 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.110s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.835 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.384 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.386 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.386 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.386 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.387 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.387 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.388 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.388 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.389 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.394 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.394 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.958 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.022.959 I llama_model_loader: - type  f32:  194 tensors
0.00.022.959 I llama_model_loader: - type q2_K:   49 tensors
0.00.022.959 I llama_model_loader: - type q3_K:   48 tensors
0.00.022.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.646 I llm_load_vocab: special tokens cache size = 25
0.00.049.526 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.529 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.529 I llm_load_print_meta: arch             = gptneox
0.00.049.530 I llm_load_print_meta: vocab type       = BPE
0.00.049.530 I llm_load_print_meta: n_vocab          = 50304
0.00.049.530 I llm_load_print_meta: n_merges         = 50009
0.00.049.530 I llm_load_print_meta: vocab_only       = 0
0.00.049.530 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.531 I llm_load_print_meta: n_embd           = 2048
0.00.049.531 I llm_load_print_meta: n_layer          = 24
0.00.049.546 I llm_load_print_meta: n_head           = 16
0.00.049.546 I llm_load_print_meta: n_head_kv        = 16
0.00.049.547 I llm_load_print_meta: n_rot            = 32
0.00.049.547 I llm_load_print_meta: n_swa            = 0
0.00.049.547 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.547 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.550 I llm_load_print_meta: n_gqa            = 1
0.00.049.551 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.551 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.552 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.553 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.553 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.553 I llm_load_print_meta: n_ff             = 8192
0.00.049.553 I llm_load_print_meta: n_expert         = 0
0.00.049.553 I llm_load_print_meta: n_expert_used    = 0
0.00.049.554 I llm_load_print_meta: causal attn      = 1
0.00.049.554 I llm_load_print_meta: pooling type     = 0
0.00.049.554 I llm_load_print_meta: rope type        = 2
0.00.049.554 I llm_load_print_meta: rope scaling     = linear
0.00.049.554 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.555 I llm_load_print_meta: freq_scale_train = 1
0.00.049.555 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.555 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.555 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.555 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.555 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.555 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.557 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.566 I llm_load_print_meta: model type       = 1.4B
0.00.049.567 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.567 I llm_load_print_meta: model params     = 1.41 B
0.00.049.567 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.568 I llm_load_print_meta: general.name     = 1.4B
0.00.049.568 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.568 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.568 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.568 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.569 I llm_load_print_meta: LF token         = 128 ''
0.00.049.569 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.569 I llm_load_print_meta: max token length = 1024
0.00.051.469 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.469 I llm_load_tensors: offloading output layer to GPU
0.00.051.470 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.480 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.481 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.390 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.390 I llama_new_context_with_model: n_ctx         = 128
0.00.052.391 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.391 I llama_new_context_with_model: n_batch       = 128
0.00.052.391 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.391 I llama_new_context_with_model: flash_attn    = 0
0.00.052.391 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.392 I llama_new_context_with_model: freq_scale    = 1
0.00.052.392 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.392 I ggml_metal_init: allocating
0.00.052.396 I ggml_metal_init: found device: Apple M4
0.00.052.398 I ggml_metal_init: picking default device: Apple M4
0.00.052.997 I ggml_metal_init: using embedded metal library
0.00.055.384 I ggml_metal_init: GPU name:   Apple M4
0.00.055.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.387 I ggml_metal_init: simdgroup reduction   = true
0.00.055.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.387 I ggml_metal_init: has bfloat            = true
0.00.055.387 I ggml_metal_init: use bfloat            = true
0.00.055.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.918 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.920 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.936 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.792 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.793 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.793 I llama_new_context_with_model: graph nodes  = 967
0.00.067.794 I llama_new_context_with_model: graph splits = 2
0.00.067.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.392.952 I 
0.00.392.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.392.998 I perplexity: tokenizing the input ..
0.00.400.580 I perplexity: tokenization took 7.579 ms
0.00.400.590 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.532.329 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.533.474 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.533.488 I llama_perf_context_print:        load time =     384.11 ms
0.00.533.489 I llama_perf_context_print: prompt eval time =     131.51 ms /   128 tokens (    1.03 ms per token,   973.34 tokens per second)
0.00.533.490 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.533.491 I llama_perf_context_print:       total time =     140.54 ms /   129 tokens
0.00.534.034 I ggml_metal_free: deallocating

real	0m0.549s
user	0m0.079s
sys	0m0.074s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.489 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.013.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.963 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.966 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.571 I llama_model_loader: - type  f32:  194 tensors
0.00.022.571 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.572 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.572 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.837 I llm_load_vocab: special tokens cache size = 25
0.00.048.762 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.765 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.765 I llm_load_print_meta: arch             = gptneox
0.00.048.766 I llm_load_print_meta: vocab type       = BPE
0.00.048.766 I llm_load_print_meta: n_vocab          = 50304
0.00.048.766 I llm_load_print_meta: n_merges         = 50009
0.00.048.766 I llm_load_print_meta: vocab_only       = 0
0.00.048.766 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.767 I llm_load_print_meta: n_embd           = 2048
0.00.048.767 I llm_load_print_meta: n_layer          = 24
0.00.048.781 I llm_load_print_meta: n_head           = 16
0.00.048.782 I llm_load_print_meta: n_head_kv        = 16
0.00.048.782 I llm_load_print_meta: n_rot            = 32
0.00.048.782 I llm_load_print_meta: n_swa            = 0
0.00.048.783 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.783 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.786 I llm_load_print_meta: n_gqa            = 1
0.00.048.787 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.787 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.788 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.789 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.789 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.790 I llm_load_print_meta: n_ff             = 8192
0.00.048.791 I llm_load_print_meta: n_expert         = 0
0.00.048.791 I llm_load_print_meta: n_expert_used    = 0
0.00.048.791 I llm_load_print_meta: causal attn      = 1
0.00.048.791 I llm_load_print_meta: pooling type     = 0
0.00.048.792 I llm_load_print_meta: rope type        = 2
0.00.048.792 I llm_load_print_meta: rope scaling     = linear
0.00.048.793 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.793 I llm_load_print_meta: freq_scale_train = 1
0.00.048.794 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.794 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.794 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.794 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.794 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.795 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.795 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.805 I llm_load_print_meta: model type       = 1.4B
0.00.048.805 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.806 I llm_load_print_meta: model params     = 1.41 B
0.00.048.807 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.807 I llm_load_print_meta: general.name     = 1.4B
0.00.048.807 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.807 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.807 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.808 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.808 I llm_load_print_meta: LF token         = 128 ''
0.00.048.809 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.809 I llm_load_print_meta: max token length = 1024
0.00.050.725 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.726 I llm_load_tensors: offloading output layer to GPU
0.00.050.726 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.736 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.738 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.645 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.646 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.646 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.646 I llama_new_context_with_model: n_batch       = 2048
0.00.051.646 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.646 I llama_new_context_with_model: flash_attn    = 0
0.00.051.647 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.647 I llama_new_context_with_model: freq_scale    = 1
0.00.051.648 I ggml_metal_init: allocating
0.00.051.651 I ggml_metal_init: found device: Apple M4
0.00.051.653 I ggml_metal_init: picking default device: Apple M4
0.00.052.226 I ggml_metal_init: using embedded metal library
0.00.054.533 I ggml_metal_init: GPU name:   Apple M4
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.535 I ggml_metal_init: simdgroup reduction   = true
0.00.054.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.535 I ggml_metal_init: has bfloat            = true
0.00.054.535 I ggml_metal_init: use bfloat            = true
0.00.054.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.347 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.357 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.375 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.323 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.324 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.324 I llama_new_context_with_model: graph nodes  = 967
0.00.084.325 I llama_new_context_with_model: graph splits = 2
0.00.084.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.091 I main: llama threadpool init, n_threads = 4
0.00.548.125 I 
0.00.548.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.178 I 
0.00.548.408 I sampler seed: 1234
0.00.548.413 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.548.424 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.548.425 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.548.425 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.294.456 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.294.457 I llama_perf_context_print:        load time =     539.60 ms
0.01.294.458 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.98 tokens per second)
0.01.294.459 I llama_perf_context_print:        eval time =     702.68 ms /    63 runs   (   11.15 ms per token,    89.66 tokens per second)
0.01.294.459 I llama_perf_context_print:       total time =     746.37 ms /    70 tokens
0.01.294.659 I ggml_metal_free: deallocating

real	0m1.310s
user	0m0.108s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.502 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.013.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.871 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.558 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.550 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.195 I llama_model_loader: - type  f32:  194 tensors
0.00.022.195 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.195 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.196 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.196 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.147 I llm_load_vocab: special tokens cache size = 25
0.00.048.065 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.068 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.068 I llm_load_print_meta: arch             = gptneox
0.00.048.068 I llm_load_print_meta: vocab type       = BPE
0.00.048.068 I llm_load_print_meta: n_vocab          = 50304
0.00.048.069 I llm_load_print_meta: n_merges         = 50009
0.00.048.069 I llm_load_print_meta: vocab_only       = 0
0.00.048.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.069 I llm_load_print_meta: n_embd           = 2048
0.00.048.069 I llm_load_print_meta: n_layer          = 24
0.00.048.084 I llm_load_print_meta: n_head           = 16
0.00.048.085 I llm_load_print_meta: n_head_kv        = 16
0.00.048.085 I llm_load_print_meta: n_rot            = 32
0.00.048.086 I llm_load_print_meta: n_swa            = 0
0.00.048.086 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.087 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.089 I llm_load_print_meta: n_gqa            = 1
0.00.048.090 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.090 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.091 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.092 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.092 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.093 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.093 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.094 I llm_load_print_meta: n_ff             = 8192
0.00.048.094 I llm_load_print_meta: n_expert         = 0
0.00.048.094 I llm_load_print_meta: n_expert_used    = 0
0.00.048.094 I llm_load_print_meta: causal attn      = 1
0.00.048.094 I llm_load_print_meta: pooling type     = 0
0.00.048.095 I llm_load_print_meta: rope type        = 2
0.00.048.095 I llm_load_print_meta: rope scaling     = linear
0.00.048.095 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.095 I llm_load_print_meta: freq_scale_train = 1
0.00.048.096 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.096 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.101 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.102 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.102 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.102 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.113 I llm_load_print_meta: model type       = 1.4B
0.00.048.114 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.114 I llm_load_print_meta: model params     = 1.41 B
0.00.048.115 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.115 I llm_load_print_meta: general.name     = 1.4B
0.00.048.115 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.115 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.115 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.115 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.116 I llm_load_print_meta: LF token         = 128 ''
0.00.048.116 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.116 I llm_load_print_meta: max token length = 1024
0.00.050.011 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.011 I llm_load_tensors: offloading output layer to GPU
0.00.050.011 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.022 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.023 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.050.901 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.901 I llama_new_context_with_model: n_ctx         = 128
0.00.050.901 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.902 I llama_new_context_with_model: n_batch       = 128
0.00.050.902 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.902 I llama_new_context_with_model: flash_attn    = 0
0.00.050.902 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.903 I llama_new_context_with_model: freq_scale    = 1
0.00.050.903 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.903 I ggml_metal_init: allocating
0.00.050.906 I ggml_metal_init: found device: Apple M4
0.00.050.908 I ggml_metal_init: picking default device: Apple M4
0.00.051.469 I ggml_metal_init: using embedded metal library
0.00.053.780 I ggml_metal_init: GPU name:   Apple M4
0.00.053.781 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.782 I ggml_metal_init: simdgroup reduction   = true
0.00.053.782 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.782 I ggml_metal_init: has bfloat            = true
0.00.053.782 I ggml_metal_init: use bfloat            = true
0.00.053.783 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.784 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.389 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.394 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.410 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.309 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.310 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.311 I llama_new_context_with_model: graph nodes  = 967
0.00.065.311 I llama_new_context_with_model: graph splits = 2
0.00.065.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.321 I 
0.00.475.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.401 I perplexity: tokenizing the input ..
0.00.482.956 I perplexity: tokenization took 7.554 ms
0.00.482.966 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.420 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.659 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.674 I llama_perf_context_print:        load time =     466.81 ms
0.00.616.675 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.08 tokens per second)
0.00.616.676 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.676 I llama_perf_context_print:       total time =     141.36 ms /   129 tokens
0.00.617.240 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.076s
sys	0m0.085s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.028 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.450 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.452 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.453 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.453 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.454 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.196 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.199 I llama_model_loader: - type  f32:  194 tensors
0.00.023.200 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.200 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.200 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.438 I llm_load_vocab: special tokens cache size = 25
0.00.049.380 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.382 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.383 I llm_load_print_meta: arch             = gptneox
0.00.049.383 I llm_load_print_meta: vocab type       = BPE
0.00.049.383 I llm_load_print_meta: n_vocab          = 50304
0.00.049.383 I llm_load_print_meta: n_merges         = 50009
0.00.049.383 I llm_load_print_meta: vocab_only       = 0
0.00.049.384 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.384 I llm_load_print_meta: n_embd           = 2048
0.00.049.384 I llm_load_print_meta: n_layer          = 24
0.00.049.398 I llm_load_print_meta: n_head           = 16
0.00.049.401 I llm_load_print_meta: n_head_kv        = 16
0.00.049.402 I llm_load_print_meta: n_rot            = 32
0.00.049.402 I llm_load_print_meta: n_swa            = 0
0.00.049.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.404 I llm_load_print_meta: n_gqa            = 1
0.00.049.404 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.406 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.406 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.407 I llm_load_print_meta: n_ff             = 8192
0.00.049.407 I llm_load_print_meta: n_expert         = 0
0.00.049.407 I llm_load_print_meta: n_expert_used    = 0
0.00.049.407 I llm_load_print_meta: causal attn      = 1
0.00.049.408 I llm_load_print_meta: pooling type     = 0
0.00.049.408 I llm_load_print_meta: rope type        = 2
0.00.049.409 I llm_load_print_meta: rope scaling     = linear
0.00.049.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.409 I llm_load_print_meta: freq_scale_train = 1
0.00.049.409 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.410 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.420 I llm_load_print_meta: model type       = 1.4B
0.00.049.420 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.420 I llm_load_print_meta: model params     = 1.41 B
0.00.049.421 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.421 I llm_load_print_meta: general.name     = 1.4B
0.00.049.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.421 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.421 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.422 I llm_load_print_meta: LF token         = 128 ''
0.00.049.422 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.422 I llm_load_print_meta: max token length = 1024
0.00.051.363 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.363 I llm_load_tensors: offloading output layer to GPU
0.00.051.364 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.374 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.375 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.285 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.285 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.285 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.286 I llama_new_context_with_model: n_batch       = 2048
0.00.052.286 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.286 I llama_new_context_with_model: flash_attn    = 0
0.00.052.287 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.287 I llama_new_context_with_model: freq_scale    = 1
0.00.052.287 I ggml_metal_init: allocating
0.00.052.294 I ggml_metal_init: found device: Apple M4
0.00.052.296 I ggml_metal_init: picking default device: Apple M4
0.00.052.891 I ggml_metal_init: using embedded metal library
0.00.055.239 I ggml_metal_init: GPU name:   Apple M4
0.00.055.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.242 I ggml_metal_init: simdgroup reduction   = true
0.00.055.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.242 I ggml_metal_init: has bfloat            = true
0.00.055.242 I ggml_metal_init: use bfloat            = true
0.00.055.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.799 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.823 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.807 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.808 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.808 I llama_new_context_with_model: graph nodes  = 967
0.00.084.809 I llama_new_context_with_model: graph splits = 2
0.00.084.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.626 I main: llama threadpool init, n_threads = 4
0.00.607.668 I 
0.00.607.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.702 I 
0.00.607.948 I sampler seed: 1234
0.00.607.952 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.607.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.607.994 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.607.994 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.372.583 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.372.584 I llama_perf_context_print:        load time =     598.59 ms
0.01.372.584 I llama_perf_context_print: prompt eval time =      52.67 ms /     7 tokens (    7.52 ms per token,   132.91 tokens per second)
0.01.372.585 I llama_perf_context_print:        eval time =     708.75 ms /    63 runs   (   11.25 ms per token,    88.89 tokens per second)
0.01.372.585 I llama_perf_context_print:       total time =     764.96 ms /    70 tokens
0.01.372.819 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.108s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.142 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.665 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.666 I llama_model_loader: - type  f32:  194 tensors
0.00.022.667 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.667 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.667 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.620 I llm_load_vocab: special tokens cache size = 25
0.00.048.604 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.607 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.607 I llm_load_print_meta: arch             = gptneox
0.00.048.607 I llm_load_print_meta: vocab type       = BPE
0.00.048.608 I llm_load_print_meta: n_vocab          = 50304
0.00.048.608 I llm_load_print_meta: n_merges         = 50009
0.00.048.608 I llm_load_print_meta: vocab_only       = 0
0.00.048.608 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.608 I llm_load_print_meta: n_embd           = 2048
0.00.048.608 I llm_load_print_meta: n_layer          = 24
0.00.048.623 I llm_load_print_meta: n_head           = 16
0.00.048.623 I llm_load_print_meta: n_head_kv        = 16
0.00.048.624 I llm_load_print_meta: n_rot            = 32
0.00.048.624 I llm_load_print_meta: n_swa            = 0
0.00.048.624 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.625 I llm_load_print_meta: n_gqa            = 1
0.00.048.626 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.627 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.627 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.628 I llm_load_print_meta: n_ff             = 8192
0.00.048.628 I llm_load_print_meta: n_expert         = 0
0.00.048.629 I llm_load_print_meta: n_expert_used    = 0
0.00.048.629 I llm_load_print_meta: causal attn      = 1
0.00.048.629 I llm_load_print_meta: pooling type     = 0
0.00.048.629 I llm_load_print_meta: rope type        = 2
0.00.048.629 I llm_load_print_meta: rope scaling     = linear
0.00.048.632 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.632 I llm_load_print_meta: freq_scale_train = 1
0.00.048.632 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.633 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.633 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.633 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.633 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.633 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.633 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.643 I llm_load_print_meta: model type       = 1.4B
0.00.048.643 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.643 I llm_load_print_meta: model params     = 1.41 B
0.00.048.644 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.644 I llm_load_print_meta: general.name     = 1.4B
0.00.048.644 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.644 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.644 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.645 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.646 I llm_load_print_meta: LF token         = 128 ''
0.00.048.646 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.646 I llm_load_print_meta: max token length = 1024
0.00.050.609 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.609 I llm_load_tensors: offloading output layer to GPU
0.00.050.610 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.620 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.621 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.549 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.549 I llama_new_context_with_model: n_ctx         = 128
0.00.051.550 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.550 I llama_new_context_with_model: n_batch       = 128
0.00.051.550 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.550 I llama_new_context_with_model: flash_attn    = 0
0.00.051.550 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.551 I llama_new_context_with_model: freq_scale    = 1
0.00.051.551 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.551 I ggml_metal_init: allocating
0.00.051.554 I ggml_metal_init: found device: Apple M4
0.00.051.556 I ggml_metal_init: picking default device: Apple M4
0.00.052.112 I ggml_metal_init: using embedded metal library
0.00.054.414 I ggml_metal_init: GPU name:   Apple M4
0.00.054.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.416 I ggml_metal_init: simdgroup reduction   = true
0.00.054.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.416 I ggml_metal_init: has bfloat            = true
0.00.054.416 I ggml_metal_init: use bfloat            = true
0.00.054.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.046 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.049 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.062 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.997 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.998 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.999 I llama_new_context_with_model: graph nodes  = 967
0.00.065.999 I llama_new_context_with_model: graph splits = 2
0.00.066.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.178 I 
0.00.558.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.221 I perplexity: tokenizing the input ..
0.00.565.691 I perplexity: tokenization took 7.469 ms
0.00.565.703 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.451 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.635 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.650 I llama_perf_context_print:        load time =     549.45 ms
0.00.700.651 I llama_perf_context_print: prompt eval time =     133.52 ms /   128 tokens (    1.04 ms per token,   958.63 tokens per second)
0.00.700.652 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.652 I llama_perf_context_print:       total time =     142.47 ms /   129 tokens
0.00.700.976 I ggml_metal_free: deallocating

real	0m0.713s
user	0m0.077s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.908 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.316 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.317 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.323 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.369 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.378 I llama_model_loader: - type  f32:  194 tensors
0.00.025.378 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.378 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.472 I llm_load_vocab: special tokens cache size = 25
0.00.052.283 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.286 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.286 I llm_load_print_meta: arch             = gptneox
0.00.052.287 I llm_load_print_meta: vocab type       = BPE
0.00.052.287 I llm_load_print_meta: n_vocab          = 50304
0.00.052.287 I llm_load_print_meta: n_merges         = 50009
0.00.052.287 I llm_load_print_meta: vocab_only       = 0
0.00.052.288 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.288 I llm_load_print_meta: n_embd           = 2048
0.00.052.288 I llm_load_print_meta: n_layer          = 24
0.00.052.302 I llm_load_print_meta: n_head           = 16
0.00.052.303 I llm_load_print_meta: n_head_kv        = 16
0.00.052.303 I llm_load_print_meta: n_rot            = 32
0.00.052.304 I llm_load_print_meta: n_swa            = 0
0.00.052.304 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.304 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.305 I llm_load_print_meta: n_gqa            = 1
0.00.052.306 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.306 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.307 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.307 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.308 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.308 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.308 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.308 I llm_load_print_meta: n_ff             = 8192
0.00.052.309 I llm_load_print_meta: n_expert         = 0
0.00.052.309 I llm_load_print_meta: n_expert_used    = 0
0.00.052.309 I llm_load_print_meta: causal attn      = 1
0.00.052.309 I llm_load_print_meta: pooling type     = 0
0.00.052.311 I llm_load_print_meta: rope type        = 2
0.00.052.311 I llm_load_print_meta: rope scaling     = linear
0.00.052.311 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.312 I llm_load_print_meta: freq_scale_train = 1
0.00.052.312 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.313 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.313 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.313 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.313 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.313 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.314 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.325 I llm_load_print_meta: model type       = 1.4B
0.00.052.325 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.325 I llm_load_print_meta: model params     = 1.41 B
0.00.052.327 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.327 I llm_load_print_meta: general.name     = 1.4B
0.00.052.327 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.327 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.328 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.328 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.328 I llm_load_print_meta: LF token         = 128 ''
0.00.052.329 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.329 I llm_load_print_meta: max token length = 1024
0.00.054.419 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.419 I llm_load_tensors: offloading output layer to GPU
0.00.054.419 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.430 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.431 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.366 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.367 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.367 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.367 I llama_new_context_with_model: n_batch       = 2048
0.00.055.367 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.367 I llama_new_context_with_model: flash_attn    = 0
0.00.055.368 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.368 I llama_new_context_with_model: freq_scale    = 1
0.00.055.369 I ggml_metal_init: allocating
0.00.055.375 I ggml_metal_init: found device: Apple M4
0.00.055.378 I ggml_metal_init: picking default device: Apple M4
0.00.055.997 I ggml_metal_init: using embedded metal library
0.00.058.345 I ggml_metal_init: GPU name:   Apple M4
0.00.058.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.347 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.347 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.347 I ggml_metal_init: simdgroup reduction   = true
0.00.058.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.347 I ggml_metal_init: has bfloat            = true
0.00.058.348 I ggml_metal_init: use bfloat            = true
0.00.058.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.466 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.477 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.494 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.547 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.549 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.549 I llama_new_context_with_model: graph nodes  = 967
0.00.089.550 I llama_new_context_with_model: graph splits = 2
0.00.089.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.491 I main: llama threadpool init, n_threads = 4
0.00.690.527 I 
0.00.690.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.581 I 
0.00.690.802 I sampler seed: 1234
0.00.690.808 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.819 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.819 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.819 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.539.821 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.539.821 I llama_perf_context_print:        load time =     681.58 ms
0.01.539.822 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.539.823 I llama_perf_context_print:        eval time =     794.43 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.539.823 I llama_perf_context_print:       total time =     849.33 ms /    70 tokens
0.01.540.015 I ggml_metal_free: deallocating

real	0m1.555s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.268 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.192 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.065 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.067 I llama_model_loader: - type  f32:  194 tensors
0.00.024.067 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.067 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.837 I llm_load_vocab: special tokens cache size = 25
0.00.050.810 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.813 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.813 I llm_load_print_meta: arch             = gptneox
0.00.050.813 I llm_load_print_meta: vocab type       = BPE
0.00.050.814 I llm_load_print_meta: n_vocab          = 50304
0.00.050.814 I llm_load_print_meta: n_merges         = 50009
0.00.050.814 I llm_load_print_meta: vocab_only       = 0
0.00.050.814 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.814 I llm_load_print_meta: n_embd           = 2048
0.00.050.815 I llm_load_print_meta: n_layer          = 24
0.00.050.829 I llm_load_print_meta: n_head           = 16
0.00.050.830 I llm_load_print_meta: n_head_kv        = 16
0.00.050.830 I llm_load_print_meta: n_rot            = 32
0.00.050.830 I llm_load_print_meta: n_swa            = 0
0.00.050.830 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.830 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.831 I llm_load_print_meta: n_gqa            = 1
0.00.050.832 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.832 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.833 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.833 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.834 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.835 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.836 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.837 I llm_load_print_meta: n_ff             = 8192
0.00.050.837 I llm_load_print_meta: n_expert         = 0
0.00.050.837 I llm_load_print_meta: n_expert_used    = 0
0.00.050.837 I llm_load_print_meta: causal attn      = 1
0.00.050.837 I llm_load_print_meta: pooling type     = 0
0.00.050.837 I llm_load_print_meta: rope type        = 2
0.00.050.837 I llm_load_print_meta: rope scaling     = linear
0.00.050.838 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.838 I llm_load_print_meta: freq_scale_train = 1
0.00.050.838 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.838 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.839 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.839 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.839 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.839 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.839 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.848 I llm_load_print_meta: model type       = 1.4B
0.00.050.849 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.849 I llm_load_print_meta: model params     = 1.41 B
0.00.050.850 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.850 I llm_load_print_meta: general.name     = 1.4B
0.00.050.850 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.850 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.850 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.851 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: LF token         = 128 ''
0.00.050.852 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: max token length = 1024
0.00.052.875 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.875 I llm_load_tensors: offloading output layer to GPU
0.00.052.875 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.885 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.887 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.794 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.795 I llama_new_context_with_model: n_ctx         = 128
0.00.053.795 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.795 I llama_new_context_with_model: n_batch       = 128
0.00.053.796 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.796 I llama_new_context_with_model: flash_attn    = 0
0.00.053.796 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.796 I llama_new_context_with_model: freq_scale    = 1
0.00.053.797 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.797 I ggml_metal_init: allocating
0.00.053.800 I ggml_metal_init: found device: Apple M4
0.00.053.802 I ggml_metal_init: picking default device: Apple M4
0.00.054.361 I ggml_metal_init: using embedded metal library
0.00.056.684 I ggml_metal_init: GPU name:   Apple M4
0.00.056.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.686 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.686 I ggml_metal_init: simdgroup reduction   = true
0.00.056.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.687 I ggml_metal_init: has bfloat            = true
0.00.056.687 I ggml_metal_init: use bfloat            = true
0.00.056.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.582 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.585 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.598 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.510 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.511 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.512 I llama_new_context_with_model: graph nodes  = 967
0.00.068.512 I llama_new_context_with_model: graph splits = 2
0.00.068.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.905 I 
0.00.662.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.949 I perplexity: tokenizing the input ..
0.00.670.707 I perplexity: tokenization took 7.757 ms
0.00.670.719 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.115 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.273 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.812.288 I llama_perf_context_print:        load time =     653.63 ms
0.00.812.289 I llama_perf_context_print: prompt eval time =     140.17 ms /   128 tokens (    1.10 ms per token,   913.18 tokens per second)
0.00.812.290 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.291 I llama_perf_context_print:       total time =     149.38 ms /   129 tokens
0.00.812.716 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.079s
sys	0m0.130s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.009.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.984 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.985 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.985 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.986 I llama_model_loader: - type  f32:  194 tensors
0.00.024.986 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.143 I llm_load_vocab: special tokens cache size = 25
0.00.051.231 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.234 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.234 I llm_load_print_meta: arch             = gptneox
0.00.051.235 I llm_load_print_meta: vocab type       = BPE
0.00.051.235 I llm_load_print_meta: n_vocab          = 50304
0.00.051.235 I llm_load_print_meta: n_merges         = 50009
0.00.051.235 I llm_load_print_meta: vocab_only       = 0
0.00.051.236 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.236 I llm_load_print_meta: n_embd           = 2048
0.00.051.236 I llm_load_print_meta: n_layer          = 24
0.00.051.250 I llm_load_print_meta: n_head           = 16
0.00.051.251 I llm_load_print_meta: n_head_kv        = 16
0.00.051.251 I llm_load_print_meta: n_rot            = 32
0.00.051.253 I llm_load_print_meta: n_swa            = 0
0.00.051.253 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.253 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.254 I llm_load_print_meta: n_gqa            = 1
0.00.051.255 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.256 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.256 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.257 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.257 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.257 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.257 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.258 I llm_load_print_meta: n_ff             = 8192
0.00.051.259 I llm_load_print_meta: n_expert         = 0
0.00.051.259 I llm_load_print_meta: n_expert_used    = 0
0.00.051.259 I llm_load_print_meta: causal attn      = 1
0.00.051.260 I llm_load_print_meta: pooling type     = 0
0.00.051.261 I llm_load_print_meta: rope type        = 2
0.00.051.261 I llm_load_print_meta: rope scaling     = linear
0.00.051.262 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.262 I llm_load_print_meta: freq_scale_train = 1
0.00.051.262 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.262 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.263 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.263 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.263 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.272 I llm_load_print_meta: model type       = 1.4B
0.00.051.272 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.273 I llm_load_print_meta: model params     = 1.41 B
0.00.051.273 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.273 I llm_load_print_meta: general.name     = 1.4B
0.00.051.274 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.275 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.275 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: LF token         = 128 ''
0.00.051.276 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.276 I llm_load_print_meta: max token length = 1024
0.00.053.335 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.335 I llm_load_tensors: offloading output layer to GPU
0.00.053.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.346 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.347 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.233 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.234 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.234 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.234 I llama_new_context_with_model: n_batch       = 2048
0.00.054.234 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.235 I llama_new_context_with_model: flash_attn    = 0
0.00.054.235 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.235 I llama_new_context_with_model: freq_scale    = 1
0.00.054.236 I ggml_metal_init: allocating
0.00.054.242 I ggml_metal_init: found device: Apple M4
0.00.054.244 I ggml_metal_init: picking default device: Apple M4
0.00.054.848 I ggml_metal_init: using embedded metal library
0.00.057.239 I ggml_metal_init: GPU name:   Apple M4
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.241 I ggml_metal_init: simdgroup reduction   = true
0.00.057.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.243 I ggml_metal_init: has bfloat            = true
0.00.057.243 I ggml_metal_init: use bfloat            = true
0.00.057.244 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.432 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.440 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.465 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.467 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.469 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.469 I llama_new_context_with_model: graph nodes  = 967
0.00.087.469 I llama_new_context_with_model: graph splits = 2
0.00.087.479 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.144 I main: llama threadpool init, n_threads = 4
0.00.750.194 I 
0.00.750.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.230 I 
0.00.750.476 I sampler seed: 1234
0.00.750.482 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.493 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.493 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.628.761 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.628.761 I llama_perf_context_print:        load time =     740.71 ms
0.01.628.762 I llama_perf_context_print: prompt eval time =      54.12 ms /     7 tokens (    7.73 ms per token,   129.33 tokens per second)
0.01.628.764 I llama_perf_context_print:        eval time =     821.50 ms /    63 runs   (   13.04 ms per token,    76.69 tokens per second)
0.01.628.764 I llama_perf_context_print:       total time =     878.62 ms /    70 tokens
0.01.629.011 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.107s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4325 (a76c56fa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.831 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.608 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.624 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.624 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.459 I llama_model_loader: - type  f32:  194 tensors
0.00.023.459 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.265 I llm_load_vocab: special tokens cache size = 25
0.00.050.256 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.259 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.259 I llm_load_print_meta: arch             = gptneox
0.00.050.260 I llm_load_print_meta: vocab type       = BPE
0.00.050.260 I llm_load_print_meta: n_vocab          = 50304
0.00.050.260 I llm_load_print_meta: n_merges         = 50009
0.00.050.260 I llm_load_print_meta: vocab_only       = 0
0.00.050.260 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.261 I llm_load_print_meta: n_embd           = 2048
0.00.050.261 I llm_load_print_meta: n_layer          = 24
0.00.050.275 I llm_load_print_meta: n_head           = 16
0.00.050.276 I llm_load_print_meta: n_head_kv        = 16
0.00.050.276 I llm_load_print_meta: n_rot            = 32
0.00.050.277 I llm_load_print_meta: n_swa            = 0
0.00.050.279 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.279 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.280 I llm_load_print_meta: n_gqa            = 1
0.00.050.281 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.282 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.282 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.283 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.283 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.283 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.283 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.284 I llm_load_print_meta: n_ff             = 8192
0.00.050.284 I llm_load_print_meta: n_expert         = 0
0.00.050.284 I llm_load_print_meta: n_expert_used    = 0
0.00.050.284 I llm_load_print_meta: causal attn      = 1
0.00.050.284 I llm_load_print_meta: pooling type     = 0
0.00.050.284 I llm_load_print_meta: rope type        = 2
0.00.050.285 I llm_load_print_meta: rope scaling     = linear
0.00.050.285 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.285 I llm_load_print_meta: freq_scale_train = 1
0.00.050.286 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.286 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.286 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.286 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.286 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.287 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.287 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.296 I llm_load_print_meta: model type       = 1.4B
0.00.050.296 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.297 I llm_load_print_meta: model params     = 1.41 B
0.00.050.297 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.297 I llm_load_print_meta: general.name     = 1.4B
0.00.050.298 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.298 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.298 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.298 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.298 I llm_load_print_meta: LF token         = 128 ''
0.00.050.299 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: max token length = 1024
0.00.052.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.250 I llm_load_tensors: offloading output layer to GPU
0.00.052.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.261 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.262 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.149 I llama_new_context_with_model: n_ctx         = 128
0.00.053.149 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.150 I llama_new_context_with_model: n_batch       = 128
0.00.053.150 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.150 I llama_new_context_with_model: flash_attn    = 0
0.00.053.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.151 I llama_new_context_with_model: freq_scale    = 1
0.00.053.151 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.151 I ggml_metal_init: allocating
0.00.053.157 I ggml_metal_init: found device: Apple M4
0.00.053.159 I ggml_metal_init: picking default device: Apple M4
0.00.053.704 I ggml_metal_init: using embedded metal library
0.00.056.018 I ggml_metal_init: GPU name:   Apple M4
0.00.056.019 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.020 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.020 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.020 I ggml_metal_init: simdgroup reduction   = true
0.00.056.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.021 I ggml_metal_init: has bfloat            = true
0.00.056.021 I ggml_metal_init: use bfloat            = true
0.00.056.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.534 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.550 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.403 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.404 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.404 I llama_new_context_with_model: graph nodes  = 967
0.00.067.405 I llama_new_context_with_model: graph splits = 2
0.00.067.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.323.713 I 
0.00.323.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.323.761 I perplexity: tokenizing the input ..
0.00.331.535 I perplexity: tokenization took 7.771 ms
0.00.331.546 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.472.007 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.473.275 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.473.298 I llama_perf_context_print:        load time =     314.88 ms
0.00.473.299 I llama_perf_context_print: prompt eval time =     140.16 ms /   128 tokens (    1.09 ms per token,   913.27 tokens per second)
0.00.473.300 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.473.300 I llama_perf_context_print:       total time =     149.59 ms /   129 tokens
0.00.473.831 I ggml_metal_free: deallocating

real	0m0.487s
user	0m0.078s
sys	0m0.076s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4325 (a76c56fa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e0a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e4f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e51200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e51750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e5c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e5f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e60660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e64c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e65d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e67d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e68050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e68660 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.146.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e0ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e0e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e0e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e0f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e0f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e0f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e0fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e10250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e11a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e12180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e12960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e13050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e13e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e14ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e15590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e16370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e2d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e33d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e35c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e36510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e37260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e37b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e37fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e38890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e38d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e39170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e39a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e39ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e3a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e3b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e3b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e3cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e3cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e3e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e3e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e3ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e3f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e40060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e40940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e40db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e41220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e41690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e42850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e42cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e45040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e45920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e45d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e46200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e46ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e46f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e47830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e47ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e48580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e48e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e49bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e4a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e58110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e58580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e58e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e5a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e5a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e5b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e5bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e5c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e5cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e5e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e5e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e5eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e60750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e60bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e61030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e61910 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136f1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136f1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136f1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136f1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136f23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136f23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136f24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136f24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136f24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136f24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136f251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136f25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136f25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136f25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136f263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136f26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136f26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136f270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136f27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136f279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136f27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136f282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136f28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136f29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136f29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136f298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136f29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136f2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136f2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136f2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136f2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136f2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136f2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136f2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136f2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136f2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136f2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136f2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136f2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136f2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136f2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136f2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136f2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136f2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136f2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136f2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136f2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136f2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136f30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136f307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136f30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136f310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136f31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136f31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136f31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136f32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136f326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136f32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136f32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136f33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136f338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136f33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136f34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136f345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136f34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136f34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136f35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136f357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136f35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136f36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136f36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136f36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136f36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136f37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136f376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136f37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136f38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136f38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136f38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136f39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136f395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136f39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136f39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136f3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136f3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136f3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136f3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136f3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136f3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136f3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136f3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136f3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136f3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136f3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136f3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136f3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136f3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136f3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136f3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136f3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136f3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136f3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136f3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136f3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136f40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136f405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136f40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136f40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136f41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136f41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136f42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136f42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136f42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136f43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136f435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136f43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136f43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136f44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136f44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136f44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136f45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136f454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136f45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136f45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136f46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136f46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136f46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136f46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136f473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136f47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136f47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136f48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136f485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136f48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136f48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136f492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136f49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136f49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136f4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136f4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136f4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136f4b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136f4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136f4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136f4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136f4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136f4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136f4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136f4cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136f4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136f4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136f4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136f4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136f4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136f4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136f4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136f4f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136f4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136f4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136f500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136f50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136f509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136f50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136f51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136f516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136f51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136f51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136f52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136f528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136f52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136f53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136f53600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136f53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136f53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136f54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136f54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136f550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136f55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136f55980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136f563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136f56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136f57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136f57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136f57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136f58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136f58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136f58c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.765s
user	0m0.290s
sys	0m0.253s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4325 (a76c56fa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14300a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14300a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14300ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14300b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14300b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14300bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14300c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14300cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14300d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14300d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14300dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14300dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14300ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14300f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14300fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1430101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1430108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143010ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143011710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143011ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143012d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143013440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143013ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143014400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1430146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143014cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143015e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143016140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1430165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1430168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143017130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143017670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143017930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143017dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143018270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143018710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143018bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143019050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1430194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143019990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143019e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14301a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14301a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14301aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14301b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14301bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14301c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14301c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14301cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14301d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14301d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14301df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14301e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14301ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14301f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14301f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14301f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143020120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1430203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143020880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143020d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1430211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143021660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143021b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143021fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143022440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1430228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143022d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143023220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1430236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1430240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143024600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143024b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1430250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1430255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143025b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143026090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1430265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143026b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143027080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1430275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143027b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143028070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1430285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143028b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143029060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1430295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143029b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14302a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14302a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14302aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14302b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14302b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14302bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14301b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14302bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14302c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14302cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14302d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14302d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14302dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14302e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14302e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14302ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14302f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14302f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14302fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143030170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1430306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143030c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1430310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143031550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1430319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143031e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143032330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1430327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143032c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143033110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1430335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143033a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143034390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143034830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143034cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143035170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143035610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143035ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143035f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1430363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143036890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143036d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1430371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143037670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143037b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143037fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143038450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1430388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143038d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143039230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1430396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143039b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14303a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14303a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14303a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14303adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14303b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14303b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14303bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14303c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14303c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14303c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14303ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14303d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14303d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14303dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14303e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14303e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14303ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14303eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14303f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14303f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14303fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143040130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1430405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143040a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143040f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1430413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143041850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143041cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143042190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143042630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143042ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143042f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1430438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143043d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1430441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143044690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143044b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143045470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143045910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143045db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1430466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143046b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143047030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1430474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143047970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143047e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143048360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1430488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143048e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143049350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143049610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143049c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14304a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14304a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14304b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14304b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14304b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14304bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14304c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14304cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14304d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14304d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14304d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14304e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14304e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14304ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14304f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14304f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14304fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143050110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143050660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143051100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143051ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1430520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143052640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143052b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1430530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143053630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143053b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1430540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143054620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143054b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1430550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143055610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143055b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1430560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143056600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143056b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1430570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1430575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143057b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1430585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143058b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143059080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1430595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143059b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14305a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14305a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14305ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14305b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14305b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14305bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14305c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14305c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14305caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14305d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14305d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14305dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14305e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14305e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14305ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14305f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14305f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14305fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143060010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143060560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143060ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143060f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1430613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143061890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143061d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1430621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143062670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143062b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143062fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143063450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1430638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143063d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143064230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1430646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143064b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143065010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143065560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143065c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1430663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143066ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1430671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1430674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143067c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143067f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143068560 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135304d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1353051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135305630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135305aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135305f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135306380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1353067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135306c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1353070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135307540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1353079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1353080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135308bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135309370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135309b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13530a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13530a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13530b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13530b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13530bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13530c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13530cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13530d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13530dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13530e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13530e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13530e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13530ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13530f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13530f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13530fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13530ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1353103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135310670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135310ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135310f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1353113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135311830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135311ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135312110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135312580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1353129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135312e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1353132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135313740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135313bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135314020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135314490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135314900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135314d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1353151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135315650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135315ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135315f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1353163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135316810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135316d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135317280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1353176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135317b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135317fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135318440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1353188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135318d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135319190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135319600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135319a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135319ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13531a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13531a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13531ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13531b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13531b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13531b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13531bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13531c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13531c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13531cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13531cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13531d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13531d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13531dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13531e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13531e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13531ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13531eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13531f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13531f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13531fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x135320080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1353204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135320960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135320dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135321240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1353216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135321b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135321f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135322400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135322870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135322ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135323150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1353235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135323a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135323ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135324310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135324780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135324bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135325060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1353254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135325940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135325db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135326220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135326690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135326b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135326f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1353273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135327850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135327cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135328130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1353285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135328a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135328e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1353292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135329760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135329bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13532a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13532a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13532a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13532ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13532b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13532b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13532bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13532bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13532c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13532c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13532cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13532d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13532d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13532d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13532de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13532e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13532e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13532ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13532f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13532f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13532f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13532fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1353301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135330650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135330ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135330f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1353313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135331810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135331c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1353320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135332560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1353329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135332e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1353332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135333720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135333b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135334000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135334470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1353348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135334d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1353351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135335630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135335aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135335f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135336380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1353367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135336c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1353370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135337540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1353379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135337e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135338290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135338700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135338b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135338fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135339450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1353398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135339d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13533a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13533a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13533aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13533aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13533b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13533b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13533bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13533c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13533c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13533c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13533ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13533d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13533d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13533db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13533dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13533e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13533e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13533ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13533f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13533f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13533fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13533fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135340340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1353407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135340d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1353411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135341620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135342170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135342430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1353426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135342b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135342fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135343440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1353438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135343d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135344190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135344600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135344a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135344ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135345350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1353457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135345c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1353460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135346510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135346980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135346df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135347260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1353476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135347b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135347fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135348420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135348890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135348d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135349170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1353495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135349a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135349ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13534a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13534a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13534ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13534b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13534b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13534b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13534bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13534c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13534c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13534cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13534cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13534d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13534d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13534dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13534e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13534e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13534ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13534eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13534f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13534f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13534fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135350060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1353504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135350940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135350db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135351220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135351690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135351b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135351f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1353523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135352850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135352cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135353130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1353535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135353a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135353e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1353542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135354760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135354bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135355040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1353554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135355920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135355d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135356800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135356f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135357640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135357d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135358020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135358490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135358a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1353590a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135304ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135305150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1353055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135305a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135305ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135306310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135306780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135306bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135307060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1353074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135307940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135307f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135308810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135308f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135309770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135309e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13530a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13530ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13530b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13530bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13530c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13530ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13530d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13530d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13530df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13530e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13530e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13530ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13530f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13530f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13530fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13530fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1353102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1353105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135310a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135310e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1353112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135311760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135311bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135312040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1353124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135312920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135312d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135313200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135313670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135313ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135313f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1353143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135314830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135314ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135315110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135315580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1353159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135315e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1353162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135316740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135316bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135317020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135317490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135317900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135317d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1353181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135318650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135318ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135318f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1353193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135319810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135319c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13531a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13531a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13531a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13531ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13531b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13531b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13531bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13531c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13531c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13531c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13531cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13531d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13531d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13531daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13531df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13531e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13531e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13531ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13531f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13531f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13531f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13531fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135320290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135320700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135320b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135320fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135321450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1353218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135321d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1353221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135322610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135322a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135322ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135323360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1353237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135323c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1353240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135324520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135324990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135324e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135325270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1353256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135325b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135325fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135326430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1353268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135326d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135327180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1353275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135327a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135327ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135328340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1353287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135328c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135329090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135329500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135329970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135329de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13532a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13532a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13532ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13532afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13532b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13532b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13532bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13532c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13532c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13532ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13532ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13532d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13532d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13532dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13532e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13532e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13532e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13532edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13532f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13532f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13532fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13532ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1353303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135330860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135330cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135331140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1353315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135331a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135331e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135332300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135332770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135332be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135333050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1353334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135333930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135333da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135334210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135334680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135334af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135334f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1353353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135335840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135335cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135336120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135336590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135336a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135336e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1353372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135337750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135337bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135338030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1353384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135338910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135338d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1353391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135339660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135339ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135339f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13533a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13533a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13533ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13533b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13533b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13533b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13533be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13533c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13533c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13533cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13533d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13533d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13533d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13533dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13533e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13533e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13533eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13533ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13533f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13533f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13533fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1353400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135340550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1353409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135340e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1353412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135341a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135341e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135342300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135342770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135342be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135343050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1353434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135343930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135343da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135344210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135344680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135344af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135344f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1353453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135345840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135345cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135346120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135346590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135346a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135346e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1353472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135347750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135347bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135348030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1353484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135348910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135348d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1353491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135349660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135349ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135349f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13534a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13534a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13534ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13534b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13534b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13534b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13534be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13534c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13534c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13534cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13534d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13534d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13534d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13534dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13534e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13534e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13534eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13534ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13534f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13534f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13534fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1353500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135350550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1353509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135350e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1353512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135351710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135351b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135351ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135352460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1353528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135352d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1353531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135353620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135353a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135353f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135354370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1353547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135354c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1353550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135355530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1353559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135356200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1353568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135356fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1353576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135357b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135357fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135358420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135358890 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.927s
user	0m0.244s
sys	0m0.142s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.53 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.24 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.15 user         0.04 sys
```
