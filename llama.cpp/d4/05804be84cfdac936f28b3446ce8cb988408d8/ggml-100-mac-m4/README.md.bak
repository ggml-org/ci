### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.54 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.38 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.18 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.19 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.24 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.29 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  176.36 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.89 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.58 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.27 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 218.91 sec*proc (27 tests)

Total Test time (real) = 218.92 sec

real	3m38.959s
user	7m30.670s
sys	0m6.585s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.28 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.21 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    1.02 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.26 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.08 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.27 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.15 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.18 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.09 sec*proc (27 tests)

Total Test time (real) =  51.11 sec

real	0m51.115s
user	1m11.444s
sys	0m5.850s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.140 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.253 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.246 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.258 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.259 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.260 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.261 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.263 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.263 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.264 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.265 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.266 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.270 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.270 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.271 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.272 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.272 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.273 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.274 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.623 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.625 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.626 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.627 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.627 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.034.628 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.628 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.034.629 I llama_model_loader: - type  f32:  124 tensors
0.00.034.629 I llama_model_loader: - type  f16:   73 tensors
0.00.039.136 I llm_load_vocab: special tokens cache size = 5
0.00.041.598 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.602 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.603 I llm_load_print_meta: arch             = bert
0.00.041.603 I llm_load_print_meta: vocab type       = WPM
0.00.041.604 I llm_load_print_meta: n_vocab          = 30522
0.00.041.604 I llm_load_print_meta: n_merges         = 0
0.00.041.604 I llm_load_print_meta: vocab_only       = 0
0.00.041.604 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.605 I llm_load_print_meta: n_embd           = 384
0.00.041.605 I llm_load_print_meta: n_layer          = 12
0.00.041.608 I llm_load_print_meta: n_head           = 12
0.00.041.610 I llm_load_print_meta: n_head_kv        = 12
0.00.041.636 I llm_load_print_meta: n_rot            = 32
0.00.041.637 I llm_load_print_meta: n_swa            = 0
0.00.041.638 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.638 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.639 I llm_load_print_meta: n_gqa            = 1
0.00.041.640 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.648 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.649 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.649 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.650 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.650 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.650 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.651 I llm_load_print_meta: n_ff             = 1536
0.00.041.652 I llm_load_print_meta: n_expert         = 0
0.00.041.652 I llm_load_print_meta: n_expert_used    = 0
0.00.041.652 I llm_load_print_meta: causal attn      = 0
0.00.041.652 I llm_load_print_meta: pooling type     = 2
0.00.041.653 I llm_load_print_meta: rope type        = 2
0.00.041.653 I llm_load_print_meta: rope scaling     = linear
0.00.041.653 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.654 I llm_load_print_meta: freq_scale_train = 1
0.00.041.654 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.654 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.655 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.655 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.655 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.655 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.655 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.667 I llm_load_print_meta: model type       = 33M
0.00.041.668 I llm_load_print_meta: model ftype      = F16
0.00.041.668 I llm_load_print_meta: model params     = 33.21 M
0.00.041.669 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.670 I llm_load_print_meta: general.name     = Bge Small
0.00.041.670 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.670 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.671 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.671 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.672 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.674 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.675 I llm_load_print_meta: max token length = 21
0.00.044.154 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.044.155 I llm_load_tensors: offloading output layer to GPU
0.00.044.156 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.044.186 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.187 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.802 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.804 I llama_new_context_with_model: n_ctx         = 512
0.00.044.804 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.804 I llama_new_context_with_model: n_batch       = 2048
0.00.044.804 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.805 I llama_new_context_with_model: flash_attn    = 0
0.00.044.806 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.806 I llama_new_context_with_model: freq_scale    = 1
0.00.044.807 I ggml_metal_init: allocating
0.00.044.817 I ggml_metal_init: found device: Apple M4
0.00.044.823 I ggml_metal_init: picking default device: Apple M4
0.00.045.673 I ggml_metal_init: using embedded metal library
0.00.050.094 I ggml_metal_init: GPU name:   Apple M4
0.00.050.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.050.097 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.050.098 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.050.098 I ggml_metal_init: simdgroup reduction   = true
0.00.050.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.050.099 I ggml_metal_init: has bfloat            = true
0.00.050.099 I ggml_metal_init: use bfloat            = true
0.00.050.099 I ggml_metal_init: hasUnifiedMemory      = true
0.00.050.100 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.872 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.063.875 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.063.876 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.064.757 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.064.759 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.064.760 I llama_new_context_with_model: graph nodes  = 429
0.00.064.760 I llama_new_context_with_model: graph splits = 2
0.00.064.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.837 I 
0.00.071.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.072.573 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.077.388 I llama_perf_context_print:        load time =      49.58 ms
0.00.077.389 I llama_perf_context_print: prompt eval time =       4.64 ms /     9 tokens (    0.52 ms per token,  1937.98 tokens per second)
0.00.077.391 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.077.391 I llama_perf_context_print:       total time =       5.55 ms /    10 tokens
0.00.077.531 I ggml_metal_free: deallocating

real	0m0.270s
user	0m0.055s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.706 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.920 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.925 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.925 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.926 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.926 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.927 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.929 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.929 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.929 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.930 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.930 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.934 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.934 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.938 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.938 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.938 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.939 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.939 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.391 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.393 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.393 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.393 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.394 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.394 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.394 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.395 I llama_model_loader: - type  f32:  124 tensors
0.00.015.395 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.949 I llm_load_vocab: special tokens cache size = 5
0.00.019.356 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.358 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.358 I llm_load_print_meta: arch             = bert
0.00.019.359 I llm_load_print_meta: vocab type       = WPM
0.00.019.359 I llm_load_print_meta: n_vocab          = 30522
0.00.019.359 I llm_load_print_meta: n_merges         = 0
0.00.019.359 I llm_load_print_meta: vocab_only       = 0
0.00.019.359 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.359 I llm_load_print_meta: n_embd           = 384
0.00.019.360 I llm_load_print_meta: n_layer          = 12
0.00.019.362 I llm_load_print_meta: n_head           = 12
0.00.019.363 I llm_load_print_meta: n_head_kv        = 12
0.00.019.370 I llm_load_print_meta: n_rot            = 32
0.00.019.370 I llm_load_print_meta: n_swa            = 0
0.00.019.370 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.370 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.371 I llm_load_print_meta: n_gqa            = 1
0.00.019.371 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.372 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.373 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.375 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.375 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.375 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.375 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.376 I llm_load_print_meta: n_ff             = 1536
0.00.019.376 I llm_load_print_meta: n_expert         = 0
0.00.019.376 I llm_load_print_meta: n_expert_used    = 0
0.00.019.376 I llm_load_print_meta: causal attn      = 0
0.00.019.376 I llm_load_print_meta: pooling type     = 2
0.00.019.376 I llm_load_print_meta: rope type        = 2
0.00.019.377 I llm_load_print_meta: rope scaling     = linear
0.00.019.377 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.377 I llm_load_print_meta: freq_scale_train = 1
0.00.019.377 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.378 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.378 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.378 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.378 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.378 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.378 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.383 I llm_load_print_meta: model type       = 33M
0.00.019.384 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.384 I llm_load_print_meta: model params     = 33.21 M
0.00.019.385 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.385 I llm_load_print_meta: general.name     = Bge Small
0.00.019.385 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.385 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.385 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.386 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.386 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.386 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.386 I llm_load_print_meta: max token length = 21
0.00.020.804 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.805 I llm_load_tensors: offloading output layer to GPU
0.00.020.805 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.813 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.814 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.170 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.171 I llama_new_context_with_model: n_ctx         = 512
0.00.021.171 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.171 I llama_new_context_with_model: n_batch       = 2048
0.00.021.171 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.171 I llama_new_context_with_model: flash_attn    = 0
0.00.021.172 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.172 I llama_new_context_with_model: freq_scale    = 1
0.00.021.172 I ggml_metal_init: allocating
0.00.021.175 I ggml_metal_init: found device: Apple M4
0.00.021.178 I ggml_metal_init: picking default device: Apple M4
0.00.021.757 I ggml_metal_init: using embedded metal library
0.00.024.308 I ggml_metal_init: GPU name:   Apple M4
0.00.024.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.310 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.311 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.311 I ggml_metal_init: simdgroup reduction   = true
0.00.024.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.312 I ggml_metal_init: has bfloat            = true
0.00.024.312 I ggml_metal_init: use bfloat            = true
0.00.024.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.846 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.848 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.849 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.436 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.436 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.437 I llama_new_context_with_model: graph nodes  = 429
0.00.035.437 I llama_new_context_with_model: graph splits = 2
0.00.035.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.923 I 
0.00.039.947 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.467 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.663 I llama_perf_context_print:        load time =      30.21 ms
0.00.044.665 I llama_perf_context_print: prompt eval time =       4.08 ms /     9 tokens (    0.45 ms per token,  2208.59 tokens per second)
0.00.044.666 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.666 I llama_perf_context_print:       total time =       4.74 ms /    10 tokens
0.00.044.860 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.138 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.463 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.640 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.648 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.652 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.653 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.653 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.655 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.655 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.656 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.657 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.657 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.661 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.661 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.662 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.731 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.732 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.733 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.733 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.733 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.734 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.734 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.734 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.735 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.735 I llama_model_loader: - type  f32:   41 tensors
0.00.049.735 I llama_model_loader: - type  f16:   29 tensors
0.00.068.036 W llm_load_vocab: empty token at index 5
0.00.072.606 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.955 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.981 I llm_load_vocab: special tokens cache size = 5
0.00.331.308 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.314 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.314 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.315 I llm_load_print_meta: vocab type       = BPE
0.00.331.315 I llm_load_print_meta: n_vocab          = 61056
0.00.331.318 I llm_load_print_meta: n_merges         = 39382
0.00.331.318 I llm_load_print_meta: vocab_only       = 0
0.00.331.318 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.319 I llm_load_print_meta: n_embd           = 384
0.00.331.319 I llm_load_print_meta: n_layer          = 4
0.00.331.324 I llm_load_print_meta: n_head           = 12
0.00.331.328 I llm_load_print_meta: n_head_kv        = 12
0.00.331.353 I llm_load_print_meta: n_rot            = 32
0.00.331.354 I llm_load_print_meta: n_swa            = 0
0.00.331.354 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.354 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.355 I llm_load_print_meta: n_gqa            = 1
0.00.331.355 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.356 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.356 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.357 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.357 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.358 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.358 I llm_load_print_meta: n_ff             = 1536
0.00.331.358 I llm_load_print_meta: n_expert         = 0
0.00.331.358 I llm_load_print_meta: n_expert_used    = 0
0.00.331.358 I llm_load_print_meta: causal attn      = 0
0.00.331.359 I llm_load_print_meta: pooling type     = -1
0.00.331.359 I llm_load_print_meta: rope type        = -1
0.00.331.359 I llm_load_print_meta: rope scaling     = linear
0.00.331.359 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.360 I llm_load_print_meta: freq_scale_train = 1
0.00.331.360 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.360 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.360 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.361 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.361 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.361 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.361 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.381 I llm_load_print_meta: model type       = 33M
0.00.331.382 I llm_load_print_meta: model ftype      = F16
0.00.331.382 I llm_load_print_meta: model params     = 32.90 M
0.00.331.383 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.383 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.383 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.383 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.383 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.385 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.385 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.385 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.385 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.386 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.386 I llm_load_print_meta: max token length = 45
0.00.332.684 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.684 I llm_load_tensors: offloading output layer to GPU
0.00.332.685 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.709 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.710 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.470 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.471 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.471 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.472 I llama_new_context_with_model: n_batch       = 2048
0.00.333.472 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.472 I llama_new_context_with_model: flash_attn    = 0
0.00.333.473 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.473 I llama_new_context_with_model: freq_scale    = 1
0.00.333.473 I ggml_metal_init: allocating
0.00.333.476 I ggml_metal_init: found device: Apple M4
0.00.333.478 I ggml_metal_init: picking default device: Apple M4
0.00.334.483 I ggml_metal_init: using embedded metal library
0.00.337.389 I ggml_metal_init: GPU name:   Apple M4
0.00.337.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.393 I ggml_metal_init: simdgroup reduction   = true
0.00.337.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.393 I ggml_metal_init: has bfloat            = true
0.00.337.393 I ggml_metal_init: use bfloat            = true
0.00.337.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.233 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.236 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.237 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.349.822 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.349.823 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.349.823 I llama_new_context_with_model: graph nodes  = 154
0.00.349.824 I llama_new_context_with_model: graph splits = 2
0.00.349.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.998 I 
0.00.360.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.173 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.174 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.177 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.177 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.179 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.179 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.701 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.406 I llama_perf_context_print:        load time =     336.53 ms
0.00.364.406 I llama_perf_context_print: prompt eval time =       3.70 ms /    62 tokens (    0.06 ms per token, 16770.35 tokens per second)
0.00.364.408 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.409 I llama_perf_context_print:       total time =       4.41 ms /    63 tokens
0.00.364.649 I ggml_metal_free: deallocating

real	0m1.052s
user	0m0.342s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.115 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.249 I main: llama backend init
0.00.000.255 I main: load the model and apply lora adapter, if any
0.00.069.800 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.086.219 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.086.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.086.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.086.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.086.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.086.252 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.086.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.086.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.086.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.086.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.086.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.086.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.086.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.086.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.086.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.086.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.086.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.096.138 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.098.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.106.783 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.106.786 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.106.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.106.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.106.787 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.106.788 I llama_model_loader: - type  f32:  194 tensors
0.00.106.788 I llama_model_loader: - type  f16:   98 tensors
0.00.137.615 I llm_load_vocab: special tokens cache size = 25
0.00.144.529 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.144.532 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.144.533 I llm_load_print_meta: arch             = gptneox
0.00.144.533 I llm_load_print_meta: vocab type       = BPE
0.00.144.533 I llm_load_print_meta: n_vocab          = 50304
0.00.144.533 I llm_load_print_meta: n_merges         = 50009
0.00.144.533 I llm_load_print_meta: vocab_only       = 0
0.00.144.534 I llm_load_print_meta: n_ctx_train      = 2048
0.00.144.534 I llm_load_print_meta: n_embd           = 2048
0.00.144.534 I llm_load_print_meta: n_layer          = 24
0.00.144.537 I llm_load_print_meta: n_head           = 16
0.00.144.538 I llm_load_print_meta: n_head_kv        = 16
0.00.144.557 I llm_load_print_meta: n_rot            = 32
0.00.144.558 I llm_load_print_meta: n_swa            = 0
0.00.144.558 I llm_load_print_meta: n_embd_head_k    = 128
0.00.144.558 I llm_load_print_meta: n_embd_head_v    = 128
0.00.144.559 I llm_load_print_meta: n_gqa            = 1
0.00.144.560 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.144.560 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.144.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.144.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.144.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.144.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.144.562 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.144.562 I llm_load_print_meta: n_ff             = 8192
0.00.144.563 I llm_load_print_meta: n_expert         = 0
0.00.144.563 I llm_load_print_meta: n_expert_used    = 0
0.00.144.563 I llm_load_print_meta: causal attn      = 1
0.00.144.563 I llm_load_print_meta: pooling type     = 0
0.00.144.563 I llm_load_print_meta: rope type        = 2
0.00.144.563 I llm_load_print_meta: rope scaling     = linear
0.00.144.564 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.144.564 I llm_load_print_meta: freq_scale_train = 1
0.00.144.564 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.144.564 I llm_load_print_meta: rope_finetuned   = unknown
0.00.144.565 I llm_load_print_meta: ssm_d_conv       = 0
0.00.144.565 I llm_load_print_meta: ssm_d_inner      = 0
0.00.144.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.144.565 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.144.565 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.144.575 I llm_load_print_meta: model type       = 1.4B
0.00.144.575 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.144.576 I llm_load_print_meta: model params     = 1.41 B
0.00.144.576 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.144.576 I llm_load_print_meta: general.name     = 1.4B
0.00.144.577 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.144.577 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.144.579 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.144.579 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.144.579 I llm_load_print_meta: LF token         = 128 ''
0.00.144.580 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.144.580 I llm_load_print_meta: max token length = 1024
0.00.147.457 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.147.457 I llm_load_tensors: offloading output layer to GPU
0.00.147.457 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.147.476 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.147.477 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.148.485 I llama_new_context_with_model: n_seq_max     = 1
0.00.148.486 I llama_new_context_with_model: n_ctx         = 2048
0.00.148.487 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.148.487 I llama_new_context_with_model: n_batch       = 2048
0.00.148.487 I llama_new_context_with_model: n_ubatch      = 512
0.00.148.487 I llama_new_context_with_model: flash_attn    = 0
0.00.148.488 I llama_new_context_with_model: freq_base     = 10000.0
0.00.148.488 I llama_new_context_with_model: freq_scale    = 1
0.00.148.489 I ggml_metal_init: allocating
0.00.148.496 I ggml_metal_init: found device: Apple M4
0.00.148.498 I ggml_metal_init: picking default device: Apple M4
0.00.149.139 I ggml_metal_init: using embedded metal library
0.00.158.320 I ggml_metal_init: GPU name:   Apple M4
0.00.158.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.158.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.158.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.158.323 I ggml_metal_init: simdgroup reduction   = true
0.00.158.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.158.323 I ggml_metal_init: has bfloat            = true
0.00.158.324 I ggml_metal_init: use bfloat            = true
0.00.158.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.158.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.207.261 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.207.267 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.207.288 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.208.181 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.208.183 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.208.183 I llama_new_context_with_model: graph nodes  = 967
0.00.208.184 I llama_new_context_with_model: graph splits = 2
0.00.208.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.279.623 I main: llama threadpool init, n_threads = 4
0.00.279.662 I 
0.00.279.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.279.695 I 
0.00.279.776 I sampler seed: 1234
0.00.279.780 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.279.803 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.279.805 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.279.805 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.122.030 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48102.98 tokens per second)
0.02.122.031 I llama_perf_context_print:        load time =     209.81 ms
0.02.122.031 I llama_perf_context_print: prompt eval time =      43.81 ms /     7 tokens (    6.26 ms per token,   159.79 tokens per second)
0.02.122.033 I llama_perf_context_print:        eval time =    1795.16 ms /    63 runs   (   28.49 ms per token,    35.09 tokens per second)
0.02.122.034 I llama_perf_context_print:       total time =    1842.41 ms /    70 tokens
0.02.122.250 I ggml_metal_free: deallocating

real	0m2.443s
user	0m0.158s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.906 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.842 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.784 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.802 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.805 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.805 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.806 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.807 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.108 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.108 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.109 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.110 I llama_model_loader: - type  f32:  194 tensors
0.00.054.110 I llama_model_loader: - type  f16:   98 tensors
0.00.083.817 I llm_load_vocab: special tokens cache size = 25
0.00.090.448 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.451 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.451 I llm_load_print_meta: arch             = gptneox
0.00.090.452 I llm_load_print_meta: vocab type       = BPE
0.00.090.452 I llm_load_print_meta: n_vocab          = 50304
0.00.090.452 I llm_load_print_meta: n_merges         = 50009
0.00.090.452 I llm_load_print_meta: vocab_only       = 0
0.00.090.453 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.453 I llm_load_print_meta: n_embd           = 2048
0.00.090.453 I llm_load_print_meta: n_layer          = 24
0.00.090.455 I llm_load_print_meta: n_head           = 16
0.00.090.456 I llm_load_print_meta: n_head_kv        = 16
0.00.090.467 I llm_load_print_meta: n_rot            = 32
0.00.090.467 I llm_load_print_meta: n_swa            = 0
0.00.090.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.470 I llm_load_print_meta: n_gqa            = 1
0.00.090.470 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.471 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.471 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.472 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.472 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.472 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.472 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.473 I llm_load_print_meta: n_ff             = 8192
0.00.090.473 I llm_load_print_meta: n_expert         = 0
0.00.090.473 I llm_load_print_meta: n_expert_used    = 0
0.00.090.473 I llm_load_print_meta: causal attn      = 1
0.00.090.475 I llm_load_print_meta: pooling type     = 0
0.00.090.475 I llm_load_print_meta: rope type        = 2
0.00.090.475 I llm_load_print_meta: rope scaling     = linear
0.00.090.475 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.475 I llm_load_print_meta: freq_scale_train = 1
0.00.090.475 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.476 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.476 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.476 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.476 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.476 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.476 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.486 I llm_load_print_meta: model type       = 1.4B
0.00.090.487 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.487 I llm_load_print_meta: model params     = 1.41 B
0.00.090.487 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.488 I llm_load_print_meta: general.name     = 1.4B
0.00.090.488 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.488 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.488 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.488 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.489 I llm_load_print_meta: LF token         = 128 ''
0.00.090.489 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.489 I llm_load_print_meta: max token length = 1024
0.00.093.301 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.301 I llm_load_tensors: offloading output layer to GPU
0.00.093.302 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.313 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.314 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.332 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.333 I llama_new_context_with_model: n_ctx         = 128
0.00.094.333 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.333 I llama_new_context_with_model: n_batch       = 128
0.00.094.333 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.333 I llama_new_context_with_model: flash_attn    = 0
0.00.094.334 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.334 I llama_new_context_with_model: freq_scale    = 1
0.00.094.334 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.335 I ggml_metal_init: allocating
0.00.094.339 I ggml_metal_init: found device: Apple M4
0.00.094.341 I ggml_metal_init: picking default device: Apple M4
0.00.094.888 I ggml_metal_init: using embedded metal library
0.00.097.374 I ggml_metal_init: GPU name:   Apple M4
0.00.097.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.376 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.376 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.376 I ggml_metal_init: simdgroup reduction   = true
0.00.097.377 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.377 I ggml_metal_init: has bfloat            = true
0.00.097.377 I ggml_metal_init: use bfloat            = true
0.00.097.377 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.378 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.902 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.905 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.919 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.741 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.742 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.742 I llama_new_context_with_model: graph nodes  = 967
0.00.109.743 I llama_new_context_with_model: graph splits = 2
0.00.109.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.060.892 I 
0.01.060.992 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.061.047 I perplexity: tokenizing the input ..
0.01.073.774 I perplexity: tokenization took 12.717 ms
0.01.073.802 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.195.047 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.196.976 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.197.013 I llama_perf_context_print:        load time =    1038.02 ms
0.01.197.015 I llama_perf_context_print: prompt eval time =     120.30 ms /   128 tokens (    0.94 ms per token,  1064.02 tokens per second)
0.01.197.016 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.197.017 I llama_perf_context_print:       total time =     136.14 ms /   129 tokens
0.01.197.864 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.130s
sys	0m0.265s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.095 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.109 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.118 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.121 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.121 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.122 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.122 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.122 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.234 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.539 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.540 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.541 I llama_model_loader: - type  f32:  194 tensors
0.00.035.542 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.685 I llm_load_vocab: special tokens cache size = 25
0.00.064.583 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.587 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.588 I llm_load_print_meta: arch             = gptneox
0.00.064.588 I llm_load_print_meta: vocab type       = BPE
0.00.064.588 I llm_load_print_meta: n_vocab          = 50304
0.00.064.588 I llm_load_print_meta: n_merges         = 50009
0.00.064.589 I llm_load_print_meta: vocab_only       = 0
0.00.064.589 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.592 I llm_load_print_meta: n_embd           = 2048
0.00.064.592 I llm_load_print_meta: n_layer          = 24
0.00.064.597 I llm_load_print_meta: n_head           = 16
0.00.064.598 I llm_load_print_meta: n_head_kv        = 16
0.00.064.612 I llm_load_print_meta: n_rot            = 32
0.00.064.613 I llm_load_print_meta: n_swa            = 0
0.00.064.613 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.613 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.614 I llm_load_print_meta: n_gqa            = 1
0.00.064.614 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.615 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.615 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.616 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.616 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.616 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.616 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.617 I llm_load_print_meta: n_ff             = 8192
0.00.064.617 I llm_load_print_meta: n_expert         = 0
0.00.064.617 I llm_load_print_meta: n_expert_used    = 0
0.00.064.617 I llm_load_print_meta: causal attn      = 1
0.00.064.617 I llm_load_print_meta: pooling type     = 0
0.00.064.617 I llm_load_print_meta: rope type        = 2
0.00.064.618 I llm_load_print_meta: rope scaling     = linear
0.00.064.618 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.618 I llm_load_print_meta: freq_scale_train = 1
0.00.064.618 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.619 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.619 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.619 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.619 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.630 I llm_load_print_meta: model type       = 1.4B
0.00.064.630 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.630 I llm_load_print_meta: model params     = 1.41 B
0.00.064.631 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.631 I llm_load_print_meta: general.name     = 1.4B
0.00.064.631 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.631 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.632 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.632 I llm_load_print_meta: LF token         = 128 ''
0.00.064.632 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.632 I llm_load_print_meta: max token length = 1024
0.00.067.252 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.252 I llm_load_tensors: offloading output layer to GPU
0.00.067.253 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.265 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.266 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.261 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.262 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.262 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.262 I llama_new_context_with_model: n_batch       = 2048
0.00.068.262 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.263 I llama_new_context_with_model: flash_attn    = 0
0.00.068.263 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.263 I llama_new_context_with_model: freq_scale    = 1
0.00.068.263 I ggml_metal_init: allocating
0.00.068.267 I ggml_metal_init: found device: Apple M4
0.00.068.268 I ggml_metal_init: picking default device: Apple M4
0.00.068.966 I ggml_metal_init: using embedded metal library
0.00.071.507 I ggml_metal_init: GPU name:   Apple M4
0.00.071.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.509 I ggml_metal_init: simdgroup reduction   = true
0.00.071.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.510 I ggml_metal_init: has bfloat            = true
0.00.071.510 I ggml_metal_init: use bfloat            = true
0.00.071.510 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.807 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.815 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.839 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.061 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.063 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.064 I llama_new_context_with_model: graph nodes  = 967
0.00.108.064 I llama_new_context_with_model: graph splits = 2
0.00.108.080 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.476.786 I main: llama threadpool init, n_threads = 4
0.01.476.829 I 
0.01.476.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.476.857 I 
0.01.477.103 I sampler seed: 1234
0.01.477.107 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.477.149 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.477.152 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.477.152 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.563.191 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.02.563.191 I llama_perf_context_print:        load time =    1466.69 ms
0.02.563.192 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.93 tokens per second)
0.02.563.193 I llama_perf_context_print:        eval time =    1038.94 ms /    63 runs   (   16.49 ms per token,    60.64 tokens per second)
0.02.563.193 I llama_perf_context_print:       total time =    1086.41 ms /    70 tokens
0.02.563.381 I ggml_metal_free: deallocating

real	0m2.583s
user	0m0.116s
sys	0m0.303s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.763 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.711 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.718 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.720 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.729 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.015 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.018 I llama_model_loader: - type  f32:  194 tensors
0.00.035.019 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.532 I llm_load_vocab: special tokens cache size = 25
0.00.068.885 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.889 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.889 I llm_load_print_meta: arch             = gptneox
0.00.068.890 I llm_load_print_meta: vocab type       = BPE
0.00.068.890 I llm_load_print_meta: n_vocab          = 50304
0.00.068.890 I llm_load_print_meta: n_merges         = 50009
0.00.068.890 I llm_load_print_meta: vocab_only       = 0
0.00.068.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.892 I llm_load_print_meta: n_embd           = 2048
0.00.068.892 I llm_load_print_meta: n_layer          = 24
0.00.068.898 I llm_load_print_meta: n_head           = 16
0.00.068.898 I llm_load_print_meta: n_head_kv        = 16
0.00.068.913 I llm_load_print_meta: n_rot            = 32
0.00.068.914 I llm_load_print_meta: n_swa            = 0
0.00.068.914 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.914 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.915 I llm_load_print_meta: n_gqa            = 1
0.00.068.916 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.916 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.917 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.917 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.917 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.917 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.917 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.920 I llm_load_print_meta: n_ff             = 8192
0.00.068.921 I llm_load_print_meta: n_expert         = 0
0.00.068.921 I llm_load_print_meta: n_expert_used    = 0
0.00.068.921 I llm_load_print_meta: causal attn      = 1
0.00.068.925 I llm_load_print_meta: pooling type     = 0
0.00.068.925 I llm_load_print_meta: rope type        = 2
0.00.068.925 I llm_load_print_meta: rope scaling     = linear
0.00.068.926 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.926 I llm_load_print_meta: freq_scale_train = 1
0.00.068.926 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.926 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.926 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.926 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.928 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.928 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.928 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.938 I llm_load_print_meta: model type       = 1.4B
0.00.068.939 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.939 I llm_load_print_meta: model params     = 1.41 B
0.00.068.940 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.940 I llm_load_print_meta: general.name     = 1.4B
0.00.068.940 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.940 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.941 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.941 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.942 I llm_load_print_meta: LF token         = 128 ''
0.00.068.942 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.942 I llm_load_print_meta: max token length = 1024
0.00.071.684 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.684 I llm_load_tensors: offloading output layer to GPU
0.00.071.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.697 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.698 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.795 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.796 I llama_new_context_with_model: n_ctx         = 128
0.00.072.796 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.796 I llama_new_context_with_model: n_batch       = 128
0.00.072.797 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.797 I llama_new_context_with_model: flash_attn    = 0
0.00.072.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.798 I llama_new_context_with_model: freq_scale    = 1
0.00.072.798 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.799 I ggml_metal_init: allocating
0.00.072.805 I ggml_metal_init: found device: Apple M4
0.00.072.808 I ggml_metal_init: picking default device: Apple M4
0.00.073.529 I ggml_metal_init: using embedded metal library
0.00.076.311 I ggml_metal_init: GPU name:   Apple M4
0.00.076.313 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.313 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.314 I ggml_metal_init: simdgroup reduction   = true
0.00.076.314 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.315 I ggml_metal_init: has bfloat            = true
0.00.076.315 I ggml_metal_init: use bfloat            = true
0.00.076.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.006 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.088.008 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.088.027 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.997 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.998 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.998 I llama_new_context_with_model: graph nodes  = 967
0.00.088.998 I llama_new_context_with_model: graph splits = 2
0.00.089.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.961.767 I 
0.00.961.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.961.874 I perplexity: tokenizing the input ..
0.00.970.107 I perplexity: tokenization took 8.233 ms
0.00.970.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.095.165 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.096.471 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.096.485 I llama_perf_context_print:        load time =     950.00 ms
0.01.096.486 I llama_perf_context_print: prompt eval time =     124.81 ms /   128 tokens (    0.98 ms per token,  1025.58 tokens per second)
0.01.096.487 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.096.487 I llama_perf_context_print:       total time =     134.72 ms /   129 tokens
0.01.097.001 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.100s
sys	0m0.212s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.011.189 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.384 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.386 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.394 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.397 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.762 I llama_model_loader: - type  f32:  194 tensors
0.00.026.762 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.762 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.952 I llm_load_vocab: special tokens cache size = 25
0.00.052.840 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.843 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.843 I llm_load_print_meta: arch             = gptneox
0.00.052.844 I llm_load_print_meta: vocab type       = BPE
0.00.052.844 I llm_load_print_meta: n_vocab          = 50304
0.00.052.844 I llm_load_print_meta: n_merges         = 50009
0.00.052.844 I llm_load_print_meta: vocab_only       = 0
0.00.052.845 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.845 I llm_load_print_meta: n_embd           = 2048
0.00.052.845 I llm_load_print_meta: n_layer          = 24
0.00.052.849 I llm_load_print_meta: n_head           = 16
0.00.052.849 I llm_load_print_meta: n_head_kv        = 16
0.00.052.863 I llm_load_print_meta: n_rot            = 32
0.00.052.863 I llm_load_print_meta: n_swa            = 0
0.00.052.863 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.863 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.864 I llm_load_print_meta: n_gqa            = 1
0.00.052.865 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.866 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.867 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.867 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.867 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.868 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.868 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.869 I llm_load_print_meta: n_ff             = 8192
0.00.052.869 I llm_load_print_meta: n_expert         = 0
0.00.052.869 I llm_load_print_meta: n_expert_used    = 0
0.00.052.869 I llm_load_print_meta: causal attn      = 1
0.00.052.872 I llm_load_print_meta: pooling type     = 0
0.00.052.872 I llm_load_print_meta: rope type        = 2
0.00.052.873 I llm_load_print_meta: rope scaling     = linear
0.00.052.873 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.873 I llm_load_print_meta: freq_scale_train = 1
0.00.052.873 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.874 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.874 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.874 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.874 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.874 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.875 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.885 I llm_load_print_meta: model type       = 1.4B
0.00.052.885 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.886 I llm_load_print_meta: model params     = 1.41 B
0.00.052.886 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.886 I llm_load_print_meta: general.name     = 1.4B
0.00.052.887 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.888 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.888 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.888 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.889 I llm_load_print_meta: LF token         = 128 ''
0.00.052.889 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.889 I llm_load_print_meta: max token length = 1024
0.00.055.365 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.365 I llm_load_tensors: offloading output layer to GPU
0.00.055.365 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.377 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.378 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.411 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.412 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.412 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.412 I llama_new_context_with_model: n_batch       = 2048
0.00.056.413 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.413 I llama_new_context_with_model: flash_attn    = 0
0.00.056.413 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.414 I llama_new_context_with_model: freq_scale    = 1
0.00.056.414 I ggml_metal_init: allocating
0.00.056.421 I ggml_metal_init: found device: Apple M4
0.00.056.424 I ggml_metal_init: picking default device: Apple M4
0.00.057.130 I ggml_metal_init: using embedded metal library
0.00.059.688 I ggml_metal_init: GPU name:   Apple M4
0.00.059.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.691 I ggml_metal_init: simdgroup reduction   = true
0.00.059.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.691 I ggml_metal_init: has bfloat            = true
0.00.059.692 I ggml_metal_init: use bfloat            = true
0.00.059.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.827 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.839 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.860 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.886 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.888 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.889 I llama_new_context_with_model: graph nodes  = 967
0.00.094.889 I llama_new_context_with_model: graph splits = 2
0.00.094.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.906 I main: llama threadpool init, n_threads = 4
0.00.692.939 I 
0.00.692.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.970 I 
0.00.693.218 I sampler seed: 1234
0.00.693.223 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.234 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.234 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.367.963 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.01.367.963 I llama_perf_context_print:        load time =     681.71 ms
0.01.367.964 I llama_perf_context_print: prompt eval time =      39.74 ms /     7 tokens (    5.68 ms per token,   176.14 tokens per second)
0.01.367.965 I llama_perf_context_print:        eval time =     631.68 ms /    63 runs   (   10.03 ms per token,    99.73 tokens per second)
0.01.367.965 I llama_perf_context_print:       total time =     675.06 ms /    70 tokens
0.01.368.178 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.110s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.751 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.462 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.466 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.472 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.483 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.315 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.051 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.052 I llama_model_loader: - type  f32:  194 tensors
0.00.025.052 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.052 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.633 I llm_load_vocab: special tokens cache size = 25
0.00.051.519 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.522 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.522 I llm_load_print_meta: arch             = gptneox
0.00.051.522 I llm_load_print_meta: vocab type       = BPE
0.00.051.523 I llm_load_print_meta: n_vocab          = 50304
0.00.051.523 I llm_load_print_meta: n_merges         = 50009
0.00.051.523 I llm_load_print_meta: vocab_only       = 0
0.00.051.523 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.523 I llm_load_print_meta: n_embd           = 2048
0.00.051.524 I llm_load_print_meta: n_layer          = 24
0.00.051.526 I llm_load_print_meta: n_head           = 16
0.00.051.527 I llm_load_print_meta: n_head_kv        = 16
0.00.051.539 I llm_load_print_meta: n_rot            = 32
0.00.051.539 I llm_load_print_meta: n_swa            = 0
0.00.051.539 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.540 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.540 I llm_load_print_meta: n_gqa            = 1
0.00.051.541 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.542 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.543 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.543 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.543 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.544 I llm_load_print_meta: n_ff             = 8192
0.00.051.544 I llm_load_print_meta: n_expert         = 0
0.00.051.544 I llm_load_print_meta: n_expert_used    = 0
0.00.051.544 I llm_load_print_meta: causal attn      = 1
0.00.051.544 I llm_load_print_meta: pooling type     = 0
0.00.051.545 I llm_load_print_meta: rope type        = 2
0.00.051.545 I llm_load_print_meta: rope scaling     = linear
0.00.051.545 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.545 I llm_load_print_meta: freq_scale_train = 1
0.00.051.546 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.546 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.546 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.546 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.546 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.546 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.546 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.556 I llm_load_print_meta: model type       = 1.4B
0.00.051.556 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.556 I llm_load_print_meta: model params     = 1.41 B
0.00.051.557 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.557 I llm_load_print_meta: general.name     = 1.4B
0.00.051.557 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: LF token         = 128 ''
0.00.051.558 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: max token length = 1024
0.00.053.683 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.683 I llm_load_tensors: offloading output layer to GPU
0.00.053.684 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.694 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.696 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.594 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.595 I llama_new_context_with_model: n_ctx         = 128
0.00.054.596 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.596 I llama_new_context_with_model: n_batch       = 128
0.00.054.596 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.596 I llama_new_context_with_model: flash_attn    = 0
0.00.054.596 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.597 I llama_new_context_with_model: freq_scale    = 1
0.00.054.597 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.597 I ggml_metal_init: allocating
0.00.054.601 I ggml_metal_init: found device: Apple M4
0.00.054.603 I ggml_metal_init: picking default device: Apple M4
0.00.055.153 I ggml_metal_init: using embedded metal library
0.00.057.462 I ggml_metal_init: GPU name:   Apple M4
0.00.057.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.464 I ggml_metal_init: simdgroup reduction   = true
0.00.057.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.465 I ggml_metal_init: has bfloat            = true
0.00.057.465 I ggml_metal_init: use bfloat            = true
0.00.057.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.374 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.376 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.389 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.337 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.339 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.339 I llama_new_context_with_model: graph nodes  = 967
0.00.069.339 I llama_new_context_with_model: graph splits = 2
0.00.069.352 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.434 I 
0.00.626.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.555 I perplexity: tokenizing the input ..
0.00.634.663 I perplexity: tokenization took 8.106 ms
0.00.634.674 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.824 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.759.177 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.759.198 I llama_perf_context_print:        load time =     615.67 ms
0.00.759.199 I llama_perf_context_print: prompt eval time =     122.92 ms /   128 tokens (    0.96 ms per token,  1041.34 tokens per second)
0.00.759.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.201 I llama_perf_context_print:       total time =     132.77 ms /   129 tokens
0.00.759.744 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.079s
sys	0m0.145s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.002 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.731 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.744 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.744 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.744 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.875 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.876 I llama_model_loader: - type  f32:  194 tensors
0.00.028.877 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.877 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.859 I llm_load_vocab: special tokens cache size = 25
0.00.054.837 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.840 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.841 I llm_load_print_meta: arch             = gptneox
0.00.054.841 I llm_load_print_meta: vocab type       = BPE
0.00.054.841 I llm_load_print_meta: n_vocab          = 50304
0.00.054.841 I llm_load_print_meta: n_merges         = 50009
0.00.054.842 I llm_load_print_meta: vocab_only       = 0
0.00.054.842 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.842 I llm_load_print_meta: n_embd           = 2048
0.00.054.842 I llm_load_print_meta: n_layer          = 24
0.00.054.845 I llm_load_print_meta: n_head           = 16
0.00.054.846 I llm_load_print_meta: n_head_kv        = 16
0.00.054.858 I llm_load_print_meta: n_rot            = 32
0.00.054.858 I llm_load_print_meta: n_swa            = 0
0.00.054.858 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.858 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.859 I llm_load_print_meta: n_gqa            = 1
0.00.054.860 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.861 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.861 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.862 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.862 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.862 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.862 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.863 I llm_load_print_meta: n_ff             = 8192
0.00.054.863 I llm_load_print_meta: n_expert         = 0
0.00.054.863 I llm_load_print_meta: n_expert_used    = 0
0.00.054.863 I llm_load_print_meta: causal attn      = 1
0.00.054.864 I llm_load_print_meta: pooling type     = 0
0.00.054.864 I llm_load_print_meta: rope type        = 2
0.00.054.864 I llm_load_print_meta: rope scaling     = linear
0.00.054.865 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.865 I llm_load_print_meta: freq_scale_train = 1
0.00.054.865 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.865 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.866 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.866 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.866 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.868 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.878 I llm_load_print_meta: model type       = 1.4B
0.00.054.878 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.879 I llm_load_print_meta: model params     = 1.41 B
0.00.054.879 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.879 I llm_load_print_meta: general.name     = 1.4B
0.00.054.880 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.880 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.880 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.880 I llm_load_print_meta: LF token         = 128 ''
0.00.054.881 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.881 I llm_load_print_meta: max token length = 1024
0.00.057.122 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.123 I llm_load_tensors: offloading output layer to GPU
0.00.057.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.134 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.057.135 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.058.115 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.116 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.116 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.116 I llama_new_context_with_model: n_batch       = 2048
0.00.058.116 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.116 I llama_new_context_with_model: flash_attn    = 0
0.00.058.117 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.117 I llama_new_context_with_model: freq_scale    = 1
0.00.058.118 I ggml_metal_init: allocating
0.00.058.124 I ggml_metal_init: found device: Apple M4
0.00.058.127 I ggml_metal_init: picking default device: Apple M4
0.00.058.681 I ggml_metal_init: using embedded metal library
0.00.060.976 I ggml_metal_init: GPU name:   Apple M4
0.00.060.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.979 I ggml_metal_init: simdgroup reduction   = true
0.00.060.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.979 I ggml_metal_init: has bfloat            = true
0.00.060.979 I ggml_metal_init: use bfloat            = true
0.00.060.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.884 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.889 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.906 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.965 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.967 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.967 I llama_new_context_with_model: graph nodes  = 967
0.00.091.967 I llama_new_context_with_model: graph splits = 2
0.00.091.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.596 I main: llama threadpool init, n_threads = 4
0.00.719.659 I 
0.00.719.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.689 I 
0.00.719.917 I sampler seed: 1234
0.00.719.921 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.719.983 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.719.985 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.719.985 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.448.510 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.448.511 I llama_perf_context_print:        load time =     709.59 ms
0.01.448.512 I llama_perf_context_print: prompt eval time =      44.90 ms /     7 tokens (    6.41 ms per token,   155.90 tokens per second)
0.01.448.513 I llama_perf_context_print:        eval time =     680.56 ms /    63 runs   (   10.80 ms per token,    92.57 tokens per second)
0.01.448.513 I llama_perf_context_print:       total time =     728.92 ms /    70 tokens
0.01.448.709 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.563 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.268 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.279 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.279 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.306 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.343 I llama_model_loader: - type  f32:  194 tensors
0.00.026.344 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.344 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.032 I llm_load_vocab: special tokens cache size = 25
0.00.051.971 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.974 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.974 I llm_load_print_meta: arch             = gptneox
0.00.051.975 I llm_load_print_meta: vocab type       = BPE
0.00.051.975 I llm_load_print_meta: n_vocab          = 50304
0.00.051.975 I llm_load_print_meta: n_merges         = 50009
0.00.051.975 I llm_load_print_meta: vocab_only       = 0
0.00.051.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.975 I llm_load_print_meta: n_embd           = 2048
0.00.051.976 I llm_load_print_meta: n_layer          = 24
0.00.051.979 I llm_load_print_meta: n_head           = 16
0.00.051.979 I llm_load_print_meta: n_head_kv        = 16
0.00.051.991 I llm_load_print_meta: n_rot            = 32
0.00.051.991 I llm_load_print_meta: n_swa            = 0
0.00.051.992 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.992 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.993 I llm_load_print_meta: n_gqa            = 1
0.00.051.993 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.994 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.994 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.995 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.995 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.995 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.995 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.996 I llm_load_print_meta: n_ff             = 8192
0.00.051.996 I llm_load_print_meta: n_expert         = 0
0.00.051.996 I llm_load_print_meta: n_expert_used    = 0
0.00.051.996 I llm_load_print_meta: causal attn      = 1
0.00.051.997 I llm_load_print_meta: pooling type     = 0
0.00.051.997 I llm_load_print_meta: rope type        = 2
0.00.051.997 I llm_load_print_meta: rope scaling     = linear
0.00.051.997 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.997 I llm_load_print_meta: freq_scale_train = 1
0.00.051.998 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.998 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.998 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.998 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.998 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.998 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.998 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.008 I llm_load_print_meta: model type       = 1.4B
0.00.052.008 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.009 I llm_load_print_meta: model params     = 1.41 B
0.00.052.009 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.009 I llm_load_print_meta: general.name     = 1.4B
0.00.052.010 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.010 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.010 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.010 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.010 I llm_load_print_meta: LF token         = 128 ''
0.00.052.011 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.011 I llm_load_print_meta: max token length = 1024
0.00.054.204 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.204 I llm_load_tensors: offloading output layer to GPU
0.00.054.205 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.215 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.217 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.162 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.163 I llama_new_context_with_model: n_ctx         = 128
0.00.055.163 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.163 I llama_new_context_with_model: n_batch       = 128
0.00.055.164 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.164 I llama_new_context_with_model: flash_attn    = 0
0.00.055.164 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.165 I llama_new_context_with_model: freq_scale    = 1
0.00.055.165 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.165 I ggml_metal_init: allocating
0.00.055.172 I ggml_metal_init: found device: Apple M4
0.00.055.175 I ggml_metal_init: picking default device: Apple M4
0.00.055.728 I ggml_metal_init: using embedded metal library
0.00.058.061 I ggml_metal_init: GPU name:   Apple M4
0.00.058.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.063 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.063 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.064 I ggml_metal_init: simdgroup reduction   = true
0.00.058.064 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.064 I ggml_metal_init: has bfloat            = true
0.00.058.064 I ggml_metal_init: use bfloat            = true
0.00.058.065 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.792 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.798 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.811 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.714 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.715 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.715 I llama_new_context_with_model: graph nodes  = 967
0.00.069.715 I llama_new_context_with_model: graph splits = 2
0.00.069.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.898 I 
0.00.640.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.000 I perplexity: tokenizing the input ..
0.00.649.290 I perplexity: tokenization took 8.289 ms
0.00.649.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.772.600 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.773.916 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.773.932 I llama_perf_context_print:        load time =     629.33 ms
0.00.773.933 I llama_perf_context_print: prompt eval time =     123.07 ms /   128 tokens (    0.96 ms per token,  1040.05 tokens per second)
0.00.773.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.773.934 I llama_perf_context_print:       total time =     133.04 ms /   129 tokens
0.00.774.339 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.079s
sys	0m0.134s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.427 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.004 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.014 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.015 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.017 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.017 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.020 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.022 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.314 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.315 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.315 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.316 I llama_model_loader: - type  f32:  194 tensors
0.00.025.316 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.317 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.277 I llm_load_vocab: special tokens cache size = 25
0.00.051.296 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.298 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.299 I llm_load_print_meta: arch             = gptneox
0.00.051.299 I llm_load_print_meta: vocab type       = BPE
0.00.051.299 I llm_load_print_meta: n_vocab          = 50304
0.00.051.300 I llm_load_print_meta: n_merges         = 50009
0.00.051.300 I llm_load_print_meta: vocab_only       = 0
0.00.051.300 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.300 I llm_load_print_meta: n_embd           = 2048
0.00.051.300 I llm_load_print_meta: n_layer          = 24
0.00.051.303 I llm_load_print_meta: n_head           = 16
0.00.051.303 I llm_load_print_meta: n_head_kv        = 16
0.00.051.315 I llm_load_print_meta: n_rot            = 32
0.00.051.317 I llm_load_print_meta: n_swa            = 0
0.00.051.317 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.317 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.318 I llm_load_print_meta: n_gqa            = 1
0.00.051.319 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.320 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.320 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.322 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.322 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.322 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.322 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.323 I llm_load_print_meta: n_ff             = 8192
0.00.051.323 I llm_load_print_meta: n_expert         = 0
0.00.051.323 I llm_load_print_meta: n_expert_used    = 0
0.00.051.323 I llm_load_print_meta: causal attn      = 1
0.00.051.323 I llm_load_print_meta: pooling type     = 0
0.00.051.323 I llm_load_print_meta: rope type        = 2
0.00.051.323 I llm_load_print_meta: rope scaling     = linear
0.00.051.324 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.325 I llm_load_print_meta: freq_scale_train = 1
0.00.051.326 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.326 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.326 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.326 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.326 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.335 I llm_load_print_meta: model type       = 1.4B
0.00.051.335 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.336 I llm_load_print_meta: model params     = 1.41 B
0.00.051.336 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.337 I llm_load_print_meta: general.name     = 1.4B
0.00.051.337 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.337 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.337 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.337 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: LF token         = 128 ''
0.00.051.339 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.339 I llm_load_print_meta: max token length = 1024
0.00.053.135 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.135 I llm_load_tensors: offloading output layer to GPU
0.00.053.136 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.146 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.147 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.968 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.969 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.969 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.969 I llama_new_context_with_model: n_batch       = 2048
0.00.053.970 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.970 I llama_new_context_with_model: flash_attn    = 0
0.00.053.970 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.970 I llama_new_context_with_model: freq_scale    = 1
0.00.053.971 I ggml_metal_init: allocating
0.00.053.977 I ggml_metal_init: found device: Apple M4
0.00.053.979 I ggml_metal_init: picking default device: Apple M4
0.00.054.527 I ggml_metal_init: using embedded metal library
0.00.056.823 I ggml_metal_init: GPU name:   Apple M4
0.00.056.824 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.825 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.825 I ggml_metal_init: simdgroup reduction   = true
0.00.056.825 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.825 I ggml_metal_init: has bfloat            = true
0.00.056.825 I ggml_metal_init: use bfloat            = true
0.00.056.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.402 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.409 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.426 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.399 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.400 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.400 I llama_new_context_with_model: graph nodes  = 967
0.00.086.400 I llama_new_context_with_model: graph splits = 2
0.00.086.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.853.022 I main: llama threadpool init, n_threads = 4
0.00.853.063 I 
0.00.853.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.115 I 
0.00.853.367 I sampler seed: 1234
0.00.853.371 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.853.382 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.853.382 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.853.382 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.640.558 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.640.558 I llama_perf_context_print:        load time =     843.59 ms
0.01.640.559 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.49 tokens per second)
0.01.640.560 I llama_perf_context_print:        eval time =     740.88 ms /    63 runs   (   11.76 ms per token,    85.03 tokens per second)
0.01.640.560 I llama_perf_context_print:       total time =     787.54 ms /    70 tokens
0.01.640.749 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.108s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.608 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.655 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.656 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.657 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.658 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.658 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.659 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.895 I llama_model_loader: - type  f32:  194 tensors
0.00.026.896 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.896 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.270 I llm_load_vocab: special tokens cache size = 25
0.00.053.188 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.190 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.190 I llm_load_print_meta: arch             = gptneox
0.00.053.191 I llm_load_print_meta: vocab type       = BPE
0.00.053.191 I llm_load_print_meta: n_vocab          = 50304
0.00.053.191 I llm_load_print_meta: n_merges         = 50009
0.00.053.192 I llm_load_print_meta: vocab_only       = 0
0.00.053.192 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.192 I llm_load_print_meta: n_embd           = 2048
0.00.053.192 I llm_load_print_meta: n_layer          = 24
0.00.053.195 I llm_load_print_meta: n_head           = 16
0.00.053.195 I llm_load_print_meta: n_head_kv        = 16
0.00.053.210 I llm_load_print_meta: n_rot            = 32
0.00.053.210 I llm_load_print_meta: n_swa            = 0
0.00.053.210 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.211 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.211 I llm_load_print_meta: n_gqa            = 1
0.00.053.213 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.213 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.214 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.214 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.214 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.214 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.214 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.215 I llm_load_print_meta: n_ff             = 8192
0.00.053.215 I llm_load_print_meta: n_expert         = 0
0.00.053.215 I llm_load_print_meta: n_expert_used    = 0
0.00.053.215 I llm_load_print_meta: causal attn      = 1
0.00.053.216 I llm_load_print_meta: pooling type     = 0
0.00.053.216 I llm_load_print_meta: rope type        = 2
0.00.053.216 I llm_load_print_meta: rope scaling     = linear
0.00.053.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.217 I llm_load_print_meta: freq_scale_train = 1
0.00.053.217 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.217 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.218 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.218 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.218 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.218 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.218 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.228 I llm_load_print_meta: model type       = 1.4B
0.00.053.228 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.229 I llm_load_print_meta: model params     = 1.41 B
0.00.053.229 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.229 I llm_load_print_meta: general.name     = 1.4B
0.00.053.230 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.230 I llm_load_print_meta: LF token         = 128 ''
0.00.053.231 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.231 I llm_load_print_meta: max token length = 1024
0.00.055.450 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.450 I llm_load_tensors: offloading output layer to GPU
0.00.055.450 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.461 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.463 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.395 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.396 I llama_new_context_with_model: n_ctx         = 128
0.00.056.396 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.396 I llama_new_context_with_model: n_batch       = 128
0.00.056.396 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.397 I llama_new_context_with_model: flash_attn    = 0
0.00.056.397 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.397 I llama_new_context_with_model: freq_scale    = 1
0.00.056.398 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.398 I ggml_metal_init: allocating
0.00.056.404 I ggml_metal_init: found device: Apple M4
0.00.056.406 I ggml_metal_init: picking default device: Apple M4
0.00.056.957 I ggml_metal_init: using embedded metal library
0.00.059.284 I ggml_metal_init: GPU name:   Apple M4
0.00.059.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.287 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.287 I ggml_metal_init: simdgroup reduction   = true
0.00.059.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.287 I ggml_metal_init: has bfloat            = true
0.00.059.287 I ggml_metal_init: use bfloat            = true
0.00.059.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.288 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.879 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.886 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.902 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.786 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.788 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.788 I llama_new_context_with_model: graph nodes  = 967
0.00.070.788 I llama_new_context_with_model: graph splits = 2
0.00.070.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.891 I 
0.00.815.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.985 I perplexity: tokenizing the input ..
0.00.823.921 I perplexity: tokenization took 7.934 ms
0.00.823.931 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.958.785 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.960.121 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.960.138 I llama_perf_context_print:        load time =     804.27 ms
0.00.960.138 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.80 tokens per second)
0.00.960.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.960.140 I llama_perf_context_print:       total time =     144.25 ms /   129 tokens
0.00.960.475 I ggml_metal_free: deallocating

real	0m0.976s
user	0m0.079s
sys	0m0.177s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.998 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.370 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.372 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.377 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.378 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.478 I llama_model_loader: - type  f32:  194 tensors
0.00.025.479 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.299 I llm_load_vocab: special tokens cache size = 25
0.00.051.189 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.191 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.192 I llm_load_print_meta: arch             = gptneox
0.00.051.192 I llm_load_print_meta: vocab type       = BPE
0.00.051.192 I llm_load_print_meta: n_vocab          = 50304
0.00.051.192 I llm_load_print_meta: n_merges         = 50009
0.00.051.193 I llm_load_print_meta: vocab_only       = 0
0.00.051.193 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.193 I llm_load_print_meta: n_embd           = 2048
0.00.051.193 I llm_load_print_meta: n_layer          = 24
0.00.051.195 I llm_load_print_meta: n_head           = 16
0.00.051.196 I llm_load_print_meta: n_head_kv        = 16
0.00.051.208 I llm_load_print_meta: n_rot            = 32
0.00.051.209 I llm_load_print_meta: n_swa            = 0
0.00.051.209 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.209 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.210 I llm_load_print_meta: n_gqa            = 1
0.00.051.210 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.211 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.212 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.212 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.212 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.213 I llm_load_print_meta: n_ff             = 8192
0.00.051.213 I llm_load_print_meta: n_expert         = 0
0.00.051.213 I llm_load_print_meta: n_expert_used    = 0
0.00.051.215 I llm_load_print_meta: causal attn      = 1
0.00.051.217 I llm_load_print_meta: pooling type     = 0
0.00.051.217 I llm_load_print_meta: rope type        = 2
0.00.051.217 I llm_load_print_meta: rope scaling     = linear
0.00.051.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.218 I llm_load_print_meta: freq_scale_train = 1
0.00.051.218 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.218 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.218 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.218 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.218 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.218 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.218 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.228 I llm_load_print_meta: model type       = 1.4B
0.00.051.229 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.229 I llm_load_print_meta: model params     = 1.41 B
0.00.051.229 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.230 I llm_load_print_meta: general.name     = 1.4B
0.00.051.230 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.231 I llm_load_print_meta: LF token         = 128 ''
0.00.051.231 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.231 I llm_load_print_meta: max token length = 1024
0.00.053.473 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.473 I llm_load_tensors: offloading output layer to GPU
0.00.053.473 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.483 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.484 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.435 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.436 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.437 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.437 I llama_new_context_with_model: n_batch       = 2048
0.00.054.437 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.437 I llama_new_context_with_model: flash_attn    = 0
0.00.054.437 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.438 I llama_new_context_with_model: freq_scale    = 1
0.00.054.438 I ggml_metal_init: allocating
0.00.054.441 I ggml_metal_init: found device: Apple M4
0.00.054.442 I ggml_metal_init: picking default device: Apple M4
0.00.054.993 I ggml_metal_init: using embedded metal library
0.00.057.283 I ggml_metal_init: GPU name:   Apple M4
0.00.057.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.286 I ggml_metal_init: simdgroup reduction   = true
0.00.057.288 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.288 I ggml_metal_init: has bfloat            = true
0.00.057.288 I ggml_metal_init: use bfloat            = true
0.00.057.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.289 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.467 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.475 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.495 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.567 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.569 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.569 I llama_new_context_with_model: graph nodes  = 967
0.00.087.570 I llama_new_context_with_model: graph splits = 2
0.00.087.584 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.919 I main: llama threadpool init, n_threads = 4
0.00.785.956 I 
0.00.785.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.986 I 
0.00.786.224 I sampler seed: 1234
0.00.786.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.260 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.261 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.261 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.628.069 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.628.070 I llama_perf_context_print:        load time =     775.92 ms
0.01.628.070 I llama_perf_context_print: prompt eval time =      46.24 ms /     7 tokens (    6.61 ms per token,   151.38 tokens per second)
0.01.628.071 I llama_perf_context_print:        eval time =     792.36 ms /    63 runs   (   12.58 ms per token,    79.51 tokens per second)
0.01.628.075 I llama_perf_context_print:       total time =     842.15 ms /    70 tokens
0.01.628.265 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.295 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.099 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.223 I llama_model_loader: - type  f32:  194 tensors
0.00.025.224 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.224 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.487 I llm_load_vocab: special tokens cache size = 25
0.00.051.224 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.227 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.227 I llm_load_print_meta: arch             = gptneox
0.00.051.228 I llm_load_print_meta: vocab type       = BPE
0.00.051.228 I llm_load_print_meta: n_vocab          = 50304
0.00.051.228 I llm_load_print_meta: n_merges         = 50009
0.00.051.228 I llm_load_print_meta: vocab_only       = 0
0.00.051.228 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.229 I llm_load_print_meta: n_embd           = 2048
0.00.051.229 I llm_load_print_meta: n_layer          = 24
0.00.051.231 I llm_load_print_meta: n_head           = 16
0.00.051.232 I llm_load_print_meta: n_head_kv        = 16
0.00.051.239 I llm_load_print_meta: n_rot            = 32
0.00.051.241 I llm_load_print_meta: n_swa            = 0
0.00.051.242 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.242 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.243 I llm_load_print_meta: n_gqa            = 1
0.00.051.243 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.244 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.244 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.245 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.245 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.245 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.245 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.246 I llm_load_print_meta: n_ff             = 8192
0.00.051.247 I llm_load_print_meta: n_expert         = 0
0.00.051.247 I llm_load_print_meta: n_expert_used    = 0
0.00.051.247 I llm_load_print_meta: causal attn      = 1
0.00.051.247 I llm_load_print_meta: pooling type     = 0
0.00.051.247 I llm_load_print_meta: rope type        = 2
0.00.051.247 I llm_load_print_meta: rope scaling     = linear
0.00.051.248 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.248 I llm_load_print_meta: freq_scale_train = 1
0.00.051.248 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.248 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.248 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.249 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.249 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.249 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.249 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.254 I llm_load_print_meta: model type       = 1.4B
0.00.051.254 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.254 I llm_load_print_meta: model params     = 1.41 B
0.00.051.255 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.255 I llm_load_print_meta: general.name     = 1.4B
0.00.051.255 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.255 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.255 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.256 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.256 I llm_load_print_meta: LF token         = 128 ''
0.00.051.256 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.256 I llm_load_print_meta: max token length = 1024
0.00.053.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.250 I llm_load_tensors: offloading output layer to GPU
0.00.053.251 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.256 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.257 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.185 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.186 I llama_new_context_with_model: n_ctx         = 128
0.00.054.186 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.186 I llama_new_context_with_model: n_batch       = 128
0.00.054.186 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.186 I llama_new_context_with_model: flash_attn    = 0
0.00.054.187 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.187 I llama_new_context_with_model: freq_scale    = 1
0.00.054.187 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.187 I ggml_metal_init: allocating
0.00.054.191 I ggml_metal_init: found device: Apple M4
0.00.054.193 I ggml_metal_init: picking default device: Apple M4
0.00.054.730 I ggml_metal_init: using embedded metal library
0.00.057.030 I ggml_metal_init: GPU name:   Apple M4
0.00.057.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.032 I ggml_metal_init: simdgroup reduction   = true
0.00.057.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.032 I ggml_metal_init: has bfloat            = true
0.00.057.033 I ggml_metal_init: use bfloat            = true
0.00.057.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.886 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.889 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.903 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.820 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.821 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.822 I llama_new_context_with_model: graph nodes  = 967
0.00.068.822 I llama_new_context_with_model: graph splits = 2
0.00.068.834 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.204 I 
0.00.774.278 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.305 I perplexity: tokenizing the input ..
0.00.782.619 I perplexity: tokenization took 8.313 ms
0.00.782.635 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.917.292 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.918.618 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.918.632 I llama_perf_context_print:        load time =     763.90 ms
0.00.918.633 I llama_perf_context_print: prompt eval time =     134.43 ms /   128 tokens (    1.05 ms per token,   952.16 tokens per second)
0.00.918.634 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.918.634 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.919.089 I ggml_metal_free: deallocating

real	0m0.932s
user	0m0.079s
sys	0m0.184s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.068 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.525 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.526 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.526 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.528 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.829 I llama_model_loader: - type  f32:  194 tensors
0.00.024.829 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.829 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.829 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.628 I llm_load_vocab: special tokens cache size = 25
0.00.051.661 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.663 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.664 I llm_load_print_meta: arch             = gptneox
0.00.051.664 I llm_load_print_meta: vocab type       = BPE
0.00.051.664 I llm_load_print_meta: n_vocab          = 50304
0.00.051.665 I llm_load_print_meta: n_merges         = 50009
0.00.051.665 I llm_load_print_meta: vocab_only       = 0
0.00.051.665 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.665 I llm_load_print_meta: n_embd           = 2048
0.00.051.665 I llm_load_print_meta: n_layer          = 24
0.00.051.669 I llm_load_print_meta: n_head           = 16
0.00.051.672 I llm_load_print_meta: n_head_kv        = 16
0.00.051.685 I llm_load_print_meta: n_rot            = 32
0.00.051.685 I llm_load_print_meta: n_swa            = 0
0.00.051.685 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.686 I llm_load_print_meta: n_gqa            = 1
0.00.051.686 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.688 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.689 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.690 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.690 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.690 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.690 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.691 I llm_load_print_meta: n_ff             = 8192
0.00.051.691 I llm_load_print_meta: n_expert         = 0
0.00.051.691 I llm_load_print_meta: n_expert_used    = 0
0.00.051.691 I llm_load_print_meta: causal attn      = 1
0.00.051.691 I llm_load_print_meta: pooling type     = 0
0.00.051.691 I llm_load_print_meta: rope type        = 2
0.00.051.692 I llm_load_print_meta: rope scaling     = linear
0.00.051.692 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.693 I llm_load_print_meta: freq_scale_train = 1
0.00.051.693 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.694 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.694 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.704 I llm_load_print_meta: model type       = 1.4B
0.00.051.704 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.704 I llm_load_print_meta: model params     = 1.41 B
0.00.051.705 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.705 I llm_load_print_meta: general.name     = 1.4B
0.00.051.705 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.706 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.706 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.706 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.707 I llm_load_print_meta: LF token         = 128 ''
0.00.051.707 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.707 I llm_load_print_meta: max token length = 1024
0.00.053.829 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.829 I llm_load_tensors: offloading output layer to GPU
0.00.053.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.840 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.842 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.778 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.779 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.779 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.779 I llama_new_context_with_model: n_batch       = 2048
0.00.054.779 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.780 I llama_new_context_with_model: flash_attn    = 0
0.00.054.780 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.780 I llama_new_context_with_model: freq_scale    = 1
0.00.054.781 I ggml_metal_init: allocating
0.00.054.784 I ggml_metal_init: found device: Apple M4
0.00.054.786 I ggml_metal_init: picking default device: Apple M4
0.00.055.344 I ggml_metal_init: using embedded metal library
0.00.057.635 I ggml_metal_init: GPU name:   Apple M4
0.00.057.636 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.637 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.637 I ggml_metal_init: simdgroup reduction   = true
0.00.057.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.638 I ggml_metal_init: has bfloat            = true
0.00.057.638 I ggml_metal_init: use bfloat            = true
0.00.057.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.976 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.981 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.000 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.067 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.069 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.069 I llama_new_context_with_model: graph nodes  = 967
0.00.088.069 I llama_new_context_with_model: graph splits = 2
0.00.088.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.828 I main: llama threadpool init, n_threads = 4
0.00.505.871 I 
0.00.505.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.901 I 
0.00.506.151 I sampler seed: 1234
0.00.506.156 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.187 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.189 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.189 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.192.369 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.192.370 I llama_perf_context_print:        load time =     495.75 ms
0.01.192.371 I llama_perf_context_print: prompt eval time =      42.65 ms /     7 tokens (    6.09 ms per token,   164.11 tokens per second)
0.01.192.375 I llama_perf_context_print:        eval time =     640.41 ms /    63 runs   (   10.17 ms per token,    98.37 tokens per second)
0.01.192.376 I llama_perf_context_print:       total time =     686.55 ms /    70 tokens
0.01.192.570 I ggml_metal_free: deallocating

real	0m1.211s
user	0m0.111s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.499 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.868 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.931 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.911 I llama_model_loader: - type  f32:  194 tensors
0.00.023.911 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.911 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.606 I llm_load_vocab: special tokens cache size = 25
0.00.049.291 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.294 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.294 I llm_load_print_meta: arch             = gptneox
0.00.049.294 I llm_load_print_meta: vocab type       = BPE
0.00.049.295 I llm_load_print_meta: n_vocab          = 50304
0.00.049.295 I llm_load_print_meta: n_merges         = 50009
0.00.049.295 I llm_load_print_meta: vocab_only       = 0
0.00.049.295 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.295 I llm_load_print_meta: n_embd           = 2048
0.00.049.296 I llm_load_print_meta: n_layer          = 24
0.00.049.298 I llm_load_print_meta: n_head           = 16
0.00.049.299 I llm_load_print_meta: n_head_kv        = 16
0.00.049.311 I llm_load_print_meta: n_rot            = 32
0.00.049.311 I llm_load_print_meta: n_swa            = 0
0.00.049.311 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.311 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.312 I llm_load_print_meta: n_gqa            = 1
0.00.049.313 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.313 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.314 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.314 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.316 I llm_load_print_meta: n_ff             = 8192
0.00.049.316 I llm_load_print_meta: n_expert         = 0
0.00.049.316 I llm_load_print_meta: n_expert_used    = 0
0.00.049.316 I llm_load_print_meta: causal attn      = 1
0.00.049.316 I llm_load_print_meta: pooling type     = 0
0.00.049.316 I llm_load_print_meta: rope type        = 2
0.00.049.316 I llm_load_print_meta: rope scaling     = linear
0.00.049.317 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.317 I llm_load_print_meta: freq_scale_train = 1
0.00.049.317 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.317 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.318 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.318 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.318 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.318 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.318 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.328 I llm_load_print_meta: model type       = 1.4B
0.00.049.328 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.328 I llm_load_print_meta: model params     = 1.41 B
0.00.049.329 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.329 I llm_load_print_meta: general.name     = 1.4B
0.00.049.329 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.329 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.329 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.331 I llm_load_print_meta: LF token         = 128 ''
0.00.049.331 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.331 I llm_load_print_meta: max token length = 1024
0.00.051.409 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.409 I llm_load_tensors: offloading output layer to GPU
0.00.051.409 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.420 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.421 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.343 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.343 I llama_new_context_with_model: n_ctx         = 128
0.00.052.344 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.344 I llama_new_context_with_model: n_batch       = 128
0.00.052.344 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.344 I llama_new_context_with_model: flash_attn    = 0
0.00.052.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.345 I llama_new_context_with_model: freq_scale    = 1
0.00.052.345 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.345 I ggml_metal_init: allocating
0.00.052.348 I ggml_metal_init: found device: Apple M4
0.00.052.350 I ggml_metal_init: picking default device: Apple M4
0.00.052.898 I ggml_metal_init: using embedded metal library
0.00.055.179 I ggml_metal_init: GPU name:   Apple M4
0.00.055.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.181 I ggml_metal_init: simdgroup reduction   = true
0.00.055.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.182 I ggml_metal_init: has bfloat            = true
0.00.055.182 I ggml_metal_init: use bfloat            = true
0.00.055.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.183 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.600 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.602 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.615 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.515 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.516 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.517 I llama_new_context_with_model: graph nodes  = 967
0.00.066.517 I llama_new_context_with_model: graph splits = 2
0.00.066.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.457.263 I 
0.00.457.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.457.353 I perplexity: tokenizing the input ..
0.00.465.285 I perplexity: tokenization took 7.929 ms
0.00.465.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.598.504 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.599.759 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.599.771 I llama_perf_context_print:        load time =     447.76 ms
0.00.599.772 I llama_perf_context_print: prompt eval time =     132.97 ms /   128 tokens (    1.04 ms per token,   962.59 tokens per second)
0.00.599.773 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.599.773 I llama_perf_context_print:       total time =     142.51 ms /   129 tokens
0.00.600.136 I ggml_metal_free: deallocating

real	0m0.615s
user	0m0.078s
sys	0m0.101s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.009 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.508 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.518 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.518 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.519 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.520 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.520 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.649 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.649 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.650 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.650 I llama_model_loader: - type  f32:  194 tensors
0.00.023.651 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.651 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.651 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.651 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.469 I llm_load_vocab: special tokens cache size = 25
0.00.049.257 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.260 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.261 I llm_load_print_meta: arch             = gptneox
0.00.049.261 I llm_load_print_meta: vocab type       = BPE
0.00.049.261 I llm_load_print_meta: n_vocab          = 50304
0.00.049.261 I llm_load_print_meta: n_merges         = 50009
0.00.049.262 I llm_load_print_meta: vocab_only       = 0
0.00.049.262 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.262 I llm_load_print_meta: n_embd           = 2048
0.00.049.262 I llm_load_print_meta: n_layer          = 24
0.00.049.264 I llm_load_print_meta: n_head           = 16
0.00.049.265 I llm_load_print_meta: n_head_kv        = 16
0.00.049.277 I llm_load_print_meta: n_rot            = 32
0.00.049.277 I llm_load_print_meta: n_swa            = 0
0.00.049.277 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.277 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.278 I llm_load_print_meta: n_gqa            = 1
0.00.049.279 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.279 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.280 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.281 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.281 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.281 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.282 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.283 I llm_load_print_meta: n_ff             = 8192
0.00.049.283 I llm_load_print_meta: n_expert         = 0
0.00.049.283 I llm_load_print_meta: n_expert_used    = 0
0.00.049.284 I llm_load_print_meta: causal attn      = 1
0.00.049.284 I llm_load_print_meta: pooling type     = 0
0.00.049.284 I llm_load_print_meta: rope type        = 2
0.00.049.285 I llm_load_print_meta: rope scaling     = linear
0.00.049.286 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.286 I llm_load_print_meta: freq_scale_train = 1
0.00.049.286 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.286 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.286 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.286 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.288 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.288 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.288 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.297 I llm_load_print_meta: model type       = 1.4B
0.00.049.297 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.298 I llm_load_print_meta: model params     = 1.41 B
0.00.049.298 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.299 I llm_load_print_meta: general.name     = 1.4B
0.00.049.299 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.299 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.299 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.299 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.300 I llm_load_print_meta: LF token         = 128 ''
0.00.049.300 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.300 I llm_load_print_meta: max token length = 1024
0.00.051.065 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.065 I llm_load_tensors: offloading output layer to GPU
0.00.051.066 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.076 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.077 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.912 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.913 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.913 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.913 I llama_new_context_with_model: n_batch       = 2048
0.00.051.913 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.914 I llama_new_context_with_model: flash_attn    = 0
0.00.051.914 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.914 I llama_new_context_with_model: freq_scale    = 1
0.00.051.914 I ggml_metal_init: allocating
0.00.051.917 I ggml_metal_init: found device: Apple M4
0.00.051.919 I ggml_metal_init: picking default device: Apple M4
0.00.052.469 I ggml_metal_init: using embedded metal library
0.00.054.744 I ggml_metal_init: GPU name:   Apple M4
0.00.054.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.747 I ggml_metal_init: simdgroup reduction   = true
0.00.054.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.747 I ggml_metal_init: has bfloat            = true
0.00.054.747 I ggml_metal_init: use bfloat            = true
0.00.054.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.224 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.230 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.250 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.189 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.191 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.191 I llama_new_context_with_model: graph nodes  = 967
0.00.084.191 I llama_new_context_with_model: graph splits = 2
0.00.084.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.336 I main: llama threadpool init, n_threads = 4
0.00.561.390 I 
0.00.561.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.424 I 
0.00.561.678 I sampler seed: 1234
0.00.561.683 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.561.695 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.561.695 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.561.695 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.310.587 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.310.588 I llama_perf_context_print:        load time =     552.32 ms
0.01.310.589 I llama_perf_context_print: prompt eval time =      45.77 ms /     7 tokens (    6.54 ms per token,   152.94 tokens per second)
0.01.310.589 I llama_perf_context_print:        eval time =     699.75 ms /    63 runs   (   11.11 ms per token,    90.03 tokens per second)
0.01.310.590 I llama_perf_context_print:       total time =     749.26 ms /    70 tokens
0.01.310.826 I ggml_metal_free: deallocating

real	0m1.328s
user	0m0.109s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.494 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.494 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.495 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.350 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.435 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.436 I llama_model_loader: - type  f32:  194 tensors
0.00.025.436 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.437 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.437 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.437 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.140 I llm_load_vocab: special tokens cache size = 25
0.00.050.972 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.975 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.975 I llm_load_print_meta: arch             = gptneox
0.00.050.975 I llm_load_print_meta: vocab type       = BPE
0.00.050.975 I llm_load_print_meta: n_vocab          = 50304
0.00.050.976 I llm_load_print_meta: n_merges         = 50009
0.00.050.976 I llm_load_print_meta: vocab_only       = 0
0.00.050.976 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.976 I llm_load_print_meta: n_embd           = 2048
0.00.050.976 I llm_load_print_meta: n_layer          = 24
0.00.050.978 I llm_load_print_meta: n_head           = 16
0.00.050.979 I llm_load_print_meta: n_head_kv        = 16
0.00.050.991 I llm_load_print_meta: n_rot            = 32
0.00.050.991 I llm_load_print_meta: n_swa            = 0
0.00.050.992 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.992 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.993 I llm_load_print_meta: n_gqa            = 1
0.00.050.993 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.994 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.995 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.995 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.995 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.996 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.997 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.997 I llm_load_print_meta: n_ff             = 8192
0.00.050.998 I llm_load_print_meta: n_expert         = 0
0.00.050.998 I llm_load_print_meta: n_expert_used    = 0
0.00.050.998 I llm_load_print_meta: causal attn      = 1
0.00.050.998 I llm_load_print_meta: pooling type     = 0
0.00.050.998 I llm_load_print_meta: rope type        = 2
0.00.051.001 I llm_load_print_meta: rope scaling     = linear
0.00.051.001 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.001 I llm_load_print_meta: freq_scale_train = 1
0.00.051.003 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.003 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.003 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.003 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.004 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.004 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.004 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.014 I llm_load_print_meta: model type       = 1.4B
0.00.051.014 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.015 I llm_load_print_meta: model params     = 1.41 B
0.00.051.015 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.015 I llm_load_print_meta: general.name     = 1.4B
0.00.051.015 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.015 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: LF token         = 128 ''
0.00.051.016 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.017 I llm_load_print_meta: max token length = 1024
0.00.052.966 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.966 I llm_load_tensors: offloading output layer to GPU
0.00.052.966 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.971 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.972 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.911 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.912 I llama_new_context_with_model: n_ctx         = 128
0.00.053.912 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.912 I llama_new_context_with_model: n_batch       = 128
0.00.053.912 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.912 I llama_new_context_with_model: flash_attn    = 0
0.00.053.913 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.913 I llama_new_context_with_model: freq_scale    = 1
0.00.053.913 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.913 I ggml_metal_init: allocating
0.00.053.916 I ggml_metal_init: found device: Apple M4
0.00.053.918 I ggml_metal_init: picking default device: Apple M4
0.00.054.455 I ggml_metal_init: using embedded metal library
0.00.056.736 I ggml_metal_init: GPU name:   Apple M4
0.00.056.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.738 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.739 I ggml_metal_init: simdgroup reduction   = true
0.00.056.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.739 I ggml_metal_init: has bfloat            = true
0.00.056.739 I ggml_metal_init: use bfloat            = true
0.00.056.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.740 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.931 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.933 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.947 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.840 I llama_new_context_with_model: graph nodes  = 967
0.00.067.840 I llama_new_context_with_model: graph splits = 2
0.00.067.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.517.337 I 
0.00.517.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.391 I perplexity: tokenizing the input ..
0.00.525.710 I perplexity: tokenization took 8.317 ms
0.00.525.720 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.658.580 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.659.889 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.659.908 I llama_perf_context_print:        load time =     506.50 ms
0.00.659.909 I llama_perf_context_print: prompt eval time =     132.63 ms /   128 tokens (    1.04 ms per token,   965.11 tokens per second)
0.00.659.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.659.911 I llama_perf_context_print:       total time =     142.57 ms /   129 tokens
0.00.660.450 I ggml_metal_free: deallocating

real	0m0.676s
user	0m0.079s
sys	0m0.112s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.111 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.039 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.052 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.052 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.053 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.056 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.056 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.056 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.166 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.167 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.167 I llama_model_loader: - type  f32:  194 tensors
0.00.026.168 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.168 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.168 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.036 I llm_load_vocab: special tokens cache size = 25
0.00.051.826 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.829 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.829 I llm_load_print_meta: arch             = gptneox
0.00.051.829 I llm_load_print_meta: vocab type       = BPE
0.00.051.830 I llm_load_print_meta: n_vocab          = 50304
0.00.051.830 I llm_load_print_meta: n_merges         = 50009
0.00.051.830 I llm_load_print_meta: vocab_only       = 0
0.00.051.830 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.830 I llm_load_print_meta: n_embd           = 2048
0.00.051.830 I llm_load_print_meta: n_layer          = 24
0.00.051.833 I llm_load_print_meta: n_head           = 16
0.00.051.834 I llm_load_print_meta: n_head_kv        = 16
0.00.051.846 I llm_load_print_meta: n_rot            = 32
0.00.051.846 I llm_load_print_meta: n_swa            = 0
0.00.051.847 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.847 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.848 I llm_load_print_meta: n_gqa            = 1
0.00.051.848 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.849 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.850 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.852 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.852 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.852 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.852 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.854 I llm_load_print_meta: n_ff             = 8192
0.00.051.854 I llm_load_print_meta: n_expert         = 0
0.00.051.857 I llm_load_print_meta: n_expert_used    = 0
0.00.051.858 I llm_load_print_meta: causal attn      = 1
0.00.051.858 I llm_load_print_meta: pooling type     = 0
0.00.051.858 I llm_load_print_meta: rope type        = 2
0.00.051.858 I llm_load_print_meta: rope scaling     = linear
0.00.051.859 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.859 I llm_load_print_meta: freq_scale_train = 1
0.00.051.859 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.859 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.860 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.860 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.860 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.860 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.860 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.870 I llm_load_print_meta: model type       = 1.4B
0.00.051.870 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.872 I llm_load_print_meta: model params     = 1.41 B
0.00.051.872 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.872 I llm_load_print_meta: general.name     = 1.4B
0.00.051.873 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.873 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.873 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.873 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.873 I llm_load_print_meta: LF token         = 128 ''
0.00.051.873 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.874 I llm_load_print_meta: max token length = 1024
0.00.054.034 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.034 I llm_load_tensors: offloading output layer to GPU
0.00.054.034 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.045 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.046 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.027 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.028 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.028 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.028 I llama_new_context_with_model: n_batch       = 2048
0.00.055.028 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.028 I llama_new_context_with_model: flash_attn    = 0
0.00.055.029 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.029 I llama_new_context_with_model: freq_scale    = 1
0.00.055.029 I ggml_metal_init: allocating
0.00.055.033 I ggml_metal_init: found device: Apple M4
0.00.055.034 I ggml_metal_init: picking default device: Apple M4
0.00.055.575 I ggml_metal_init: using embedded metal library
0.00.057.867 I ggml_metal_init: GPU name:   Apple M4
0.00.057.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.871 I ggml_metal_init: simdgroup reduction   = true
0.00.057.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.872 I ggml_metal_init: has bfloat            = true
0.00.057.872 I ggml_metal_init: use bfloat            = true
0.00.057.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.605 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.631 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.700 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.701 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.702 I llama_new_context_with_model: graph nodes  = 967
0.00.087.702 I llama_new_context_with_model: graph splits = 2
0.00.087.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.569 I main: llama threadpool init, n_threads = 4
0.00.685.608 I 
0.00.685.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.639 I 
0.00.685.871 I sampler seed: 1234
0.00.685.876 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.685.919 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.923 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.924 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.444.092 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48630.14 tokens per second)
0.01.444.093 I llama_perf_context_print:        load time =     675.45 ms
0.01.444.094 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.52 tokens per second)
0.01.444.094 I llama_perf_context_print:        eval time =     707.66 ms /    63 runs   (   11.23 ms per token,    89.03 tokens per second)
0.01.444.094 I llama_perf_context_print:       total time =     758.53 ms /    70 tokens
0.01.444.275 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.110s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.837 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.547 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.581 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.702 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.705 I llama_model_loader: - type  f32:  194 tensors
0.00.024.706 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.706 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.706 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.344 I llm_load_vocab: special tokens cache size = 25
0.00.051.170 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.173 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.173 I llm_load_print_meta: arch             = gptneox
0.00.051.174 I llm_load_print_meta: vocab type       = BPE
0.00.051.174 I llm_load_print_meta: n_vocab          = 50304
0.00.051.174 I llm_load_print_meta: n_merges         = 50009
0.00.051.174 I llm_load_print_meta: vocab_only       = 0
0.00.051.175 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.175 I llm_load_print_meta: n_embd           = 2048
0.00.051.175 I llm_load_print_meta: n_layer          = 24
0.00.051.177 I llm_load_print_meta: n_head           = 16
0.00.051.178 I llm_load_print_meta: n_head_kv        = 16
0.00.051.190 I llm_load_print_meta: n_rot            = 32
0.00.051.190 I llm_load_print_meta: n_swa            = 0
0.00.051.190 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.191 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.192 I llm_load_print_meta: n_gqa            = 1
0.00.051.193 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.194 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.194 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.194 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.195 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.195 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.196 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.197 I llm_load_print_meta: n_ff             = 8192
0.00.051.197 I llm_load_print_meta: n_expert         = 0
0.00.051.197 I llm_load_print_meta: n_expert_used    = 0
0.00.051.197 I llm_load_print_meta: causal attn      = 1
0.00.051.197 I llm_load_print_meta: pooling type     = 0
0.00.051.197 I llm_load_print_meta: rope type        = 2
0.00.051.197 I llm_load_print_meta: rope scaling     = linear
0.00.051.199 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.199 I llm_load_print_meta: freq_scale_train = 1
0.00.051.199 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.199 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.200 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.200 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.200 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.200 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.200 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.210 I llm_load_print_meta: model type       = 1.4B
0.00.051.211 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.212 I llm_load_print_meta: model params     = 1.41 B
0.00.051.212 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.212 I llm_load_print_meta: general.name     = 1.4B
0.00.051.213 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.213 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.213 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.214 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.214 I llm_load_print_meta: LF token         = 128 ''
0.00.051.214 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.214 I llm_load_print_meta: max token length = 1024
0.00.053.408 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.408 I llm_load_tensors: offloading output layer to GPU
0.00.053.408 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.418 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.420 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.348 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.349 I llama_new_context_with_model: n_ctx         = 128
0.00.054.349 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.349 I llama_new_context_with_model: n_batch       = 128
0.00.054.349 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.349 I llama_new_context_with_model: flash_attn    = 0
0.00.054.350 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.350 I llama_new_context_with_model: freq_scale    = 1
0.00.054.350 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.351 I ggml_metal_init: allocating
0.00.054.355 I ggml_metal_init: found device: Apple M4
0.00.054.358 I ggml_metal_init: picking default device: Apple M4
0.00.054.888 I ggml_metal_init: using embedded metal library
0.00.057.233 I ggml_metal_init: GPU name:   Apple M4
0.00.057.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.235 I ggml_metal_init: simdgroup reduction   = true
0.00.057.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.235 I ggml_metal_init: has bfloat            = true
0.00.057.236 I ggml_metal_init: use bfloat            = true
0.00.057.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.783 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.795 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.817 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.648 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.650 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.650 I llama_new_context_with_model: graph nodes  = 967
0.00.068.650 I llama_new_context_with_model: graph splits = 2
0.00.068.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.943 I 
0.00.627.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.001 I perplexity: tokenizing the input ..
0.00.635.573 I perplexity: tokenization took 7.57 ms
0.00.635.583 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.485 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.771.848 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.771.862 I llama_perf_context_print:        load time =     618.10 ms
0.00.771.863 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.39 tokens per second)
0.00.771.864 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.865 I llama_perf_context_print:       total time =     143.92 ms /   129 tokens
0.00.772.368 I ggml_metal_free: deallocating

real	0m0.789s
user	0m0.080s
sys	0m0.150s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.899 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.976 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.983 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.983 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.984 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.984 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.985 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.985 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.986 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.986 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.987 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.987 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.988 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.989 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.896 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.898 I llama_model_loader: - type  f32:  194 tensors
0.00.023.899 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.899 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.591 I llm_load_vocab: special tokens cache size = 25
0.00.049.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.499 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.499 I llm_load_print_meta: arch             = gptneox
0.00.049.500 I llm_load_print_meta: vocab type       = BPE
0.00.049.500 I llm_load_print_meta: n_vocab          = 50304
0.00.049.500 I llm_load_print_meta: n_merges         = 50009
0.00.049.500 I llm_load_print_meta: vocab_only       = 0
0.00.049.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.501 I llm_load_print_meta: n_embd           = 2048
0.00.049.501 I llm_load_print_meta: n_layer          = 24
0.00.049.504 I llm_load_print_meta: n_head           = 16
0.00.049.504 I llm_load_print_meta: n_head_kv        = 16
0.00.049.511 I llm_load_print_meta: n_rot            = 32
0.00.049.511 I llm_load_print_meta: n_swa            = 0
0.00.049.511 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.512 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.512 I llm_load_print_meta: n_gqa            = 1
0.00.049.513 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.514 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.514 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.515 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.515 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.515 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.515 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.516 I llm_load_print_meta: n_ff             = 8192
0.00.049.516 I llm_load_print_meta: n_expert         = 0
0.00.049.516 I llm_load_print_meta: n_expert_used    = 0
0.00.049.516 I llm_load_print_meta: causal attn      = 1
0.00.049.517 I llm_load_print_meta: pooling type     = 0
0.00.049.517 I llm_load_print_meta: rope type        = 2
0.00.049.517 I llm_load_print_meta: rope scaling     = linear
0.00.049.517 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.518 I llm_load_print_meta: freq_scale_train = 1
0.00.049.518 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.518 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.518 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.521 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.521 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.521 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.525 I llm_load_print_meta: model type       = 1.4B
0.00.049.525 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.525 I llm_load_print_meta: model params     = 1.41 B
0.00.049.526 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.526 I llm_load_print_meta: general.name     = 1.4B
0.00.049.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.526 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.527 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.527 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.527 I llm_load_print_meta: LF token         = 128 ''
0.00.049.527 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.528 I llm_load_print_meta: max token length = 1024
0.00.051.240 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.240 I llm_load_tensors: offloading output layer to GPU
0.00.051.240 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.245 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.247 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.091 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.092 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.092 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.092 I llama_new_context_with_model: n_batch       = 2048
0.00.052.093 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.093 I llama_new_context_with_model: flash_attn    = 0
0.00.052.093 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.094 I llama_new_context_with_model: freq_scale    = 1
0.00.052.094 I ggml_metal_init: allocating
0.00.052.100 I ggml_metal_init: found device: Apple M4
0.00.052.102 I ggml_metal_init: picking default device: Apple M4
0.00.052.626 I ggml_metal_init: using embedded metal library
0.00.055.330 I ggml_metal_init: GPU name:   Apple M4
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.332 I ggml_metal_init: simdgroup reduction   = true
0.00.055.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.333 I ggml_metal_init: has bfloat            = true
0.00.055.333 I ggml_metal_init: use bfloat            = true
0.00.055.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.446 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.453 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.472 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.428 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.429 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.429 I llama_new_context_with_model: graph nodes  = 967
0.00.085.429 I llama_new_context_with_model: graph splits = 2
0.00.085.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.977 I main: llama threadpool init, n_threads = 4
0.00.764.016 I 
0.00.764.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.076 I 
0.00.764.321 I sampler seed: 1234
0.00.764.325 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.369 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.369 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.610.605 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.610.605 I llama_perf_context_print:        load time =     755.07 ms
0.01.610.606 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.66 tokens per second)
0.01.610.607 I llama_perf_context_print:        eval time =     791.45 ms /    63 runs   (   12.56 ms per token,    79.60 tokens per second)
0.01.610.608 I llama_perf_context_print:       total time =     846.63 ms /    70 tokens
0.01.610.804 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.402 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.356 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.357 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.486 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.588 I llama_model_loader: - type  f32:  194 tensors
0.00.024.588 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.588 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.110 I llm_load_vocab: special tokens cache size = 25
0.00.049.955 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.958 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.959 I llm_load_print_meta: arch             = gptneox
0.00.049.959 I llm_load_print_meta: vocab type       = BPE
0.00.049.959 I llm_load_print_meta: n_vocab          = 50304
0.00.049.959 I llm_load_print_meta: n_merges         = 50009
0.00.049.960 I llm_load_print_meta: vocab_only       = 0
0.00.049.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.960 I llm_load_print_meta: n_embd           = 2048
0.00.049.960 I llm_load_print_meta: n_layer          = 24
0.00.049.963 I llm_load_print_meta: n_head           = 16
0.00.049.964 I llm_load_print_meta: n_head_kv        = 16
0.00.049.971 I llm_load_print_meta: n_rot            = 32
0.00.049.971 I llm_load_print_meta: n_swa            = 0
0.00.049.971 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.971 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.972 I llm_load_print_meta: n_gqa            = 1
0.00.049.973 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.974 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.974 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.975 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.975 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.975 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.975 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.976 I llm_load_print_meta: n_ff             = 8192
0.00.049.976 I llm_load_print_meta: n_expert         = 0
0.00.049.976 I llm_load_print_meta: n_expert_used    = 0
0.00.049.976 I llm_load_print_meta: causal attn      = 1
0.00.049.976 I llm_load_print_meta: pooling type     = 0
0.00.049.976 I llm_load_print_meta: rope type        = 2
0.00.049.977 I llm_load_print_meta: rope scaling     = linear
0.00.049.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.978 I llm_load_print_meta: freq_scale_train = 1
0.00.049.978 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.979 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.979 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.979 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.983 I llm_load_print_meta: model type       = 1.4B
0.00.049.984 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.986 I llm_load_print_meta: model params     = 1.41 B
0.00.049.987 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.987 I llm_load_print_meta: general.name     = 1.4B
0.00.049.987 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.987 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: LF token         = 128 ''
0.00.049.989 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.989 I llm_load_print_meta: max token length = 1024
0.00.051.971 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.971 I llm_load_tensors: offloading output layer to GPU
0.00.051.971 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.977 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.978 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.904 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.904 I llama_new_context_with_model: n_ctx         = 128
0.00.052.905 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.905 I llama_new_context_with_model: n_batch       = 128
0.00.052.905 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.905 I llama_new_context_with_model: flash_attn    = 0
0.00.052.906 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.906 I llama_new_context_with_model: freq_scale    = 1
0.00.052.906 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.906 I ggml_metal_init: allocating
0.00.052.910 I ggml_metal_init: found device: Apple M4
0.00.052.912 I ggml_metal_init: picking default device: Apple M4
0.00.053.456 I ggml_metal_init: using embedded metal library
0.00.055.730 I ggml_metal_init: GPU name:   Apple M4
0.00.055.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.732 I ggml_metal_init: simdgroup reduction   = true
0.00.055.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.733 I ggml_metal_init: has bfloat            = true
0.00.055.733 I ggml_metal_init: use bfloat            = true
0.00.055.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.227 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.229 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.254 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.184 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.185 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.186 I llama_new_context_with_model: graph nodes  = 967
0.00.066.186 I llama_new_context_with_model: graph splits = 2
0.00.066.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.818 I 
0.00.740.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.902 I perplexity: tokenizing the input ..
0.00.748.512 I perplexity: tokenization took 7.609 ms
0.00.748.522 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.046 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.890.354 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.890.374 I llama_perf_context_print:        load time =     731.41 ms
0.00.890.375 I llama_perf_context_print: prompt eval time =     140.30 ms /   128 tokens (    1.10 ms per token,   912.34 tokens per second)
0.00.890.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.376 I llama_perf_context_print:       total time =     149.56 ms /   129 tokens
0.00.890.881 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.077s
sys	0m0.167s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.458 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.753 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.778 I llama_model_loader: - type  f32:  194 tensors
0.00.025.778 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.756 I llm_load_vocab: special tokens cache size = 25
0.00.051.582 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.585 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.586 I llm_load_print_meta: arch             = gptneox
0.00.051.586 I llm_load_print_meta: vocab type       = BPE
0.00.051.586 I llm_load_print_meta: n_vocab          = 50304
0.00.051.586 I llm_load_print_meta: n_merges         = 50009
0.00.051.587 I llm_load_print_meta: vocab_only       = 0
0.00.051.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.587 I llm_load_print_meta: n_embd           = 2048
0.00.051.587 I llm_load_print_meta: n_layer          = 24
0.00.051.590 I llm_load_print_meta: n_head           = 16
0.00.051.591 I llm_load_print_meta: n_head_kv        = 16
0.00.051.603 I llm_load_print_meta: n_rot            = 32
0.00.051.603 I llm_load_print_meta: n_swa            = 0
0.00.051.603 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.604 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.604 I llm_load_print_meta: n_gqa            = 1
0.00.051.605 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.606 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.606 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.607 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.607 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.607 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.607 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.608 I llm_load_print_meta: n_ff             = 8192
0.00.051.608 I llm_load_print_meta: n_expert         = 0
0.00.051.608 I llm_load_print_meta: n_expert_used    = 0
0.00.051.608 I llm_load_print_meta: causal attn      = 1
0.00.051.608 I llm_load_print_meta: pooling type     = 0
0.00.051.608 I llm_load_print_meta: rope type        = 2
0.00.051.609 I llm_load_print_meta: rope scaling     = linear
0.00.051.609 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.609 I llm_load_print_meta: freq_scale_train = 1
0.00.051.609 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.610 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.610 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.610 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.610 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.610 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.610 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.619 I llm_load_print_meta: model type       = 1.4B
0.00.051.620 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.620 I llm_load_print_meta: model params     = 1.41 B
0.00.051.620 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.620 I llm_load_print_meta: general.name     = 1.4B
0.00.051.621 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.623 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.623 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.624 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.624 I llm_load_print_meta: LF token         = 128 ''
0.00.051.624 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.624 I llm_load_print_meta: max token length = 1024
0.00.053.893 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.893 I llm_load_tensors: offloading output layer to GPU
0.00.053.893 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.904 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.905 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.838 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.838 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.838 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.839 I llama_new_context_with_model: n_batch       = 2048
0.00.054.839 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.839 I llama_new_context_with_model: flash_attn    = 0
0.00.054.839 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.840 I llama_new_context_with_model: freq_scale    = 1
0.00.054.840 I ggml_metal_init: allocating
0.00.054.846 I ggml_metal_init: found device: Apple M4
0.00.054.848 I ggml_metal_init: picking default device: Apple M4
0.00.055.387 I ggml_metal_init: using embedded metal library
0.00.057.740 I ggml_metal_init: GPU name:   Apple M4
0.00.057.742 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.743 I ggml_metal_init: simdgroup reduction   = true
0.00.057.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.743 I ggml_metal_init: has bfloat            = true
0.00.057.743 I ggml_metal_init: use bfloat            = true
0.00.057.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.539 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.549 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.574 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.596 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.597 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.598 I llama_new_context_with_model: graph nodes  = 967
0.00.088.598 I llama_new_context_with_model: graph splits = 2
0.00.088.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.218 I main: llama threadpool init, n_threads = 4
0.00.831.260 I 
0.00.831.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.831.300 I 
0.00.831.546 I sampler seed: 1234
0.00.831.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.597 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.600 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.600 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.715.247 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47333.33 tokens per second)
0.01.715.248 I llama_perf_context_print:        load time =     820.75 ms
0.01.715.250 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.715.252 I llama_perf_context_print:        eval time =     825.79 ms /    63 runs   (   13.11 ms per token,    76.29 tokens per second)
0.01.715.253 I llama_perf_context_print:       total time =     884.04 ms /    70 tokens
0.01.715.473 I ggml_metal_free: deallocating

real	0m1.735s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4268 (d405804b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.750 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.190 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.191 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.106 I llama_model_loader: - type  f32:  194 tensors
0.00.024.107 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.557 I llm_load_vocab: special tokens cache size = 25
0.00.050.373 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.375 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.376 I llm_load_print_meta: arch             = gptneox
0.00.050.376 I llm_load_print_meta: vocab type       = BPE
0.00.050.376 I llm_load_print_meta: n_vocab          = 50304
0.00.050.376 I llm_load_print_meta: n_merges         = 50009
0.00.050.377 I llm_load_print_meta: vocab_only       = 0
0.00.050.377 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.377 I llm_load_print_meta: n_embd           = 2048
0.00.050.377 I llm_load_print_meta: n_layer          = 24
0.00.050.380 I llm_load_print_meta: n_head           = 16
0.00.050.381 I llm_load_print_meta: n_head_kv        = 16
0.00.050.393 I llm_load_print_meta: n_rot            = 32
0.00.050.393 I llm_load_print_meta: n_swa            = 0
0.00.050.394 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.394 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.394 I llm_load_print_meta: n_gqa            = 1
0.00.050.395 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.396 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.397 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.397 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.397 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.397 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.397 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.398 I llm_load_print_meta: n_ff             = 8192
0.00.050.398 I llm_load_print_meta: n_expert         = 0
0.00.050.398 I llm_load_print_meta: n_expert_used    = 0
0.00.050.398 I llm_load_print_meta: causal attn      = 1
0.00.050.398 I llm_load_print_meta: pooling type     = 0
0.00.050.398 I llm_load_print_meta: rope type        = 2
0.00.050.399 I llm_load_print_meta: rope scaling     = linear
0.00.050.399 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.399 I llm_load_print_meta: freq_scale_train = 1
0.00.050.399 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.400 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.400 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.400 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.400 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.400 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.400 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.410 I llm_load_print_meta: model type       = 1.4B
0.00.050.410 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.411 I llm_load_print_meta: model params     = 1.41 B
0.00.050.411 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.411 I llm_load_print_meta: general.name     = 1.4B
0.00.050.411 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.411 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: LF token         = 128 ''
0.00.050.412 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: max token length = 1024
0.00.052.708 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.708 I llm_load_tensors: offloading output layer to GPU
0.00.052.708 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.719 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.720 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.675 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.675 I llama_new_context_with_model: n_ctx         = 128
0.00.053.676 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.676 I llama_new_context_with_model: n_batch       = 128
0.00.053.676 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.676 I llama_new_context_with_model: flash_attn    = 0
0.00.053.676 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.677 I llama_new_context_with_model: freq_scale    = 1
0.00.053.677 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.677 I ggml_metal_init: allocating
0.00.053.680 I ggml_metal_init: found device: Apple M4
0.00.053.683 I ggml_metal_init: picking default device: Apple M4
0.00.054.233 I ggml_metal_init: using embedded metal library
0.00.056.533 I ggml_metal_init: GPU name:   Apple M4
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.536 I ggml_metal_init: simdgroup reduction   = true
0.00.056.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.536 I ggml_metal_init: has bfloat            = true
0.00.056.536 I ggml_metal_init: use bfloat            = true
0.00.056.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.212 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.216 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.238 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.174 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.175 I llama_new_context_with_model: graph nodes  = 967
0.00.068.175 I llama_new_context_with_model: graph splits = 2
0.00.068.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.604 I 
0.00.243.678 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.706 I perplexity: tokenizing the input ..
0.00.252.004 I perplexity: tokenization took 8.297 ms
0.00.252.019 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.392.533 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.393.797 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.393.831 I llama_perf_context_print:        load time =     233.85 ms
0.00.393.832 I llama_perf_context_print: prompt eval time =     140.27 ms /   128 tokens (    1.10 ms per token,   912.53 tokens per second)
0.00.393.833 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.393.834 I llama_perf_context_print:       total time =     150.23 ms /   129 tokens
0.00.394.232 I ggml_metal_free: deallocating

real	0m0.410s
user	0m0.080s
sys	0m0.057s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4268 (d405804b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12050aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12050b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12050b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12050bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12050c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12050c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12050ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12050d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12050d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12050ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12050e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12050e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12050f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12050fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1205103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120510ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1205111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120511910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120512030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120512800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120512f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120513640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120513d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120514600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120514d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120514fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1205155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120516260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1205167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120516a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120516f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1205171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120517a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120517f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120518250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1205186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120518b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120519030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1205194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120519970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120519e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12051a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12051a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12051abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12051aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12051b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12051bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12051c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12051ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12051d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12051d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12051dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12051e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12051e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12051f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12051f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12051f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12051fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120520250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120520a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120520d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1205211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120521640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120521ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120521f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120522420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1205228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120522d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120523200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1205236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120523b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120523fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120524480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1205249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120524f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120525470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1205259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120525f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120526460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1205269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120526f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120527450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1205279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120527ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120528440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120528990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120528ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120529430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120529980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120529ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12052a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12052a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12052aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12052b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12052b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12052beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12052c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12051c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12052c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12052d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12052d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12052dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12052e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12052e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12052eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12052f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12052f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12052faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12052fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120530540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120530a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120530fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120531530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1205319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120531e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120532310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1205327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120532c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1205330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120533590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120533a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120533ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120534370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120534810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120534cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120535150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1205355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120535a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120535f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1205363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120536870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120536d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1205371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120537650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120537af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120537f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120538430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1205388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120538d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120539210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1205396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120539b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120539ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12053a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12053a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12053add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12053b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12053b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12053bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12053c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12053c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12053c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12053ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12053d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12053d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12053dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12053e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12053e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12053e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12053ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12053f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12053f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12053fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120540110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1205405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120540a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120540ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120541390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120541830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120541cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120542170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120542610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120542ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120542f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1205433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120543890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120543d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1205441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120544670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120544b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120544fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120545450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1205458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120545d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120546230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1205466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120546b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120547010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1205474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120547950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120547df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120548290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120548730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120548c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1205491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120549720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120549c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120549f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12054a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12054ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12054b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12054b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12054bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12054c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12054c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12054ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12054d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12054d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12054dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12054e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12054e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12054eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12054f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12054f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12054fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120550420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120550970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120550ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120551410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120551960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120551eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120552400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120552950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120552ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1205533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120553940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120553e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1205543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120554930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120554e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1205553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120555920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120555e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1205563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120556910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120556e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1205573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120557900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120557e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1205583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1205588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120558e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120559390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1205598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120559e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12055a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12055a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12055ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12055b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12055b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12055be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12055c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12055c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12055ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12055d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12055d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12055ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12055e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12055e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12055ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12055f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12055f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12055fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120560320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120560870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120560dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120561260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120561700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120561ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120562040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1205624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120562980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120562e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1205632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120563760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120563c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1205640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120564540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1205649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120564f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120565650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120565d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120566490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120566bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120566e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120567660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120567920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120567f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.192.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120306100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120306570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1203069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120306e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1203072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120307730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120307ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120308010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120308480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1203088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120308d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120309450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120309f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12030a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12030af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12030b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12030bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12030c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12030cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12030d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12030da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12030e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12030e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12030ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12030f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12030f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12030fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120310070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1203104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120310950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120310dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1203112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120311760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120311a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120311e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120312300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120312770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120312be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120313050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1203134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120313930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120313da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120314210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120314680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120314af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120314f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1203153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120315840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120315cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120316120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120316590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120316a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120316e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1203172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120317750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120317bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120318130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120318630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120318aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120318f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120319380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1203197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120319c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12031a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12031a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12031a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12031ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12031b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12031b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12031bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12031bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12031c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12031c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12031cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12031d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12031d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12031da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12031def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12031e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12031e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12031ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12031f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12031f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12031f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12031fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120320270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1203206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120320b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120320fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120321430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1203218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120321d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120322180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1203225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120322a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120322ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120323340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1203237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120323c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120324090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120324500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120324970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120324de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120325250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1203256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120325b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120325fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120326410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120326880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120326cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120327160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1203275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120327a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120327eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120328320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120328790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120328c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120329070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1203294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120329950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120329dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12032a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12032a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12032ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12032af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12032b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12032b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12032bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12032c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12032c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12032ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12032ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12032d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12032d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12032dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12032e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12032e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12032e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12032eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12032f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12032f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12032faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12032ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1203303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120330840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120330cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120331120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120331590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120331a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120331e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1203322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120332750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120332bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120333030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1203334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120333910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120333d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1203341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120334660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120334ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120334f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1203353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120335820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120335c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120336100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120336570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1203369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120336e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1203372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120337730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120337ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120338010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120338480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1203388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120338d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1203391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120339640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120339ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120339f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12033a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12033a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12033ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12033b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12033b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12033b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12033be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12033c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12033c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12033cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12033cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12033d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12033d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12033dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12033e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12033e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12033ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12033ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12033f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12033f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12033fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1203400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120340530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1203409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120340e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120341280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1203416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120341b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120341fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120342440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120342fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120343270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120343530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1203439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120343e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120344280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1203446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120344b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120344fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120345440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1203458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120345d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120346190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120346600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120346a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120346ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120347350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1203477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120347c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1203480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120348510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120348980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120348df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120349260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1203496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120349b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120349fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12034a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12034a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12034ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12034b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12034b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12034ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12034bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12034c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12034c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12034cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12034d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12034d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12034d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12034ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12034e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12034e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12034eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12034ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12034f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12034f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12034fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120350150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1203505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120350a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120350ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120351310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120351780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120351bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120352060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1203524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120352940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120352db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120353220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120353690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120353b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120353f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1203543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120354850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120354cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120355130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1203555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120355a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120355e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1203562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120356e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120357550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120357c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120358390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120358650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120358910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120358d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1203591f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120306010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120306480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1203068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120306d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1203071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120307640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120307ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120307f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120308390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120308800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120308c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120309250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120309b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12030a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12030aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12030b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12030b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12030bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12030c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12030cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12030d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12030ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12030e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12030eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12030f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12030f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12030fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12030ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120310450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1203108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120310d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1203111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120311610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1203118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120311d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1203121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120312a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120312f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1203137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120313c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1203140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120314530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1203149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120314e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120315280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1203156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120316440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1203168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120316d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120317190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120317600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120317a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120317ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120318350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1203187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120318c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1203190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120319510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120319980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120319df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12031a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12031a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12031ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12031afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12031b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12031b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12031bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12031c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12031c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12031ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12031cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12031d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12031d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12031dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12031e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12031e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12031e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12031edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12031f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12031f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12031fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12031ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120320400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120320870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120320ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120321150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1203215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120321a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120321ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120322310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120322780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120322bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120323060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1203234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120323940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120323db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120324220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120324690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120324b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120324f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1203253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120325850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120325cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120326130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1203265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120326a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120326e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1203272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120327760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120327bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120328040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1203284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120328920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120328d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120329200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120329670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120329ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120329f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12032a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12032a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12032aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12032b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12032b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12032b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12032be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12032c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12032c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12032cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12032d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12032d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12032d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12032dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12032e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12032e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12032eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12032ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12032f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12032f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12032fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1203300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120330560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1203309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120330e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1203312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120331720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120331b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120332000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120332470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1203328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120332d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1203331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120333630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120333aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120333f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120334380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1203347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120334c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1203350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120335540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1203359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120335e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120336290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120336700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120336b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120336fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120337450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1203378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120337d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1203381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120338610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120338a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120338ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120339360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1203397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120339c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12033a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12033a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12033a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12033ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12033b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12033b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12033bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12033bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12033c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12033c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12033cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12033d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12033d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12033da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12033ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12033e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12033e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12033ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12033f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12033f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12033f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12033fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120340250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1203406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120340b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120340fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120341410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120341880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120341cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120342160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1203428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120342d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1203431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120343630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120343aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120343f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120344380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1203447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120344c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1203450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120345540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1203459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120345e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120346290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120346700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120346b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120346fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120347450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1203478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120347d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1203481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120348610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120348a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120348ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120349360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1203497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120349c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12034a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12034a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12034a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12034ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12034b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12034b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12034bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12034bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12034c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12034c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12034cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12034d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12034d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12034da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12034ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12034e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12034e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12034ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12034f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12034f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12034f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12034fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120350250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1203506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120350b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120350fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120351410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120351880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120351cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120352160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1203525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120352a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120352eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120353320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120353790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120353c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120354070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1203544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120354950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120354dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120355230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1203556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120355b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120355f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1203567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120356ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1203575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120357cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120358120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120358590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120358a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120358e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.910s
user	0m0.297s
sys	0m0.358s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4268 (d405804b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a60d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a60dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a60e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a60e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a60ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a60f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a60f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a60fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a6104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a6109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a610ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a6113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a611ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a612670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a6135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a613cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a6143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a614b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a6152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a6159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a616110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a616830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a6170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a6177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a617ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a6180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a619270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a6199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a619c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a61a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a61aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a61ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a61b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a61b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a61bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a61bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a61c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a61c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a61cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a61d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a61d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a61d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a61df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a61e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a61eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a61f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a61fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a6200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a620d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a621b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a622450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a622710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a622d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a6237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a623c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a624110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a6245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a624a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a624ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a625cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a626610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a626ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a626f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a6274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a6279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a627f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a628490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a6289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a628f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a629480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a6299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a629f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a62a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a62a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a62af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a62b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a62b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a62bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a62c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a62c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a62cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a62d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a62d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a62dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a62e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a62e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a62eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a61ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a62f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a62faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a630590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a630ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a631030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a631580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a631ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a632570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a632ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a633010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a633560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a634000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a6344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a634940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a636060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a636500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a6369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a636e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a6372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a637780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a637c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a6380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a638560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a638ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a6397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a63a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a63a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a63aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a63af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a63b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a63b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a63bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a63c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a63c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a63cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a63cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a63d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a63d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a63dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a63e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a63e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a63eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a63efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a63f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a63f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a63fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a640240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a6406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a640b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a641020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a6414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a641960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a6422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a642740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a643080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a643520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a6439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a643e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a644300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a6447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a644c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a6450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a645a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a646360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a646800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a646ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a647140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a6475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a647a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a647f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a6483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a648860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a648d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a6491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a649ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a649f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a64a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a64a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a64ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a64b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a64b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a64bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a64c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a64c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a64ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a64d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a64d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a64dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a64e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a64e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a64eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a64f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a64f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a64fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a6502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a650760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a650f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a651460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a6519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a6529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a6583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a6593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a65a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a65a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a65ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a65b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a65b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a65be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a65c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a65c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a65ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a65d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a65d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a65de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a65e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a65e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a65ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a65f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a65f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a65fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a6608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a6618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a662350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a6628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a662df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a663340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a663890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a663d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a6641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a664670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a664b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a664fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a665450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a6658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a665d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a666230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a6666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a666b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a667010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a6674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a667a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a668120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a668840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a668f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a669680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a669940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a66a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a66a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a66aa00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a7087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a708c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a7090c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a709530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a7099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a709e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a70a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a70a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a70ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a70b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a70b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a70bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a70c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a70cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a70d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a70ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a70e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a70ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a70f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a70fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a710220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a710940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a711060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a711780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a711ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a712160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a712420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a712890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a712d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a713170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a713670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a713b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a713ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a7142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a714720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a714b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a7150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a7155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a715af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a715ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a7164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a7169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a716ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a7173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a7178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a717d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a7181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a718640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a718ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a718f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a719390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a719800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a719c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a71a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a71a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a71ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a71b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a71b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a71ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a71c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a71c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a71cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a71d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a71d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a71d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a71de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a71e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a71e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a71ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a71f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a71f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a71fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a71fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a7203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a720940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a720e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a7213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a721930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a7223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a722e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a7233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a723910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a723e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a7243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a724900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a724e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a7253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a7258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a725e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a726390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a7268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a726e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a727380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a7278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a727e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a7288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a728e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a729360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a7298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a729e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a72a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a72a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a72adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a72b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a72b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a72bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a72c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a72c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a72cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a72d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a72d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a72dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a72e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a72e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a72ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a72eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a72f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a72f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a72fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a730600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a730aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a730f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a7313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a731880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a731d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a7321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a732660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a732b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a732fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a733440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a7338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a733d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a734220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a7346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a734b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a735000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a7354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a735940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a735de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a736280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a736720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a736bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a737060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a737500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a7379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a737e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a7382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a738780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a738c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a7390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a739560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a739a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a739ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a73a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a73a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a73ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a73b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a73b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a73ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a73bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a73c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a73c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a73cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a73d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a73d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a73dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a73df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a73e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a73e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a73ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a73f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a73f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a73fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a73ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a740460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a740900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a741240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a7416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a741b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a742020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a7424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a742960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a742e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a7432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a743be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a744080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a744520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a744a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a744fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a745510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a745a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a745d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a746330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a746940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a746f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a747740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a747be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a747ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a7484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a748ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a749140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a7495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a749a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a74a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a74a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a74acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a74b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a74b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a74bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a74c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a74c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a74ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a74d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a74d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a74dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a74e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a74e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a74ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a74f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a74f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a74fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a7501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a750720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a750c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a7511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a751710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a751c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a7521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a752700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a752c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a7531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a7536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a753c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a754190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a7546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a754c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a755180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a7556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a755c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a756170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a7566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a756c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a757160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a7576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a757c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a758150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a7586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a758bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a759140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a759690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a759be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a75a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a75a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a75abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a75b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a75b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a75bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a75c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a75c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a75cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a75d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a75d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a75d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a75de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a75e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a75e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a75ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a75f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a75f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a75f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a75fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a760330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a7607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a760d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a761440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a761b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a762280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a7629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a762c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a763450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a763710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a763d20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a7087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a708c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a7090c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a709530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a7099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a709e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a70a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a70a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a70ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a70afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a70b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a70ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a70c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a70ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a70d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a70d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a70e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a70e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a70ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a70f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a70fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a710590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a710c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a711370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a711a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a711ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a712340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a7127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a712c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a713090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a713500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a713970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a713de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a7140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a7156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a716d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a7175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a717a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a717ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a718330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a7187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a718c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a719080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a7194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a719960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a71a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a71a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a71ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a71af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a71b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a71b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a71bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a71c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a71c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a71ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a71cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a71d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a71d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a71dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a71e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a71e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a71e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a71edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a71f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a71f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a71fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a71ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a7203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a720850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a720cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a721130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a7215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a721a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a7222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a722760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a722bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a723040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a7234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a723920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a723d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a724200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a724670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a724ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a7253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a725830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a725ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a726110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a7269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a726e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a7272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a727740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a727bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a728020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a728490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a728900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a728d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a7291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a729650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a729ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a729f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a72a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a72a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a72ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a72b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a72b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a72b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a72be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a72c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a72c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a72cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a72d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a72d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a72d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a72dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a72e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a72e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a72eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a72ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a72f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a72f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a72fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a7300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a730540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a7309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a730e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a731290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a731700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a731b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a731fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a732450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a7328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a732d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a7331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a733610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a733a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a733ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a734360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a7347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a734c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a7350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a735520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a735990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a735e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a736270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a7366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a736b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a737430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a7378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a737d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a738180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a7385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a738a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a738ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a739340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a7397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a739c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a73a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a73a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a73a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a73ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a73b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a73b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a73bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a73bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a73c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a73c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a73ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a73d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a73d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a73da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a73deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a73e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a73e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a73ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a73f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a73f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a73f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a73fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a740230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a7406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a740b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a740f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a7413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a741860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a7425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a742a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a742e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a743300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a743770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a743be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a744050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a7444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a744930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a7450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a745520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a745e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a746270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a7466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a746b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a746fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a747430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a7478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a747d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a748180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a7485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a748a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a748ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a749340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a7497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a749c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a74a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a74a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a74a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a74ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a74b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a74b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a74bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a74bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a74c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a74c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a74ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a74d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a74d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a74da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a74deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a74e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a74e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a74ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a74f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a74f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a74f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a74fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a750230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a750b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a750f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a7513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a751860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a751cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a752140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a7525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a752a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a752e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a753300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a753770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a753be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a754050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a7544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a754930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a754da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a755210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a755680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a755af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a755f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a7563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a756840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a756cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a757120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a757590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a757e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a7582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a758750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a758e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a759530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a759c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a75a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a75a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a75abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a75b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a75b4d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.968s
user	0m0.243s
sys	0m0.156s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.79 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.34 sec*proc (2 tests)

Total Test time (real) =   1.35 sec
        1.37 real         0.73 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
