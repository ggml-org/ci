### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.71 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.66 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.33 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.95 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.25 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.96 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.98 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  191.04 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.49 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 237.62 sec*proc (28 tests)

Total Test time (real) = 237.63 sec

real	3m57.782s
user	8m15.483s
sys	0m7.180s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.10 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.44 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.43 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   28.89 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.16 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.28 sec*proc (28 tests)

Total Test time (real) =  51.30 sec

real	0m51.306s
user	1m16.438s
sys	0m5.311s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.079 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.144 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.154 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.155 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.156 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.157 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.159 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.159 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.160 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.160 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.161 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.164 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.165 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.168 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.169 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.169 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.170 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.171 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.491 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.582 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.584 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.584 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.585 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.585 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.585 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.586 I llama_model_loader: - type  f32:  124 tensors
0.00.027.586 I llama_model_loader: - type  f16:   73 tensors
0.00.027.587 I print_info: file format = GGUF V3 (latest)
0.00.027.588 I print_info: file type   = F16
0.00.027.589 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.771 I load: special tokens cache size = 5
0.00.033.991 I load: token to piece cache size = 0.2032 MB
0.00.033.995 I print_info: arch             = bert
0.00.033.996 I print_info: vocab_only       = 0
0.00.033.996 I print_info: n_ctx_train      = 512
0.00.033.996 I print_info: n_embd           = 384
0.00.033.997 I print_info: n_layer          = 12
0.00.034.000 I print_info: n_head           = 12
0.00.034.001 I print_info: n_head_kv        = 12
0.00.034.001 I print_info: n_rot            = 32
0.00.034.002 I print_info: n_swa            = 0
0.00.034.002 I print_info: n_embd_head_k    = 32
0.00.034.002 I print_info: n_embd_head_v    = 32
0.00.034.003 I print_info: n_gqa            = 1
0.00.034.004 I print_info: n_embd_k_gqa     = 384
0.00.034.006 I print_info: n_embd_v_gqa     = 384
0.00.034.006 I print_info: f_norm_eps       = 1.0e-12
0.00.034.007 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.008 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.008 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.008 I print_info: f_logit_scale    = 0.0e+00
0.00.034.009 I print_info: n_ff             = 1536
0.00.034.009 I print_info: n_expert         = 0
0.00.034.009 I print_info: n_expert_used    = 0
0.00.034.010 I print_info: causal attn      = 0
0.00.034.010 I print_info: pooling type     = 2
0.00.034.010 I print_info: rope type        = 2
0.00.034.010 I print_info: rope scaling     = linear
0.00.034.011 I print_info: freq_base_train  = 10000.0
0.00.034.011 I print_info: freq_scale_train = 1
0.00.034.012 I print_info: n_ctx_orig_yarn  = 512
0.00.034.012 I print_info: rope_finetuned   = unknown
0.00.034.012 I print_info: ssm_d_conv       = 0
0.00.034.012 I print_info: ssm_d_inner      = 0
0.00.034.013 I print_info: ssm_d_state      = 0
0.00.034.013 I print_info: ssm_dt_rank      = 0
0.00.034.013 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.013 I print_info: model type       = 33M
0.00.034.014 I print_info: model params     = 33.21 M
0.00.034.014 I print_info: general.name     = Bge Small
0.00.034.015 I print_info: vocab type       = WPM
0.00.034.015 I print_info: n_vocab          = 30522
0.00.034.015 I print_info: n_merges         = 0
0.00.034.016 I print_info: BOS token        = 101 '[CLS]'
0.00.034.016 I print_info: UNK token        = 100 '[UNK]'
0.00.034.016 I print_info: SEP token        = 102 '[SEP]'
0.00.034.019 I print_info: PAD token        = 0 '[PAD]'
0.00.034.019 I print_info: MASK token       = 103 '[MASK]'
0.00.034.019 I print_info: LF token         = 0 '[PAD]'
0.00.034.020 I print_info: max token length = 21
0.00.037.042 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.043 I load_tensors: offloading output layer to GPU
0.00.037.044 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.067 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.069 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.037.303 I llama_init_from_model: n_seq_max     = 1
0.00.037.304 I llama_init_from_model: n_ctx         = 512
0.00.037.304 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.304 I llama_init_from_model: n_batch       = 2048
0.00.037.305 I llama_init_from_model: n_ubatch      = 2048
0.00.037.305 I llama_init_from_model: flash_attn    = 0
0.00.037.305 I llama_init_from_model: freq_base     = 10000.0
0.00.037.306 I llama_init_from_model: freq_scale    = 1
0.00.037.306 I ggml_metal_init: allocating
0.00.037.311 I ggml_metal_init: found device: Apple M4
0.00.037.314 I ggml_metal_init: picking default device: Apple M4
0.00.037.967 I ggml_metal_init: using embedded metal library
0.00.041.823 I ggml_metal_init: GPU name:   Apple M4
0.00.041.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.827 I ggml_metal_init: simdgroup reduction   = true
0.00.041.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.827 I ggml_metal_init: has residency sets    = true
0.00.041.827 I ggml_metal_init: has bfloat            = true
0.00.041.828 I ggml_metal_init: use bfloat            = true
0.00.041.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.880 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.559 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.562 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.563 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.055.691 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.055.692 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.055.692 I llama_init_from_model: graph nodes  = 429
0.00.055.693 I llama_init_from_model: graph splits = 2
0.00.055.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.055.694 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.348 I 
0.00.061.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.061.998 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.048 I llama_perf_context_print:        load time =      44.53 ms
0.00.067.049 I llama_perf_context_print: prompt eval time =       4.89 ms /     9 tokens (    0.54 ms per token,  1840.11 tokens per second)
0.00.067.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.050 I llama_perf_context_print:       total time =       5.70 ms /    10 tokens
0.00.067.194 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.048s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.379 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.101 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.014.105 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.107 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.014.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.108 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.014.108 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.014.108 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.014.109 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.014.110 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.014.110 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.014.110 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.014.112 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.014.114 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.014.114 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.014.115 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.014.115 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.014.115 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.014.116 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.599 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.017.249 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.017.250 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.017.250 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.017.250 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.017.251 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.017.251 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.017.251 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.017.252 I llama_model_loader: - type  f32:  124 tensors
0.00.017.252 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.253 I print_info: file format = GGUF V3 (latest)
0.00.017.253 I print_info: file type   = Q8_0
0.00.017.254 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.019.687 I load: special tokens cache size = 5
0.00.020.996 I load: token to piece cache size = 0.2032 MB
0.00.020.999 I print_info: arch             = bert
0.00.020.999 I print_info: vocab_only       = 0
0.00.021.000 I print_info: n_ctx_train      = 512
0.00.021.000 I print_info: n_embd           = 384
0.00.021.000 I print_info: n_layer          = 12
0.00.021.004 I print_info: n_head           = 12
0.00.021.004 I print_info: n_head_kv        = 12
0.00.021.004 I print_info: n_rot            = 32
0.00.021.005 I print_info: n_swa            = 0
0.00.021.005 I print_info: n_embd_head_k    = 32
0.00.021.005 I print_info: n_embd_head_v    = 32
0.00.021.006 I print_info: n_gqa            = 1
0.00.021.006 I print_info: n_embd_k_gqa     = 384
0.00.021.007 I print_info: n_embd_v_gqa     = 384
0.00.021.008 I print_info: f_norm_eps       = 1.0e-12
0.00.021.008 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.021.008 I print_info: f_clamp_kqv      = 0.0e+00
0.00.021.008 I print_info: f_max_alibi_bias = 0.0e+00
0.00.021.009 I print_info: f_logit_scale    = 0.0e+00
0.00.021.009 I print_info: n_ff             = 1536
0.00.021.009 I print_info: n_expert         = 0
0.00.021.009 I print_info: n_expert_used    = 0
0.00.021.010 I print_info: causal attn      = 0
0.00.021.010 I print_info: pooling type     = 2
0.00.021.010 I print_info: rope type        = 2
0.00.021.010 I print_info: rope scaling     = linear
0.00.021.010 I print_info: freq_base_train  = 10000.0
0.00.021.011 I print_info: freq_scale_train = 1
0.00.021.011 I print_info: n_ctx_orig_yarn  = 512
0.00.021.011 I print_info: rope_finetuned   = unknown
0.00.021.011 I print_info: ssm_d_conv       = 0
0.00.021.011 I print_info: ssm_d_inner      = 0
0.00.021.011 I print_info: ssm_d_state      = 0
0.00.021.012 I print_info: ssm_dt_rank      = 0
0.00.021.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.021.012 I print_info: model type       = 33M
0.00.021.012 I print_info: model params     = 33.21 M
0.00.021.012 I print_info: general.name     = Bge Small
0.00.021.015 I print_info: vocab type       = WPM
0.00.021.015 I print_info: n_vocab          = 30522
0.00.021.016 I print_info: n_merges         = 0
0.00.021.016 I print_info: BOS token        = 101 '[CLS]'
0.00.021.016 I print_info: UNK token        = 100 '[UNK]'
0.00.021.016 I print_info: SEP token        = 102 '[SEP]'
0.00.021.016 I print_info: PAD token        = 0 '[PAD]'
0.00.021.016 I print_info: MASK token       = 103 '[MASK]'
0.00.021.017 I print_info: LF token         = 0 '[PAD]'
0.00.021.017 I print_info: max token length = 21
0.00.022.872 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.873 I load_tensors: offloading output layer to GPU
0.00.022.873 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.880 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.880 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.023.077 I llama_init_from_model: n_seq_max     = 1
0.00.023.077 I llama_init_from_model: n_ctx         = 512
0.00.023.077 I llama_init_from_model: n_ctx_per_seq = 512
0.00.023.078 I llama_init_from_model: n_batch       = 2048
0.00.023.078 I llama_init_from_model: n_ubatch      = 2048
0.00.023.078 I llama_init_from_model: flash_attn    = 0
0.00.023.078 I llama_init_from_model: freq_base     = 10000.0
0.00.023.079 I llama_init_from_model: freq_scale    = 1
0.00.023.079 I ggml_metal_init: allocating
0.00.023.083 I ggml_metal_init: found device: Apple M4
0.00.023.085 I ggml_metal_init: picking default device: Apple M4
0.00.023.609 I ggml_metal_init: using embedded metal library
0.00.026.119 I ggml_metal_init: GPU name:   Apple M4
0.00.026.121 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.026.121 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.026.122 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.026.122 I ggml_metal_init: simdgroup reduction   = true
0.00.026.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.026.122 I ggml_metal_init: has residency sets    = true
0.00.026.122 I ggml_metal_init: has bfloat            = true
0.00.026.123 I ggml_metal_init: use bfloat            = true
0.00.026.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.026.124 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.036.678 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.302 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.304 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.307 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.038.351 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.038.352 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.038.352 I llama_init_from_model: graph nodes  = 429
0.00.038.353 I llama_init_from_model: graph splits = 2
0.00.038.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.038.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.495 I 
0.00.042.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.043.051 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.047.489 I llama_perf_context_print:        load time =      31.11 ms
0.00.047.491 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2093.51 tokens per second)
0.00.047.492 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.492 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.047.652 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.161 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.095 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.015 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.024 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.029 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.030 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.030 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.032 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.032 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.033 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.034 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.034 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.038 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.038 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.039 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.040 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.237 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.743 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.744 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.744 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.744 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.745 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.745 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.745 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.746 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.746 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.747 I llama_model_loader: - type  f32:   40 tensors
0.00.051.747 I llama_model_loader: - type  f16:   30 tensors
0.00.051.748 I print_info: file format = GGUF V3 (latest)
0.00.051.749 I print_info: file type   = F16
0.00.051.750 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.068.064 W load: empty token at index 5
0.00.072.651 W load: model vocab missing newline token, using special_pad_id instead
0.00.074.062 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.094 I load: special tokens cache size = 5
0.00.337.459 I load: token to piece cache size = 1.5060 MB
0.00.337.465 I print_info: arch             = jina-bert-v2
0.00.337.465 I print_info: vocab_only       = 0
0.00.337.466 I print_info: n_ctx_train      = 8192
0.00.337.466 I print_info: n_embd           = 384
0.00.337.466 I print_info: n_layer          = 4
0.00.337.473 I print_info: n_head           = 12
0.00.337.473 I print_info: n_head_kv        = 12
0.00.337.473 I print_info: n_rot            = 32
0.00.337.474 I print_info: n_swa            = 0
0.00.337.474 I print_info: n_embd_head_k    = 32
0.00.337.474 I print_info: n_embd_head_v    = 32
0.00.337.474 I print_info: n_gqa            = 1
0.00.337.477 I print_info: n_embd_k_gqa     = 384
0.00.337.480 I print_info: n_embd_v_gqa     = 384
0.00.337.480 I print_info: f_norm_eps       = 1.0e-12
0.00.337.481 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.337.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.337.482 I print_info: f_max_alibi_bias = 8.0e+00
0.00.337.482 I print_info: f_logit_scale    = 0.0e+00
0.00.337.483 I print_info: n_ff             = 1536
0.00.337.483 I print_info: n_expert         = 0
0.00.337.483 I print_info: n_expert_used    = 0
0.00.337.483 I print_info: causal attn      = 0
0.00.337.484 I print_info: pooling type     = -1
0.00.337.484 I print_info: rope type        = -1
0.00.337.484 I print_info: rope scaling     = linear
0.00.337.485 I print_info: freq_base_train  = 10000.0
0.00.337.485 I print_info: freq_scale_train = 1
0.00.337.486 I print_info: n_ctx_orig_yarn  = 8192
0.00.337.486 I print_info: rope_finetuned   = unknown
0.00.337.486 I print_info: ssm_d_conv       = 0
0.00.337.486 I print_info: ssm_d_inner      = 0
0.00.337.487 I print_info: ssm_d_state      = 0
0.00.337.487 I print_info: ssm_dt_rank      = 0
0.00.337.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.337.487 I print_info: model type       = 33M
0.00.337.488 I print_info: model params     = 32.90 M
0.00.337.488 I print_info: general.name     = Jina Bert Implementation
0.00.337.489 I print_info: vocab type       = BPE
0.00.337.489 I print_info: n_vocab          = 61056
0.00.337.489 I print_info: n_merges         = 39382
0.00.337.490 I print_info: BOS token        = 0 '<s>'
0.00.337.490 I print_info: EOS token        = 2 '</s>'
0.00.337.490 I print_info: UNK token        = 3 '<unk>'
0.00.337.490 I print_info: SEP token        = 2 '</s>'
0.00.337.490 I print_info: PAD token        = 1 '<pad>'
0.00.337.491 I print_info: MASK token       = 4 '<mask>'
0.00.337.491 I print_info: EOG token        = 2 '</s>'
0.00.337.492 I print_info: max token length = 45
0.00.339.857 I load_tensors: offloading 4 repeating layers to GPU
0.00.339.859 I load_tensors: offloading output layer to GPU
0.00.339.859 I load_tensors: offloaded 5/5 layers to GPU
0.00.339.885 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.339.886 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.340.359 I llama_init_from_model: n_seq_max     = 1
0.00.340.360 I llama_init_from_model: n_ctx         = 8192
0.00.340.360 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.340.361 I llama_init_from_model: n_batch       = 2048
0.00.340.361 I llama_init_from_model: n_ubatch      = 2048
0.00.340.361 I llama_init_from_model: flash_attn    = 0
0.00.340.361 I llama_init_from_model: freq_base     = 10000.0
0.00.340.362 I llama_init_from_model: freq_scale    = 1
0.00.340.362 I ggml_metal_init: allocating
0.00.340.366 I ggml_metal_init: found device: Apple M4
0.00.340.368 I ggml_metal_init: picking default device: Apple M4
0.00.341.298 I ggml_metal_init: using embedded metal library
0.00.344.207 I ggml_metal_init: GPU name:   Apple M4
0.00.344.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.210 I ggml_metal_init: simdgroup reduction   = true
0.00.344.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.210 I ggml_metal_init: has residency sets    = true
0.00.344.210 I ggml_metal_init: has bfloat            = true
0.00.344.212 I ggml_metal_init: use bfloat            = true
0.00.344.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.806 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.356.879 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.356.881 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.356.883 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.363.290 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.363.292 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.363.292 I llama_init_from_model: graph nodes  = 154
0.00.363.293 I llama_init_from_model: graph splits = 2
0.00.363.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.363.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.370.630 I 
0.00.370.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.370.965 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.370.966 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.370.977 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.370.977 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.370.981 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.370.981 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.371.501 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.375.104 I llama_perf_context_print:        load time =     346.53 ms
0.00.375.105 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17308.77 tokens per second)
0.00.375.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.375.107 I llama_perf_context_print:       total time =       4.48 ms /    63 tokens
0.00.375.352 I ggml_metal_free: deallocating

real	0m1.110s
user	0m0.345s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.157 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.325 I main: llama backend init
0.00.000.331 I main: load the model and apply lora adapter, if any
0.00.027.636 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.791 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.818 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.818 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.819 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.819 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.830 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.831 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.833 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.839 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.755 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.509 I llama_model_loader: - type  f32:  194 tensors
0.00.059.509 I llama_model_loader: - type  f16:   98 tensors
0.00.059.514 I print_info: file format = GGUF V3 (latest)
0.00.059.515 I print_info: file type   = all F32 (guessed)
0.00.059.517 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.235 I load: special tokens cache size = 25
0.00.096.647 I load: token to piece cache size = 0.2984 MB
0.00.096.650 I print_info: arch             = gptneox
0.00.096.650 I print_info: vocab_only       = 0
0.00.096.650 I print_info: n_ctx_train      = 2048
0.00.096.650 I print_info: n_embd           = 2048
0.00.096.650 I print_info: n_layer          = 24
0.00.096.653 I print_info: n_head           = 16
0.00.096.654 I print_info: n_head_kv        = 16
0.00.096.654 I print_info: n_rot            = 32
0.00.096.654 I print_info: n_swa            = 0
0.00.096.655 I print_info: n_embd_head_k    = 128
0.00.096.655 I print_info: n_embd_head_v    = 128
0.00.096.655 I print_info: n_gqa            = 1
0.00.096.656 I print_info: n_embd_k_gqa     = 2048
0.00.096.657 I print_info: n_embd_v_gqa     = 2048
0.00.096.657 I print_info: f_norm_eps       = 1.0e-05
0.00.096.659 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.659 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.659 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.660 I print_info: f_logit_scale    = 0.0e+00
0.00.096.660 I print_info: n_ff             = 8192
0.00.096.660 I print_info: n_expert         = 0
0.00.096.661 I print_info: n_expert_used    = 0
0.00.096.661 I print_info: causal attn      = 1
0.00.096.661 I print_info: pooling type     = 0
0.00.096.661 I print_info: rope type        = 2
0.00.096.661 I print_info: rope scaling     = linear
0.00.096.662 I print_info: freq_base_train  = 10000.0
0.00.096.662 I print_info: freq_scale_train = 1
0.00.096.662 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.664 I print_info: rope_finetuned   = unknown
0.00.096.664 I print_info: ssm_d_conv       = 0
0.00.096.664 I print_info: ssm_d_inner      = 0
0.00.096.665 I print_info: ssm_d_state      = 0
0.00.096.665 I print_info: ssm_dt_rank      = 0
0.00.096.665 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.665 I print_info: model type       = 1.4B
0.00.096.665 I print_info: model params     = 1.41 B
0.00.096.665 I print_info: general.name     = 1.4B
0.00.096.666 I print_info: vocab type       = BPE
0.00.096.666 I print_info: n_vocab          = 50304
0.00.096.666 I print_info: n_merges         = 50009
0.00.096.667 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.667 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.667 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.667 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.672 I print_info: LF token         = 128 'Ä'
0.00.096.673 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.673 I print_info: max token length = 1024
0.00.131.819 I load_tensors: offloading 24 repeating layers to GPU
0.00.131.823 I load_tensors: offloading output layer to GPU
0.00.131.824 I load_tensors: offloaded 25/25 layers to GPU
0.00.131.847 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.131.848 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.132.123 I llama_init_from_model: n_seq_max     = 1
0.00.132.124 I llama_init_from_model: n_ctx         = 2048
0.00.132.124 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.132.125 I llama_init_from_model: n_batch       = 2048
0.00.132.125 I llama_init_from_model: n_ubatch      = 512
0.00.132.125 I llama_init_from_model: flash_attn    = 0
0.00.132.125 I llama_init_from_model: freq_base     = 10000.0
0.00.132.126 I llama_init_from_model: freq_scale    = 1
0.00.132.126 I ggml_metal_init: allocating
0.00.132.144 I ggml_metal_init: found device: Apple M4
0.00.132.147 I ggml_metal_init: picking default device: Apple M4
0.00.132.716 I ggml_metal_init: using embedded metal library
0.00.141.667 I ggml_metal_init: GPU name:   Apple M4
0.00.141.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.141.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.141.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.141.670 I ggml_metal_init: simdgroup reduction   = true
0.00.141.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.141.670 I ggml_metal_init: has residency sets    = true
0.00.141.670 I ggml_metal_init: has bfloat            = true
0.00.141.670 I ggml_metal_init: use bfloat            = true
0.00.141.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.141.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.166.437 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.194.101 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.194.108 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.194.129 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.197.998 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.198.002 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.198.002 I llama_init_from_model: graph nodes  = 967
0.00.198.002 I llama_init_from_model: graph splits = 2
0.00.198.006 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.198.134 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.198.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.264.024 I main: llama threadpool init, n_threads = 4
0.00.264.071 I 
0.00.264.101 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.264.103 I 
0.00.264.166 I sampler seed: 1234
0.00.264.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.264.195 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.264.196 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.264.196 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.102.107 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.02.102.108 I llama_perf_context_print:        load time =     235.36 ms
0.02.102.110 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.06 tokens per second)
0.02.102.111 I llama_perf_context_print:        eval time =    1791.26 ms /    63 runs   (   28.43 ms per token,    35.17 tokens per second)
0.02.102.112 I llama_perf_context_print:       total time =    1839.11 ms /    70 tokens
0.02.102.357 I ggml_metal_free: deallocating

real	0m2.404s
user	0m0.146s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.304 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.141 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.157 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.158 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.159 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.159 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.936 I llama_model_loader: - type  f32:  194 tensors
0.00.056.937 I llama_model_loader: - type  f16:   98 tensors
0.00.056.937 I print_info: file format = GGUF V3 (latest)
0.00.056.938 I print_info: file type   = all F32 (guessed)
0.00.056.940 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.328 I load: special tokens cache size = 25
0.00.091.178 I load: token to piece cache size = 0.2984 MB
0.00.091.182 I print_info: arch             = gptneox
0.00.091.182 I print_info: vocab_only       = 0
0.00.091.182 I print_info: n_ctx_train      = 2048
0.00.091.182 I print_info: n_embd           = 2048
0.00.091.182 I print_info: n_layer          = 24
0.00.091.186 I print_info: n_head           = 16
0.00.091.187 I print_info: n_head_kv        = 16
0.00.091.187 I print_info: n_rot            = 32
0.00.091.187 I print_info: n_swa            = 0
0.00.091.187 I print_info: n_embd_head_k    = 128
0.00.091.187 I print_info: n_embd_head_v    = 128
0.00.091.190 I print_info: n_gqa            = 1
0.00.091.191 I print_info: n_embd_k_gqa     = 2048
0.00.091.191 I print_info: n_embd_v_gqa     = 2048
0.00.091.192 I print_info: f_norm_eps       = 1.0e-05
0.00.091.192 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.192 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.193 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.193 I print_info: f_logit_scale    = 0.0e+00
0.00.091.193 I print_info: n_ff             = 8192
0.00.091.193 I print_info: n_expert         = 0
0.00.091.193 I print_info: n_expert_used    = 0
0.00.091.194 I print_info: causal attn      = 1
0.00.091.194 I print_info: pooling type     = 0
0.00.091.194 I print_info: rope type        = 2
0.00.091.194 I print_info: rope scaling     = linear
0.00.091.194 I print_info: freq_base_train  = 10000.0
0.00.091.195 I print_info: freq_scale_train = 1
0.00.091.196 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.196 I print_info: rope_finetuned   = unknown
0.00.091.196 I print_info: ssm_d_conv       = 0
0.00.091.197 I print_info: ssm_d_inner      = 0
0.00.091.197 I print_info: ssm_d_state      = 0
0.00.091.197 I print_info: ssm_dt_rank      = 0
0.00.091.197 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.197 I print_info: model type       = 1.4B
0.00.091.198 I print_info: model params     = 1.41 B
0.00.091.198 I print_info: general.name     = 1.4B
0.00.091.198 I print_info: vocab type       = BPE
0.00.091.198 I print_info: n_vocab          = 50304
0.00.091.198 I print_info: n_merges         = 50009
0.00.091.199 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.203 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.204 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.205 I print_info: LF token         = 128 'Ä'
0.00.091.207 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.207 I print_info: max token length = 1024
0.00.953.009 I load_tensors: offloading 24 repeating layers to GPU
0.00.953.016 I load_tensors: offloading output layer to GPU
0.00.953.018 I load_tensors: offloaded 25/25 layers to GPU
0.00.953.048 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.953.049 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.953.751 I llama_init_from_model: n_seq_max     = 1
0.00.953.752 I llama_init_from_model: n_ctx         = 128
0.00.953.753 I llama_init_from_model: n_ctx_per_seq = 128
0.00.953.753 I llama_init_from_model: n_batch       = 128
0.00.953.753 I llama_init_from_model: n_ubatch      = 128
0.00.953.753 I llama_init_from_model: flash_attn    = 0
0.00.953.754 I llama_init_from_model: freq_base     = 10000.0
0.00.953.754 I llama_init_from_model: freq_scale    = 1
0.00.953.755 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.953.756 I ggml_metal_init: allocating
0.00.953.796 I ggml_metal_init: found device: Apple M4
0.00.953.800 I ggml_metal_init: picking default device: Apple M4
0.00.954.849 I ggml_metal_init: using embedded metal library
0.00.958.820 I ggml_metal_init: GPU name:   Apple M4
0.00.958.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.958.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.958.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.958.824 I ggml_metal_init: simdgroup reduction   = true
0.00.958.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.958.824 I ggml_metal_init: has residency sets    = true
0.00.958.824 I ggml_metal_init: has bfloat            = true
0.00.958.824 I ggml_metal_init: use bfloat            = true
0.00.958.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.958.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.970.069 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.971.785 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.971.790 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.971.804 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.973.474 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.973.475 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.973.475 I llama_init_from_model: graph nodes  = 967
0.00.973.476 I llama_init_from_model: graph splits = 2
0.00.973.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.973.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.020.973 I 
0.01.021.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.021.044 I perplexity: tokenizing the input ..
0.01.031.914 I perplexity: tokenization took 10.868 ms
0.01.031.939 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.150.583 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.151.893 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.151.913 I llama_perf_context_print:        load time =     997.66 ms
0.01.151.914 I llama_perf_context_print: prompt eval time =     118.33 ms /   128 tokens (    0.92 ms per token,  1081.69 tokens per second)
0.01.151.918 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.151.918 I llama_perf_context_print:       total time =     130.94 ms /   129 tokens
0.01.152.302 I ggml_metal_free: deallocating

real	0m1.343s
user	0m0.120s
sys	0m0.211s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.064 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.071 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.075 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.078 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.372 I llama_model_loader: - type  f32:  194 tensors
0.00.027.372 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.373 I print_info: file format = GGUF V3 (latest)
0.00.027.373 I print_info: file type   = Q8_0
0.00.027.375 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.264 I load: special tokens cache size = 25
0.00.053.409 I load: token to piece cache size = 0.2984 MB
0.00.053.414 I print_info: arch             = gptneox
0.00.053.415 I print_info: vocab_only       = 0
0.00.053.415 I print_info: n_ctx_train      = 2048
0.00.053.417 I print_info: n_embd           = 2048
0.00.053.418 I print_info: n_layer          = 24
0.00.053.422 I print_info: n_head           = 16
0.00.053.422 I print_info: n_head_kv        = 16
0.00.053.423 I print_info: n_rot            = 32
0.00.053.423 I print_info: n_swa            = 0
0.00.053.423 I print_info: n_embd_head_k    = 128
0.00.053.423 I print_info: n_embd_head_v    = 128
0.00.053.424 I print_info: n_gqa            = 1
0.00.053.425 I print_info: n_embd_k_gqa     = 2048
0.00.053.425 I print_info: n_embd_v_gqa     = 2048
0.00.053.426 I print_info: f_norm_eps       = 1.0e-05
0.00.053.427 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.427 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.427 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.427 I print_info: f_logit_scale    = 0.0e+00
0.00.053.428 I print_info: n_ff             = 8192
0.00.053.428 I print_info: n_expert         = 0
0.00.053.428 I print_info: n_expert_used    = 0
0.00.053.428 I print_info: causal attn      = 1
0.00.053.428 I print_info: pooling type     = 0
0.00.053.429 I print_info: rope type        = 2
0.00.053.430 I print_info: rope scaling     = linear
0.00.053.430 I print_info: freq_base_train  = 10000.0
0.00.053.431 I print_info: freq_scale_train = 1
0.00.053.431 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.431 I print_info: rope_finetuned   = unknown
0.00.053.431 I print_info: ssm_d_conv       = 0
0.00.053.432 I print_info: ssm_d_inner      = 0
0.00.053.432 I print_info: ssm_d_state      = 0
0.00.053.432 I print_info: ssm_dt_rank      = 0
0.00.053.432 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.432 I print_info: model type       = 1.4B
0.00.053.434 I print_info: model params     = 1.41 B
0.00.053.434 I print_info: general.name     = 1.4B
0.00.053.435 I print_info: vocab type       = BPE
0.00.053.435 I print_info: n_vocab          = 50304
0.00.053.435 I print_info: n_merges         = 50009
0.00.053.435 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.435 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.436 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.436 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.436 I print_info: LF token         = 128 'Ä'
0.00.053.436 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.436 I print_info: max token length = 1024
0.00.945.755 I load_tensors: offloading 24 repeating layers to GPU
0.00.945.759 I load_tensors: offloading output layer to GPU
0.00.945.760 I load_tensors: offloaded 25/25 layers to GPU
0.00.945.781 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.945.784 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.946.470 I llama_init_from_model: n_seq_max     = 1
0.00.946.472 I llama_init_from_model: n_ctx         = 2048
0.00.946.472 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.946.473 I llama_init_from_model: n_batch       = 2048
0.00.946.473 I llama_init_from_model: n_ubatch      = 512
0.00.946.473 I llama_init_from_model: flash_attn    = 0
0.00.946.474 I llama_init_from_model: freq_base     = 10000.0
0.00.946.475 I llama_init_from_model: freq_scale    = 1
0.00.946.476 I ggml_metal_init: allocating
0.00.946.495 I ggml_metal_init: found device: Apple M4
0.00.946.500 I ggml_metal_init: picking default device: Apple M4
0.00.947.825 I ggml_metal_init: using embedded metal library
0.00.953.639 I ggml_metal_init: GPU name:   Apple M4
0.00.953.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.953.644 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.953.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.953.645 I ggml_metal_init: simdgroup reduction   = true
0.00.953.645 I ggml_metal_init: simdgroup matrix mul. = true
0.00.953.645 I ggml_metal_init: has residency sets    = true
0.00.953.645 I ggml_metal_init: has bfloat            = true
0.00.953.646 I ggml_metal_init: use bfloat            = true
0.00.953.646 I ggml_metal_init: hasUnifiedMemory      = true
0.00.953.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.969.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.023.150 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.023.157 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.023.224 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.027.905 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.027.907 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.027.907 I llama_init_from_model: graph nodes  = 967
0.01.027.908 I llama_init_from_model: graph splits = 2
0.01.027.913 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.028.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.028.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.085.544 I main: llama threadpool init, n_threads = 4
0.01.085.594 I 
0.01.085.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.085.617 I 
0.01.085.827 I sampler seed: 1234
0.01.085.831 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.085.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.085.843 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.085.843 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.171.472 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.02.171.473 I llama_perf_context_print:        load time =    1074.74 ms
0.02.171.475 I llama_perf_context_print: prompt eval time =      44.59 ms /     7 tokens (    6.37 ms per token,   156.98 tokens per second)
0.02.171.475 I llama_perf_context_print:        eval time =    1038.06 ms /    63 runs   (   16.48 ms per token,    60.69 tokens per second)
0.02.171.476 I llama_perf_context_print:       total time =    1086.84 ms /    70 tokens
0.02.171.746 I ggml_metal_free: deallocating

real	0m2.192s
user	0m0.120s
sys	0m0.253s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.096 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.104 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.104 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.105 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.105 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.105 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.106 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.108 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.108 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.528 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.162 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.163 I llama_model_loader: - type  f32:  194 tensors
0.00.027.163 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.164 I print_info: file format = GGUF V3 (latest)
0.00.027.165 I print_info: file type   = Q8_0
0.00.027.166 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.029 I load: special tokens cache size = 25
0.00.057.386 I load: token to piece cache size = 0.2984 MB
0.00.057.389 I print_info: arch             = gptneox
0.00.057.389 I print_info: vocab_only       = 0
0.00.057.390 I print_info: n_ctx_train      = 2048
0.00.057.390 I print_info: n_embd           = 2048
0.00.057.390 I print_info: n_layer          = 24
0.00.057.393 I print_info: n_head           = 16
0.00.057.394 I print_info: n_head_kv        = 16
0.00.057.394 I print_info: n_rot            = 32
0.00.057.395 I print_info: n_swa            = 0
0.00.057.395 I print_info: n_embd_head_k    = 128
0.00.057.395 I print_info: n_embd_head_v    = 128
0.00.057.395 I print_info: n_gqa            = 1
0.00.057.396 I print_info: n_embd_k_gqa     = 2048
0.00.057.397 I print_info: n_embd_v_gqa     = 2048
0.00.057.397 I print_info: f_norm_eps       = 1.0e-05
0.00.057.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.398 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.398 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.398 I print_info: f_logit_scale    = 0.0e+00
0.00.057.399 I print_info: n_ff             = 8192
0.00.057.399 I print_info: n_expert         = 0
0.00.057.399 I print_info: n_expert_used    = 0
0.00.057.399 I print_info: causal attn      = 1
0.00.057.399 I print_info: pooling type     = 0
0.00.057.399 I print_info: rope type        = 2
0.00.057.400 I print_info: rope scaling     = linear
0.00.057.400 I print_info: freq_base_train  = 10000.0
0.00.057.400 I print_info: freq_scale_train = 1
0.00.057.400 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.400 I print_info: rope_finetuned   = unknown
0.00.057.401 I print_info: ssm_d_conv       = 0
0.00.057.403 I print_info: ssm_d_inner      = 0
0.00.057.403 I print_info: ssm_d_state      = 0
0.00.057.403 I print_info: ssm_dt_rank      = 0
0.00.057.403 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.403 I print_info: model type       = 1.4B
0.00.057.404 I print_info: model params     = 1.41 B
0.00.057.404 I print_info: general.name     = 1.4B
0.00.057.404 I print_info: vocab type       = BPE
0.00.057.405 I print_info: n_vocab          = 50304
0.00.057.405 I print_info: n_merges         = 50009
0.00.057.405 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.405 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.405 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.405 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.406 I print_info: LF token         = 128 'Ä'
0.00.057.406 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.410 I print_info: max token length = 1024
0.00.836.105 I load_tensors: offloading 24 repeating layers to GPU
0.00.836.110 I load_tensors: offloading output layer to GPU
0.00.836.111 I load_tensors: offloaded 25/25 layers to GPU
0.00.836.143 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.836.145 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.837.492 I llama_init_from_model: n_seq_max     = 1
0.00.837.494 I llama_init_from_model: n_ctx         = 128
0.00.837.494 I llama_init_from_model: n_ctx_per_seq = 128
0.00.837.494 I llama_init_from_model: n_batch       = 128
0.00.837.494 I llama_init_from_model: n_ubatch      = 128
0.00.837.495 I llama_init_from_model: flash_attn    = 0
0.00.837.496 I llama_init_from_model: freq_base     = 10000.0
0.00.837.496 I llama_init_from_model: freq_scale    = 1
0.00.837.497 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.837.498 I ggml_metal_init: allocating
0.00.837.565 I ggml_metal_init: found device: Apple M4
0.00.837.571 I ggml_metal_init: picking default device: Apple M4
0.00.838.879 I ggml_metal_init: using embedded metal library
0.00.844.206 I ggml_metal_init: GPU name:   Apple M4
0.00.844.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.844.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.844.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.844.210 I ggml_metal_init: simdgroup reduction   = true
0.00.844.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.844.211 I ggml_metal_init: has residency sets    = true
0.00.844.211 I ggml_metal_init: has bfloat            = true
0.00.844.211 I ggml_metal_init: use bfloat            = true
0.00.844.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.844.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.859.155 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.862.446 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.862.449 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.862.478 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.865.467 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.865.469 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.865.469 I llama_init_from_model: graph nodes  = 967
0.00.865.470 I llama_init_from_model: graph splits = 2
0.00.865.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.865.472 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.892.711 I 
0.00.892.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.892.793 I perplexity: tokenizing the input ..
0.00.901.191 I perplexity: tokenization took 8.396 ms
0.00.901.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.024.397 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.025.746 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.025.766 I llama_perf_context_print:        load time =     883.07 ms
0.01.025.767 I llama_perf_context_print: prompt eval time =     122.96 ms /   128 tokens (    0.96 ms per token,  1040.98 tokens per second)
0.01.025.772 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.025.772 I llama_perf_context_print:       total time =     133.06 ms /   129 tokens
0.01.026.125 I ggml_metal_free: deallocating

real	0m1.041s
user	0m0.094s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.500 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.505 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.514 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.514 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.516 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.811 I llama_model_loader: - type  f32:  194 tensors
0.00.028.811 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.812 I print_info: file format = GGUF V3 (latest)
0.00.028.813 I print_info: file type   = Q4_0
0.00.028.814 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.061 I load: special tokens cache size = 25
0.00.054.125 I load: token to piece cache size = 0.2984 MB
0.00.054.129 I print_info: arch             = gptneox
0.00.054.129 I print_info: vocab_only       = 0
0.00.054.129 I print_info: n_ctx_train      = 2048
0.00.054.130 I print_info: n_embd           = 2048
0.00.054.130 I print_info: n_layer          = 24
0.00.054.136 I print_info: n_head           = 16
0.00.054.136 I print_info: n_head_kv        = 16
0.00.054.137 I print_info: n_rot            = 32
0.00.054.137 I print_info: n_swa            = 0
0.00.054.137 I print_info: n_embd_head_k    = 128
0.00.054.137 I print_info: n_embd_head_v    = 128
0.00.054.138 I print_info: n_gqa            = 1
0.00.054.138 I print_info: n_embd_k_gqa     = 2048
0.00.054.139 I print_info: n_embd_v_gqa     = 2048
0.00.054.140 I print_info: f_norm_eps       = 1.0e-05
0.00.054.140 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.141 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.141 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.141 I print_info: f_logit_scale    = 0.0e+00
0.00.054.142 I print_info: n_ff             = 8192
0.00.054.142 I print_info: n_expert         = 0
0.00.054.142 I print_info: n_expert_used    = 0
0.00.054.142 I print_info: causal attn      = 1
0.00.054.142 I print_info: pooling type     = 0
0.00.054.142 I print_info: rope type        = 2
0.00.054.143 I print_info: rope scaling     = linear
0.00.054.143 I print_info: freq_base_train  = 10000.0
0.00.054.144 I print_info: freq_scale_train = 1
0.00.054.144 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.144 I print_info: rope_finetuned   = unknown
0.00.054.144 I print_info: ssm_d_conv       = 0
0.00.054.144 I print_info: ssm_d_inner      = 0
0.00.054.145 I print_info: ssm_d_state      = 0
0.00.054.145 I print_info: ssm_dt_rank      = 0
0.00.054.145 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.145 I print_info: model type       = 1.4B
0.00.054.147 I print_info: model params     = 1.41 B
0.00.054.148 I print_info: general.name     = 1.4B
0.00.054.148 I print_info: vocab type       = BPE
0.00.054.149 I print_info: n_vocab          = 50304
0.00.054.151 I print_info: n_merges         = 50009
0.00.054.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.151 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.152 I print_info: LF token         = 128 'Ä'
0.00.054.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.152 I print_info: max token length = 1024
0.00.594.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.838 I load_tensors: offloading output layer to GPU
0.00.594.839 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.873 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.594.875 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.596.386 I llama_init_from_model: n_seq_max     = 1
0.00.596.391 I llama_init_from_model: n_ctx         = 2048
0.00.596.391 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.596.392 I llama_init_from_model: n_batch       = 2048
0.00.596.392 I llama_init_from_model: n_ubatch      = 512
0.00.596.393 I llama_init_from_model: flash_attn    = 0
0.00.596.395 I llama_init_from_model: freq_base     = 10000.0
0.00.596.395 I llama_init_from_model: freq_scale    = 1
0.00.596.397 I ggml_metal_init: allocating
0.00.596.474 I ggml_metal_init: found device: Apple M4
0.00.596.483 I ggml_metal_init: picking default device: Apple M4
0.00.598.234 I ggml_metal_init: using embedded metal library
0.00.604.564 I ggml_metal_init: GPU name:   Apple M4
0.00.604.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.573 I ggml_metal_init: simdgroup reduction   = true
0.00.604.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.573 I ggml_metal_init: has residency sets    = true
0.00.604.574 I ggml_metal_init: has bfloat            = true
0.00.604.574 I ggml_metal_init: use bfloat            = true
0.00.604.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.704 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.801 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.808 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.829 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.853 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.855 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.855 I llama_init_from_model: graph nodes  = 967
0.00.686.856 I llama_init_from_model: graph splits = 2
0.00.686.862 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.945 I main: llama threadpool init, n_threads = 4
0.00.742.992 I 
0.00.743.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.019 I 
0.00.743.254 I sampler seed: 1234
0.00.743.258 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.269 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.269 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.269 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.430.693 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.430.694 I llama_perf_context_print:        load time =     730.18 ms
0.01.430.695 I llama_perf_context_print: prompt eval time =      44.42 ms /     7 tokens (    6.35 ms per token,   157.59 tokens per second)
0.01.430.696 I llama_perf_context_print:        eval time =     640.19 ms /    63 runs   (   10.16 ms per token,    98.41 tokens per second)
0.01.430.696 I llama_perf_context_print:       total time =     688.67 ms /    70 tokens
0.01.430.960 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.124s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.602 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.175 I llama_model_loader: - type  f32:  194 tensors
0.00.026.175 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.176 I print_info: file format = GGUF V3 (latest)
0.00.026.177 I print_info: file type   = Q4_0
0.00.026.178 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.890 I load: special tokens cache size = 25
0.00.051.963 I load: token to piece cache size = 0.2984 MB
0.00.051.966 I print_info: arch             = gptneox
0.00.051.966 I print_info: vocab_only       = 0
0.00.051.966 I print_info: n_ctx_train      = 2048
0.00.051.966 I print_info: n_embd           = 2048
0.00.051.966 I print_info: n_layer          = 24
0.00.051.970 I print_info: n_head           = 16
0.00.051.970 I print_info: n_head_kv        = 16
0.00.051.971 I print_info: n_rot            = 32
0.00.051.971 I print_info: n_swa            = 0
0.00.051.971 I print_info: n_embd_head_k    = 128
0.00.051.971 I print_info: n_embd_head_v    = 128
0.00.051.972 I print_info: n_gqa            = 1
0.00.051.973 I print_info: n_embd_k_gqa     = 2048
0.00.051.973 I print_info: n_embd_v_gqa     = 2048
0.00.051.974 I print_info: f_norm_eps       = 1.0e-05
0.00.051.974 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.975 I print_info: f_logit_scale    = 0.0e+00
0.00.051.976 I print_info: n_ff             = 8192
0.00.051.976 I print_info: n_expert         = 0
0.00.051.976 I print_info: n_expert_used    = 0
0.00.051.976 I print_info: causal attn      = 1
0.00.051.976 I print_info: pooling type     = 0
0.00.051.977 I print_info: rope type        = 2
0.00.051.977 I print_info: rope scaling     = linear
0.00.051.977 I print_info: freq_base_train  = 10000.0
0.00.051.977 I print_info: freq_scale_train = 1
0.00.051.978 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.978 I print_info: rope_finetuned   = unknown
0.00.051.978 I print_info: ssm_d_conv       = 0
0.00.051.978 I print_info: ssm_d_inner      = 0
0.00.051.978 I print_info: ssm_d_state      = 0
0.00.051.979 I print_info: ssm_dt_rank      = 0
0.00.051.979 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.979 I print_info: model type       = 1.4B
0.00.051.979 I print_info: model params     = 1.41 B
0.00.051.980 I print_info: general.name     = 1.4B
0.00.051.980 I print_info: vocab type       = BPE
0.00.051.980 I print_info: n_vocab          = 50304
0.00.051.980 I print_info: n_merges         = 50009
0.00.051.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.983 I print_info: LF token         = 128 'Ä'
0.00.051.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.983 I print_info: max token length = 1024
0.00.588.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.804 I load_tensors: offloading output layer to GPU
0.00.588.805 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.836 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.838 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.590.262 I llama_init_from_model: n_seq_max     = 1
0.00.590.267 I llama_init_from_model: n_ctx         = 128
0.00.590.268 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.268 I llama_init_from_model: n_batch       = 128
0.00.590.269 I llama_init_from_model: n_ubatch      = 128
0.00.590.269 I llama_init_from_model: flash_attn    = 0
0.00.590.270 I llama_init_from_model: freq_base     = 10000.0
0.00.590.271 I llama_init_from_model: freq_scale    = 1
0.00.590.271 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.274 I ggml_metal_init: allocating
0.00.590.324 I ggml_metal_init: found device: Apple M4
0.00.590.333 I ggml_metal_init: picking default device: Apple M4
0.00.592.405 I ggml_metal_init: using embedded metal library
0.00.598.620 I ggml_metal_init: GPU name:   Apple M4
0.00.598.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.629 I ggml_metal_init: simdgroup reduction   = true
0.00.598.629 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.629 I ggml_metal_init: has residency sets    = true
0.00.598.630 I ggml_metal_init: has bfloat            = true
0.00.598.630 I ggml_metal_init: use bfloat            = true
0.00.598.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.872 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.395 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.402 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.465 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.710 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.712 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.713 I llama_init_from_model: graph nodes  = 967
0.00.624.713 I llama_init_from_model: graph splits = 2
0.00.624.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.271 I 
0.00.652.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.369 I perplexity: tokenizing the input ..
0.00.660.348 I perplexity: tokenization took 7.978 ms
0.00.660.362 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.323 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.783.937 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.783.952 I llama_perf_context_print:        load time =     642.66 ms
0.00.783.953 I llama_perf_context_print: prompt eval time =     121.71 ms /   128 tokens (    0.95 ms per token,  1051.65 tokens per second)
0.00.783.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.954 I llama_perf_context_print:       total time =     131.69 ms /   129 tokens
0.00.784.417 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.092s
sys	0m0.119s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.718 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.854 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.869 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.870 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.838 I llama_model_loader: - type  f32:  194 tensors
0.00.026.838 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.839 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.840 I print_info: file format = GGUF V3 (latest)
0.00.026.840 I print_info: file type   = Q4_1
0.00.026.841 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.255 I load: special tokens cache size = 25
0.00.052.443 I load: token to piece cache size = 0.2984 MB
0.00.052.447 I print_info: arch             = gptneox
0.00.052.447 I print_info: vocab_only       = 0
0.00.052.447 I print_info: n_ctx_train      = 2048
0.00.052.448 I print_info: n_embd           = 2048
0.00.052.448 I print_info: n_layer          = 24
0.00.052.451 I print_info: n_head           = 16
0.00.052.452 I print_info: n_head_kv        = 16
0.00.052.452 I print_info: n_rot            = 32
0.00.052.453 I print_info: n_swa            = 0
0.00.052.453 I print_info: n_embd_head_k    = 128
0.00.052.453 I print_info: n_embd_head_v    = 128
0.00.052.454 I print_info: n_gqa            = 1
0.00.052.454 I print_info: n_embd_k_gqa     = 2048
0.00.052.455 I print_info: n_embd_v_gqa     = 2048
0.00.052.456 I print_info: f_norm_eps       = 1.0e-05
0.00.052.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.458 I print_info: f_logit_scale    = 0.0e+00
0.00.052.461 I print_info: n_ff             = 8192
0.00.052.461 I print_info: n_expert         = 0
0.00.052.461 I print_info: n_expert_used    = 0
0.00.052.461 I print_info: causal attn      = 1
0.00.052.461 I print_info: pooling type     = 0
0.00.052.461 I print_info: rope type        = 2
0.00.052.463 I print_info: rope scaling     = linear
0.00.052.463 I print_info: freq_base_train  = 10000.0
0.00.052.467 I print_info: freq_scale_train = 1
0.00.052.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.467 I print_info: rope_finetuned   = unknown
0.00.052.468 I print_info: ssm_d_conv       = 0
0.00.052.468 I print_info: ssm_d_inner      = 0
0.00.052.468 I print_info: ssm_d_state      = 0
0.00.052.468 I print_info: ssm_dt_rank      = 0
0.00.052.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.468 I print_info: model type       = 1.4B
0.00.052.469 I print_info: model params     = 1.41 B
0.00.052.469 I print_info: general.name     = 1.4B
0.00.052.469 I print_info: vocab type       = BPE
0.00.052.470 I print_info: n_vocab          = 50304
0.00.052.470 I print_info: n_merges         = 50009
0.00.052.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.470 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.471 I print_info: LF token         = 128 'Ä'
0.00.052.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.471 I print_info: max token length = 1024
0.00.600.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.569 I load_tensors: offloading output layer to GPU
0.00.600.569 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.586 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.600.589 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.601.232 I llama_init_from_model: n_seq_max     = 1
0.00.601.238 I llama_init_from_model: n_ctx         = 2048
0.00.601.238 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.238 I llama_init_from_model: n_batch       = 2048
0.00.601.239 I llama_init_from_model: n_ubatch      = 512
0.00.601.239 I llama_init_from_model: flash_attn    = 0
0.00.601.240 I llama_init_from_model: freq_base     = 10000.0
0.00.601.240 I llama_init_from_model: freq_scale    = 1
0.00.601.241 I ggml_metal_init: allocating
0.00.601.285 I ggml_metal_init: found device: Apple M4
0.00.601.292 I ggml_metal_init: picking default device: Apple M4
0.00.602.323 I ggml_metal_init: using embedded metal library
0.00.606.422 I ggml_metal_init: GPU name:   Apple M4
0.00.606.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.431 I ggml_metal_init: simdgroup reduction   = true
0.00.606.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.432 I ggml_metal_init: has residency sets    = true
0.00.606.432 I ggml_metal_init: has bfloat            = true
0.00.606.432 I ggml_metal_init: use bfloat            = true
0.00.606.434 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.261 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.625 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.652.631 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.652.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.128 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.657.131 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.657.131 I llama_init_from_model: graph nodes  = 967
0.00.657.131 I llama_init_from_model: graph splits = 2
0.00.657.140 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.657.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.657.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.460 I main: llama threadpool init, n_threads = 4
0.00.705.498 I 
0.00.705.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.521 I 
0.00.705.749 I sampler seed: 1234
0.00.705.754 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.786 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.789 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.789 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.437.218 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.437.220 I llama_perf_context_print:        load time =     694.85 ms
0.01.437.221 I llama_perf_context_print: prompt eval time =      44.58 ms /     7 tokens (    6.37 ms per token,   157.02 tokens per second)
0.01.437.222 I llama_perf_context_print:        eval time =     684.56 ms /    63 runs   (   10.87 ms per token,    92.03 tokens per second)
0.01.437.222 I llama_perf_context_print:       total time =     732.65 ms /    70 tokens
0.01.437.439 I ggml_metal_free: deallocating

real	0m1.458s
user	0m0.117s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.397 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.411 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.412 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.412 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.280 I llama_model_loader: - type  f32:  194 tensors
0.00.025.281 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.281 I print_info: file format = GGUF V3 (latest)
0.00.025.282 I print_info: file type   = Q4_1
0.00.025.283 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.977 I load: special tokens cache size = 25
0.00.051.119 I load: token to piece cache size = 0.2984 MB
0.00.051.122 I print_info: arch             = gptneox
0.00.051.123 I print_info: vocab_only       = 0
0.00.051.123 I print_info: n_ctx_train      = 2048
0.00.051.123 I print_info: n_embd           = 2048
0.00.051.123 I print_info: n_layer          = 24
0.00.051.126 I print_info: n_head           = 16
0.00.051.127 I print_info: n_head_kv        = 16
0.00.051.127 I print_info: n_rot            = 32
0.00.051.127 I print_info: n_swa            = 0
0.00.051.128 I print_info: n_embd_head_k    = 128
0.00.051.128 I print_info: n_embd_head_v    = 128
0.00.051.128 I print_info: n_gqa            = 1
0.00.051.129 I print_info: n_embd_k_gqa     = 2048
0.00.051.130 I print_info: n_embd_v_gqa     = 2048
0.00.051.130 I print_info: f_norm_eps       = 1.0e-05
0.00.051.131 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.131 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.131 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.131 I print_info: f_logit_scale    = 0.0e+00
0.00.051.132 I print_info: n_ff             = 8192
0.00.051.132 I print_info: n_expert         = 0
0.00.051.132 I print_info: n_expert_used    = 0
0.00.051.132 I print_info: causal attn      = 1
0.00.051.132 I print_info: pooling type     = 0
0.00.051.132 I print_info: rope type        = 2
0.00.051.133 I print_info: rope scaling     = linear
0.00.051.133 I print_info: freq_base_train  = 10000.0
0.00.051.133 I print_info: freq_scale_train = 1
0.00.051.134 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.134 I print_info: rope_finetuned   = unknown
0.00.051.134 I print_info: ssm_d_conv       = 0
0.00.051.136 I print_info: ssm_d_inner      = 0
0.00.051.136 I print_info: ssm_d_state      = 0
0.00.051.136 I print_info: ssm_dt_rank      = 0
0.00.051.137 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.137 I print_info: model type       = 1.4B
0.00.051.137 I print_info: model params     = 1.41 B
0.00.051.137 I print_info: general.name     = 1.4B
0.00.051.138 I print_info: vocab type       = BPE
0.00.051.138 I print_info: n_vocab          = 50304
0.00.051.138 I print_info: n_merges         = 50009
0.00.051.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.139 I print_info: LF token         = 128 'Ä'
0.00.051.144 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.144 I print_info: max token length = 1024
0.00.600.141 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.155 I load_tensors: offloading output layer to GPU
0.00.600.156 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.190 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.600.191 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.601.706 I llama_init_from_model: n_seq_max     = 1
0.00.601.711 I llama_init_from_model: n_ctx         = 128
0.00.601.712 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.712 I llama_init_from_model: n_batch       = 128
0.00.601.713 I llama_init_from_model: n_ubatch      = 128
0.00.601.713 I llama_init_from_model: flash_attn    = 0
0.00.601.715 I llama_init_from_model: freq_base     = 10000.0
0.00.601.715 I llama_init_from_model: freq_scale    = 1
0.00.601.716 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.718 I ggml_metal_init: allocating
0.00.601.793 I ggml_metal_init: found device: Apple M4
0.00.601.802 I ggml_metal_init: picking default device: Apple M4
0.00.603.490 I ggml_metal_init: using embedded metal library
0.00.610.286 I ggml_metal_init: GPU name:   Apple M4
0.00.610.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.293 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.296 I ggml_metal_init: simdgroup reduction   = true
0.00.610.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.297 I ggml_metal_init: has residency sets    = true
0.00.610.297 I ggml_metal_init: has bfloat            = true
0.00.610.298 I ggml_metal_init: use bfloat            = true
0.00.610.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.080 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.563 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.567 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.715 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.717 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.717 I llama_init_from_model: graph nodes  = 967
0.00.634.717 I llama_init_from_model: graph splits = 2
0.00.634.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.157 I 
0.00.663.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.259 I perplexity: tokenizing the input ..
0.00.673.021 I perplexity: tokenization took 9.76 ms
0.00.673.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.969 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.798.296 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.798.314 I llama_perf_context_print:        load time =     654.26 ms
0.00.798.315 I llama_perf_context_print: prompt eval time =     123.70 ms /   128 tokens (    0.97 ms per token,  1034.74 tokens per second)
0.00.798.316 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.316 I llama_perf_context_print:       total time =     135.16 ms /   129 tokens
0.00.798.693 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.094s
sys	0m0.124s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.083 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.084 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.089 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.095 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.096 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.497 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.500 I llama_model_loader: - type  f32:  194 tensors
0.00.026.500 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.501 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.501 I print_info: file format = GGUF V3 (latest)
0.00.026.502 I print_info: file type   = Q5_0
0.00.026.503 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.758 I load: special tokens cache size = 25
0.00.053.141 I load: token to piece cache size = 0.2984 MB
0.00.053.147 I print_info: arch             = gptneox
0.00.053.147 I print_info: vocab_only       = 0
0.00.053.147 I print_info: n_ctx_train      = 2048
0.00.053.147 I print_info: n_embd           = 2048
0.00.053.147 I print_info: n_layer          = 24
0.00.053.152 I print_info: n_head           = 16
0.00.053.153 I print_info: n_head_kv        = 16
0.00.053.153 I print_info: n_rot            = 32
0.00.053.153 I print_info: n_swa            = 0
0.00.053.153 I print_info: n_embd_head_k    = 128
0.00.053.153 I print_info: n_embd_head_v    = 128
0.00.053.154 I print_info: n_gqa            = 1
0.00.053.154 I print_info: n_embd_k_gqa     = 2048
0.00.053.155 I print_info: n_embd_v_gqa     = 2048
0.00.053.155 I print_info: f_norm_eps       = 1.0e-05
0.00.053.156 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.156 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.156 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.156 I print_info: f_logit_scale    = 0.0e+00
0.00.053.157 I print_info: n_ff             = 8192
0.00.053.157 I print_info: n_expert         = 0
0.00.053.157 I print_info: n_expert_used    = 0
0.00.053.157 I print_info: causal attn      = 1
0.00.053.157 I print_info: pooling type     = 0
0.00.053.158 I print_info: rope type        = 2
0.00.053.158 I print_info: rope scaling     = linear
0.00.053.158 I print_info: freq_base_train  = 10000.0
0.00.053.158 I print_info: freq_scale_train = 1
0.00.053.158 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.169 I print_info: rope_finetuned   = unknown
0.00.053.169 I print_info: ssm_d_conv       = 0
0.00.053.169 I print_info: ssm_d_inner      = 0
0.00.053.170 I print_info: ssm_d_state      = 0
0.00.053.170 I print_info: ssm_dt_rank      = 0
0.00.053.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.170 I print_info: model type       = 1.4B
0.00.053.170 I print_info: model params     = 1.41 B
0.00.053.170 I print_info: general.name     = 1.4B
0.00.053.171 I print_info: vocab type       = BPE
0.00.053.171 I print_info: n_vocab          = 50304
0.00.053.171 I print_info: n_merges         = 50009
0.00.053.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.172 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.172 I print_info: LF token         = 128 'Ä'
0.00.053.173 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.173 I print_info: max token length = 1024
0.00.653.724 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.741 I load_tensors: offloading output layer to GPU
0.00.653.741 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.776 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.653.777 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.654.759 I llama_init_from_model: n_seq_max     = 1
0.00.654.765 I llama_init_from_model: n_ctx         = 2048
0.00.654.765 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.766 I llama_init_from_model: n_batch       = 2048
0.00.654.766 I llama_init_from_model: n_ubatch      = 512
0.00.654.767 I llama_init_from_model: flash_attn    = 0
0.00.654.769 I llama_init_from_model: freq_base     = 10000.0
0.00.654.769 I llama_init_from_model: freq_scale    = 1
0.00.654.772 I ggml_metal_init: allocating
0.00.654.855 I ggml_metal_init: found device: Apple M4
0.00.654.864 I ggml_metal_init: picking default device: Apple M4
0.00.656.658 I ggml_metal_init: using embedded metal library
0.00.663.591 I ggml_metal_init: GPU name:   Apple M4
0.00.663.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.602 I ggml_metal_init: simdgroup reduction   = true
0.00.663.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.603 I ggml_metal_init: has residency sets    = true
0.00.663.603 I ggml_metal_init: has bfloat            = true
0.00.663.604 I ggml_metal_init: use bfloat            = true
0.00.663.605 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.778 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.350 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.356 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.379 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.395 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.397 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.397 I llama_init_from_model: graph nodes  = 967
0.00.735.398 I llama_init_from_model: graph splits = 2
0.00.735.403 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.212 I main: llama threadpool init, n_threads = 4
0.00.796.260 I 
0.00.796.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.283 I 
0.00.796.511 I sampler seed: 1234
0.00.796.515 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.528 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.590.192 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.590.193 I llama_perf_context_print:        load time =     786.08 ms
0.01.590.193 I llama_perf_context_print: prompt eval time =      48.08 ms /     7 tokens (    6.87 ms per token,   145.58 tokens per second)
0.01.590.194 I llama_perf_context_print:        eval time =     742.60 ms /    63 runs   (   11.79 ms per token,    84.84 tokens per second)
0.01.590.194 I llama_perf_context_print:       total time =     794.86 ms /    70 tokens
0.01.590.413 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.126s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.927 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.242 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.244 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.247 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.247 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.252 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.221 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.222 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.222 I llama_model_loader: - type  f32:  194 tensors
0.00.026.223 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.223 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.223 I print_info: file format = GGUF V3 (latest)
0.00.026.224 I print_info: file type   = Q5_0
0.00.026.225 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.183 I load: special tokens cache size = 25
0.00.051.066 I load: token to piece cache size = 0.2984 MB
0.00.051.068 I print_info: arch             = gptneox
0.00.051.069 I print_info: vocab_only       = 0
0.00.051.069 I print_info: n_ctx_train      = 2048
0.00.051.069 I print_info: n_embd           = 2048
0.00.051.069 I print_info: n_layer          = 24
0.00.051.073 I print_info: n_head           = 16
0.00.051.074 I print_info: n_head_kv        = 16
0.00.051.074 I print_info: n_rot            = 32
0.00.051.074 I print_info: n_swa            = 0
0.00.051.074 I print_info: n_embd_head_k    = 128
0.00.051.074 I print_info: n_embd_head_v    = 128
0.00.051.075 I print_info: n_gqa            = 1
0.00.051.076 I print_info: n_embd_k_gqa     = 2048
0.00.051.076 I print_info: n_embd_v_gqa     = 2048
0.00.051.077 I print_info: f_norm_eps       = 1.0e-05
0.00.051.077 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.077 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.078 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.078 I print_info: f_logit_scale    = 0.0e+00
0.00.051.078 I print_info: n_ff             = 8192
0.00.051.078 I print_info: n_expert         = 0
0.00.051.079 I print_info: n_expert_used    = 0
0.00.051.079 I print_info: causal attn      = 1
0.00.051.079 I print_info: pooling type     = 0
0.00.051.079 I print_info: rope type        = 2
0.00.051.079 I print_info: rope scaling     = linear
0.00.051.080 I print_info: freq_base_train  = 10000.0
0.00.051.080 I print_info: freq_scale_train = 1
0.00.051.080 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.080 I print_info: rope_finetuned   = unknown
0.00.051.081 I print_info: ssm_d_conv       = 0
0.00.051.081 I print_info: ssm_d_inner      = 0
0.00.051.082 I print_info: ssm_d_state      = 0
0.00.051.082 I print_info: ssm_dt_rank      = 0
0.00.051.082 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.082 I print_info: model type       = 1.4B
0.00.051.083 I print_info: model params     = 1.41 B
0.00.051.083 I print_info: general.name     = 1.4B
0.00.051.083 I print_info: vocab type       = BPE
0.00.051.084 I print_info: n_vocab          = 50304
0.00.051.084 I print_info: n_merges         = 50009
0.00.051.084 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.084 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: LF token         = 128 'Ä'
0.00.051.085 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.086 I print_info: max token length = 1024
0.00.651.064 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.081 I load_tensors: offloading output layer to GPU
0.00.651.081 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.114 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.651.115 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.652.681 I llama_init_from_model: n_seq_max     = 1
0.00.652.686 I llama_init_from_model: n_ctx         = 128
0.00.652.687 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.687 I llama_init_from_model: n_batch       = 128
0.00.652.687 I llama_init_from_model: n_ubatch      = 128
0.00.652.688 I llama_init_from_model: flash_attn    = 0
0.00.652.690 I llama_init_from_model: freq_base     = 10000.0
0.00.652.690 I llama_init_from_model: freq_scale    = 1
0.00.652.691 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.693 I ggml_metal_init: allocating
0.00.652.765 I ggml_metal_init: found device: Apple M4
0.00.652.774 I ggml_metal_init: picking default device: Apple M4
0.00.654.420 I ggml_metal_init: using embedded metal library
0.00.660.980 I ggml_metal_init: GPU name:   Apple M4
0.00.660.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.989 I ggml_metal_init: simdgroup reduction   = true
0.00.660.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.989 I ggml_metal_init: has residency sets    = true
0.00.660.990 I ggml_metal_init: has bfloat            = true
0.00.660.990 I ggml_metal_init: use bfloat            = true
0.00.660.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.660.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.895 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.681.314 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.681.339 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.826 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.684.828 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.684.828 I llama_init_from_model: graph nodes  = 967
0.00.684.829 I llama_init_from_model: graph splits = 2
0.00.684.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.684.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.355 I 
0.00.716.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.453 I perplexity: tokenizing the input ..
0.00.725.482 I perplexity: tokenization took 9.027 ms
0.00.725.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.646 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.861.065 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.861.080 I llama_perf_context_print:        load time =     706.42 ms
0.00.861.081 I llama_perf_context_print: prompt eval time =     133.92 ms /   128 tokens (    1.05 ms per token,   955.79 tokens per second)
0.00.861.082 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.082 I llama_perf_context_print:       total time =     144.73 ms /   129 tokens
0.00.861.465 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.091s
sys	0m0.133s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.701 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.701 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.704 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.705 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.805 I llama_model_loader: - type  f32:  194 tensors
0.00.027.806 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.806 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.807 I print_info: file format = GGUF V3 (latest)
0.00.027.807 I print_info: file type   = Q5_1
0.00.027.808 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.794 I load: special tokens cache size = 25
0.00.052.895 I load: token to piece cache size = 0.2984 MB
0.00.052.898 I print_info: arch             = gptneox
0.00.052.898 I print_info: vocab_only       = 0
0.00.052.898 I print_info: n_ctx_train      = 2048
0.00.052.898 I print_info: n_embd           = 2048
0.00.052.898 I print_info: n_layer          = 24
0.00.052.901 I print_info: n_head           = 16
0.00.052.902 I print_info: n_head_kv        = 16
0.00.052.902 I print_info: n_rot            = 32
0.00.052.903 I print_info: n_swa            = 0
0.00.052.903 I print_info: n_embd_head_k    = 128
0.00.052.903 I print_info: n_embd_head_v    = 128
0.00.052.904 I print_info: n_gqa            = 1
0.00.052.905 I print_info: n_embd_k_gqa     = 2048
0.00.052.905 I print_info: n_embd_v_gqa     = 2048
0.00.052.907 I print_info: f_norm_eps       = 1.0e-05
0.00.052.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.908 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.908 I print_info: f_logit_scale    = 0.0e+00
0.00.052.909 I print_info: n_ff             = 8192
0.00.052.909 I print_info: n_expert         = 0
0.00.052.909 I print_info: n_expert_used    = 0
0.00.052.909 I print_info: causal attn      = 1
0.00.052.909 I print_info: pooling type     = 0
0.00.052.911 I print_info: rope type        = 2
0.00.052.912 I print_info: rope scaling     = linear
0.00.052.913 I print_info: freq_base_train  = 10000.0
0.00.052.913 I print_info: freq_scale_train = 1
0.00.052.913 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.914 I print_info: rope_finetuned   = unknown
0.00.052.914 I print_info: ssm_d_conv       = 0
0.00.052.914 I print_info: ssm_d_inner      = 0
0.00.052.914 I print_info: ssm_d_state      = 0
0.00.052.914 I print_info: ssm_dt_rank      = 0
0.00.052.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.915 I print_info: model type       = 1.4B
0.00.052.915 I print_info: model params     = 1.41 B
0.00.052.915 I print_info: general.name     = 1.4B
0.00.052.916 I print_info: vocab type       = BPE
0.00.052.916 I print_info: n_vocab          = 50304
0.00.052.916 I print_info: n_merges         = 50009
0.00.052.920 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.920 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.920 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.920 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.921 I print_info: LF token         = 128 'Ä'
0.00.052.921 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.921 I print_info: max token length = 1024
0.00.701.090 I load_tensors: offloading 24 repeating layers to GPU
0.00.701.104 I load_tensors: offloading output layer to GPU
0.00.701.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.701.141 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.701.142 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.702.572 I llama_init_from_model: n_seq_max     = 1
0.00.702.579 I llama_init_from_model: n_ctx         = 2048
0.00.702.579 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.702.580 I llama_init_from_model: n_batch       = 2048
0.00.702.580 I llama_init_from_model: n_ubatch      = 512
0.00.702.581 I llama_init_from_model: flash_attn    = 0
0.00.702.583 I llama_init_from_model: freq_base     = 10000.0
0.00.702.584 I llama_init_from_model: freq_scale    = 1
0.00.702.587 I ggml_metal_init: allocating
0.00.702.638 I ggml_metal_init: found device: Apple M4
0.00.702.646 I ggml_metal_init: picking default device: Apple M4
0.00.704.481 I ggml_metal_init: using embedded metal library
0.00.711.004 I ggml_metal_init: GPU name:   Apple M4
0.00.711.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.711.010 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.711.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.711.011 I ggml_metal_init: simdgroup reduction   = true
0.00.711.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.711.012 I ggml_metal_init: has residency sets    = true
0.00.711.012 I ggml_metal_init: has bfloat            = true
0.00.711.012 I ggml_metal_init: use bfloat            = true
0.00.711.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.711.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.923 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.773.747 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.773.754 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.773.775 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.657 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.777.659 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.777.659 I llama_init_from_model: graph nodes  = 967
0.00.777.659 I llama_init_from_model: graph splits = 2
0.00.777.665 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.777.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.777.781 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.839.225 I main: llama threadpool init, n_threads = 4
0.00.839.265 I 
0.00.839.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.839.290 I 
0.00.839.523 I sampler seed: 1234
0.00.839.527 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.839.548 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.839.548 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.839.548 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.677.840 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.677.841 I llama_perf_context_print:        load time =     827.38 ms
0.01.677.842 I llama_perf_context_print: prompt eval time =      46.57 ms /     7 tokens (    6.65 ms per token,   150.32 tokens per second)
0.01.677.842 I llama_perf_context_print:        eval time =     788.99 ms /    63 runs   (   12.52 ms per token,    79.85 tokens per second)
0.01.677.843 I llama_perf_context_print:       total time =     839.49 ms /    70 tokens
0.01.678.130 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.121s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.407 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.817 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.851 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.840 I llama_model_loader: - type  f32:  194 tensors
0.00.025.840 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.841 I print_info: file format = GGUF V3 (latest)
0.00.025.841 I print_info: file type   = Q5_1
0.00.025.845 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.989 I load: special tokens cache size = 25
0.00.051.005 I load: token to piece cache size = 0.2984 MB
0.00.051.008 I print_info: arch             = gptneox
0.00.051.009 I print_info: vocab_only       = 0
0.00.051.009 I print_info: n_ctx_train      = 2048
0.00.051.009 I print_info: n_embd           = 2048
0.00.051.009 I print_info: n_layer          = 24
0.00.051.012 I print_info: n_head           = 16
0.00.051.013 I print_info: n_head_kv        = 16
0.00.051.013 I print_info: n_rot            = 32
0.00.051.014 I print_info: n_swa            = 0
0.00.051.014 I print_info: n_embd_head_k    = 128
0.00.051.014 I print_info: n_embd_head_v    = 128
0.00.051.015 I print_info: n_gqa            = 1
0.00.051.015 I print_info: n_embd_k_gqa     = 2048
0.00.051.016 I print_info: n_embd_v_gqa     = 2048
0.00.051.017 I print_info: f_norm_eps       = 1.0e-05
0.00.051.017 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.019 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.019 I print_info: f_logit_scale    = 0.0e+00
0.00.051.020 I print_info: n_ff             = 8192
0.00.051.020 I print_info: n_expert         = 0
0.00.051.021 I print_info: n_expert_used    = 0
0.00.051.021 I print_info: causal attn      = 1
0.00.051.022 I print_info: pooling type     = 0
0.00.051.022 I print_info: rope type        = 2
0.00.051.022 I print_info: rope scaling     = linear
0.00.051.023 I print_info: freq_base_train  = 10000.0
0.00.051.023 I print_info: freq_scale_train = 1
0.00.051.023 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.023 I print_info: rope_finetuned   = unknown
0.00.051.024 I print_info: ssm_d_conv       = 0
0.00.051.024 I print_info: ssm_d_inner      = 0
0.00.051.024 I print_info: ssm_d_state      = 0
0.00.051.024 I print_info: ssm_dt_rank      = 0
0.00.051.024 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.024 I print_info: model type       = 1.4B
0.00.051.025 I print_info: model params     = 1.41 B
0.00.051.025 I print_info: general.name     = 1.4B
0.00.051.026 I print_info: vocab type       = BPE
0.00.051.026 I print_info: n_vocab          = 50304
0.00.051.026 I print_info: n_merges         = 50009
0.00.051.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.028 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.028 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.028 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.028 I print_info: LF token         = 128 'Ä'
0.00.051.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.029 I print_info: max token length = 1024
0.00.706.823 I load_tensors: offloading 24 repeating layers to GPU
0.00.706.838 I load_tensors: offloading output layer to GPU
0.00.706.839 I load_tensors: offloaded 25/25 layers to GPU
0.00.706.874 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.706.876 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.708.142 I llama_init_from_model: n_seq_max     = 1
0.00.708.147 I llama_init_from_model: n_ctx         = 128
0.00.708.148 I llama_init_from_model: n_ctx_per_seq = 128
0.00.708.148 I llama_init_from_model: n_batch       = 128
0.00.708.149 I llama_init_from_model: n_ubatch      = 128
0.00.708.149 I llama_init_from_model: flash_attn    = 0
0.00.708.151 I llama_init_from_model: freq_base     = 10000.0
0.00.708.152 I llama_init_from_model: freq_scale    = 1
0.00.708.152 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.708.154 I ggml_metal_init: allocating
0.00.708.235 I ggml_metal_init: found device: Apple M4
0.00.708.243 I ggml_metal_init: picking default device: Apple M4
0.00.709.947 I ggml_metal_init: using embedded metal library
0.00.716.688 I ggml_metal_init: GPU name:   Apple M4
0.00.716.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.716.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.716.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.716.696 I ggml_metal_init: simdgroup reduction   = true
0.00.716.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.716.696 I ggml_metal_init: has residency sets    = true
0.00.716.697 I ggml_metal_init: has bfloat            = true
0.00.716.697 I ggml_metal_init: use bfloat            = true
0.00.716.698 I ggml_metal_init: hasUnifiedMemory      = true
0.00.716.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.733.938 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.465 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.737.471 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.737.526 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.642 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.740.644 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.740.645 I llama_init_from_model: graph nodes  = 967
0.00.740.645 I llama_init_from_model: graph splits = 2
0.00.740.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.740.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.350 I 
0.00.773.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.454 I perplexity: tokenizing the input ..
0.00.782.993 I perplexity: tokenization took 9.537 ms
0.00.783.006 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.917.596 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.918.908 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.918.925 I llama_perf_context_print:        load time =     763.93 ms
0.00.918.926 I llama_perf_context_print: prompt eval time =     134.36 ms /   128 tokens (    1.05 ms per token,   952.67 tokens per second)
0.00.918.927 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.918.927 I llama_perf_context_print:       total time =     145.58 ms /   129 tokens
0.00.919.334 I ggml_metal_free: deallocating

real	0m0.934s
user	0m0.093s
sys	0m0.141s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.621 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.241 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.245 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.865 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.865 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.866 I llama_model_loader: - type  f32:  194 tensors
0.00.023.866 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.866 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.867 I print_info: file format = GGUF V3 (latest)
0.00.023.868 I print_info: file type   = Q2_K - Medium
0.00.023.869 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.837 I load: special tokens cache size = 25
0.00.048.887 I load: token to piece cache size = 0.2984 MB
0.00.048.889 I print_info: arch             = gptneox
0.00.048.889 I print_info: vocab_only       = 0
0.00.048.890 I print_info: n_ctx_train      = 2048
0.00.048.890 I print_info: n_embd           = 2048
0.00.048.890 I print_info: n_layer          = 24
0.00.048.893 I print_info: n_head           = 16
0.00.048.894 I print_info: n_head_kv        = 16
0.00.048.894 I print_info: n_rot            = 32
0.00.048.894 I print_info: n_swa            = 0
0.00.048.894 I print_info: n_embd_head_k    = 128
0.00.048.894 I print_info: n_embd_head_v    = 128
0.00.048.895 I print_info: n_gqa            = 1
0.00.048.896 I print_info: n_embd_k_gqa     = 2048
0.00.048.897 I print_info: n_embd_v_gqa     = 2048
0.00.048.897 I print_info: f_norm_eps       = 1.0e-05
0.00.048.898 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.898 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.898 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.898 I print_info: f_logit_scale    = 0.0e+00
0.00.048.899 I print_info: n_ff             = 8192
0.00.048.899 I print_info: n_expert         = 0
0.00.048.899 I print_info: n_expert_used    = 0
0.00.048.899 I print_info: causal attn      = 1
0.00.048.899 I print_info: pooling type     = 0
0.00.048.899 I print_info: rope type        = 2
0.00.048.900 I print_info: rope scaling     = linear
0.00.048.900 I print_info: freq_base_train  = 10000.0
0.00.048.900 I print_info: freq_scale_train = 1
0.00.048.901 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.901 I print_info: rope_finetuned   = unknown
0.00.048.901 I print_info: ssm_d_conv       = 0
0.00.048.901 I print_info: ssm_d_inner      = 0
0.00.048.901 I print_info: ssm_d_state      = 0
0.00.048.901 I print_info: ssm_dt_rank      = 0
0.00.048.901 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.902 I print_info: model type       = 1.4B
0.00.048.902 I print_info: model params     = 1.41 B
0.00.048.902 I print_info: general.name     = 1.4B
0.00.048.903 I print_info: vocab type       = BPE
0.00.048.903 I print_info: n_vocab          = 50304
0.00.048.903 I print_info: n_merges         = 50009
0.00.048.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.904 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.904 I print_info: LF token         = 128 'Ä'
0.00.048.905 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.905 I print_info: max token length = 1024
0.00.387.220 I load_tensors: offloading 24 repeating layers to GPU
0.00.387.234 I load_tensors: offloading output layer to GPU
0.00.387.234 I load_tensors: offloaded 25/25 layers to GPU
0.00.387.270 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.387.271 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.388.729 I llama_init_from_model: n_seq_max     = 1
0.00.388.734 I llama_init_from_model: n_ctx         = 2048
0.00.388.734 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.388.735 I llama_init_from_model: n_batch       = 2048
0.00.388.735 I llama_init_from_model: n_ubatch      = 512
0.00.388.735 I llama_init_from_model: flash_attn    = 0
0.00.388.738 I llama_init_from_model: freq_base     = 10000.0
0.00.388.738 I llama_init_from_model: freq_scale    = 1
0.00.388.743 I ggml_metal_init: allocating
0.00.388.832 I ggml_metal_init: found device: Apple M4
0.00.388.841 I ggml_metal_init: picking default device: Apple M4
0.00.390.607 I ggml_metal_init: using embedded metal library
0.00.396.767 I ggml_metal_init: GPU name:   Apple M4
0.00.396.775 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.396.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.396.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.396.778 I ggml_metal_init: simdgroup reduction   = true
0.00.396.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.396.779 I ggml_metal_init: has residency sets    = true
0.00.396.779 I ggml_metal_init: has bfloat            = true
0.00.396.779 I ggml_metal_init: use bfloat            = true
0.00.396.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.396.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.417.926 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.475.559 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.475.565 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.475.587 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.480.630 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.480.632 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.480.632 I llama_init_from_model: graph nodes  = 967
0.00.480.633 I llama_init_from_model: graph splits = 2
0.00.480.637 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.480.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.480.769 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.469 I main: llama threadpool init, n_threads = 4
0.00.541.509 I 
0.00.541.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.532 I 
0.00.541.763 I sampler seed: 1234
0.00.541.767 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.541.805 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.541.808 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.541.808 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.223.103 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.223.104 I llama_perf_context_print:        load time =     531.92 ms
0.01.223.105 I llama_perf_context_print: prompt eval time =      43.36 ms /     7 tokens (    6.19 ms per token,   161.45 tokens per second)
0.01.223.106 I llama_perf_context_print:        eval time =     635.17 ms /    63 runs   (   10.08 ms per token,    99.19 tokens per second)
0.01.223.106 I llama_perf_context_print:       total time =     682.55 ms /    70 tokens
0.01.223.300 I ggml_metal_free: deallocating

real	0m1.243s
user	0m0.124s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.825 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.626 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.554 I llama_model_loader: - type  f32:  194 tensors
0.00.026.554 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.554 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.555 I print_info: file format = GGUF V3 (latest)
0.00.026.555 I print_info: file type   = Q2_K - Medium
0.00.026.556 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.428 I load: special tokens cache size = 25
0.00.051.571 I load: token to piece cache size = 0.2984 MB
0.00.051.574 I print_info: arch             = gptneox
0.00.051.574 I print_info: vocab_only       = 0
0.00.051.574 I print_info: n_ctx_train      = 2048
0.00.051.575 I print_info: n_embd           = 2048
0.00.051.575 I print_info: n_layer          = 24
0.00.051.578 I print_info: n_head           = 16
0.00.051.578 I print_info: n_head_kv        = 16
0.00.051.580 I print_info: n_rot            = 32
0.00.051.581 I print_info: n_swa            = 0
0.00.051.581 I print_info: n_embd_head_k    = 128
0.00.051.581 I print_info: n_embd_head_v    = 128
0.00.051.582 I print_info: n_gqa            = 1
0.00.051.582 I print_info: n_embd_k_gqa     = 2048
0.00.051.583 I print_info: n_embd_v_gqa     = 2048
0.00.051.583 I print_info: f_norm_eps       = 1.0e-05
0.00.051.584 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.584 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.584 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.584 I print_info: f_logit_scale    = 0.0e+00
0.00.051.585 I print_info: n_ff             = 8192
0.00.051.585 I print_info: n_expert         = 0
0.00.051.585 I print_info: n_expert_used    = 0
0.00.051.586 I print_info: causal attn      = 1
0.00.051.586 I print_info: pooling type     = 0
0.00.051.586 I print_info: rope type        = 2
0.00.051.586 I print_info: rope scaling     = linear
0.00.051.586 I print_info: freq_base_train  = 10000.0
0.00.051.587 I print_info: freq_scale_train = 1
0.00.051.587 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.591 I print_info: rope_finetuned   = unknown
0.00.051.591 I print_info: ssm_d_conv       = 0
0.00.051.591 I print_info: ssm_d_inner      = 0
0.00.051.591 I print_info: ssm_d_state      = 0
0.00.051.591 I print_info: ssm_dt_rank      = 0
0.00.051.592 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.592 I print_info: model type       = 1.4B
0.00.051.592 I print_info: model params     = 1.41 B
0.00.051.592 I print_info: general.name     = 1.4B
0.00.051.593 I print_info: vocab type       = BPE
0.00.051.593 I print_info: n_vocab          = 50304
0.00.051.593 I print_info: n_merges         = 50009
0.00.051.594 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.595 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.595 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.595 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.596 I print_info: LF token         = 128 'Ä'
0.00.051.596 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.596 I print_info: max token length = 1024
0.00.382.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.382.845 I load_tensors: offloading output layer to GPU
0.00.382.846 I load_tensors: offloaded 25/25 layers to GPU
0.00.382.877 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.382.879 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.384.418 I llama_init_from_model: n_seq_max     = 1
0.00.384.422 I llama_init_from_model: n_ctx         = 128
0.00.384.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.384.423 I llama_init_from_model: n_batch       = 128
0.00.384.424 I llama_init_from_model: n_ubatch      = 128
0.00.384.424 I llama_init_from_model: flash_attn    = 0
0.00.384.426 I llama_init_from_model: freq_base     = 10000.0
0.00.384.427 I llama_init_from_model: freq_scale    = 1
0.00.384.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.384.430 I ggml_metal_init: allocating
0.00.384.497 I ggml_metal_init: found device: Apple M4
0.00.384.506 I ggml_metal_init: picking default device: Apple M4
0.00.386.174 I ggml_metal_init: using embedded metal library
0.00.391.686 I ggml_metal_init: GPU name:   Apple M4
0.00.391.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.391.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.391.704 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.391.705 I ggml_metal_init: simdgroup reduction   = true
0.00.391.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.391.705 I ggml_metal_init: has residency sets    = true
0.00.391.706 I ggml_metal_init: has bfloat            = true
0.00.391.706 I ggml_metal_init: use bfloat            = true
0.00.391.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.391.712 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.412.583 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.416.268 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.416.276 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.416.338 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.419.697 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.419.699 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.419.700 I llama_init_from_model: graph nodes  = 967
0.00.419.700 I llama_init_from_model: graph splits = 2
0.00.419.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.419.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.450.671 I 
0.00.450.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.450.764 I perplexity: tokenizing the input ..
0.00.459.038 I perplexity: tokenization took 8.273 ms
0.00.459.050 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.590.475 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.591.810 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.591.823 I llama_perf_context_print:        load time =     439.84 ms
0.00.591.824 I llama_perf_context_print: prompt eval time =     131.19 ms /   128 tokens (    1.02 ms per token,   975.65 tokens per second)
0.00.591.825 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.591.826 I llama_perf_context_print:       total time =     141.16 ms /   129 tokens
0.00.592.219 I ggml_metal_free: deallocating

real	0m0.608s
user	0m0.092s
sys	0m0.091s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.439 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.443 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.589 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.508 I llama_model_loader: - type  f32:  194 tensors
0.00.025.509 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.509 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.509 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.510 I print_info: file format = GGUF V3 (latest)
0.00.025.510 I print_info: file type   = Q3_K - Medium
0.00.025.511 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.527 I load: special tokens cache size = 25
0.00.050.798 I load: token to piece cache size = 0.2984 MB
0.00.050.801 I print_info: arch             = gptneox
0.00.050.801 I print_info: vocab_only       = 0
0.00.050.802 I print_info: n_ctx_train      = 2048
0.00.050.802 I print_info: n_embd           = 2048
0.00.050.802 I print_info: n_layer          = 24
0.00.050.805 I print_info: n_head           = 16
0.00.050.806 I print_info: n_head_kv        = 16
0.00.050.806 I print_info: n_rot            = 32
0.00.050.806 I print_info: n_swa            = 0
0.00.050.806 I print_info: n_embd_head_k    = 128
0.00.050.806 I print_info: n_embd_head_v    = 128
0.00.050.807 I print_info: n_gqa            = 1
0.00.050.808 I print_info: n_embd_k_gqa     = 2048
0.00.050.809 I print_info: n_embd_v_gqa     = 2048
0.00.050.809 I print_info: f_norm_eps       = 1.0e-05
0.00.050.809 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.810 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.810 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.810 I print_info: f_logit_scale    = 0.0e+00
0.00.050.811 I print_info: n_ff             = 8192
0.00.050.811 I print_info: n_expert         = 0
0.00.050.811 I print_info: n_expert_used    = 0
0.00.050.811 I print_info: causal attn      = 1
0.00.050.811 I print_info: pooling type     = 0
0.00.050.812 I print_info: rope type        = 2
0.00.050.812 I print_info: rope scaling     = linear
0.00.050.812 I print_info: freq_base_train  = 10000.0
0.00.050.812 I print_info: freq_scale_train = 1
0.00.050.813 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.813 I print_info: rope_finetuned   = unknown
0.00.050.813 I print_info: ssm_d_conv       = 0
0.00.050.813 I print_info: ssm_d_inner      = 0
0.00.050.813 I print_info: ssm_d_state      = 0
0.00.050.814 I print_info: ssm_dt_rank      = 0
0.00.050.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.814 I print_info: model type       = 1.4B
0.00.050.814 I print_info: model params     = 1.41 B
0.00.050.814 I print_info: general.name     = 1.4B
0.00.050.817 I print_info: vocab type       = BPE
0.00.050.817 I print_info: n_vocab          = 50304
0.00.050.817 I print_info: n_merges         = 50009
0.00.050.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.818 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.818 I print_info: LF token         = 128 'Ä'
0.00.050.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.819 I print_info: max token length = 1024
0.00.458.692 I load_tensors: offloading 24 repeating layers to GPU
0.00.458.704 I load_tensors: offloading output layer to GPU
0.00.458.705 I load_tensors: offloaded 25/25 layers to GPU
0.00.458.747 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.458.748 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.460.157 I llama_init_from_model: n_seq_max     = 1
0.00.460.161 I llama_init_from_model: n_ctx         = 2048
0.00.460.162 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.460.162 I llama_init_from_model: n_batch       = 2048
0.00.460.163 I llama_init_from_model: n_ubatch      = 512
0.00.460.163 I llama_init_from_model: flash_attn    = 0
0.00.460.165 I llama_init_from_model: freq_base     = 10000.0
0.00.460.165 I llama_init_from_model: freq_scale    = 1
0.00.460.168 I ggml_metal_init: allocating
0.00.460.242 I ggml_metal_init: found device: Apple M4
0.00.460.250 I ggml_metal_init: picking default device: Apple M4
0.00.462.076 I ggml_metal_init: using embedded metal library
0.00.467.634 I ggml_metal_init: GPU name:   Apple M4
0.00.467.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.648 I ggml_metal_init: simdgroup reduction   = true
0.00.467.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.649 I ggml_metal_init: has residency sets    = true
0.00.467.649 I ggml_metal_init: has bfloat            = true
0.00.467.649 I ggml_metal_init: use bfloat            = true
0.00.467.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.487.688 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.024 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.546.031 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.546.055 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.550.822 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.550.825 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.550.825 I llama_init_from_model: graph nodes  = 967
0.00.550.825 I llama_init_from_model: graph splits = 2
0.00.550.832 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.550.958 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.550.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.821 I main: llama threadpool init, n_threads = 4
0.00.604.874 I 
0.00.604.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.898 I 
0.00.605.072 I sampler seed: 1234
0.00.605.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.605.086 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.605.087 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.605.087 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.347.607 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.347.608 I llama_perf_context_print:        load time =     595.20 ms
0.01.347.608 I llama_perf_context_print: prompt eval time =      40.20 ms /     7 tokens (    5.74 ms per token,   174.13 tokens per second)
0.01.347.609 I llama_perf_context_print:        eval time =     699.38 ms /    63 runs   (   11.10 ms per token,    90.08 tokens per second)
0.01.347.610 I llama_perf_context_print:       total time =     743.68 ms /    70 tokens
0.01.347.873 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.122s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.949 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.719 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.737 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.738 I llama_model_loader: - type  f32:  194 tensors
0.00.024.738 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.738 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.738 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.739 I print_info: file format = GGUF V3 (latest)
0.00.024.740 I print_info: file type   = Q3_K - Medium
0.00.024.741 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.826 I load: special tokens cache size = 25
0.00.049.709 I load: token to piece cache size = 0.2984 MB
0.00.049.712 I print_info: arch             = gptneox
0.00.049.712 I print_info: vocab_only       = 0
0.00.049.712 I print_info: n_ctx_train      = 2048
0.00.049.712 I print_info: n_embd           = 2048
0.00.049.713 I print_info: n_layer          = 24
0.00.049.715 I print_info: n_head           = 16
0.00.049.716 I print_info: n_head_kv        = 16
0.00.049.716 I print_info: n_rot            = 32
0.00.049.717 I print_info: n_swa            = 0
0.00.049.717 I print_info: n_embd_head_k    = 128
0.00.049.717 I print_info: n_embd_head_v    = 128
0.00.049.718 I print_info: n_gqa            = 1
0.00.049.718 I print_info: n_embd_k_gqa     = 2048
0.00.049.719 I print_info: n_embd_v_gqa     = 2048
0.00.049.720 I print_info: f_norm_eps       = 1.0e-05
0.00.049.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.720 I print_info: f_logit_scale    = 0.0e+00
0.00.049.721 I print_info: n_ff             = 8192
0.00.049.721 I print_info: n_expert         = 0
0.00.049.721 I print_info: n_expert_used    = 0
0.00.049.721 I print_info: causal attn      = 1
0.00.049.722 I print_info: pooling type     = 0
0.00.049.722 I print_info: rope type        = 2
0.00.049.722 I print_info: rope scaling     = linear
0.00.049.722 I print_info: freq_base_train  = 10000.0
0.00.049.723 I print_info: freq_scale_train = 1
0.00.049.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.723 I print_info: rope_finetuned   = unknown
0.00.049.723 I print_info: ssm_d_conv       = 0
0.00.049.723 I print_info: ssm_d_inner      = 0
0.00.049.723 I print_info: ssm_d_state      = 0
0.00.049.726 I print_info: ssm_dt_rank      = 0
0.00.049.726 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.726 I print_info: model type       = 1.4B
0.00.049.727 I print_info: model params     = 1.41 B
0.00.049.727 I print_info: general.name     = 1.4B
0.00.049.727 I print_info: vocab type       = BPE
0.00.049.727 I print_info: n_vocab          = 50304
0.00.049.728 I print_info: n_merges         = 50009
0.00.049.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.728 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.729 I print_info: LF token         = 128 'Ä'
0.00.049.733 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.733 I print_info: max token length = 1024
0.00.462.373 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.386 I load_tensors: offloading output layer to GPU
0.00.462.387 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.423 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.425 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.463.925 I llama_init_from_model: n_seq_max     = 1
0.00.463.929 I llama_init_from_model: n_ctx         = 128
0.00.463.930 I llama_init_from_model: n_ctx_per_seq = 128
0.00.463.930 I llama_init_from_model: n_batch       = 128
0.00.463.930 I llama_init_from_model: n_ubatch      = 128
0.00.463.931 I llama_init_from_model: flash_attn    = 0
0.00.463.932 I llama_init_from_model: freq_base     = 10000.0
0.00.463.932 I llama_init_from_model: freq_scale    = 1
0.00.463.933 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.463.938 I ggml_metal_init: allocating
0.00.464.038 I ggml_metal_init: found device: Apple M4
0.00.464.048 I ggml_metal_init: picking default device: Apple M4
0.00.466.218 I ggml_metal_init: using embedded metal library
0.00.472.594 I ggml_metal_init: GPU name:   Apple M4
0.00.472.607 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.472.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.472.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.472.609 I ggml_metal_init: simdgroup reduction   = true
0.00.472.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.472.610 I ggml_metal_init: has residency sets    = true
0.00.472.610 I ggml_metal_init: has bfloat            = true
0.00.472.610 I ggml_metal_init: use bfloat            = true
0.00.472.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.472.616 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.493.719 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.497.327 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.497.334 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.497.378 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.500.709 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.500.711 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.500.711 I llama_init_from_model: graph nodes  = 967
0.00.500.712 I llama_init_from_model: graph splits = 2
0.00.500.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.500.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.530.555 I 
0.00.530.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.530.656 I perplexity: tokenizing the input ..
0.00.539.373 I perplexity: tokenization took 8.717 ms
0.00.539.386 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.670.757 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.672.266 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.672.274 I llama_perf_context_print:        load time =     521.60 ms
0.00.672.276 I llama_perf_context_print: prompt eval time =     131.13 ms /   128 tokens (    1.02 ms per token,   976.15 tokens per second)
0.00.672.276 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.672.277 I llama_perf_context_print:       total time =     141.72 ms /   129 tokens
0.00.672.696 I ggml_metal_free: deallocating

real	0m0.687s
user	0m0.092s
sys	0m0.113s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.783 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.392 I llama_model_loader: - type  f32:  194 tensors
0.00.026.392 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.393 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.393 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.393 I print_info: file format = GGUF V3 (latest)
0.00.026.394 I print_info: file type   = Q4_K - Medium
0.00.026.399 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.092 I load: special tokens cache size = 25
0.00.051.887 I load: token to piece cache size = 0.2984 MB
0.00.051.889 I print_info: arch             = gptneox
0.00.051.890 I print_info: vocab_only       = 0
0.00.051.890 I print_info: n_ctx_train      = 2048
0.00.051.890 I print_info: n_embd           = 2048
0.00.051.890 I print_info: n_layer          = 24
0.00.051.894 I print_info: n_head           = 16
0.00.051.894 I print_info: n_head_kv        = 16
0.00.051.895 I print_info: n_rot            = 32
0.00.051.895 I print_info: n_swa            = 0
0.00.051.895 I print_info: n_embd_head_k    = 128
0.00.051.895 I print_info: n_embd_head_v    = 128
0.00.051.896 I print_info: n_gqa            = 1
0.00.051.897 I print_info: n_embd_k_gqa     = 2048
0.00.051.897 I print_info: n_embd_v_gqa     = 2048
0.00.051.898 I print_info: f_norm_eps       = 1.0e-05
0.00.051.898 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.899 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.899 I print_info: f_logit_scale    = 0.0e+00
0.00.051.899 I print_info: n_ff             = 8192
0.00.051.900 I print_info: n_expert         = 0
0.00.051.900 I print_info: n_expert_used    = 0
0.00.051.900 I print_info: causal attn      = 1
0.00.051.900 I print_info: pooling type     = 0
0.00.051.900 I print_info: rope type        = 2
0.00.051.901 I print_info: rope scaling     = linear
0.00.051.901 I print_info: freq_base_train  = 10000.0
0.00.051.901 I print_info: freq_scale_train = 1
0.00.051.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.902 I print_info: rope_finetuned   = unknown
0.00.051.902 I print_info: ssm_d_conv       = 0
0.00.051.902 I print_info: ssm_d_inner      = 0
0.00.051.902 I print_info: ssm_d_state      = 0
0.00.051.903 I print_info: ssm_dt_rank      = 0
0.00.051.903 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.903 I print_info: model type       = 1.4B
0.00.051.903 I print_info: model params     = 1.41 B
0.00.051.904 I print_info: general.name     = 1.4B
0.00.051.904 I print_info: vocab type       = BPE
0.00.051.904 I print_info: n_vocab          = 50304
0.00.051.904 I print_info: n_merges         = 50009
0.00.051.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.905 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.905 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.906 I print_info: LF token         = 128 'Ä'
0.00.051.906 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.906 I print_info: max token length = 1024
0.00.526.856 I load_tensors: offloading 24 repeating layers to GPU
0.00.526.869 I load_tensors: offloading output layer to GPU
0.00.526.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.526.903 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.526.904 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.528.303 I llama_init_from_model: n_seq_max     = 1
0.00.528.307 I llama_init_from_model: n_ctx         = 2048
0.00.528.308 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.528.308 I llama_init_from_model: n_batch       = 2048
0.00.528.308 I llama_init_from_model: n_ubatch      = 512
0.00.528.309 I llama_init_from_model: flash_attn    = 0
0.00.528.311 I llama_init_from_model: freq_base     = 10000.0
0.00.528.312 I llama_init_from_model: freq_scale    = 1
0.00.528.314 I ggml_metal_init: allocating
0.00.528.385 I ggml_metal_init: found device: Apple M4
0.00.528.395 I ggml_metal_init: picking default device: Apple M4
0.00.530.176 I ggml_metal_init: using embedded metal library
0.00.536.750 I ggml_metal_init: GPU name:   Apple M4
0.00.536.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.757 I ggml_metal_init: simdgroup reduction   = true
0.00.536.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.758 I ggml_metal_init: has residency sets    = true
0.00.536.758 I ggml_metal_init: has bfloat            = true
0.00.536.758 I ggml_metal_init: use bfloat            = true
0.00.536.759 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.428 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.608.396 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.421 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.863 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.612.866 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.612.866 I llama_init_from_model: graph nodes  = 967
0.00.612.866 I llama_init_from_model: graph splits = 2
0.00.612.872 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.612.982 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.612.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.040 I main: llama threadpool init, n_threads = 4
0.00.669.089 I 
0.00.669.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.112 I 
0.00.669.331 I sampler seed: 1234
0.00.669.336 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.347 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.347 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.348 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.431.569 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.431.570 I llama_perf_context_print:        load time =     658.36 ms
0.01.431.571 I llama_perf_context_print: prompt eval time =      47.09 ms /     7 tokens (    6.73 ms per token,   148.64 tokens per second)
0.01.431.572 I llama_perf_context_print:        eval time =     712.15 ms /    63 runs   (   11.30 ms per token,    88.46 tokens per second)
0.01.431.572 I llama_perf_context_print:       total time =     763.43 ms /    70 tokens
0.01.431.848 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.122s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.531 I llama_model_loader: - type  f32:  194 tensors
0.00.026.531 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.532 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.532 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.532 I print_info: file format = GGUF V3 (latest)
0.00.026.533 I print_info: file type   = Q4_K - Medium
0.00.026.535 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.356 I load: special tokens cache size = 25
0.00.051.405 I load: token to piece cache size = 0.2984 MB
0.00.051.408 I print_info: arch             = gptneox
0.00.051.408 I print_info: vocab_only       = 0
0.00.051.409 I print_info: n_ctx_train      = 2048
0.00.051.409 I print_info: n_embd           = 2048
0.00.051.409 I print_info: n_layer          = 24
0.00.051.412 I print_info: n_head           = 16
0.00.051.413 I print_info: n_head_kv        = 16
0.00.051.413 I print_info: n_rot            = 32
0.00.051.413 I print_info: n_swa            = 0
0.00.051.413 I print_info: n_embd_head_k    = 128
0.00.051.413 I print_info: n_embd_head_v    = 128
0.00.051.414 I print_info: n_gqa            = 1
0.00.051.415 I print_info: n_embd_k_gqa     = 2048
0.00.051.416 I print_info: n_embd_v_gqa     = 2048
0.00.051.416 I print_info: f_norm_eps       = 1.0e-05
0.00.051.417 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.417 I print_info: f_logit_scale    = 0.0e+00
0.00.051.418 I print_info: n_ff             = 8192
0.00.051.418 I print_info: n_expert         = 0
0.00.051.418 I print_info: n_expert_used    = 0
0.00.051.418 I print_info: causal attn      = 1
0.00.051.418 I print_info: pooling type     = 0
0.00.051.418 I print_info: rope type        = 2
0.00.051.419 I print_info: rope scaling     = linear
0.00.051.419 I print_info: freq_base_train  = 10000.0
0.00.051.419 I print_info: freq_scale_train = 1
0.00.051.420 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.420 I print_info: rope_finetuned   = unknown
0.00.051.420 I print_info: ssm_d_conv       = 0
0.00.051.420 I print_info: ssm_d_inner      = 0
0.00.051.420 I print_info: ssm_d_state      = 0
0.00.051.422 I print_info: ssm_dt_rank      = 0
0.00.051.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.423 I print_info: model type       = 1.4B
0.00.051.423 I print_info: model params     = 1.41 B
0.00.051.423 I print_info: general.name     = 1.4B
0.00.051.424 I print_info: vocab type       = BPE
0.00.051.424 I print_info: n_vocab          = 50304
0.00.051.424 I print_info: n_merges         = 50009
0.00.051.424 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.425 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.425 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.425 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.425 I print_info: LF token         = 128 'Ä'
0.00.051.425 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.426 I print_info: max token length = 1024
0.00.526.149 I load_tensors: offloading 24 repeating layers to GPU
0.00.526.166 I load_tensors: offloading output layer to GPU
0.00.526.166 I load_tensors: offloaded 25/25 layers to GPU
0.00.526.199 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.526.200 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.527.704 I llama_init_from_model: n_seq_max     = 1
0.00.527.710 I llama_init_from_model: n_ctx         = 128
0.00.527.710 I llama_init_from_model: n_ctx_per_seq = 128
0.00.527.711 I llama_init_from_model: n_batch       = 128
0.00.527.711 I llama_init_from_model: n_ubatch      = 128
0.00.527.712 I llama_init_from_model: flash_attn    = 0
0.00.527.713 I llama_init_from_model: freq_base     = 10000.0
0.00.527.714 I llama_init_from_model: freq_scale    = 1
0.00.527.714 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.527.717 I ggml_metal_init: allocating
0.00.527.794 I ggml_metal_init: found device: Apple M4
0.00.527.803 I ggml_metal_init: picking default device: Apple M4
0.00.529.530 I ggml_metal_init: using embedded metal library
0.00.536.175 I ggml_metal_init: GPU name:   Apple M4
0.00.536.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.182 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.183 I ggml_metal_init: simdgroup reduction   = true
0.00.536.183 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.183 I ggml_metal_init: has residency sets    = true
0.00.536.184 I ggml_metal_init: has bfloat            = true
0.00.536.184 I ggml_metal_init: use bfloat            = true
0.00.536.185 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.741 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.558.246 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.558.254 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.558.324 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.561.686 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.561.688 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.561.688 I llama_init_from_model: graph nodes  = 967
0.00.561.689 I llama_init_from_model: graph splits = 2
0.00.561.692 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.561.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.481 I 
0.00.590.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.582 I perplexity: tokenizing the input ..
0.00.600.297 I perplexity: tokenization took 9.713 ms
0.00.600.308 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.804 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.736.097 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.736.116 I llama_perf_context_print:        load time =     579.97 ms
0.00.736.117 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.33 tokens per second)
0.00.736.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.118 I llama_perf_context_print:       total time =     145.64 ms /   129 tokens
0.00.736.543 I ggml_metal_free: deallocating

real	0m0.753s
user	0m0.094s
sys	0m0.119s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.555 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.320 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.324 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.503 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.503 I llama_model_loader: - type  f32:  194 tensors
0.00.028.504 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.504 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.505 I print_info: file format = GGUF V3 (latest)
0.00.028.505 I print_info: file type   = Q5_K - Medium
0.00.028.506 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.048.190 I load: special tokens cache size = 25
0.00.054.495 I load: token to piece cache size = 0.2984 MB
0.00.054.498 I print_info: arch             = gptneox
0.00.054.498 I print_info: vocab_only       = 0
0.00.054.498 I print_info: n_ctx_train      = 2048
0.00.054.499 I print_info: n_embd           = 2048
0.00.054.499 I print_info: n_layer          = 24
0.00.054.502 I print_info: n_head           = 16
0.00.054.503 I print_info: n_head_kv        = 16
0.00.054.503 I print_info: n_rot            = 32
0.00.054.503 I print_info: n_swa            = 0
0.00.054.503 I print_info: n_embd_head_k    = 128
0.00.054.504 I print_info: n_embd_head_v    = 128
0.00.054.504 I print_info: n_gqa            = 1
0.00.054.505 I print_info: n_embd_k_gqa     = 2048
0.00.054.506 I print_info: n_embd_v_gqa     = 2048
0.00.054.506 I print_info: f_norm_eps       = 1.0e-05
0.00.054.507 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.507 I print_info: f_logit_scale    = 0.0e+00
0.00.054.508 I print_info: n_ff             = 8192
0.00.054.508 I print_info: n_expert         = 0
0.00.054.508 I print_info: n_expert_used    = 0
0.00.054.509 I print_info: causal attn      = 1
0.00.054.509 I print_info: pooling type     = 0
0.00.054.510 I print_info: rope type        = 2
0.00.054.512 I print_info: rope scaling     = linear
0.00.054.512 I print_info: freq_base_train  = 10000.0
0.00.054.512 I print_info: freq_scale_train = 1
0.00.054.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.513 I print_info: rope_finetuned   = unknown
0.00.054.513 I print_info: ssm_d_conv       = 0
0.00.054.513 I print_info: ssm_d_inner      = 0
0.00.054.513 I print_info: ssm_d_state      = 0
0.00.054.513 I print_info: ssm_dt_rank      = 0
0.00.054.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.514 I print_info: model type       = 1.4B
0.00.054.514 I print_info: model params     = 1.41 B
0.00.054.514 I print_info: general.name     = 1.4B
0.00.054.515 I print_info: vocab type       = BPE
0.00.054.515 I print_info: n_vocab          = 50304
0.00.054.515 I print_info: n_merges         = 50009
0.00.054.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.516 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.517 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.518 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.518 I print_info: LF token         = 128 'Ä'
0.00.054.518 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.518 I print_info: max token length = 1024
0.00.610.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.815 I load_tensors: offloading output layer to GPU
0.00.610.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.843 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.610.846 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.612.272 I llama_init_from_model: n_seq_max     = 1
0.00.612.274 I llama_init_from_model: n_ctx         = 2048
0.00.612.275 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.612.275 I llama_init_from_model: n_batch       = 2048
0.00.612.276 I llama_init_from_model: n_ubatch      = 512
0.00.612.276 I llama_init_from_model: flash_attn    = 0
0.00.612.277 I llama_init_from_model: freq_base     = 10000.0
0.00.612.278 I llama_init_from_model: freq_scale    = 1
0.00.612.279 I ggml_metal_init: allocating
0.00.612.296 I ggml_metal_init: found device: Apple M4
0.00.612.301 I ggml_metal_init: picking default device: Apple M4
0.00.613.768 I ggml_metal_init: using embedded metal library
0.00.619.906 I ggml_metal_init: GPU name:   Apple M4
0.00.619.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.911 I ggml_metal_init: simdgroup reduction   = true
0.00.619.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.911 I ggml_metal_init: has residency sets    = true
0.00.619.911 I ggml_metal_init: has bfloat            = true
0.00.619.912 I ggml_metal_init: use bfloat            = true
0.00.619.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.419 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.215 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.221 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.247 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.824 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.826 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.827 I llama_init_from_model: graph nodes  = 967
0.00.698.827 I llama_init_from_model: graph splits = 2
0.00.698.832 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.954 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.464 I main: llama threadpool init, n_threads = 4
0.00.762.512 I 
0.00.762.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.537 I 
0.00.762.764 I sampler seed: 1234
0.00.762.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.782 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.782 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.613.791 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49719.89 tokens per second)
0.01.613.792 I llama_perf_context_print:        load time =     749.99 ms
0.01.613.793 I llama_perf_context_print: prompt eval time =      51.57 ms /     7 tokens (    7.37 ms per token,   135.74 tokens per second)
0.01.613.794 I llama_perf_context_print:        eval time =     796.72 ms /    63 runs   (   12.65 ms per token,    79.07 tokens per second)
0.01.613.794 I llama_perf_context_print:       total time =     852.24 ms /    70 tokens
0.01.614.074 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.122s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.220 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.221 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.224 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.215 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.183 I llama_model_loader: - type  f32:  194 tensors
0.00.025.183 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.184 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.184 I print_info: file format = GGUF V3 (latest)
0.00.025.185 I print_info: file type   = Q5_K - Medium
0.00.025.186 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.810 I load: special tokens cache size = 25
0.00.050.995 I load: token to piece cache size = 0.2984 MB
0.00.050.999 I print_info: arch             = gptneox
0.00.050.999 I print_info: vocab_only       = 0
0.00.050.999 I print_info: n_ctx_train      = 2048
0.00.050.999 I print_info: n_embd           = 2048
0.00.050.999 I print_info: n_layer          = 24
0.00.051.003 I print_info: n_head           = 16
0.00.051.004 I print_info: n_head_kv        = 16
0.00.051.004 I print_info: n_rot            = 32
0.00.051.007 I print_info: n_swa            = 0
0.00.051.007 I print_info: n_embd_head_k    = 128
0.00.051.007 I print_info: n_embd_head_v    = 128
0.00.051.008 I print_info: n_gqa            = 1
0.00.051.010 I print_info: n_embd_k_gqa     = 2048
0.00.051.010 I print_info: n_embd_v_gqa     = 2048
0.00.051.011 I print_info: f_norm_eps       = 1.0e-05
0.00.051.011 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.011 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.012 I print_info: f_logit_scale    = 0.0e+00
0.00.051.014 I print_info: n_ff             = 8192
0.00.051.015 I print_info: n_expert         = 0
0.00.051.015 I print_info: n_expert_used    = 0
0.00.051.015 I print_info: causal attn      = 1
0.00.051.015 I print_info: pooling type     = 0
0.00.051.015 I print_info: rope type        = 2
0.00.051.015 I print_info: rope scaling     = linear
0.00.051.016 I print_info: freq_base_train  = 10000.0
0.00.051.016 I print_info: freq_scale_train = 1
0.00.051.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.016 I print_info: rope_finetuned   = unknown
0.00.051.017 I print_info: ssm_d_conv       = 0
0.00.051.017 I print_info: ssm_d_inner      = 0
0.00.051.017 I print_info: ssm_d_state      = 0
0.00.051.017 I print_info: ssm_dt_rank      = 0
0.00.051.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.023 I print_info: model type       = 1.4B
0.00.051.024 I print_info: model params     = 1.41 B
0.00.051.024 I print_info: general.name     = 1.4B
0.00.051.024 I print_info: vocab type       = BPE
0.00.051.025 I print_info: n_vocab          = 50304
0.00.051.025 I print_info: n_merges         = 50009
0.00.051.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.026 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.026 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.026 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.026 I print_info: LF token         = 128 'Ä'
0.00.051.027 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.027 I print_info: max token length = 1024
0.00.602.391 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.395 I load_tensors: offloading output layer to GPU
0.00.602.395 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.418 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.602.420 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.603.874 I llama_init_from_model: n_seq_max     = 1
0.00.603.877 I llama_init_from_model: n_ctx         = 128
0.00.603.877 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.878 I llama_init_from_model: n_batch       = 128
0.00.603.878 I llama_init_from_model: n_ubatch      = 128
0.00.603.878 I llama_init_from_model: flash_attn    = 0
0.00.603.879 I llama_init_from_model: freq_base     = 10000.0
0.00.603.880 I llama_init_from_model: freq_scale    = 1
0.00.603.881 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.882 I ggml_metal_init: allocating
0.00.603.949 I ggml_metal_init: found device: Apple M4
0.00.603.957 I ggml_metal_init: picking default device: Apple M4
0.00.605.407 I ggml_metal_init: using embedded metal library
0.00.611.303 I ggml_metal_init: GPU name:   Apple M4
0.00.611.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.308 I ggml_metal_init: simdgroup reduction   = true
0.00.611.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.309 I ggml_metal_init: has residency sets    = true
0.00.611.309 I ggml_metal_init: has bfloat            = true
0.00.611.309 I ggml_metal_init: use bfloat            = true
0.00.611.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.343 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.011 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.038 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.327 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.329 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.329 I llama_init_from_model: graph nodes  = 967
0.00.635.330 I llama_init_from_model: graph splits = 2
0.00.635.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.071 I 
0.00.665.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.154 I perplexity: tokenizing the input ..
0.00.672.980 I perplexity: tokenization took 7.824 ms
0.00.672.993 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.838 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.274 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.289 I llama_perf_context_print:        load time =     656.04 ms
0.00.814.291 I llama_perf_context_print: prompt eval time =     139.60 ms /   128 tokens (    1.09 ms per token,   916.87 tokens per second)
0.00.814.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.292 I llama_perf_context_print:       total time =     149.22 ms /   129 tokens
0.00.814.680 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.090s
sys	0m0.131s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.421 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.523 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.527 I llama_model_loader: - type  f32:  194 tensors
0.00.026.527 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.528 I print_info: file format = GGUF V3 (latest)
0.00.026.529 I print_info: file type   = Q6_K
0.00.026.530 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.819 I load: special tokens cache size = 25
0.00.053.020 I load: token to piece cache size = 0.2984 MB
0.00.053.025 I print_info: arch             = gptneox
0.00.053.026 I print_info: vocab_only       = 0
0.00.053.026 I print_info: n_ctx_train      = 2048
0.00.053.026 I print_info: n_embd           = 2048
0.00.053.026 I print_info: n_layer          = 24
0.00.053.030 I print_info: n_head           = 16
0.00.053.031 I print_info: n_head_kv        = 16
0.00.053.031 I print_info: n_rot            = 32
0.00.053.031 I print_info: n_swa            = 0
0.00.053.031 I print_info: n_embd_head_k    = 128
0.00.053.032 I print_info: n_embd_head_v    = 128
0.00.053.032 I print_info: n_gqa            = 1
0.00.053.033 I print_info: n_embd_k_gqa     = 2048
0.00.053.034 I print_info: n_embd_v_gqa     = 2048
0.00.053.035 I print_info: f_norm_eps       = 1.0e-05
0.00.053.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.037 I print_info: f_logit_scale    = 0.0e+00
0.00.053.039 I print_info: n_ff             = 8192
0.00.053.040 I print_info: n_expert         = 0
0.00.053.040 I print_info: n_expert_used    = 0
0.00.053.040 I print_info: causal attn      = 1
0.00.053.040 I print_info: pooling type     = 0
0.00.053.040 I print_info: rope type        = 2
0.00.053.040 I print_info: rope scaling     = linear
0.00.053.041 I print_info: freq_base_train  = 10000.0
0.00.053.041 I print_info: freq_scale_train = 1
0.00.053.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.041 I print_info: rope_finetuned   = unknown
0.00.053.041 I print_info: ssm_d_conv       = 0
0.00.053.041 I print_info: ssm_d_inner      = 0
0.00.053.042 I print_info: ssm_d_state      = 0
0.00.053.043 I print_info: ssm_dt_rank      = 0
0.00.053.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.044 I print_info: model type       = 1.4B
0.00.053.044 I print_info: model params     = 1.41 B
0.00.053.044 I print_info: general.name     = 1.4B
0.00.053.045 I print_info: vocab type       = BPE
0.00.053.045 I print_info: n_vocab          = 50304
0.00.053.045 I print_info: n_merges         = 50009
0.00.053.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.046 I print_info: LF token         = 128 'Ä'
0.00.053.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.047 I print_info: max token length = 1024
0.00.657.224 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.229 I load_tensors: offloading output layer to GPU
0.00.657.229 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.255 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.657.256 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.658.582 I llama_init_from_model: n_seq_max     = 1
0.00.658.585 I llama_init_from_model: n_ctx         = 2048
0.00.658.585 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.586 I llama_init_from_model: n_batch       = 2048
0.00.658.586 I llama_init_from_model: n_ubatch      = 512
0.00.658.586 I llama_init_from_model: flash_attn    = 0
0.00.658.588 I llama_init_from_model: freq_base     = 10000.0
0.00.658.589 I llama_init_from_model: freq_scale    = 1
0.00.658.590 I ggml_metal_init: allocating
0.00.658.634 I ggml_metal_init: found device: Apple M4
0.00.658.639 I ggml_metal_init: picking default device: Apple M4
0.00.660.132 I ggml_metal_init: using embedded metal library
0.00.666.202 I ggml_metal_init: GPU name:   Apple M4
0.00.666.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.207 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.208 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.209 I ggml_metal_init: simdgroup reduction   = true
0.00.666.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.209 I ggml_metal_init: has residency sets    = true
0.00.666.210 I ggml_metal_init: has bfloat            = true
0.00.666.210 I ggml_metal_init: use bfloat            = true
0.00.666.211 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.255 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.656 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.663 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.684 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.988 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.990 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.990 I llama_init_from_model: graph nodes  = 967
0.00.741.990 I llama_init_from_model: graph splits = 2
0.00.741.995 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.119 I main: llama threadpool init, n_threads = 4
0.00.806.155 I 
0.00.806.177 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.177 I 
0.00.806.342 I sampler seed: 1234
0.00.806.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.381 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.384 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.385 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.687.993 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.687.993 I llama_perf_context_print:        load time =     795.77 ms
0.01.687.994 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.74 tokens per second)
0.01.687.998 I llama_perf_context_print:        eval time =     824.28 ms /    63 runs   (   13.08 ms per token,    76.43 tokens per second)
0.01.688.000 I llama_perf_context_print:       total time =     882.80 ms /    70 tokens
0.01.688.218 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.123s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4563 (225d2e0c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.178 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.129 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.967 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.770 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.772 I llama_model_loader: - type  f32:  194 tensors
0.00.025.773 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.773 I print_info: file format = GGUF V3 (latest)
0.00.025.774 I print_info: file type   = Q6_K
0.00.025.774 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.389 I load: special tokens cache size = 25
0.00.051.703 I load: token to piece cache size = 0.2984 MB
0.00.051.706 I print_info: arch             = gptneox
0.00.051.706 I print_info: vocab_only       = 0
0.00.051.706 I print_info: n_ctx_train      = 2048
0.00.051.706 I print_info: n_embd           = 2048
0.00.051.707 I print_info: n_layer          = 24
0.00.051.709 I print_info: n_head           = 16
0.00.051.710 I print_info: n_head_kv        = 16
0.00.051.710 I print_info: n_rot            = 32
0.00.051.711 I print_info: n_swa            = 0
0.00.051.711 I print_info: n_embd_head_k    = 128
0.00.051.711 I print_info: n_embd_head_v    = 128
0.00.051.712 I print_info: n_gqa            = 1
0.00.051.712 I print_info: n_embd_k_gqa     = 2048
0.00.051.713 I print_info: n_embd_v_gqa     = 2048
0.00.051.714 I print_info: f_norm_eps       = 1.0e-05
0.00.051.714 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.714 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.714 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.715 I print_info: f_logit_scale    = 0.0e+00
0.00.051.715 I print_info: n_ff             = 8192
0.00.051.715 I print_info: n_expert         = 0
0.00.051.716 I print_info: n_expert_used    = 0
0.00.051.716 I print_info: causal attn      = 1
0.00.051.716 I print_info: pooling type     = 0
0.00.051.716 I print_info: rope type        = 2
0.00.051.716 I print_info: rope scaling     = linear
0.00.051.717 I print_info: freq_base_train  = 10000.0
0.00.051.717 I print_info: freq_scale_train = 1
0.00.051.717 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.717 I print_info: rope_finetuned   = unknown
0.00.051.718 I print_info: ssm_d_conv       = 0
0.00.051.718 I print_info: ssm_d_inner      = 0
0.00.051.718 I print_info: ssm_d_state      = 0
0.00.051.720 I print_info: ssm_dt_rank      = 0
0.00.051.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.720 I print_info: model type       = 1.4B
0.00.051.721 I print_info: model params     = 1.41 B
0.00.051.721 I print_info: general.name     = 1.4B
0.00.051.721 I print_info: vocab type       = BPE
0.00.051.721 I print_info: n_vocab          = 50304
0.00.051.722 I print_info: n_merges         = 50009
0.00.051.722 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.722 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.722 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.722 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.723 I print_info: LF token         = 128 'Ä'
0.00.051.723 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.725 I print_info: max token length = 1024
0.00.230.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.230.726 I load_tensors: offloading output layer to GPU
0.00.230.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.230.740 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.230.741 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.231.787 I llama_init_from_model: n_seq_max     = 1
0.00.231.790 I llama_init_from_model: n_ctx         = 128
0.00.231.790 I llama_init_from_model: n_ctx_per_seq = 128
0.00.231.790 I llama_init_from_model: n_batch       = 128
0.00.231.790 I llama_init_from_model: n_ubatch      = 128
0.00.231.791 I llama_init_from_model: flash_attn    = 0
0.00.231.791 I llama_init_from_model: freq_base     = 10000.0
0.00.231.792 I llama_init_from_model: freq_scale    = 1
0.00.231.792 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.231.793 I ggml_metal_init: allocating
0.00.231.824 I ggml_metal_init: found device: Apple M4
0.00.231.829 I ggml_metal_init: picking default device: Apple M4
0.00.232.897 I ggml_metal_init: using embedded metal library
0.00.237.456 I ggml_metal_init: GPU name:   Apple M4
0.00.237.460 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.237.461 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.237.461 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.237.462 I ggml_metal_init: simdgroup reduction   = true
0.00.237.462 I ggml_metal_init: simdgroup matrix mul. = true
0.00.237.462 I ggml_metal_init: has residency sets    = true
0.00.237.462 I ggml_metal_init: has bfloat            = true
0.00.237.463 I ggml_metal_init: use bfloat            = true
0.00.237.463 I ggml_metal_init: hasUnifiedMemory      = true
0.00.237.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.250.526 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.252.403 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.252.406 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.252.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.254.233 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.254.234 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.254.235 I llama_init_from_model: graph nodes  = 967
0.00.254.235 I llama_init_from_model: graph splits = 2
0.00.254.236 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.254.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.283.042 I 
0.00.283.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.283.084 I perplexity: tokenizing the input ..
0.00.291.024 I perplexity: tokenization took 7.938 ms
0.00.291.038 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.429.931 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.431.369 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.431.378 I llama_perf_context_print:        load time =     272.86 ms
0.00.431.379 I llama_perf_context_print: prompt eval time =     138.66 ms /   128 tokens (    1.08 ms per token,   923.12 tokens per second)
0.00.431.380 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.431.381 I llama_perf_context_print:       total time =     148.34 ms /   129 tokens
0.00.431.741 I ggml_metal_free: deallocating

real	0m0.448s
user	0m0.084s
sys	0m0.075s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4563 (225d2e0c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1059045f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105904d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105905320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1059058d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105905e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105906430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1059069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105906f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105907540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105907a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105907f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105908440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105908f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105909710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105909f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10590a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10590ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10590b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10590bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10590c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10590ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10590d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10590d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10590e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10590e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10590eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10590f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10590fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105910310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1059105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105910a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105910d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1059115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105911b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105911dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105912260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105912700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105912ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105913040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1059134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105913980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105913e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1059142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105914760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105914a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105915030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105915640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105915f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105916570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105916b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105917190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1059177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105917db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1059183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105918bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105919050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1059194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1059197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105919dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10591a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10591a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10591ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10591b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10591b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10591baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10591bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10591c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10591c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10591cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10591d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10591d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10591db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10591dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10591e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10591ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10591efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10591f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10591fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10591ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105920520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105920a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105920fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105921510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105921a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105921fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105922500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105922a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105922fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1059234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105923a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105923f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1059244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105924a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105924f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1059254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105925a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105925f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105915c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1059263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105926b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1059270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105927630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105927b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1059280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105928620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105928b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1059290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105929610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105929b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10592a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10592a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10592ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10592b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10592b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10592b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10592be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10592c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10592c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10592cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10592d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10592d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10592da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10592dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10592e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10592e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10592ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10592f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10592f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10592faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10592ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1059303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105930880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105930d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1059311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105931660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105931b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105931fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105932440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1059328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105932d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105933220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1059336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105933b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105934000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1059344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105934940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105934de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105935280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105935720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105935bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105936060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105936500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1059369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105936e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1059372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105937780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105937c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1059380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105938560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105938a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105938ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105939340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1059397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105939c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10593a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10593a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10593aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10593af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10593b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10593b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10593bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10593c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10593c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10593cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10593cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10593d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10593d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10593dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10593e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10593e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10593eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10593efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10593f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10593f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10593fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105940240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1059406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105940b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105941020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1059414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105941960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105941e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1059422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1059427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105942d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105943290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1059437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105943aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1059440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1059446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105944cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1059454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105945960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105945c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105946230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105946840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105947030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1059474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105947970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105947e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1059485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105948b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105949060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1059495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105949b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10594a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10594a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10594aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10594b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10594b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10594bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10594c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10594c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10594cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10594d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10594d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10594dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10594e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10594e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10594eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10594f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10594f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10594faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10594fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105950540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105950a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105950fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105951530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105951a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105951fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105952520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105952a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105952fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105953510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105953a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105953fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105954500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105954a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105954fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1059554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105955a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105955f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1059564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105956a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105956f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1059574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105957a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105957f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1059584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105958a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105958f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1059594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105959a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105959f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10595a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10595a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10595af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10595b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10595b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10595bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10595c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10595c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10595cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10595cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10595d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10595d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10595dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10595e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10595e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10595eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10595f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10595f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10595f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105960110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105960830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105960f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105961670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105961930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105962120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1059623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1059629f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.883.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.883.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106004dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106005240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1060056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106005b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106005f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106006400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106006870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106006ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106007150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1060075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106007a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106008120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106008c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1060093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106009c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10600a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10600aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10600b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10600b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10600bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10600c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10600cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10600d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10600dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10600e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10600e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10600e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10600ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10600f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10600f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10600fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10600ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106010430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1060106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106010b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106010fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106011440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1060118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106011d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106012190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106012a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106012ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106013350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1060137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106013c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1060140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106014510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106014980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106014df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106015260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1060156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106015b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106015fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106016420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106016890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106016e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106017300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106017770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106017be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106018050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1060184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106018930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106018da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106019210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106019680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106019af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106019f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10601a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10601a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10601acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10601b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10601b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10601ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10601be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10601c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10601c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10601cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10601d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10601d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10601d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10601dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10601e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10601e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10601ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10601ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10601f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10601f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10601fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106020100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106020570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1060209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106020e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1060212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106021730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106021ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106022010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106022480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1060228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106022d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1060231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106023640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106023ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106023f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106024390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106024800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106024c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1060250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106025550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1060259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106025e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1060262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106026710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106026b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106026ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106027460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1060278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106027d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1060281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106028620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106028a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106028f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106029370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1060297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106029c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10602a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10602a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10602a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10602ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10602b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10602b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10602bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10602bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10602c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10602c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10602cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10602d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10602d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10602da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10602dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10602e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10602e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10602ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10602f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10602f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10602f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10602fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106030260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1060306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106030b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106030fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106031420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106031890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106031d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106032170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1060325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106032a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106032ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106033330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1060337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106033c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106034080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1060344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106034960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106034dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106035240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106035e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106036130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1060363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106036860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106036cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106037140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1060375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106037a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106037e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106038300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106038770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106038be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106039050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1060394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106039930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106039da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10603a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10603a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10603aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10603af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10603b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10603b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10603bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10603c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10603c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10603ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10603ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10603d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10603d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10603dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10603e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10603e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10603e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10603ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10603f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10603f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10603fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1060400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106040540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1060409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106040e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106041290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1060417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106041cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106042830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106042af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1060430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106043670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106043c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1060441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1060447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106045330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1060458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106045eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106046470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106046a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106046ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1060475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106047b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106048130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1060486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106048cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106049270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106049830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106049df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10604a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10604a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10604af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10604b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10604bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10604c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10604c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10604cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10604d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10604d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10604dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10604e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10604e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10604ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10604f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10604f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10604ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106050570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106050b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1060510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1060516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106051c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106052230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1060527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106052db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106053370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106053930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106053ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1060544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106054a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106055030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1060555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106055bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106056170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106056730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106056cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1060571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1060576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106057bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1060580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1060585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106058af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106058ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1060594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1060599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106059ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10605a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10605a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10605adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10605b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10605b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10605c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10605c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10605d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10605d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10605da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10605e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10605e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10605eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117f05140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117f055b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117f05a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117f05e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117f06300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117f06770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117f06be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117f07050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117f074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117f07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117f07da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117f08420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117f08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117f096f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117f09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117f0a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117f0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117f0b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117f0bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117f0c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117f0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117f0d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117f0d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117f0dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117f0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117f0e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117f0ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117f0f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117f0f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117f0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117f0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117f10360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117f107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117f10a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117f10f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117f11370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117f117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117f11c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117f120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117f12530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117f129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117f12e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117f13280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117f136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117f13b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117f13fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117f14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117f148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117f14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117f15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117f15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117f15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117f15ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117f16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117f167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117f16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117f171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117f176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117f17b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117f17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117f183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117f18860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117f18cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117f19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117f195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117f19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117f19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117f1a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117f1a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117f1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117f1b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117f1b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117f1b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117f1bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117f1c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117f1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117f1caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117f1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117f1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117f1d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117f1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117f1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117f1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117f1ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117f1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117f1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117f1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117f1fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117f20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117f204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117f20910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117f20d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117f211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117f21660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117f21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117f21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117f223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117f22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117f22c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117f23100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117f23570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117f239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117f23e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117f246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117f249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117f24e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117f25280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117f256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117f25b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117f25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117f26440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117f268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117f26d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117f27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117f27600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117f27a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117f27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117f28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117f287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117f28c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117f290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117f29510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117f29980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117f29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117f2a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117f2a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117f2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117f2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117f2b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117f2b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117f2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117f2c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117f2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117f2ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117f2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117f2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117f2d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117f2dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117f2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117f2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117f2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117f2edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117f2f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117f2f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117f2fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117f2ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117f30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117f30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117f30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117f31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117f315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117f31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117f31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117f32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117f32780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117f32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117f33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117f334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117f33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117f33db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117f34220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117f34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117f34b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117f34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117f353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117f35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117f35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117f36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117f365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117f36a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117f36e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117f372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117f37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117f37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117f38040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117f384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117f38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117f38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117f39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117f39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117f39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117f39f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117f3a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117f3a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117f3aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117f3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117f3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117f3be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117f3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117f3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117f3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117f3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117f3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117f3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10605bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10604c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10604b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1060483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106045bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1060552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106052ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106050830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10604e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106046730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106043ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10604a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10604f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10604c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1060541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106046cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106047e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10604f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1060513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106049af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10604ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106050270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106042db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10604ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10604d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106047870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1060489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1060558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106053070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10604dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106043370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106043930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1060455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106055e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10604b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106053630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106049530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10604bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10604fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1060472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106051970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106046170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106054770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106051f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10604da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1060569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106045030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106056430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1060444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106054d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10604eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106050df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106053bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1060524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10604a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1060083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106035500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106041f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106004880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10605dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10600bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10605ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10605f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10605f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10605f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10605fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10605fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10605ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106060280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106060540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106060800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106060ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106060d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106061040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106061300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1060615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106061880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106061b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106061e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1060620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106062380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106062640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106062900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106062bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106062e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106063140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106063400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106063940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106063e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106064140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106064400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1060646c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.945s
user	0m0.290s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4563 (225d2e0c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13370f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13370fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1337102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133710880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133710e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1337113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133711990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1337124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1337129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133712ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1337133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133713f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1337146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133714ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1337155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133715d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133716430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133716b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133717320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133717a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133718160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133718880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133719120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133719840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133719b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13371a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13371ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13371b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13371b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13371ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13371bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13371c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13371cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13371cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13371d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13371d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13371db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13371dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13371e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13371e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13371edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13371f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13371f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13371f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13371ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1337205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133721520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133721b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133722140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133724000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1337244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133724760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133724d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133725560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133725820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133725cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133726160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133726600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133726aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133726f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1337273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133727880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1337281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133728660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133728fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1337294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133729a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133729f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13372a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13372aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13372af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13372b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13372ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13372bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13372c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13372ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13372cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13372d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13372da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13372df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13372e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13372e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13372ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13372f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13372f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13372ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133730480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1337309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133730f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133731390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133731b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133732090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1337325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133732b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133733080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1337335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133733b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133734070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1337345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133734b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133735060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1337355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133735b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133736050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1337364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133736990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133736e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1337372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133737c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1337380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1337389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133738e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133739330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1337397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133739c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13373a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13373a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13373aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13373aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13373b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13373b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13373bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13373c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13373c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13373cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13373cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13373d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13373d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13373dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13373e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13373e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13373eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13373efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13373f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13373f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13373fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133740230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1337406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133740b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1337414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133741950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133741df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133742290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133742730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133742bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133743070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133743510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1337439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133743e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133744790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133744c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1337450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133745570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133745a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133746350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1337467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133746c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133747130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1337475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133747a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133747f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1337483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133748850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133748cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133749190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133749630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133749ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133749f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13374a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13374a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13374ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13374b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13374b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13374bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13374bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13374c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13374c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13374cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13374d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13374d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13374dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13374e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13374e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13374ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13374f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13374f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13374fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133750470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133750910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133750bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1337511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1337517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133751fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133752920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133752dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133753570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133753ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133754010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133754560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133754ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133755000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133755550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133755aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133755ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133756540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133756a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133756fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133757530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133757a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133757fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133758520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133758a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133758fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133759510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133759a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133759fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13375a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13375aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13375afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13375b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13375ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13375bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13375c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13375ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13375cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13375d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13375da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13375df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13375e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13375ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13375ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13375f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13375fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13375ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1337604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1337609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133760f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133761490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1337619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133605f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133606400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133606870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133606ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133607150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1336075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133607a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133607ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133608310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133608780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133608bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133609060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1336094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133609940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133609e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13360a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13360a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13360abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13360b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13360b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13360b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13360bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13360c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13360c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13360cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13360cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13360d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13360d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13360dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13360e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13360ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13360f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13360fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133610010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1336102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133610740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133610bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.120.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1336130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1336133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133613930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133613bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133614150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1336146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133614bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133614eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133615170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133615430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133615af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133616610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133616dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1336175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133617cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133618b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133619250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133619a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13361a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13361a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13361af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13361b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13361bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13361c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13361c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13361c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13361cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13361d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13361d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13361daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13361df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13361e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13361e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13361eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13361f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13361f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13361fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13361ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133620410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133620910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133620e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133621310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133621810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1336220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133622560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1336229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1336232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133623b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133624000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133624470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133624c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1336250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1336253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1336259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1336261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133626640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133626ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133627420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1336278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133628200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1336286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133628b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133628fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133629480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133629920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13362a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13362a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13362adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13362b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13362b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13362bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13362c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13362c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13362cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13362d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13362d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13362dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13362e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13362e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13362ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13362f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13362f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13362fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1336302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133630800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133630d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1336312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1336317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133631d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133632290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1336327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133633280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1336337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133633d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133634270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1336347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133635260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1336357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133635d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133636250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1336367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133637240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1336376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133637b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133638020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1336384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133638960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133638e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1336392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133639740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133639be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13363a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13363a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13363a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13363ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13363b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13363b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13363bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13363c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13363c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13363ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13363cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13363d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13363d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13363dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13363e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13363e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13363ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13363ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13363f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13363f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13363fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1336401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133640640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133640ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133640f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133641420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1336418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133641d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133642200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1336426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133642b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133642fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133643480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133643920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133643dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133644260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133644700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133644ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133645040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1336454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133645980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1336462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133646760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133646c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1336470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133647540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1336479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133647e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133648320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1336487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133648c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133649100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1336495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133649a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133649ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13364a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13364a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13364acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13364b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13364b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13364baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13364bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13364c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13364c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13364cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13364d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13364d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13364db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13364dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13364e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13364e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13364eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13364f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13364f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13364fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133650860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133650e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133651660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133651b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133651dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1336523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1336529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1336531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133653b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133653fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133654760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133654cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133655200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133655750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133655ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1336561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133656740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133656c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1336571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133657730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133657c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1336581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133658720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133658c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1336591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133659710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133659c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13365a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13365a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13365ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13365b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13365b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13365bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13365c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13365c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13365cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13365d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13365d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13365dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13365e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13365e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13365ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13365f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13365f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13365fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133660150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1336606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133660bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133661140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133661690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133661be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133662130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133662680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133662bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133663120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133663670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133663bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133664110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133664660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133665100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133665650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133665ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1336660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133666640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133666b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1336670e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133667580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133667a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133667ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133668360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133668800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133668ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133669140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1336695e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133669a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133669f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13366a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13366a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13366ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13366b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13366b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13366bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13366c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13366c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13366d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13366d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13366dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13366e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13366e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13366eb90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13374ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13374f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13374f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133722a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133722400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1337514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133719dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1337208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1337211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1337217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1337202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133723020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133718dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13370ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133723630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133725030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133731650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13371bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13371c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133751ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13374ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13371a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13371a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13371a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133761ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133761f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133762220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1337624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1337627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133762a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133762d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133762fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1337632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133763560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133763820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133763ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133763da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133764060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133764320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1337645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1337648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133764b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133764e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1337650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1337653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133765920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133765be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133765ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133766160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133766420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1337666e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1337669a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133766c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133766f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1337671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1337674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133767760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133767a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133767ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133767fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133768260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133768520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1337687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133768aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133768d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133769020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1337692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1337695a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133769860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133769b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133769de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13376a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13376a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13376a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13376a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13376aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13376ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13376b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13376b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13376b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13376b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13376bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13376bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13376c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13376c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13376c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13376c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13376cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13376cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13376d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13376d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13376d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13376da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13376dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13376dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13376e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13376e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13376e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13376eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13376eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13376f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13376f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13376f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13376f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13376fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13376fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1337700e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1337703a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133770660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133770920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133770be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133770ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133771160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133771420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1337716e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1337719a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133771c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133771f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1337721e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1337724a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133772760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133772a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133772ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133772fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133773260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133773520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1337737e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133773aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133773d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133774020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1337742e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1337745a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133774860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133774b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133774de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1337750a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133775360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133775620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1337758e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133775ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133775e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133776120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1337763e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1337766a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133776960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133776c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133776ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1337771a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133777460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133777720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1337779e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133777ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133777f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133778220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1337784e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1337787a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133778a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133778d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133778fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337792a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133779560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133779820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133779ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133779da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13377a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13377a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13377a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13377a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13377ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13377ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13377b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13377b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13377b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13377b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13377bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13377bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13377c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13377c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13377c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13377c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13377cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13377cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13377d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13377d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13377d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13377da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13377dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13377dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13377e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13377e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13377e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13377eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13377ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13377f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13377f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13377f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13377f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13377fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13377fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1337800a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133780360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133780620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1337808e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133780ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133780e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133781120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1337813e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1337816a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133781c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1337821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133782710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133782c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1337831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133783700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133783c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1337841a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1337846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133784c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133785190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1337856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133785c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133786180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1337866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133786c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133787170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1337876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133787c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133788160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1337886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133788c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133789150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1337896a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133789bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13378a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13378a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13378abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13378b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13378b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13378bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13378c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13378c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13378cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13378d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13378d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13378dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13378e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13378e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13378eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13378f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13378f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13378fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1337900e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133790630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133790b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1337910d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133791620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133791b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1337920c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133792610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133792b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1337930b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133793600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133793b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1337940a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1337945f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1337948b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133794b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133795070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133795570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133795a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133795f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133796470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133796970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133796e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133797370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133797870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133797d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133798270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133798770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133798c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133799170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133799b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13379a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13379a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13379b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13379b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13379bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13379be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13379c460 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.985s
user	0m0.250s
sys	0m0.191s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    1.87 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.39 sec*proc (2 tests)

Total Test time (real) =   2.40 sec
        2.42 real         0.70 user         0.28 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.56 sec*proc (2 tests)

Total Test time (real) =   0.57 sec
        0.58 real         0.15 user         0.08 sys
```
