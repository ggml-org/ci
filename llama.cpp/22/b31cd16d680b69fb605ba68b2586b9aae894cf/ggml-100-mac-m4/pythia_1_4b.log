Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.540s
user	0m0.870s
sys	0m1.214s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking C executable ../bin/test-c
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Built target llava_shared
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-backend-ops
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-embedding
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-lookup
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Built target llama-lookup-merge
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-cli
[ 84%] Built target llama-parallel
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-lookup-stats
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Built target llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.010s
user	0m6.049s
sys	0m9.857s

main: quantize time =  3751.65 ms
main:    total time =  3751.65 ms

main: quantize time =  1333.65 ms
main:    total time =  1333.65 ms

main: quantize time =  1313.54 ms
main:    total time =  1313.54 ms

main: quantize time =  1473.34 ms
main:    total time =  1473.34 ms

main: quantize time =  1539.88 ms
main:    total time =  1539.88 ms

main: quantize time =  5071.64 ms
main:    total time =  5071.64 ms

main: quantize time =  5698.10 ms
main:    total time =  5698.10 ms

main: quantize time =  7213.83 ms
main:    total time =  7213.83 ms

main: quantize time =  5939.33 ms
main:    total time =  5939.33 ms

main: quantize time =  4590.23 ms
main:    total time =  4590.23 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.157 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.269 I main: llama backend init
0.00.000.275 I main: load the model and apply lora adapter, if any
0.00.032.032 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.897 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.914 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.921 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.925 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.925 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.926 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.927 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.063.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.328 I llama_model_loader: - type  f32:  194 tensors
0.00.063.329 I llama_model_loader: - type  f16:   98 tensors
0.00.063.337 I print_info: file format = GGUF V3 (latest)
0.00.063.339 I print_info: file type   = all F32 (guessed)
0.00.063.341 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.686 I load: special tokens cache size = 25
0.00.098.663 I load: token to piece cache size = 0.2984 MB
0.00.098.685 I print_info: arch             = gptneox
0.00.098.685 I print_info: n_vocab (hp)     = 50304
0.00.098.686 I print_info: vocab_only       = 0
0.00.098.686 I print_info: n_ctx_train      = 2048
0.00.098.686 I print_info: n_embd           = 2048
0.00.098.686 I print_info: n_layer          = 24
0.00.098.689 I print_info: n_head           = 16
0.00.098.690 I print_info: n_head_kv        = 16
0.00.098.690 I print_info: n_rot            = 32
0.00.098.691 I print_info: n_swa            = 0
0.00.098.691 I print_info: n_embd_head_k    = 128
0.00.098.691 I print_info: n_embd_head_v    = 128
0.00.098.692 I print_info: n_gqa            = 1
0.00.098.692 I print_info: n_embd_k_gqa     = 2048
0.00.098.693 I print_info: n_embd_v_gqa     = 2048
0.00.098.694 I print_info: f_norm_eps       = 1.0e-05
0.00.098.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.098.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.098.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.098.695 I print_info: f_logit_scale    = 0.0e+00
0.00.098.695 I print_info: n_ff             = 8192
0.00.098.695 I print_info: n_expert         = 0
0.00.098.695 I print_info: n_expert_used    = 0
0.00.098.696 I print_info: causal attn      = 1
0.00.098.696 I print_info: pooling type     = 0
0.00.098.698 I print_info: rope type        = 2
0.00.098.698 I print_info: rope scaling     = linear
0.00.098.698 I print_info: freq_base_train  = 10000.0
0.00.098.699 I print_info: freq_scale_train = 1
0.00.098.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.098.699 I print_info: rope_finetuned   = unknown
0.00.098.700 I print_info: ssm_d_conv       = 0
0.00.098.701 I print_info: ssm_d_inner      = 0
0.00.098.701 I print_info: ssm_d_state      = 0
0.00.098.701 I print_info: ssm_dt_rank      = 0
0.00.098.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.098.701 I print_info: model type       = 1.4B
0.00.098.701 I print_info: model params     = 1.41 B
0.00.098.702 I print_info: general.name     = 1.4B
0.00.098.702 I print_info: vocab type       = BPE
0.00.098.702 I print_info: n_vocab          = 50304
0.00.098.702 I print_info: n_merges         = 50009
0.00.098.702 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.098.702 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.098.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.098.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.098.703 I print_info: LF token         = 128 'Ä'
0.00.098.703 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.098.703 I print_info: max token length = 1024
0.00.101.343 I load_tensors: offloading 24 repeating layers to GPU
0.00.101.343 I load_tensors: offloading output layer to GPU
0.00.101.343 I load_tensors: offloaded 25/25 layers to GPU
0.00.101.362 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.363 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.101.656 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.657 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.657 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.657 I llama_new_context_with_model: n_batch       = 2048
0.00.101.657 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.658 I llama_new_context_with_model: flash_attn    = 0
0.00.101.658 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.658 I llama_new_context_with_model: freq_scale    = 1
0.00.101.659 I ggml_metal_init: allocating
0.00.101.662 I ggml_metal_init: found device: Apple M4
0.00.101.664 I ggml_metal_init: picking default device: Apple M4
0.00.102.323 I ggml_metal_init: using embedded metal library
0.00.114.165 I ggml_metal_init: GPU name:   Apple M4
0.00.114.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.114.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.114.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.114.168 I ggml_metal_init: simdgroup reduction   = true
0.00.114.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.114.168 I ggml_metal_init: has bfloat            = true
0.00.114.168 I ggml_metal_init: use bfloat            = true
0.00.114.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.114.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.137.771 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.158.358 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.158.364 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.158.408 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.159.370 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.159.371 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.159.372 I llama_new_context_with_model: graph nodes  = 967
0.00.159.372 I llama_new_context_with_model: graph splits = 2
0.00.159.375 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.159.499 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.499 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.237.256 I main: llama threadpool init, n_threads = 4
0.00.237.303 I 
0.00.237.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.237.329 I 
0.00.237.407 I sampler seed: 1234
0.00.237.411 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.237.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.237.437 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.237.437 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.077.582 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.02.077.582 I llama_perf_context_print:        load time =     205.21 ms
0.02.077.585 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.87 tokens per second)
0.02.077.586 I llama_perf_context_print:        eval time =    1793.60 ms /    63 runs   (   28.47 ms per token,    35.12 tokens per second)
0.02.077.586 I llama_perf_context_print:       total time =    1840.33 ms /    70 tokens
0.02.077.805 I ggml_metal_free: deallocating

real	0m2.387s
user	0m0.142s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.912 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.004 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.032.011 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.015 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.783 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.786 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.787 I llama_model_loader: - type  f32:  194 tensors
0.00.041.787 I llama_model_loader: - type q8_0:   98 tensors
0.00.041.788 I print_info: file format = GGUF V3 (latest)
0.00.041.788 I print_info: file type   = Q8_0
0.00.041.789 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.065.947 I load: special tokens cache size = 25
0.00.072.678 I load: token to piece cache size = 0.2984 MB
0.00.072.696 I print_info: arch             = gptneox
0.00.072.697 I print_info: n_vocab (hp)     = 50304
0.00.072.697 I print_info: vocab_only       = 0
0.00.072.697 I print_info: n_ctx_train      = 2048
0.00.072.697 I print_info: n_embd           = 2048
0.00.072.697 I print_info: n_layer          = 24
0.00.072.703 I print_info: n_head           = 16
0.00.072.704 I print_info: n_head_kv        = 16
0.00.072.704 I print_info: n_rot            = 32
0.00.072.704 I print_info: n_swa            = 0
0.00.072.704 I print_info: n_embd_head_k    = 128
0.00.072.704 I print_info: n_embd_head_v    = 128
0.00.072.705 I print_info: n_gqa            = 1
0.00.072.705 I print_info: n_embd_k_gqa     = 2048
0.00.072.706 I print_info: n_embd_v_gqa     = 2048
0.00.072.709 I print_info: f_norm_eps       = 1.0e-05
0.00.072.710 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.710 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.710 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.710 I print_info: f_logit_scale    = 0.0e+00
0.00.072.711 I print_info: n_ff             = 8192
0.00.072.711 I print_info: n_expert         = 0
0.00.072.711 I print_info: n_expert_used    = 0
0.00.072.712 I print_info: causal attn      = 1
0.00.072.712 I print_info: pooling type     = 0
0.00.072.714 I print_info: rope type        = 2
0.00.072.714 I print_info: rope scaling     = linear
0.00.072.715 I print_info: freq_base_train  = 10000.0
0.00.072.715 I print_info: freq_scale_train = 1
0.00.072.715 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.715 I print_info: rope_finetuned   = unknown
0.00.072.715 I print_info: ssm_d_conv       = 0
0.00.072.716 I print_info: ssm_d_inner      = 0
0.00.072.716 I print_info: ssm_d_state      = 0
0.00.072.716 I print_info: ssm_dt_rank      = 0
0.00.072.716 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.716 I print_info: model type       = 1.4B
0.00.072.717 I print_info: model params     = 1.41 B
0.00.072.717 I print_info: general.name     = 1.4B
0.00.072.717 I print_info: vocab type       = BPE
0.00.072.719 I print_info: n_vocab          = 50304
0.00.072.719 I print_info: n_merges         = 50009
0.00.072.719 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.719 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.719 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.720 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.720 I print_info: LF token         = 128 'Ä'
0.00.072.720 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.720 I print_info: max token length = 1024
0.00.075.283 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.284 I load_tensors: offloading output layer to GPU
0.00.075.284 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.296 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.297 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.075.625 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.626 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.626 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.626 I llama_new_context_with_model: n_batch       = 2048
0.00.075.626 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.626 I llama_new_context_with_model: flash_attn    = 0
0.00.075.627 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.627 I llama_new_context_with_model: freq_scale    = 1
0.00.075.628 I ggml_metal_init: allocating
0.00.075.631 I ggml_metal_init: found device: Apple M4
0.00.075.633 I ggml_metal_init: picking default device: Apple M4
0.00.076.391 I ggml_metal_init: using embedded metal library
0.00.079.154 I ggml_metal_init: GPU name:   Apple M4
0.00.079.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.157 I ggml_metal_init: simdgroup reduction   = true
0.00.079.157 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.157 I ggml_metal_init: has bfloat            = true
0.00.079.157 I ggml_metal_init: use bfloat            = true
0.00.079.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.007 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.117.618 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.630 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.823 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.825 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.825 I llama_new_context_with_model: graph nodes  = 967
0.00.118.825 I llama_new_context_with_model: graph splits = 2
0.00.118.830 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.118.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.841.014 I main: llama threadpool init, n_threads = 4
0.01.841.047 I 
0.01.841.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.841.071 I 
0.01.841.306 I sampler seed: 1234
0.01.841.310 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.841.321 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.841.322 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.841.322 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.932.371 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.02.932.372 I llama_perf_context_print:        load time =    1831.10 ms
0.02.932.373 I llama_perf_context_print: prompt eval time =      43.15 ms /     7 tokens (    6.16 ms per token,   162.24 tokens per second)
0.02.932.374 I llama_perf_context_print:        eval time =    1045.20 ms /    63 runs   (   16.59 ms per token,    60.28 tokens per second)
0.02.932.374 I llama_perf_context_print:       total time =    1091.36 ms /    70 tokens
0.02.932.663 I ggml_metal_free: deallocating

real	0m2.954s
user	0m0.121s
sys	0m0.238s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.677 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.641 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.688 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.688 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.689 I llama_model_loader: - type  f32:  194 tensors
0.00.037.689 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.689 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.690 I print_info: file format = GGUF V3 (latest)
0.00.037.690 I print_info: file type   = Q4_0
0.00.037.691 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.061.420 I load: special tokens cache size = 25
0.00.069.852 I load: token to piece cache size = 0.2984 MB
0.00.069.868 I print_info: arch             = gptneox
0.00.069.870 I print_info: n_vocab (hp)     = 50304
0.00.069.870 I print_info: vocab_only       = 0
0.00.069.870 I print_info: n_ctx_train      = 2048
0.00.069.871 I print_info: n_embd           = 2048
0.00.069.871 I print_info: n_layer          = 24
0.00.069.879 I print_info: n_head           = 16
0.00.069.880 I print_info: n_head_kv        = 16
0.00.069.880 I print_info: n_rot            = 32
0.00.069.881 I print_info: n_swa            = 0
0.00.069.881 I print_info: n_embd_head_k    = 128
0.00.069.881 I print_info: n_embd_head_v    = 128
0.00.069.882 I print_info: n_gqa            = 1
0.00.069.884 I print_info: n_embd_k_gqa     = 2048
0.00.069.885 I print_info: n_embd_v_gqa     = 2048
0.00.069.886 I print_info: f_norm_eps       = 1.0e-05
0.00.069.886 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.887 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.887 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.887 I print_info: f_logit_scale    = 0.0e+00
0.00.069.888 I print_info: n_ff             = 8192
0.00.069.888 I print_info: n_expert         = 0
0.00.069.889 I print_info: n_expert_used    = 0
0.00.069.889 I print_info: causal attn      = 1
0.00.069.889 I print_info: pooling type     = 0
0.00.069.889 I print_info: rope type        = 2
0.00.069.889 I print_info: rope scaling     = linear
0.00.069.890 I print_info: freq_base_train  = 10000.0
0.00.069.893 I print_info: freq_scale_train = 1
0.00.069.893 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.893 I print_info: rope_finetuned   = unknown
0.00.069.894 I print_info: ssm_d_conv       = 0
0.00.069.894 I print_info: ssm_d_inner      = 0
0.00.069.894 I print_info: ssm_d_state      = 0
0.00.069.894 I print_info: ssm_dt_rank      = 0
0.00.069.894 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.894 I print_info: model type       = 1.4B
0.00.069.895 I print_info: model params     = 1.41 B
0.00.069.895 I print_info: general.name     = 1.4B
0.00.069.896 I print_info: vocab type       = BPE
0.00.069.896 I print_info: n_vocab          = 50304
0.00.069.896 I print_info: n_merges         = 50009
0.00.069.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.898 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.898 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.898 I print_info: LF token         = 128 'Ä'
0.00.069.899 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.899 I print_info: max token length = 1024
0.00.072.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.237 I load_tensors: offloading output layer to GPU
0.00.072.237 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.243 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.245 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.567 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.568 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.568 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.568 I llama_new_context_with_model: n_batch       = 2048
0.00.072.569 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.569 I llama_new_context_with_model: flash_attn    = 0
0.00.072.569 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.569 I llama_new_context_with_model: freq_scale    = 1
0.00.072.570 I ggml_metal_init: allocating
0.00.072.573 I ggml_metal_init: found device: Apple M4
0.00.072.575 I ggml_metal_init: picking default device: Apple M4
0.00.073.349 I ggml_metal_init: using embedded metal library
0.00.076.530 I ggml_metal_init: GPU name:   Apple M4
0.00.076.532 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.534 I ggml_metal_init: simdgroup reduction   = true
0.00.076.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.534 I ggml_metal_init: has bfloat            = true
0.00.076.534 I ggml_metal_init: use bfloat            = true
0.00.076.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.726 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.117.044 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.057 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.095 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.346 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.348 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.348 I llama_new_context_with_model: graph nodes  = 967
0.00.118.348 I llama_new_context_with_model: graph splits = 2
0.00.118.352 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.118.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.821 I main: llama threadpool init, n_threads = 4
0.00.726.860 I 
0.00.726.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.888 I 
0.00.727.128 I sampler seed: 1234
0.00.727.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.152 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.153 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.153 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.410.805 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.410.806 I llama_perf_context_print:        load time =     716.14 ms
0.01.410.807 I llama_perf_context_print: prompt eval time =      39.78 ms /     7 tokens (    5.68 ms per token,   175.98 tokens per second)
0.01.410.807 I llama_perf_context_print:        eval time =     640.86 ms /    63 runs   (   10.17 ms per token,    98.31 tokens per second)
0.01.410.808 I llama_perf_context_print:       total time =     683.99 ms /    70 tokens
0.01.411.043 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.123s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.129 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.894 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.894 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.896 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.896 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.897 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.897 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.900 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.566 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.236 I llama_model_loader: - type  f32:  194 tensors
0.00.033.236 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.237 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.237 I print_info: file format = GGUF V3 (latest)
0.00.033.238 I print_info: file type   = Q4_1
0.00.033.238 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.052.893 I load: special tokens cache size = 25
0.00.058.933 I load: token to piece cache size = 0.2984 MB
0.00.058.943 I print_info: arch             = gptneox
0.00.058.944 I print_info: n_vocab (hp)     = 50304
0.00.058.945 I print_info: vocab_only       = 0
0.00.058.945 I print_info: n_ctx_train      = 2048
0.00.058.945 I print_info: n_embd           = 2048
0.00.058.945 I print_info: n_layer          = 24
0.00.058.948 I print_info: n_head           = 16
0.00.058.949 I print_info: n_head_kv        = 16
0.00.058.949 I print_info: n_rot            = 32
0.00.058.949 I print_info: n_swa            = 0
0.00.058.950 I print_info: n_embd_head_k    = 128
0.00.058.950 I print_info: n_embd_head_v    = 128
0.00.058.951 I print_info: n_gqa            = 1
0.00.058.951 I print_info: n_embd_k_gqa     = 2048
0.00.058.952 I print_info: n_embd_v_gqa     = 2048
0.00.058.953 I print_info: f_norm_eps       = 1.0e-05
0.00.058.953 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.953 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.953 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.953 I print_info: f_logit_scale    = 0.0e+00
0.00.058.954 I print_info: n_ff             = 8192
0.00.058.954 I print_info: n_expert         = 0
0.00.058.954 I print_info: n_expert_used    = 0
0.00.058.955 I print_info: causal attn      = 1
0.00.058.955 I print_info: pooling type     = 0
0.00.058.955 I print_info: rope type        = 2
0.00.058.955 I print_info: rope scaling     = linear
0.00.058.956 I print_info: freq_base_train  = 10000.0
0.00.058.956 I print_info: freq_scale_train = 1
0.00.058.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.956 I print_info: rope_finetuned   = unknown
0.00.058.957 I print_info: ssm_d_conv       = 0
0.00.058.959 I print_info: ssm_d_inner      = 0
0.00.058.959 I print_info: ssm_d_state      = 0
0.00.058.959 I print_info: ssm_dt_rank      = 0
0.00.058.959 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.959 I print_info: model type       = 1.4B
0.00.058.959 I print_info: model params     = 1.41 B
0.00.058.960 I print_info: general.name     = 1.4B
0.00.058.960 I print_info: vocab type       = BPE
0.00.058.960 I print_info: n_vocab          = 50304
0.00.058.960 I print_info: n_merges         = 50009
0.00.058.960 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.961 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.961 I print_info: LF token         = 128 'Ä'
0.00.058.961 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.961 I print_info: max token length = 1024
0.00.060.679 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.679 I load_tensors: offloading output layer to GPU
0.00.060.680 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.685 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.060.686 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.060.952 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.953 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.953 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.954 I llama_new_context_with_model: n_batch       = 2048
0.00.060.954 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.954 I llama_new_context_with_model: flash_attn    = 0
0.00.060.954 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.955 I llama_new_context_with_model: freq_scale    = 1
0.00.060.955 I ggml_metal_init: allocating
0.00.060.958 I ggml_metal_init: found device: Apple M4
0.00.060.960 I ggml_metal_init: picking default device: Apple M4
0.00.061.550 I ggml_metal_init: using embedded metal library
0.00.063.868 I ggml_metal_init: GPU name:   Apple M4
0.00.063.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.870 I ggml_metal_init: simdgroup reduction   = true
0.00.063.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.871 I ggml_metal_init: has bfloat            = true
0.00.063.871 I ggml_metal_init: use bfloat            = true
0.00.063.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.872 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.982 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.994 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.026 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.072 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.074 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.074 I llama_new_context_with_model: graph nodes  = 967
0.00.095.074 I llama_new_context_with_model: graph splits = 2
0.00.095.077 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.217 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.217 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.196 I main: llama threadpool init, n_threads = 4
0.00.775.238 I 
0.00.775.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.275 I 
0.00.775.514 I sampler seed: 1234
0.00.775.518 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.567 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.569 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.510.528 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63449.51 tokens per second)
0.01.510.529 I llama_perf_context_print:        load time =     766.06 ms
0.01.510.529 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.37 tokens per second)
0.01.510.530 I llama_perf_context_print:        eval time =     688.41 ms /    63 runs   (   10.93 ms per token,    91.52 tokens per second)
0.01.510.530 I llama_perf_context_print:       total time =     735.33 ms /    70 tokens
0.01.510.786 I ggml_metal_free: deallocating

real	0m1.527s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.847 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.557 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.562 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.562 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.563 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.074 I llama_model_loader: - type  f32:  194 tensors
0.00.027.075 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.075 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.076 I print_info: file format = GGUF V3 (latest)
0.00.027.076 I print_info: file type   = Q5_0
0.00.027.077 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.887 I load: special tokens cache size = 25
0.00.051.903 I load: token to piece cache size = 0.2984 MB
0.00.051.917 I print_info: arch             = gptneox
0.00.051.919 I print_info: n_vocab (hp)     = 50304
0.00.051.919 I print_info: vocab_only       = 0
0.00.051.919 I print_info: n_ctx_train      = 2048
0.00.051.919 I print_info: n_embd           = 2048
0.00.051.919 I print_info: n_layer          = 24
0.00.051.923 I print_info: n_head           = 16
0.00.051.924 I print_info: n_head_kv        = 16
0.00.051.924 I print_info: n_rot            = 32
0.00.051.924 I print_info: n_swa            = 0
0.00.051.924 I print_info: n_embd_head_k    = 128
0.00.051.924 I print_info: n_embd_head_v    = 128
0.00.051.925 I print_info: n_gqa            = 1
0.00.051.926 I print_info: n_embd_k_gqa     = 2048
0.00.051.926 I print_info: n_embd_v_gqa     = 2048
0.00.051.927 I print_info: f_norm_eps       = 1.0e-05
0.00.051.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.927 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.928 I print_info: f_logit_scale    = 0.0e+00
0.00.051.928 I print_info: n_ff             = 8192
0.00.051.929 I print_info: n_expert         = 0
0.00.051.930 I print_info: n_expert_used    = 0
0.00.051.931 I print_info: causal attn      = 1
0.00.051.931 I print_info: pooling type     = 0
0.00.051.931 I print_info: rope type        = 2
0.00.051.932 I print_info: rope scaling     = linear
0.00.051.932 I print_info: freq_base_train  = 10000.0
0.00.051.932 I print_info: freq_scale_train = 1
0.00.051.932 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.932 I print_info: rope_finetuned   = unknown
0.00.051.933 I print_info: ssm_d_conv       = 0
0.00.051.937 I print_info: ssm_d_inner      = 0
0.00.051.937 I print_info: ssm_d_state      = 0
0.00.051.938 I print_info: ssm_dt_rank      = 0
0.00.051.938 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.939 I print_info: model type       = 1.4B
0.00.051.939 I print_info: model params     = 1.41 B
0.00.051.939 I print_info: general.name     = 1.4B
0.00.051.939 I print_info: vocab type       = BPE
0.00.051.940 I print_info: n_vocab          = 50304
0.00.051.940 I print_info: n_merges         = 50009
0.00.051.940 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.940 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.940 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.940 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.941 I print_info: LF token         = 128 'Ä'
0.00.051.941 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.942 I print_info: max token length = 1024
0.00.053.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.913 I load_tensors: offloading output layer to GPU
0.00.053.913 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.923 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.924 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.205 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.206 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.206 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.206 I llama_new_context_with_model: n_batch       = 2048
0.00.054.207 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.207 I llama_new_context_with_model: flash_attn    = 0
0.00.054.207 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.207 I llama_new_context_with_model: freq_scale    = 1
0.00.054.208 I ggml_metal_init: allocating
0.00.054.211 I ggml_metal_init: found device: Apple M4
0.00.054.213 I ggml_metal_init: picking default device: Apple M4
0.00.054.789 I ggml_metal_init: using embedded metal library
0.00.057.113 I ggml_metal_init: GPU name:   Apple M4
0.00.057.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.115 I ggml_metal_init: simdgroup reduction   = true
0.00.057.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.116 I ggml_metal_init: has bfloat            = true
0.00.057.116 I ggml_metal_init: use bfloat            = true
0.00.057.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.840 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.077 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.086 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.118 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.170 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.171 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.171 I llama_new_context_with_model: graph nodes  = 967
0.00.087.172 I llama_new_context_with_model: graph splits = 2
0.00.087.175 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.311 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.482 I main: llama threadpool init, n_threads = 4
0.00.769.518 I 
0.00.769.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.540 I 
0.00.769.778 I sampler seed: 1234
0.00.769.782 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.805 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.805 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.805 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.559.570 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.559.571 I llama_perf_context_print:        load time =     758.63 ms
0.01.559.572 I llama_perf_context_print: prompt eval time =      43.06 ms /     7 tokens (    6.15 ms per token,   162.58 tokens per second)
0.01.559.572 I llama_perf_context_print:        eval time =     743.65 ms /    63 runs   (   11.80 ms per token,    84.72 tokens per second)
0.01.559.574 I llama_perf_context_print:       total time =     790.09 ms /    70 tokens
0.01.559.830 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.108s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.642 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.129 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.144 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.144 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.957 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.710 I llama_model_loader: - type  f32:  194 tensors
0.00.024.710 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.711 I print_info: file format = GGUF V3 (latest)
0.00.024.712 I print_info: file type   = Q5_1
0.00.024.713 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.457 I load: special tokens cache size = 25
0.00.049.388 I load: token to piece cache size = 0.2984 MB
0.00.049.403 I print_info: arch             = gptneox
0.00.049.404 I print_info: n_vocab (hp)     = 50304
0.00.049.404 I print_info: vocab_only       = 0
0.00.049.404 I print_info: n_ctx_train      = 2048
0.00.049.404 I print_info: n_embd           = 2048
0.00.049.405 I print_info: n_layer          = 24
0.00.049.408 I print_info: n_head           = 16
0.00.049.408 I print_info: n_head_kv        = 16
0.00.049.409 I print_info: n_rot            = 32
0.00.049.409 I print_info: n_swa            = 0
0.00.049.409 I print_info: n_embd_head_k    = 128
0.00.049.409 I print_info: n_embd_head_v    = 128
0.00.049.412 I print_info: n_gqa            = 1
0.00.049.412 I print_info: n_embd_k_gqa     = 2048
0.00.049.413 I print_info: n_embd_v_gqa     = 2048
0.00.049.414 I print_info: f_norm_eps       = 1.0e-05
0.00.049.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.416 I print_info: f_logit_scale    = 0.0e+00
0.00.049.417 I print_info: n_ff             = 8192
0.00.049.417 I print_info: n_expert         = 0
0.00.049.417 I print_info: n_expert_used    = 0
0.00.049.417 I print_info: causal attn      = 1
0.00.049.417 I print_info: pooling type     = 0
0.00.049.417 I print_info: rope type        = 2
0.00.049.418 I print_info: rope scaling     = linear
0.00.049.418 I print_info: freq_base_train  = 10000.0
0.00.049.419 I print_info: freq_scale_train = 1
0.00.049.419 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.419 I print_info: rope_finetuned   = unknown
0.00.049.419 I print_info: ssm_d_conv       = 0
0.00.049.419 I print_info: ssm_d_inner      = 0
0.00.049.420 I print_info: ssm_d_state      = 0
0.00.049.420 I print_info: ssm_dt_rank      = 0
0.00.049.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.420 I print_info: model type       = 1.4B
0.00.049.420 I print_info: model params     = 1.41 B
0.00.049.420 I print_info: general.name     = 1.4B
0.00.049.421 I print_info: vocab type       = BPE
0.00.049.421 I print_info: n_vocab          = 50304
0.00.049.421 I print_info: n_merges         = 50009
0.00.049.422 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.422 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.423 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.423 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.423 I print_info: LF token         = 128 'Ä'
0.00.049.423 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.423 I print_info: max token length = 1024
0.00.051.423 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.424 I load_tensors: offloading output layer to GPU
0.00.051.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.435 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.436 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.711 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.712 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.712 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.712 I llama_new_context_with_model: n_batch       = 2048
0.00.051.712 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.712 I llama_new_context_with_model: flash_attn    = 0
0.00.051.713 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.713 I llama_new_context_with_model: freq_scale    = 1
0.00.051.713 I ggml_metal_init: allocating
0.00.051.716 I ggml_metal_init: found device: Apple M4
0.00.051.718 I ggml_metal_init: picking default device: Apple M4
0.00.052.311 I ggml_metal_init: using embedded metal library
0.00.054.632 I ggml_metal_init: GPU name:   Apple M4
0.00.054.633 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.634 I ggml_metal_init: simdgroup reduction   = true
0.00.054.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.634 I ggml_metal_init: has bfloat            = true
0.00.054.635 I ggml_metal_init: use bfloat            = true
0.00.054.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.402 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.886 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.929 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.889 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.890 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.891 I llama_new_context_with_model: graph nodes  = 967
0.00.083.891 I llama_new_context_with_model: graph splits = 2
0.00.083.893 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.023 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.394 I main: llama threadpool init, n_threads = 4
0.00.720.438 I 
0.00.720.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.468 I 
0.00.720.713 I sampler seed: 1234
0.00.720.722 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.784 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.784 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.548.869 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.01.548.869 I llama_perf_context_print:        load time =     711.75 ms
0.01.548.870 I llama_perf_context_print: prompt eval time =      42.21 ms /     7 tokens (    6.03 ms per token,   165.84 tokens per second)
0.01.548.871 I llama_perf_context_print:        eval time =     782.82 ms /    63 runs   (   12.43 ms per token,    80.48 tokens per second)
0.01.548.871 I llama_perf_context_print:       total time =     828.48 ms /    70 tokens
0.01.549.111 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.707 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.311 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.316 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.319 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.320 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.320 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.321 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.321 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.322 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.322 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.325 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.905 I llama_model_loader: - type  f32:  194 tensors
0.00.024.906 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.906 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.906 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.907 I print_info: file format = GGUF V3 (latest)
0.00.024.907 I print_info: file type   = Q2_K - Medium
0.00.024.908 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.858 I load: special tokens cache size = 25
0.00.049.821 I load: token to piece cache size = 0.2984 MB
0.00.049.836 I print_info: arch             = gptneox
0.00.049.837 I print_info: n_vocab (hp)     = 50304
0.00.049.837 I print_info: vocab_only       = 0
0.00.049.837 I print_info: n_ctx_train      = 2048
0.00.049.837 I print_info: n_embd           = 2048
0.00.049.838 I print_info: n_layer          = 24
0.00.049.840 I print_info: n_head           = 16
0.00.049.841 I print_info: n_head_kv        = 16
0.00.049.841 I print_info: n_rot            = 32
0.00.049.841 I print_info: n_swa            = 0
0.00.049.841 I print_info: n_embd_head_k    = 128
0.00.049.841 I print_info: n_embd_head_v    = 128
0.00.049.842 I print_info: n_gqa            = 1
0.00.049.843 I print_info: n_embd_k_gqa     = 2048
0.00.049.844 I print_info: n_embd_v_gqa     = 2048
0.00.049.846 I print_info: f_norm_eps       = 1.0e-05
0.00.049.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.847 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.847 I print_info: f_logit_scale    = 0.0e+00
0.00.049.848 I print_info: n_ff             = 8192
0.00.049.848 I print_info: n_expert         = 0
0.00.049.848 I print_info: n_expert_used    = 0
0.00.049.848 I print_info: causal attn      = 1
0.00.049.848 I print_info: pooling type     = 0
0.00.049.848 I print_info: rope type        = 2
0.00.049.849 I print_info: rope scaling     = linear
0.00.049.849 I print_info: freq_base_train  = 10000.0
0.00.049.849 I print_info: freq_scale_train = 1
0.00.049.849 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.850 I print_info: rope_finetuned   = unknown
0.00.049.850 I print_info: ssm_d_conv       = 0
0.00.049.850 I print_info: ssm_d_inner      = 0
0.00.049.850 I print_info: ssm_d_state      = 0
0.00.049.850 I print_info: ssm_dt_rank      = 0
0.00.049.850 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.851 I print_info: model type       = 1.4B
0.00.049.851 I print_info: model params     = 1.41 B
0.00.049.891 I print_info: general.name     = 1.4B
0.00.049.892 I print_info: vocab type       = BPE
0.00.049.892 I print_info: n_vocab          = 50304
0.00.049.893 I print_info: n_merges         = 50009
0.00.049.893 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.893 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.893 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.896 I print_info: LF token         = 128 'Ä'
0.00.049.896 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.896 I print_info: max token length = 1024
0.00.051.652 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.652 I load_tensors: offloading output layer to GPU
0.00.051.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.656 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.657 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.911 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.912 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.913 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.913 I llama_new_context_with_model: n_batch       = 2048
0.00.051.913 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.913 I llama_new_context_with_model: flash_attn    = 0
0.00.051.913 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.914 I llama_new_context_with_model: freq_scale    = 1
0.00.051.914 I ggml_metal_init: allocating
0.00.051.917 I ggml_metal_init: found device: Apple M4
0.00.051.919 I ggml_metal_init: picking default device: Apple M4
0.00.052.488 I ggml_metal_init: using embedded metal library
0.00.054.802 I ggml_metal_init: GPU name:   Apple M4
0.00.054.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.805 I ggml_metal_init: simdgroup reduction   = true
0.00.054.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.805 I ggml_metal_init: has bfloat            = true
0.00.054.805 I ggml_metal_init: use bfloat            = true
0.00.054.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.567 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.202 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.207 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.234 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.208 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.209 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.209 I llama_new_context_with_model: graph nodes  = 967
0.00.085.210 I llama_new_context_with_model: graph splits = 2
0.00.085.213 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.809 I main: llama threadpool init, n_threads = 4
0.00.442.848 I 
0.00.442.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.871 I 
0.00.443.118 I sampler seed: 1234
0.00.443.124 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.159 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.159 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.112.599 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.112.600 I llama_perf_context_print:        load time =     433.10 ms
0.01.112.601 I llama_perf_context_print: prompt eval time =      35.59 ms /     7 tokens (    5.08 ms per token,   196.70 tokens per second)
0.01.112.601 I llama_perf_context_print:        eval time =     631.27 ms /    63 runs   (   10.02 ms per token,    99.80 tokens per second)
0.01.112.602 I llama_perf_context_print:       total time =     669.79 ms /    70 tokens
0.01.112.854 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.110s
sys	0m0.109s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.409 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.164 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.165 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.166 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.174 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.175 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.643 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.643 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.644 I llama_model_loader: - type  f32:  194 tensors
0.00.025.644 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.644 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.645 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.645 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.645 I print_info: file format = GGUF V3 (latest)
0.00.025.646 I print_info: file type   = Q3_K - Medium
0.00.025.647 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.467 I load: special tokens cache size = 25
0.00.051.570 I load: token to piece cache size = 0.2984 MB
0.00.051.585 I print_info: arch             = gptneox
0.00.051.586 I print_info: n_vocab (hp)     = 50304
0.00.051.586 I print_info: vocab_only       = 0
0.00.051.587 I print_info: n_ctx_train      = 2048
0.00.051.587 I print_info: n_embd           = 2048
0.00.051.587 I print_info: n_layer          = 24
0.00.051.591 I print_info: n_head           = 16
0.00.051.591 I print_info: n_head_kv        = 16
0.00.051.591 I print_info: n_rot            = 32
0.00.051.592 I print_info: n_swa            = 0
0.00.051.593 I print_info: n_embd_head_k    = 128
0.00.051.593 I print_info: n_embd_head_v    = 128
0.00.051.594 I print_info: n_gqa            = 1
0.00.051.595 I print_info: n_embd_k_gqa     = 2048
0.00.051.596 I print_info: n_embd_v_gqa     = 2048
0.00.051.596 I print_info: f_norm_eps       = 1.0e-05
0.00.051.596 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.596 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.597 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.597 I print_info: f_logit_scale    = 0.0e+00
0.00.051.599 I print_info: n_ff             = 8192
0.00.051.599 I print_info: n_expert         = 0
0.00.051.599 I print_info: n_expert_used    = 0
0.00.051.599 I print_info: causal attn      = 1
0.00.051.599 I print_info: pooling type     = 0
0.00.051.599 I print_info: rope type        = 2
0.00.051.599 I print_info: rope scaling     = linear
0.00.051.600 I print_info: freq_base_train  = 10000.0
0.00.051.600 I print_info: freq_scale_train = 1
0.00.051.600 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.600 I print_info: rope_finetuned   = unknown
0.00.051.600 I print_info: ssm_d_conv       = 0
0.00.051.600 I print_info: ssm_d_inner      = 0
0.00.051.600 I print_info: ssm_d_state      = 0
0.00.051.601 I print_info: ssm_dt_rank      = 0
0.00.051.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.601 I print_info: model type       = 1.4B
0.00.051.601 I print_info: model params     = 1.41 B
0.00.051.601 I print_info: general.name     = 1.4B
0.00.051.602 I print_info: vocab type       = BPE
0.00.051.602 I print_info: n_vocab          = 50304
0.00.051.603 I print_info: n_merges         = 50009
0.00.051.604 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.604 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.604 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.604 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.604 I print_info: LF token         = 128 'Ä'
0.00.051.605 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.605 I print_info: max token length = 1024
0.00.053.642 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.642 I load_tensors: offloading output layer to GPU
0.00.053.643 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.653 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.654 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.054.035 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.036 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.036 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.036 I llama_new_context_with_model: n_batch       = 2048
0.00.054.036 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.037 I llama_new_context_with_model: flash_attn    = 0
0.00.054.037 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.037 I llama_new_context_with_model: freq_scale    = 1
0.00.054.038 I ggml_metal_init: allocating
0.00.054.041 I ggml_metal_init: found device: Apple M4
0.00.054.042 I ggml_metal_init: picking default device: Apple M4
0.00.054.648 I ggml_metal_init: using embedded metal library
0.00.057.066 I ggml_metal_init: GPU name:   Apple M4
0.00.057.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.068 I ggml_metal_init: simdgroup reduction   = true
0.00.057.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.069 I ggml_metal_init: has bfloat            = true
0.00.057.069 I ggml_metal_init: use bfloat            = true
0.00.057.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.125 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.260 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.268 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.308 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.341 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.342 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.343 I llama_new_context_with_model: graph nodes  = 967
0.00.087.343 I llama_new_context_with_model: graph splits = 2
0.00.087.346 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.121 I main: llama threadpool init, n_threads = 4
0.00.532.157 I 
0.00.532.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.195 I 
0.00.532.427 I sampler seed: 1234
0.00.532.432 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.456 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.456 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.278.805 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.278.806 I llama_perf_context_print:        load time =     522.71 ms
0.01.278.806 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.01.278.807 I llama_perf_context_print:        eval time =     699.65 ms /    63 runs   (   11.11 ms per token,    90.05 tokens per second)
0.01.278.808 I llama_perf_context_print:       total time =     746.69 ms /    70 tokens
0.01.279.004 I ggml_metal_free: deallocating

real	0m1.295s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.640 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.940 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.941 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.945 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.945 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.946 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.531 I llama_model_loader: - type  f32:  194 tensors
0.00.026.532 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.532 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.532 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.532 I print_info: file format = GGUF V3 (latest)
0.00.026.533 I print_info: file type   = Q4_K - Medium
0.00.026.534 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.476 I load: special tokens cache size = 25
0.00.051.411 I load: token to piece cache size = 0.2984 MB
0.00.051.425 I print_info: arch             = gptneox
0.00.051.426 I print_info: n_vocab (hp)     = 50304
0.00.051.426 I print_info: vocab_only       = 0
0.00.051.427 I print_info: n_ctx_train      = 2048
0.00.051.427 I print_info: n_embd           = 2048
0.00.051.427 I print_info: n_layer          = 24
0.00.051.430 I print_info: n_head           = 16
0.00.051.431 I print_info: n_head_kv        = 16
0.00.051.431 I print_info: n_rot            = 32
0.00.051.431 I print_info: n_swa            = 0
0.00.051.431 I print_info: n_embd_head_k    = 128
0.00.051.431 I print_info: n_embd_head_v    = 128
0.00.051.432 I print_info: n_gqa            = 1
0.00.051.433 I print_info: n_embd_k_gqa     = 2048
0.00.051.433 I print_info: n_embd_v_gqa     = 2048
0.00.051.434 I print_info: f_norm_eps       = 1.0e-05
0.00.051.434 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.434 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.435 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.435 I print_info: f_logit_scale    = 0.0e+00
0.00.051.435 I print_info: n_ff             = 8192
0.00.051.437 I print_info: n_expert         = 0
0.00.051.438 I print_info: n_expert_used    = 0
0.00.051.438 I print_info: causal attn      = 1
0.00.051.439 I print_info: pooling type     = 0
0.00.051.439 I print_info: rope type        = 2
0.00.051.439 I print_info: rope scaling     = linear
0.00.051.439 I print_info: freq_base_train  = 10000.0
0.00.051.439 I print_info: freq_scale_train = 1
0.00.051.439 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.440 I print_info: rope_finetuned   = unknown
0.00.051.440 I print_info: ssm_d_conv       = 0
0.00.051.440 I print_info: ssm_d_inner      = 0
0.00.051.440 I print_info: ssm_d_state      = 0
0.00.051.440 I print_info: ssm_dt_rank      = 0
0.00.051.440 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.440 I print_info: model type       = 1.4B
0.00.051.441 I print_info: model params     = 1.41 B
0.00.051.444 I print_info: general.name     = 1.4B
0.00.051.444 I print_info: vocab type       = BPE
0.00.051.444 I print_info: n_vocab          = 50304
0.00.051.444 I print_info: n_merges         = 50009
0.00.051.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.446 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.446 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.447 I print_info: LF token         = 128 'Ä'
0.00.051.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.447 I print_info: max token length = 1024
0.00.053.453 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.453 I load_tensors: offloading output layer to GPU
0.00.053.453 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.464 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.465 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.762 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.763 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.763 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.763 I llama_new_context_with_model: n_batch       = 2048
0.00.053.763 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.763 I llama_new_context_with_model: flash_attn    = 0
0.00.053.764 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.764 I llama_new_context_with_model: freq_scale    = 1
0.00.053.764 I ggml_metal_init: allocating
0.00.053.767 I ggml_metal_init: found device: Apple M4
0.00.053.769 I ggml_metal_init: picking default device: Apple M4
0.00.054.373 I ggml_metal_init: using embedded metal library
0.00.056.718 I ggml_metal_init: GPU name:   Apple M4
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.720 I ggml_metal_init: simdgroup reduction   = true
0.00.056.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.720 I ggml_metal_init: has bfloat            = true
0.00.056.720 I ggml_metal_init: use bfloat            = true
0.00.056.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.473 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.295 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.332 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.485 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.486 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.487 I llama_new_context_with_model: graph nodes  = 967
0.00.087.487 I llama_new_context_with_model: graph splits = 2
0.00.087.490 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.456 I main: llama threadpool init, n_threads = 4
0.00.611.494 I 
0.00.611.517 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.517 I 
0.00.611.749 I sampler seed: 1234
0.00.611.755 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.795 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.799 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.799 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.080 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.373.081 I llama_perf_context_print:        load time =     600.81 ms
0.01.373.086 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.60 tokens per second)
0.01.373.086 I llama_perf_context_print:        eval time =     711.08 ms /    63 runs   (   11.29 ms per token,    88.60 tokens per second)
0.01.373.087 I llama_perf_context_print:       total time =     761.63 ms /    70 tokens
0.01.373.327 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.109s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.755 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.899 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.919 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.919 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.920 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.495 I llama_model_loader: - type  f32:  194 tensors
0.00.025.496 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.496 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.497 I print_info: file format = GGUF V3 (latest)
0.00.025.497 I print_info: file type   = Q5_K - Medium
0.00.025.501 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.318 I load: special tokens cache size = 25
0.00.050.249 I load: token to piece cache size = 0.2984 MB
0.00.050.263 I print_info: arch             = gptneox
0.00.050.264 I print_info: n_vocab (hp)     = 50304
0.00.050.265 I print_info: vocab_only       = 0
0.00.050.265 I print_info: n_ctx_train      = 2048
0.00.050.265 I print_info: n_embd           = 2048
0.00.050.265 I print_info: n_layer          = 24
0.00.050.269 I print_info: n_head           = 16
0.00.050.269 I print_info: n_head_kv        = 16
0.00.050.269 I print_info: n_rot            = 32
0.00.050.270 I print_info: n_swa            = 0
0.00.050.270 I print_info: n_embd_head_k    = 128
0.00.050.270 I print_info: n_embd_head_v    = 128
0.00.050.272 I print_info: n_gqa            = 1
0.00.050.272 I print_info: n_embd_k_gqa     = 2048
0.00.050.273 I print_info: n_embd_v_gqa     = 2048
0.00.050.274 I print_info: f_norm_eps       = 1.0e-05
0.00.050.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.275 I print_info: f_logit_scale    = 0.0e+00
0.00.050.275 I print_info: n_ff             = 8192
0.00.050.275 I print_info: n_expert         = 0
0.00.050.275 I print_info: n_expert_used    = 0
0.00.050.276 I print_info: causal attn      = 1
0.00.050.276 I print_info: pooling type     = 0
0.00.050.276 I print_info: rope type        = 2
0.00.050.276 I print_info: rope scaling     = linear
0.00.050.276 I print_info: freq_base_train  = 10000.0
0.00.050.277 I print_info: freq_scale_train = 1
0.00.050.277 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.277 I print_info: rope_finetuned   = unknown
0.00.050.279 I print_info: ssm_d_conv       = 0
0.00.050.279 I print_info: ssm_d_inner      = 0
0.00.050.279 I print_info: ssm_d_state      = 0
0.00.050.279 I print_info: ssm_dt_rank      = 0
0.00.050.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.279 I print_info: model type       = 1.4B
0.00.050.280 I print_info: model params     = 1.41 B
0.00.050.280 I print_info: general.name     = 1.4B
0.00.050.280 I print_info: vocab type       = BPE
0.00.050.280 I print_info: n_vocab          = 50304
0.00.050.280 I print_info: n_merges         = 50009
0.00.050.281 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.281 I print_info: LF token         = 128 'Ä'
0.00.050.281 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.282 I print_info: max token length = 1024
0.00.052.274 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.274 I load_tensors: offloading output layer to GPU
0.00.052.274 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.285 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.286 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.566 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.566 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.567 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.567 I llama_new_context_with_model: n_batch       = 2048
0.00.052.567 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.567 I llama_new_context_with_model: flash_attn    = 0
0.00.052.568 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.568 I llama_new_context_with_model: freq_scale    = 1
0.00.052.568 I ggml_metal_init: allocating
0.00.052.571 I ggml_metal_init: found device: Apple M4
0.00.052.573 I ggml_metal_init: picking default device: Apple M4
0.00.053.172 I ggml_metal_init: using embedded metal library
0.00.055.506 I ggml_metal_init: GPU name:   Apple M4
0.00.055.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.509 I ggml_metal_init: simdgroup reduction   = true
0.00.055.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.509 I ggml_metal_init: has bfloat            = true
0.00.055.509 I ggml_metal_init: use bfloat            = true
0.00.055.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.218 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.740 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.751 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.796 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.728 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.730 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.730 I llama_new_context_with_model: graph nodes  = 967
0.00.085.730 I llama_new_context_with_model: graph splits = 2
0.00.085.733 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.190 I main: llama threadpool init, n_threads = 4
0.00.681.225 I 
0.00.681.261 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.263 I 
0.00.681.483 I sampler seed: 1234
0.00.681.487 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.531 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.533 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.533 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.532.328 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.532.329 I llama_perf_context_print:        load time =     672.43 ms
0.01.532.329 I llama_perf_context_print: prompt eval time =      51.49 ms /     7 tokens (    7.36 ms per token,   135.94 tokens per second)
0.01.532.330 I llama_perf_context_print:        eval time =     796.21 ms /    63 runs   (   12.64 ms per token,    79.13 tokens per second)
0.01.532.330 I llama_perf_context_print:       total time =     851.14 ms /    70 tokens
0.01.532.530 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.108s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.729 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.234 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.235 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.974 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.762 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.763 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.764 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.764 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.764 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.765 I llama_model_loader: - type  f32:  194 tensors
0.00.025.765 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.766 I print_info: file format = GGUF V3 (latest)
0.00.025.766 I print_info: file type   = Q6_K
0.00.025.767 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.686 I load: special tokens cache size = 25
0.00.050.604 I load: token to piece cache size = 0.2984 MB
0.00.050.619 I print_info: arch             = gptneox
0.00.050.620 I print_info: n_vocab (hp)     = 50304
0.00.050.621 I print_info: vocab_only       = 0
0.00.050.621 I print_info: n_ctx_train      = 2048
0.00.050.621 I print_info: n_embd           = 2048
0.00.050.621 I print_info: n_layer          = 24
0.00.050.625 I print_info: n_head           = 16
0.00.050.626 I print_info: n_head_kv        = 16
0.00.050.626 I print_info: n_rot            = 32
0.00.050.626 I print_info: n_swa            = 0
0.00.050.628 I print_info: n_embd_head_k    = 128
0.00.050.628 I print_info: n_embd_head_v    = 128
0.00.050.629 I print_info: n_gqa            = 1
0.00.050.630 I print_info: n_embd_k_gqa     = 2048
0.00.050.630 I print_info: n_embd_v_gqa     = 2048
0.00.050.631 I print_info: f_norm_eps       = 1.0e-05
0.00.050.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.632 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.633 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.633 I print_info: f_logit_scale    = 0.0e+00
0.00.050.633 I print_info: n_ff             = 8192
0.00.050.633 I print_info: n_expert         = 0
0.00.050.634 I print_info: n_expert_used    = 0
0.00.050.634 I print_info: causal attn      = 1
0.00.050.634 I print_info: pooling type     = 0
0.00.050.634 I print_info: rope type        = 2
0.00.050.634 I print_info: rope scaling     = linear
0.00.050.634 I print_info: freq_base_train  = 10000.0
0.00.050.635 I print_info: freq_scale_train = 1
0.00.050.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.635 I print_info: rope_finetuned   = unknown
0.00.050.635 I print_info: ssm_d_conv       = 0
0.00.050.635 I print_info: ssm_d_inner      = 0
0.00.050.635 I print_info: ssm_d_state      = 0
0.00.050.635 I print_info: ssm_dt_rank      = 0
0.00.050.636 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.641 I print_info: model type       = 1.4B
0.00.050.641 I print_info: model params     = 1.41 B
0.00.050.641 I print_info: general.name     = 1.4B
0.00.050.641 I print_info: vocab type       = BPE
0.00.050.642 I print_info: n_vocab          = 50304
0.00.050.642 I print_info: n_merges         = 50009
0.00.050.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.643 I print_info: LF token         = 128 'Ä'
0.00.050.644 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.644 I print_info: max token length = 1024
0.00.052.610 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.610 I load_tensors: offloading output layer to GPU
0.00.052.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.621 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.622 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.904 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.905 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.905 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.905 I llama_new_context_with_model: n_batch       = 2048
0.00.052.906 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.906 I llama_new_context_with_model: flash_attn    = 0
0.00.052.906 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.906 I llama_new_context_with_model: freq_scale    = 1
0.00.052.907 I ggml_metal_init: allocating
0.00.052.913 I ggml_metal_init: found device: Apple M4
0.00.052.915 I ggml_metal_init: picking default device: Apple M4
0.00.053.497 I ggml_metal_init: using embedded metal library
0.00.055.898 I ggml_metal_init: GPU name:   Apple M4
0.00.055.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.900 I ggml_metal_init: simdgroup reduction   = true
0.00.055.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.901 I ggml_metal_init: has bfloat            = true
0.00.055.901 I ggml_metal_init: use bfloat            = true
0.00.055.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.658 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.576 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.587 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.631 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.682 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.684 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.684 I llama_new_context_with_model: graph nodes  = 967
0.00.086.684 I llama_new_context_with_model: graph splits = 2
0.00.086.687 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.091 I main: llama threadpool init, n_threads = 4
0.00.761.129 I 
0.00.761.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.154 I 
0.00.761.395 I sampler seed: 1234
0.00.761.401 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.421 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.422 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.422 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.632.392 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.632.393 I llama_perf_context_print:        load time =     751.36 ms
0.01.632.394 I llama_perf_context_print: prompt eval time =      54.34 ms /     7 tokens (    7.76 ms per token,   128.81 tokens per second)
0.01.632.395 I llama_perf_context_print:        eval time =     813.64 ms /    63 runs   (   12.91 ms per token,    77.43 tokens per second)
0.01.632.395 I llama_perf_context_print:       total time =     871.31 ms /    70 tokens
0.01.632.575 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.109s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.583 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.587 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.461 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.578 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.580 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.580 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.581 I llama_model_loader: - type  f32:  194 tensors
0.00.057.581 I llama_model_loader: - type  f16:   98 tensors
0.00.057.582 I print_info: file format = GGUF V3 (latest)
0.00.057.583 I print_info: file type   = all F32 (guessed)
0.00.057.585 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.864 I load: special tokens cache size = 25
0.00.092.813 I load: token to piece cache size = 0.2984 MB
0.00.092.830 I print_info: arch             = gptneox
0.00.092.831 I print_info: n_vocab (hp)     = 50304
0.00.092.832 I print_info: vocab_only       = 0
0.00.092.832 I print_info: n_ctx_train      = 2048
0.00.092.832 I print_info: n_embd           = 2048
0.00.092.832 I print_info: n_layer          = 24
0.00.092.836 I print_info: n_head           = 16
0.00.092.836 I print_info: n_head_kv        = 16
0.00.092.837 I print_info: n_rot            = 32
0.00.092.837 I print_info: n_swa            = 0
0.00.092.840 I print_info: n_embd_head_k    = 128
0.00.092.840 I print_info: n_embd_head_v    = 128
0.00.092.840 I print_info: n_gqa            = 1
0.00.092.841 I print_info: n_embd_k_gqa     = 2048
0.00.092.842 I print_info: n_embd_v_gqa     = 2048
0.00.092.842 I print_info: f_norm_eps       = 1.0e-05
0.00.092.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.843 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.843 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.843 I print_info: f_logit_scale    = 0.0e+00
0.00.092.844 I print_info: n_ff             = 8192
0.00.092.844 I print_info: n_expert         = 0
0.00.092.844 I print_info: n_expert_used    = 0
0.00.092.844 I print_info: causal attn      = 1
0.00.092.844 I print_info: pooling type     = 0
0.00.092.844 I print_info: rope type        = 2
0.00.092.845 I print_info: rope scaling     = linear
0.00.092.845 I print_info: freq_base_train  = 10000.0
0.00.092.845 I print_info: freq_scale_train = 1
0.00.092.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.846 I print_info: rope_finetuned   = unknown
0.00.092.847 I print_info: ssm_d_conv       = 0
0.00.092.847 I print_info: ssm_d_inner      = 0
0.00.092.847 I print_info: ssm_d_state      = 0
0.00.092.847 I print_info: ssm_dt_rank      = 0
0.00.092.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.847 I print_info: model type       = 1.4B
0.00.092.848 I print_info: model params     = 1.41 B
0.00.092.848 I print_info: general.name     = 1.4B
0.00.092.848 I print_info: vocab type       = BPE
0.00.092.849 I print_info: n_vocab          = 50304
0.00.092.849 I print_info: n_merges         = 50009
0.00.092.849 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.849 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.849 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.851 I print_info: LF token         = 128 'Ä'
0.00.092.851 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.851 I print_info: max token length = 1024
0.00.095.608 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.608 I load_tensors: offloading output layer to GPU
0.00.095.609 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.620 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.621 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.095.994 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.995 I llama_new_context_with_model: n_ctx         = 128
0.00.095.995 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.995 I llama_new_context_with_model: n_batch       = 128
0.00.095.996 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.996 I llama_new_context_with_model: flash_attn    = 0
0.00.095.996 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.996 I llama_new_context_with_model: freq_scale    = 1
0.00.095.997 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.997 I ggml_metal_init: allocating
0.00.096.000 I ggml_metal_init: found device: Apple M4
0.00.096.002 I ggml_metal_init: picking default device: Apple M4
0.00.096.649 I ggml_metal_init: using embedded metal library
0.00.099.311 I ggml_metal_init: GPU name:   Apple M4
0.00.099.313 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.313 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.314 I ggml_metal_init: simdgroup reduction   = true
0.00.099.314 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.314 I ggml_metal_init: has bfloat            = true
0.00.099.314 I ggml_metal_init: use bfloat            = true
0.00.099.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.409 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.413 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.442 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.360 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.361 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.362 I llama_new_context_with_model: graph nodes  = 967
0.00.111.362 I llama_new_context_with_model: graph splits = 2
0.00.111.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.111.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.116.513 I 
0.01.116.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.116.571 I perplexity: tokenizing the input ..
0.01.130.513 I perplexity: tokenization took 13.939 ms
0.01.130.520 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.251.326 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.252.660 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.252.688 I llama_perf_context_print:        load time =    1091.91 ms
0.01.252.690 I llama_perf_context_print: prompt eval time =     119.88 ms /   128 tokens (    0.94 ms per token,  1067.78 tokens per second)
0.01.252.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.252.696 I llama_perf_context_print:       total time =     136.18 ms /   129 tokens
0.01.253.033 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.122s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.480 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.176 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.181 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.187 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.856 I llama_model_loader: - type  f32:  194 tensors
0.00.027.857 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.857 I print_info: file format = GGUF V3 (latest)
0.00.027.858 I print_info: file type   = Q8_0
0.00.027.859 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.390 I load: special tokens cache size = 25
0.00.053.469 I load: token to piece cache size = 0.2984 MB
0.00.053.486 I print_info: arch             = gptneox
0.00.053.487 I print_info: n_vocab (hp)     = 50304
0.00.053.487 I print_info: vocab_only       = 0
0.00.053.487 I print_info: n_ctx_train      = 2048
0.00.053.487 I print_info: n_embd           = 2048
0.00.053.487 I print_info: n_layer          = 24
0.00.053.492 I print_info: n_head           = 16
0.00.053.492 I print_info: n_head_kv        = 16
0.00.053.492 I print_info: n_rot            = 32
0.00.053.493 I print_info: n_swa            = 0
0.00.053.493 I print_info: n_embd_head_k    = 128
0.00.053.493 I print_info: n_embd_head_v    = 128
0.00.053.493 I print_info: n_gqa            = 1
0.00.053.494 I print_info: n_embd_k_gqa     = 2048
0.00.053.495 I print_info: n_embd_v_gqa     = 2048
0.00.053.495 I print_info: f_norm_eps       = 1.0e-05
0.00.053.495 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.496 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.496 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.496 I print_info: f_logit_scale    = 0.0e+00
0.00.053.496 I print_info: n_ff             = 8192
0.00.053.497 I print_info: n_expert         = 0
0.00.053.497 I print_info: n_expert_used    = 0
0.00.053.497 I print_info: causal attn      = 1
0.00.053.497 I print_info: pooling type     = 0
0.00.053.497 I print_info: rope type        = 2
0.00.053.497 I print_info: rope scaling     = linear
0.00.053.498 I print_info: freq_base_train  = 10000.0
0.00.053.498 I print_info: freq_scale_train = 1
0.00.053.498 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.498 I print_info: rope_finetuned   = unknown
0.00.053.498 I print_info: ssm_d_conv       = 0
0.00.053.499 I print_info: ssm_d_inner      = 0
0.00.053.499 I print_info: ssm_d_state      = 0
0.00.053.499 I print_info: ssm_dt_rank      = 0
0.00.053.499 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.499 I print_info: model type       = 1.4B
0.00.053.499 I print_info: model params     = 1.41 B
0.00.053.499 I print_info: general.name     = 1.4B
0.00.053.501 I print_info: vocab type       = BPE
0.00.053.501 I print_info: n_vocab          = 50304
0.00.053.502 I print_info: n_merges         = 50009
0.00.053.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.504 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.505 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.505 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.505 I print_info: LF token         = 128 'Ä'
0.00.053.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.505 I print_info: max token length = 1024
0.00.055.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.602 I load_tensors: offloading output layer to GPU
0.00.055.602 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.613 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.614 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.055.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.922 I llama_new_context_with_model: n_ctx         = 128
0.00.055.922 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.922 I llama_new_context_with_model: n_batch       = 128
0.00.055.922 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.922 I llama_new_context_with_model: flash_attn    = 0
0.00.055.923 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.923 I llama_new_context_with_model: freq_scale    = 1
0.00.055.923 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.924 I ggml_metal_init: allocating
0.00.055.927 I ggml_metal_init: found device: Apple M4
0.00.055.929 I ggml_metal_init: picking default device: Apple M4
0.00.056.522 I ggml_metal_init: using embedded metal library
0.00.058.942 I ggml_metal_init: GPU name:   Apple M4
0.00.058.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.943 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.944 I ggml_metal_init: simdgroup reduction   = true
0.00.058.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.944 I ggml_metal_init: has bfloat            = true
0.00.058.945 I ggml_metal_init: use bfloat            = true
0.00.058.945 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.135 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.647 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.673 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.669 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.670 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.670 I llama_new_context_with_model: graph nodes  = 967
0.00.071.671 I llama_new_context_with_model: graph splits = 2
0.00.071.672 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.953.989 I 
0.00.954.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.954.026 I perplexity: tokenizing the input ..
0.00.961.360 I perplexity: tokenization took 7.332 ms
0.00.961.363 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.084.935 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.086.365 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.086.391 I llama_perf_context_print:        load time =     944.50 ms
0.01.086.392 I llama_perf_context_print: prompt eval time =     123.34 ms /   128 tokens (    0.96 ms per token,  1037.81 tokens per second)
0.01.086.392 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.086.393 I llama_perf_context_print:       total time =     132.40 ms /   129 tokens
0.01.086.729 I ggml_metal_free: deallocating

real	0m1.105s
user	0m0.082s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.234 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.974 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.975 I llama_model_loader: - type  f32:  194 tensors
0.00.025.975 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.975 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.976 I print_info: file format = GGUF V3 (latest)
0.00.025.977 I print_info: file type   = Q4_0
0.00.025.978 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.221 I load: special tokens cache size = 25
0.00.052.394 I load: token to piece cache size = 0.2984 MB
0.00.052.409 I print_info: arch             = gptneox
0.00.052.410 I print_info: n_vocab (hp)     = 50304
0.00.052.410 I print_info: vocab_only       = 0
0.00.052.410 I print_info: n_ctx_train      = 2048
0.00.052.410 I print_info: n_embd           = 2048
0.00.052.410 I print_info: n_layer          = 24
0.00.052.414 I print_info: n_head           = 16
0.00.052.415 I print_info: n_head_kv        = 16
0.00.052.415 I print_info: n_rot            = 32
0.00.052.416 I print_info: n_swa            = 0
0.00.052.416 I print_info: n_embd_head_k    = 128
0.00.052.418 I print_info: n_embd_head_v    = 128
0.00.052.419 I print_info: n_gqa            = 1
0.00.052.419 I print_info: n_embd_k_gqa     = 2048
0.00.052.420 I print_info: n_embd_v_gqa     = 2048
0.00.052.422 I print_info: f_norm_eps       = 1.0e-05
0.00.052.422 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.422 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.423 I print_info: f_logit_scale    = 0.0e+00
0.00.052.424 I print_info: n_ff             = 8192
0.00.052.424 I print_info: n_expert         = 0
0.00.052.424 I print_info: n_expert_used    = 0
0.00.052.424 I print_info: causal attn      = 1
0.00.052.424 I print_info: pooling type     = 0
0.00.052.424 I print_info: rope type        = 2
0.00.052.424 I print_info: rope scaling     = linear
0.00.052.425 I print_info: freq_base_train  = 10000.0
0.00.052.425 I print_info: freq_scale_train = 1
0.00.052.425 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.425 I print_info: rope_finetuned   = unknown
0.00.052.426 I print_info: ssm_d_conv       = 0
0.00.052.426 I print_info: ssm_d_inner      = 0
0.00.052.426 I print_info: ssm_d_state      = 0
0.00.052.426 I print_info: ssm_dt_rank      = 0
0.00.052.426 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.426 I print_info: model type       = 1.4B
0.00.052.427 I print_info: model params     = 1.41 B
0.00.052.427 I print_info: general.name     = 1.4B
0.00.052.427 I print_info: vocab type       = BPE
0.00.052.427 I print_info: n_vocab          = 50304
0.00.052.429 I print_info: n_merges         = 50009
0.00.052.429 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.430 I print_info: LF token         = 128 'Ä'
0.00.052.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.431 I print_info: max token length = 1024
0.00.054.386 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.387 I load_tensors: offloading output layer to GPU
0.00.054.388 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.398 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.400 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.778 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.779 I llama_new_context_with_model: n_ctx         = 128
0.00.054.779 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.779 I llama_new_context_with_model: n_batch       = 128
0.00.054.779 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.780 I llama_new_context_with_model: flash_attn    = 0
0.00.054.780 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.809 I llama_new_context_with_model: freq_scale    = 1
0.00.054.830 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.831 I ggml_metal_init: allocating
0.00.054.834 I ggml_metal_init: found device: Apple M4
0.00.054.835 I ggml_metal_init: picking default device: Apple M4
0.00.055.437 I ggml_metal_init: using embedded metal library
0.00.060.099 I ggml_metal_init: GPU name:   Apple M4
0.00.060.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.102 I ggml_metal_init: simdgroup reduction   = true
0.00.060.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.102 I ggml_metal_init: has bfloat            = true
0.00.060.102 I ggml_metal_init: use bfloat            = true
0.00.060.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.391 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.687 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.690 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.721 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.652 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.653 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.653 I llama_new_context_with_model: graph nodes  = 967
0.00.072.653 I llama_new_context_with_model: graph splits = 2
0.00.072.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.655 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.574 I 
0.00.637.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.620 I perplexity: tokenizing the input ..
0.00.645.896 I perplexity: tokenization took 8.275 ms
0.00.645.900 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.092 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.770.332 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.770.355 I llama_perf_context_print:        load time =     627.33 ms
0.00.770.356 I llama_perf_context_print: prompt eval time =     122.95 ms /   128 tokens (    0.96 ms per token,  1041.05 tokens per second)
0.00.770.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.357 I llama_perf_context_print:       total time =     132.78 ms /   129 tokens
0.00.770.791 I ggml_metal_free: deallocating

real	0m0.788s
user	0m0.079s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.893 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.740 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.164 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.165 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.165 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.165 I llama_model_loader: - type  f32:  194 tensors
0.00.024.166 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.166 I print_info: file format = GGUF V3 (latest)
0.00.024.167 I print_info: file type   = Q4_1
0.00.024.168 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.889 I load: special tokens cache size = 25
0.00.048.908 I load: token to piece cache size = 0.2984 MB
0.00.048.923 I print_info: arch             = gptneox
0.00.048.924 I print_info: n_vocab (hp)     = 50304
0.00.048.924 I print_info: vocab_only       = 0
0.00.048.924 I print_info: n_ctx_train      = 2048
0.00.048.924 I print_info: n_embd           = 2048
0.00.048.925 I print_info: n_layer          = 24
0.00.048.928 I print_info: n_head           = 16
0.00.048.928 I print_info: n_head_kv        = 16
0.00.048.929 I print_info: n_rot            = 32
0.00.048.929 I print_info: n_swa            = 0
0.00.048.929 I print_info: n_embd_head_k    = 128
0.00.048.929 I print_info: n_embd_head_v    = 128
0.00.048.930 I print_info: n_gqa            = 1
0.00.048.931 I print_info: n_embd_k_gqa     = 2048
0.00.048.931 I print_info: n_embd_v_gqa     = 2048
0.00.048.932 I print_info: f_norm_eps       = 1.0e-05
0.00.048.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.933 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.933 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.933 I print_info: f_logit_scale    = 0.0e+00
0.00.048.934 I print_info: n_ff             = 8192
0.00.048.934 I print_info: n_expert         = 0
0.00.048.934 I print_info: n_expert_used    = 0
0.00.048.934 I print_info: causal attn      = 1
0.00.048.934 I print_info: pooling type     = 0
0.00.048.934 I print_info: rope type        = 2
0.00.048.934 I print_info: rope scaling     = linear
0.00.048.936 I print_info: freq_base_train  = 10000.0
0.00.048.936 I print_info: freq_scale_train = 1
0.00.048.936 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.936 I print_info: rope_finetuned   = unknown
0.00.048.936 I print_info: ssm_d_conv       = 0
0.00.048.937 I print_info: ssm_d_inner      = 0
0.00.048.937 I print_info: ssm_d_state      = 0
0.00.048.937 I print_info: ssm_dt_rank      = 0
0.00.048.937 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.937 I print_info: model type       = 1.4B
0.00.048.937 I print_info: model params     = 1.41 B
0.00.048.938 I print_info: general.name     = 1.4B
0.00.048.938 I print_info: vocab type       = BPE
0.00.048.938 I print_info: n_vocab          = 50304
0.00.048.938 I print_info: n_merges         = 50009
0.00.048.938 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.939 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.939 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.939 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.939 I print_info: LF token         = 128 'Ä'
0.00.048.939 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.940 I print_info: max token length = 1024
0.00.050.910 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.910 I load_tensors: offloading output layer to GPU
0.00.050.910 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.921 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.922 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.193 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.194 I llama_new_context_with_model: n_ctx         = 128
0.00.051.194 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.194 I llama_new_context_with_model: n_batch       = 128
0.00.051.194 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.194 I llama_new_context_with_model: flash_attn    = 0
0.00.051.195 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.195 I llama_new_context_with_model: freq_scale    = 1
0.00.051.195 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.196 I ggml_metal_init: allocating
0.00.051.199 I ggml_metal_init: found device: Apple M4
0.00.051.201 I ggml_metal_init: picking default device: Apple M4
0.00.051.743 I ggml_metal_init: using embedded metal library
0.00.054.091 I ggml_metal_init: GPU name:   Apple M4
0.00.054.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.093 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.094 I ggml_metal_init: simdgroup reduction   = true
0.00.054.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.094 I ggml_metal_init: has bfloat            = true
0.00.054.094 I ggml_metal_init: use bfloat            = true
0.00.054.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.688 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.213 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.218 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.245 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.130 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.131 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.131 I llama_new_context_with_model: graph nodes  = 967
0.00.066.132 I llama_new_context_with_model: graph splits = 2
0.00.066.133 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.133 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.074 I 
0.00.670.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.372 I perplexity: tokenizing the input ..
0.00.687.749 I perplexity: tokenization took 17.37 ms
0.00.687.762 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.469 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.832.836 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.832.899 I llama_perf_context_print:        load time =     661.15 ms
0.00.832.902 I llama_perf_context_print: prompt eval time =     140.81 ms /   128 tokens (    1.10 ms per token,   909.01 tokens per second)
0.00.832.904 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.904 I llama_perf_context_print:       total time =     162.84 ms /   129 tokens
0.00.834.253 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.106s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.189 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.516 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.855 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.856 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.863 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.637 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.639 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.640 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.641 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.041.641 I llama_model_loader: - type  f32:  194 tensors
0.00.041.642 I llama_model_loader: - type q5_0:   97 tensors
0.00.041.642 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.642 I print_info: file format = GGUF V3 (latest)
0.00.041.643 I print_info: file type   = Q5_0
0.00.041.644 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.065.068 I load: special tokens cache size = 25
0.00.071.198 I load: token to piece cache size = 0.2984 MB
0.00.071.212 I print_info: arch             = gptneox
0.00.071.213 I print_info: n_vocab (hp)     = 50304
0.00.071.214 I print_info: vocab_only       = 0
0.00.071.214 I print_info: n_ctx_train      = 2048
0.00.071.214 I print_info: n_embd           = 2048
0.00.071.214 I print_info: n_layer          = 24
0.00.071.217 I print_info: n_head           = 16
0.00.071.218 I print_info: n_head_kv        = 16
0.00.071.218 I print_info: n_rot            = 32
0.00.071.220 I print_info: n_swa            = 0
0.00.071.220 I print_info: n_embd_head_k    = 128
0.00.071.220 I print_info: n_embd_head_v    = 128
0.00.071.221 I print_info: n_gqa            = 1
0.00.071.222 I print_info: n_embd_k_gqa     = 2048
0.00.071.223 I print_info: n_embd_v_gqa     = 2048
0.00.071.223 I print_info: f_norm_eps       = 1.0e-05
0.00.071.223 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.224 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.224 I print_info: f_logit_scale    = 0.0e+00
0.00.071.225 I print_info: n_ff             = 8192
0.00.071.225 I print_info: n_expert         = 0
0.00.071.225 I print_info: n_expert_used    = 0
0.00.071.225 I print_info: causal attn      = 1
0.00.071.225 I print_info: pooling type     = 0
0.00.071.225 I print_info: rope type        = 2
0.00.071.226 I print_info: rope scaling     = linear
0.00.071.226 I print_info: freq_base_train  = 10000.0
0.00.071.226 I print_info: freq_scale_train = 1
0.00.071.226 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.227 I print_info: rope_finetuned   = unknown
0.00.071.227 I print_info: ssm_d_conv       = 0
0.00.071.228 I print_info: ssm_d_inner      = 0
0.00.071.228 I print_info: ssm_d_state      = 0
0.00.071.228 I print_info: ssm_dt_rank      = 0
0.00.071.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.229 I print_info: model type       = 1.4B
0.00.071.229 I print_info: model params     = 1.41 B
0.00.071.229 I print_info: general.name     = 1.4B
0.00.071.229 I print_info: vocab type       = BPE
0.00.071.229 I print_info: n_vocab          = 50304
0.00.071.230 I print_info: n_merges         = 50009
0.00.071.230 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.230 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.230 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.231 I print_info: LF token         = 128 'Ä'
0.00.071.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.231 I print_info: max token length = 1024
0.00.073.220 I load_tensors: offloading 24 repeating layers to GPU
0.00.073.220 I load_tensors: offloading output layer to GPU
0.00.073.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.073.231 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.073.232 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.073.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.500 I llama_new_context_with_model: n_ctx         = 128
0.00.073.500 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.073.500 I llama_new_context_with_model: n_batch       = 128
0.00.073.500 I llama_new_context_with_model: n_ubatch      = 128
0.00.073.500 I llama_new_context_with_model: flash_attn    = 0
0.00.073.501 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.501 I llama_new_context_with_model: freq_scale    = 1
0.00.073.501 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.073.502 I ggml_metal_init: allocating
0.00.073.505 I ggml_metal_init: found device: Apple M4
0.00.073.507 I ggml_metal_init: picking default device: Apple M4
0.00.074.060 I ggml_metal_init: using embedded metal library
0.00.076.395 I ggml_metal_init: GPU name:   Apple M4
0.00.076.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.397 I ggml_metal_init: simdgroup reduction   = true
0.00.076.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.397 I ggml_metal_init: has bfloat            = true
0.00.076.397 I ggml_metal_init: use bfloat            = true
0.00.076.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.615 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.918 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.944 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.087.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.087.875 I llama_new_context_with_model: graph nodes  = 967
0.00.087.875 I llama_new_context_with_model: graph splits = 2
0.00.087.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.087.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.631 I 
0.00.754.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.682 I perplexity: tokenizing the input ..
0.00.765.828 I perplexity: tokenization took 11.144 ms
0.00.765.833 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.910.775 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.911.966 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.911.993 I llama_perf_context_print:        load time =     738.11 ms
0.00.911.994 I llama_perf_context_print: prompt eval time =     144.51 ms /   128 tokens (    1.13 ms per token,   885.73 tokens per second)
0.00.911.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.911.996 I llama_perf_context_print:       total time =     157.36 ms /   129 tokens
0.00.912.513 I ggml_metal_free: deallocating

real	0m0.936s
user	0m0.101s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.096 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.104 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.690 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.692 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.692 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.693 I llama_model_loader: - type  f32:  194 tensors
0.00.024.693 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.694 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.694 I print_info: file format = GGUF V3 (latest)
0.00.024.695 I print_info: file type   = Q5_1
0.00.024.696 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.161 I load: special tokens cache size = 25
0.00.050.170 I load: token to piece cache size = 0.2984 MB
0.00.050.184 I print_info: arch             = gptneox
0.00.050.185 I print_info: n_vocab (hp)     = 50304
0.00.050.185 I print_info: vocab_only       = 0
0.00.050.186 I print_info: n_ctx_train      = 2048
0.00.050.186 I print_info: n_embd           = 2048
0.00.050.186 I print_info: n_layer          = 24
0.00.050.189 I print_info: n_head           = 16
0.00.050.190 I print_info: n_head_kv        = 16
0.00.050.190 I print_info: n_rot            = 32
0.00.050.190 I print_info: n_swa            = 0
0.00.050.190 I print_info: n_embd_head_k    = 128
0.00.050.192 I print_info: n_embd_head_v    = 128
0.00.050.193 I print_info: n_gqa            = 1
0.00.050.194 I print_info: n_embd_k_gqa     = 2048
0.00.050.195 I print_info: n_embd_v_gqa     = 2048
0.00.050.195 I print_info: f_norm_eps       = 1.0e-05
0.00.050.196 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.196 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.196 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.196 I print_info: f_logit_scale    = 0.0e+00
0.00.050.197 I print_info: n_ff             = 8192
0.00.050.197 I print_info: n_expert         = 0
0.00.050.197 I print_info: n_expert_used    = 0
0.00.050.197 I print_info: causal attn      = 1
0.00.050.197 I print_info: pooling type     = 0
0.00.050.197 I print_info: rope type        = 2
0.00.050.198 I print_info: rope scaling     = linear
0.00.050.199 I print_info: freq_base_train  = 10000.0
0.00.050.199 I print_info: freq_scale_train = 1
0.00.050.199 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.199 I print_info: rope_finetuned   = unknown
0.00.050.199 I print_info: ssm_d_conv       = 0
0.00.050.199 I print_info: ssm_d_inner      = 0
0.00.050.200 I print_info: ssm_d_state      = 0
0.00.050.200 I print_info: ssm_dt_rank      = 0
0.00.050.200 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.200 I print_info: model type       = 1.4B
0.00.050.200 I print_info: model params     = 1.41 B
0.00.050.200 I print_info: general.name     = 1.4B
0.00.050.201 I print_info: vocab type       = BPE
0.00.050.201 I print_info: n_vocab          = 50304
0.00.050.201 I print_info: n_merges         = 50009
0.00.050.201 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.202 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.202 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.203 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.203 I print_info: LF token         = 128 'Ä'
0.00.050.203 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.204 I print_info: max token length = 1024
0.00.051.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.860 I load_tensors: offloading output layer to GPU
0.00.051.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.871 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.872 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.154 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.155 I llama_new_context_with_model: n_ctx         = 128
0.00.052.155 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.155 I llama_new_context_with_model: n_batch       = 128
0.00.052.156 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.156 I llama_new_context_with_model: flash_attn    = 0
0.00.052.156 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.156 I llama_new_context_with_model: freq_scale    = 1
0.00.052.157 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.157 I ggml_metal_init: allocating
0.00.052.160 I ggml_metal_init: found device: Apple M4
0.00.052.162 I ggml_metal_init: picking default device: Apple M4
0.00.052.749 I ggml_metal_init: using embedded metal library
0.00.055.137 I ggml_metal_init: GPU name:   Apple M4
0.00.055.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.140 I ggml_metal_init: simdgroup reduction   = true
0.00.055.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.140 I ggml_metal_init: has bfloat            = true
0.00.055.140 I ggml_metal_init: use bfloat            = true
0.00.055.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.047 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.291 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.294 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.320 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.224 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.225 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.226 I llama_new_context_with_model: graph nodes  = 967
0.00.067.226 I llama_new_context_with_model: graph splits = 2
0.00.067.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.517 I 
0.00.665.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.553 I perplexity: tokenizing the input ..
0.00.673.701 I perplexity: tokenization took 8.146 ms
0.00.673.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.498 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.809.654 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.809.677 I llama_perf_context_print:        load time =     656.42 ms
0.00.809.677 I llama_perf_context_print: prompt eval time =     134.53 ms /   128 tokens (    1.05 ms per token,   951.45 tokens per second)
0.00.809.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.679 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.810.037 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.224 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.815 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.451 I llama_model_loader: - type  f32:  194 tensors
0.00.025.452 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.452 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.452 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.453 I print_info: file format = GGUF V3 (latest)
0.00.025.455 I print_info: file type   = Q2_K - Medium
0.00.025.456 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.080 I load: special tokens cache size = 25
0.00.051.110 I load: token to piece cache size = 0.2984 MB
0.00.051.125 I print_info: arch             = gptneox
0.00.051.126 I print_info: n_vocab (hp)     = 50304
0.00.051.126 I print_info: vocab_only       = 0
0.00.051.126 I print_info: n_ctx_train      = 2048
0.00.051.127 I print_info: n_embd           = 2048
0.00.051.127 I print_info: n_layer          = 24
0.00.051.130 I print_info: n_head           = 16
0.00.051.131 I print_info: n_head_kv        = 16
0.00.051.131 I print_info: n_rot            = 32
0.00.051.131 I print_info: n_swa            = 0
0.00.051.131 I print_info: n_embd_head_k    = 128
0.00.051.132 I print_info: n_embd_head_v    = 128
0.00.051.132 I print_info: n_gqa            = 1
0.00.051.133 I print_info: n_embd_k_gqa     = 2048
0.00.051.134 I print_info: n_embd_v_gqa     = 2048
0.00.051.134 I print_info: f_norm_eps       = 1.0e-05
0.00.051.135 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.135 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.135 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.135 I print_info: f_logit_scale    = 0.0e+00
0.00.051.136 I print_info: n_ff             = 8192
0.00.051.136 I print_info: n_expert         = 0
0.00.051.136 I print_info: n_expert_used    = 0
0.00.051.136 I print_info: causal attn      = 1
0.00.051.136 I print_info: pooling type     = 0
0.00.051.136 I print_info: rope type        = 2
0.00.051.137 I print_info: rope scaling     = linear
0.00.051.137 I print_info: freq_base_train  = 10000.0
0.00.051.138 I print_info: freq_scale_train = 1
0.00.051.138 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.138 I print_info: rope_finetuned   = unknown
0.00.051.138 I print_info: ssm_d_conv       = 0
0.00.051.138 I print_info: ssm_d_inner      = 0
0.00.051.139 I print_info: ssm_d_state      = 0
0.00.051.139 I print_info: ssm_dt_rank      = 0
0.00.051.139 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.139 I print_info: model type       = 1.4B
0.00.051.139 I print_info: model params     = 1.41 B
0.00.051.140 I print_info: general.name     = 1.4B
0.00.051.140 I print_info: vocab type       = BPE
0.00.051.140 I print_info: n_vocab          = 50304
0.00.051.140 I print_info: n_merges         = 50009
0.00.051.140 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.141 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.141 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.141 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.141 I print_info: LF token         = 128 'Ä'
0.00.051.141 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.141 I print_info: max token length = 1024
0.00.052.787 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.787 I load_tensors: offloading output layer to GPU
0.00.052.788 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.798 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.800 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.099 I llama_new_context_with_model: n_ctx         = 128
0.00.053.099 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.099 I llama_new_context_with_model: n_batch       = 128
0.00.053.099 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.099 I llama_new_context_with_model: flash_attn    = 0
0.00.053.100 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.100 I llama_new_context_with_model: freq_scale    = 1
0.00.053.100 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.101 I ggml_metal_init: allocating
0.00.053.104 I ggml_metal_init: found device: Apple M4
0.00.053.106 I ggml_metal_init: picking default device: Apple M4
0.00.053.676 I ggml_metal_init: using embedded metal library
0.00.056.071 I ggml_metal_init: GPU name:   Apple M4
0.00.056.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.073 I ggml_metal_init: simdgroup reduction   = true
0.00.056.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.073 I ggml_metal_init: has bfloat            = true
0.00.056.073 I ggml_metal_init: use bfloat            = true
0.00.056.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.798 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.025 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.028 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.054 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.979 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.980 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.980 I llama_new_context_with_model: graph nodes  = 967
0.00.067.981 I llama_new_context_with_model: graph splits = 2
0.00.067.982 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.384.806 I 
0.00.384.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.384.855 I perplexity: tokenizing the input ..
0.00.391.927 I perplexity: tokenization took 7.07 ms
0.00.391.930 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.524.169 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.525.358 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.525.383 I llama_perf_context_print:        load time =     374.58 ms
0.00.525.384 I llama_perf_context_print: prompt eval time =     131.98 ms /   128 tokens (    1.03 ms per token,   969.83 tokens per second)
0.00.525.385 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.525.385 I llama_perf_context_print:       total time =     140.58 ms /   129 tokens
0.00.525.926 I ggml_metal_free: deallocating

real	0m0.541s
user	0m0.078s
sys	0m0.065s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.102 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.245 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.997 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.703 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.704 I llama_model_loader: - type  f32:  194 tensors
0.00.024.704 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.705 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.705 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.705 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.706 I print_info: file format = GGUF V3 (latest)
0.00.024.706 I print_info: file type   = Q3_K - Medium
0.00.024.707 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.412 I load: special tokens cache size = 25
0.00.049.453 I load: token to piece cache size = 0.2984 MB
0.00.049.467 I print_info: arch             = gptneox
0.00.049.468 I print_info: n_vocab (hp)     = 50304
0.00.049.468 I print_info: vocab_only       = 0
0.00.049.468 I print_info: n_ctx_train      = 2048
0.00.049.469 I print_info: n_embd           = 2048
0.00.049.469 I print_info: n_layer          = 24
0.00.049.472 I print_info: n_head           = 16
0.00.049.473 I print_info: n_head_kv        = 16
0.00.049.473 I print_info: n_rot            = 32
0.00.049.473 I print_info: n_swa            = 0
0.00.049.473 I print_info: n_embd_head_k    = 128
0.00.049.473 I print_info: n_embd_head_v    = 128
0.00.049.474 I print_info: n_gqa            = 1
0.00.049.475 I print_info: n_embd_k_gqa     = 2048
0.00.049.476 I print_info: n_embd_v_gqa     = 2048
0.00.049.476 I print_info: f_norm_eps       = 1.0e-05
0.00.049.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.477 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.477 I print_info: f_logit_scale    = 0.0e+00
0.00.049.478 I print_info: n_ff             = 8192
0.00.049.478 I print_info: n_expert         = 0
0.00.049.478 I print_info: n_expert_used    = 0
0.00.049.478 I print_info: causal attn      = 1
0.00.049.478 I print_info: pooling type     = 0
0.00.049.478 I print_info: rope type        = 2
0.00.049.479 I print_info: rope scaling     = linear
0.00.049.479 I print_info: freq_base_train  = 10000.0
0.00.049.479 I print_info: freq_scale_train = 1
0.00.049.479 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.479 I print_info: rope_finetuned   = unknown
0.00.049.480 I print_info: ssm_d_conv       = 0
0.00.049.480 I print_info: ssm_d_inner      = 0
0.00.049.480 I print_info: ssm_d_state      = 0
0.00.049.480 I print_info: ssm_dt_rank      = 0
0.00.049.480 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.480 I print_info: model type       = 1.4B
0.00.049.480 I print_info: model params     = 1.41 B
0.00.049.481 I print_info: general.name     = 1.4B
0.00.049.481 I print_info: vocab type       = BPE
0.00.049.481 I print_info: n_vocab          = 50304
0.00.049.482 I print_info: n_merges         = 50009
0.00.049.482 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.482 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.482 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.483 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.483 I print_info: LF token         = 128 'Ä'
0.00.049.483 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.483 I print_info: max token length = 1024
0.00.051.068 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.069 I load_tensors: offloading output layer to GPU
0.00.051.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.079 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.080 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.353 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.354 I llama_new_context_with_model: n_ctx         = 128
0.00.051.354 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.354 I llama_new_context_with_model: n_batch       = 128
0.00.051.354 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.355 I llama_new_context_with_model: flash_attn    = 0
0.00.051.355 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.355 I llama_new_context_with_model: freq_scale    = 1
0.00.051.356 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.356 I ggml_metal_init: allocating
0.00.051.359 I ggml_metal_init: found device: Apple M4
0.00.051.361 I ggml_metal_init: picking default device: Apple M4
0.00.051.915 I ggml_metal_init: using embedded metal library
0.00.054.230 I ggml_metal_init: GPU name:   Apple M4
0.00.054.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.233 I ggml_metal_init: simdgroup reduction   = true
0.00.054.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.233 I ggml_metal_init: has bfloat            = true
0.00.054.233 I ggml_metal_init: use bfloat            = true
0.00.054.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.814 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.064 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.066 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.991 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.992 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.993 I llama_new_context_with_model: graph nodes  = 967
0.00.065.993 I llama_new_context_with_model: graph splits = 2
0.00.065.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.109 I 
0.00.477.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.170 I perplexity: tokenizing the input ..
0.00.484.462 I perplexity: tokenization took 7.291 ms
0.00.484.466 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.515 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.617.695 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.617.715 I llama_perf_context_print:        load time =     468.00 ms
0.00.617.716 I llama_perf_context_print: prompt eval time =     131.79 ms /   128 tokens (    1.03 ms per token,   971.22 tokens per second)
0.00.617.717 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.717 I llama_perf_context_print:       total time =     140.61 ms /   129 tokens
0.00.618.264 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.077s
sys	0m0.083s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.969 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.988 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.989 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.993 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.994 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.728 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.499 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.501 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.502 I llama_model_loader: - type  f32:  194 tensors
0.00.025.502 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.502 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.502 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.503 I print_info: file format = GGUF V3 (latest)
0.00.025.503 I print_info: file type   = Q4_K - Medium
0.00.025.505 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.157 I load: special tokens cache size = 25
0.00.050.071 I load: token to piece cache size = 0.2984 MB
0.00.050.085 I print_info: arch             = gptneox
0.00.050.086 I print_info: n_vocab (hp)     = 50304
0.00.050.086 I print_info: vocab_only       = 0
0.00.050.086 I print_info: n_ctx_train      = 2048
0.00.050.087 I print_info: n_embd           = 2048
0.00.050.087 I print_info: n_layer          = 24
0.00.050.090 I print_info: n_head           = 16
0.00.050.091 I print_info: n_head_kv        = 16
0.00.050.091 I print_info: n_rot            = 32
0.00.050.091 I print_info: n_swa            = 0
0.00.050.092 I print_info: n_embd_head_k    = 128
0.00.050.095 I print_info: n_embd_head_v    = 128
0.00.050.096 I print_info: n_gqa            = 1
0.00.050.096 I print_info: n_embd_k_gqa     = 2048
0.00.050.098 I print_info: n_embd_v_gqa     = 2048
0.00.050.098 I print_info: f_norm_eps       = 1.0e-05
0.00.050.099 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.099 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.099 I print_info: f_logit_scale    = 0.0e+00
0.00.050.100 I print_info: n_ff             = 8192
0.00.050.100 I print_info: n_expert         = 0
0.00.050.100 I print_info: n_expert_used    = 0
0.00.050.100 I print_info: causal attn      = 1
0.00.050.100 I print_info: pooling type     = 0
0.00.050.102 I print_info: rope type        = 2
0.00.050.102 I print_info: rope scaling     = linear
0.00.050.102 I print_info: freq_base_train  = 10000.0
0.00.050.102 I print_info: freq_scale_train = 1
0.00.050.102 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.103 I print_info: rope_finetuned   = unknown
0.00.050.103 I print_info: ssm_d_conv       = 0
0.00.050.103 I print_info: ssm_d_inner      = 0
0.00.050.103 I print_info: ssm_d_state      = 0
0.00.050.103 I print_info: ssm_dt_rank      = 0
0.00.050.103 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.103 I print_info: model type       = 1.4B
0.00.050.104 I print_info: model params     = 1.41 B
0.00.050.104 I print_info: general.name     = 1.4B
0.00.050.104 I print_info: vocab type       = BPE
0.00.050.104 I print_info: n_vocab          = 50304
0.00.050.105 I print_info: n_merges         = 50009
0.00.050.105 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.105 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.105 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.105 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.105 I print_info: LF token         = 128 'Ä'
0.00.050.106 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.106 I print_info: max token length = 1024
0.00.051.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.722 I load_tensors: offloading output layer to GPU
0.00.051.723 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.733 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.734 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.012 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.012 I llama_new_context_with_model: n_ctx         = 128
0.00.052.012 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.013 I llama_new_context_with_model: n_batch       = 128
0.00.052.013 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.013 I llama_new_context_with_model: flash_attn    = 0
0.00.052.013 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.014 I llama_new_context_with_model: freq_scale    = 1
0.00.052.014 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.014 I ggml_metal_init: allocating
0.00.052.017 I ggml_metal_init: found device: Apple M4
0.00.052.019 I ggml_metal_init: picking default device: Apple M4
0.00.052.581 I ggml_metal_init: using embedded metal library
0.00.054.899 I ggml_metal_init: GPU name:   Apple M4
0.00.054.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.901 I ggml_metal_init: simdgroup reduction   = true
0.00.054.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.901 I ggml_metal_init: has bfloat            = true
0.00.054.901 I ggml_metal_init: use bfloat            = true
0.00.054.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.601 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.861 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.865 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.891 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.788 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.789 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.789 I llama_new_context_with_model: graph nodes  = 967
0.00.066.789 I llama_new_context_with_model: graph splits = 2
0.00.066.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.791 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.953 I 
0.00.563.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.992 I perplexity: tokenizing the input ..
0.00.571.908 I perplexity: tokenization took 7.913 ms
0.00.571.912 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.069 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.707.287 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.707.310 I llama_perf_context_print:        load time =     553.98 ms
0.00.707.311 I llama_perf_context_print: prompt eval time =     133.90 ms /   128 tokens (    1.05 ms per token,   955.95 tokens per second)
0.00.707.312 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.313 I llama_perf_context_print:       total time =     143.36 ms /   129 tokens
0.00.707.745 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.077s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.920 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.482 I llama_model_loader: - type  f32:  194 tensors
0.00.024.482 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.483 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.483 I print_info: file format = GGUF V3 (latest)
0.00.024.484 I print_info: file type   = Q5_K - Medium
0.00.024.485 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.242 I load: special tokens cache size = 25
0.00.049.278 I load: token to piece cache size = 0.2984 MB
0.00.049.294 I print_info: arch             = gptneox
0.00.049.295 I print_info: n_vocab (hp)     = 50304
0.00.049.295 I print_info: vocab_only       = 0
0.00.049.295 I print_info: n_ctx_train      = 2048
0.00.049.295 I print_info: n_embd           = 2048
0.00.049.296 I print_info: n_layer          = 24
0.00.049.298 I print_info: n_head           = 16
0.00.049.299 I print_info: n_head_kv        = 16
0.00.049.299 I print_info: n_rot            = 32
0.00.049.299 I print_info: n_swa            = 0
0.00.049.299 I print_info: n_embd_head_k    = 128
0.00.049.301 I print_info: n_embd_head_v    = 128
0.00.049.302 I print_info: n_gqa            = 1
0.00.049.307 I print_info: n_embd_k_gqa     = 2048
0.00.049.308 I print_info: n_embd_v_gqa     = 2048
0.00.049.308 I print_info: f_norm_eps       = 1.0e-05
0.00.049.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.309 I print_info: f_logit_scale    = 0.0e+00
0.00.049.310 I print_info: n_ff             = 8192
0.00.049.310 I print_info: n_expert         = 0
0.00.049.311 I print_info: n_expert_used    = 0
0.00.049.311 I print_info: causal attn      = 1
0.00.049.311 I print_info: pooling type     = 0
0.00.049.311 I print_info: rope type        = 2
0.00.049.311 I print_info: rope scaling     = linear
0.00.049.312 I print_info: freq_base_train  = 10000.0
0.00.049.312 I print_info: freq_scale_train = 1
0.00.049.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.312 I print_info: rope_finetuned   = unknown
0.00.049.312 I print_info: ssm_d_conv       = 0
0.00.049.313 I print_info: ssm_d_inner      = 0
0.00.049.314 I print_info: ssm_d_state      = 0
0.00.049.314 I print_info: ssm_dt_rank      = 0
0.00.049.314 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.314 I print_info: model type       = 1.4B
0.00.049.315 I print_info: model params     = 1.41 B
0.00.049.315 I print_info: general.name     = 1.4B
0.00.049.315 I print_info: vocab type       = BPE
0.00.049.315 I print_info: n_vocab          = 50304
0.00.049.316 I print_info: n_merges         = 50009
0.00.049.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.316 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.317 I print_info: LF token         = 128 'Ä'
0.00.049.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.317 I print_info: max token length = 1024
0.00.051.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.127 I load_tensors: offloading output layer to GPU
0.00.051.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.137 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.139 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.429 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.430 I llama_new_context_with_model: n_ctx         = 128
0.00.051.430 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.430 I llama_new_context_with_model: n_batch       = 128
0.00.051.430 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.430 I llama_new_context_with_model: flash_attn    = 0
0.00.051.431 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.431 I llama_new_context_with_model: freq_scale    = 1
0.00.051.431 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.432 I ggml_metal_init: allocating
0.00.051.435 I ggml_metal_init: found device: Apple M4
0.00.051.437 I ggml_metal_init: picking default device: Apple M4
0.00.051.999 I ggml_metal_init: using embedded metal library
0.00.054.324 I ggml_metal_init: GPU name:   Apple M4
0.00.054.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.326 I ggml_metal_init: simdgroup reduction   = true
0.00.054.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.326 I ggml_metal_init: has bfloat            = true
0.00.054.327 I ggml_metal_init: use bfloat            = true
0.00.054.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.411 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.413 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.446 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.381 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.382 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.382 I llama_new_context_with_model: graph nodes  = 967
0.00.066.382 I llama_new_context_with_model: graph splits = 2
0.00.066.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.345 I 
0.00.647.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.415 I perplexity: tokenizing the input ..
0.00.655.333 I perplexity: tokenization took 7.916 ms
0.00.655.336 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.621 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.804 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.848 I llama_perf_context_print:        load time =     638.42 ms
0.00.796.849 I llama_perf_context_print: prompt eval time =     140.06 ms /   128 tokens (    1.09 ms per token,   913.90 tokens per second)
0.00.796.849 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.850 I llama_perf_context_print:       total time =     149.51 ms /   129 tokens
0.00.797.320 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.836 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.730 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.737 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.739 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.339 I llama_model_loader: - type  f32:  194 tensors
0.00.025.339 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.339 I print_info: file format = GGUF V3 (latest)
0.00.025.340 I print_info: file type   = Q6_K
0.00.025.341 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.746 I load: special tokens cache size = 25
0.00.050.690 I load: token to piece cache size = 0.2984 MB
0.00.050.705 I print_info: arch             = gptneox
0.00.050.706 I print_info: n_vocab (hp)     = 50304
0.00.050.707 I print_info: vocab_only       = 0
0.00.050.707 I print_info: n_ctx_train      = 2048
0.00.050.707 I print_info: n_embd           = 2048
0.00.050.707 I print_info: n_layer          = 24
0.00.050.710 I print_info: n_head           = 16
0.00.050.711 I print_info: n_head_kv        = 16
0.00.050.711 I print_info: n_rot            = 32
0.00.050.711 I print_info: n_swa            = 0
0.00.050.716 I print_info: n_embd_head_k    = 128
0.00.050.717 I print_info: n_embd_head_v    = 128
0.00.050.717 I print_info: n_gqa            = 1
0.00.050.718 I print_info: n_embd_k_gqa     = 2048
0.00.050.719 I print_info: n_embd_v_gqa     = 2048
0.00.050.720 I print_info: f_norm_eps       = 1.0e-05
0.00.050.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.720 I print_info: f_logit_scale    = 0.0e+00
0.00.050.721 I print_info: n_ff             = 8192
0.00.050.721 I print_info: n_expert         = 0
0.00.050.721 I print_info: n_expert_used    = 0
0.00.050.721 I print_info: causal attn      = 1
0.00.050.722 I print_info: pooling type     = 0
0.00.050.722 I print_info: rope type        = 2
0.00.050.722 I print_info: rope scaling     = linear
0.00.050.722 I print_info: freq_base_train  = 10000.0
0.00.050.723 I print_info: freq_scale_train = 1
0.00.050.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.723 I print_info: rope_finetuned   = unknown
0.00.050.723 I print_info: ssm_d_conv       = 0
0.00.050.723 I print_info: ssm_d_inner      = 0
0.00.050.724 I print_info: ssm_d_state      = 0
0.00.050.724 I print_info: ssm_dt_rank      = 0
0.00.050.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.726 I print_info: model type       = 1.4B
0.00.050.726 I print_info: model params     = 1.41 B
0.00.050.726 I print_info: general.name     = 1.4B
0.00.050.726 I print_info: vocab type       = BPE
0.00.050.727 I print_info: n_vocab          = 50304
0.00.050.727 I print_info: n_merges         = 50009
0.00.050.727 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.728 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.728 I print_info: LF token         = 128 'Ä'
0.00.050.731 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.731 I print_info: max token length = 1024
0.00.052.773 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.773 I load_tensors: offloading output layer to GPU
0.00.052.773 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.784 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.785 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.057 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.058 I llama_new_context_with_model: n_ctx         = 128
0.00.053.058 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.058 I llama_new_context_with_model: n_batch       = 128
0.00.053.059 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.059 I llama_new_context_with_model: flash_attn    = 0
0.00.053.059 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.059 I llama_new_context_with_model: freq_scale    = 1
0.00.053.060 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.060 I ggml_metal_init: allocating
0.00.053.063 I ggml_metal_init: found device: Apple M4
0.00.053.065 I ggml_metal_init: picking default device: Apple M4
0.00.053.629 I ggml_metal_init: using embedded metal library
0.00.055.975 I ggml_metal_init: GPU name:   Apple M4
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.978 I ggml_metal_init: simdgroup reduction   = true
0.00.055.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.978 I ggml_metal_init: has bfloat            = true
0.00.055.978 I ggml_metal_init: use bfloat            = true
0.00.055.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.788 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.044 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.079 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.035 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.036 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.036 I llama_new_context_with_model: graph nodes  = 967
0.00.068.036 I llama_new_context_with_model: graph splits = 2
0.00.068.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.154 I 
0.00.371.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.197 I perplexity: tokenizing the input ..
0.00.378.861 I perplexity: tokenization took 7.661 ms
0.00.378.869 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.518.388 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.519.558 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.519.577 I llama_perf_context_print:        load time =     361.31 ms
0.00.519.578 I llama_perf_context_print: prompt eval time =     139.29 ms /   128 tokens (    1.09 ms per token,   918.93 tokens per second)
0.00.519.579 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.519.580 I llama_perf_context_print:       total time =     148.43 ms /   129 tokens
0.00.519.966 I ggml_metal_free: deallocating

real	0m0.535s
user	0m0.078s
sys	0m0.079s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.291 I build: 4473 (22b31cd1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.153 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.565 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.594 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.207 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.209 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.209 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.210 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.210 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.211 I llama_model_loader: - type  f32:  194 tensors
0.00.060.211 I llama_model_loader: - type  f16:   98 tensors
0.00.060.212 I print_info: file format = GGUF V3 (latest)
0.00.060.213 I print_info: file type   = all F32 (guessed)
0.00.060.214 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.997 I load: special tokens cache size = 25
0.00.092.442 I load: token to piece cache size = 0.2984 MB
0.00.092.457 I print_info: arch             = gptneox
0.00.092.458 I print_info: n_vocab (hp)     = 50304
0.00.092.459 I print_info: vocab_only       = 0
0.00.092.459 I print_info: n_ctx_train      = 2048
0.00.092.459 I print_info: n_embd           = 2048
0.00.092.459 I print_info: n_layer          = 24
0.00.092.461 I print_info: n_head           = 16
0.00.092.462 I print_info: n_head_kv        = 16
0.00.092.462 I print_info: n_rot            = 32
0.00.092.462 I print_info: n_swa            = 0
0.00.092.462 I print_info: n_embd_head_k    = 128
0.00.092.463 I print_info: n_embd_head_v    = 128
0.00.092.463 I print_info: n_gqa            = 1
0.00.092.464 I print_info: n_embd_k_gqa     = 2048
0.00.092.465 I print_info: n_embd_v_gqa     = 2048
0.00.092.465 I print_info: f_norm_eps       = 1.0e-05
0.00.092.467 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.467 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.467 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.467 I print_info: f_logit_scale    = 0.0e+00
0.00.092.468 I print_info: n_ff             = 8192
0.00.092.468 I print_info: n_expert         = 0
0.00.092.468 I print_info: n_expert_used    = 0
0.00.092.469 I print_info: causal attn      = 1
0.00.092.469 I print_info: pooling type     = 0
0.00.092.469 I print_info: rope type        = 2
0.00.092.469 I print_info: rope scaling     = linear
0.00.092.471 I print_info: freq_base_train  = 10000.0
0.00.092.471 I print_info: freq_scale_train = 1
0.00.092.471 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.472 I print_info: rope_finetuned   = unknown
0.00.092.477 I print_info: ssm_d_conv       = 0
0.00.092.478 I print_info: ssm_d_inner      = 0
0.00.092.480 I print_info: ssm_d_state      = 0
0.00.092.480 I print_info: ssm_dt_rank      = 0
0.00.092.480 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.480 I print_info: model type       = 1.4B
0.00.092.481 I print_info: model params     = 1.41 B
0.00.092.481 I print_info: general.name     = 1.4B
0.00.092.481 I print_info: vocab type       = BPE
0.00.092.482 I print_info: n_vocab          = 50304
0.00.092.482 I print_info: n_merges         = 50009
0.00.092.482 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.482 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.482 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.482 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.483 I print_info: LF token         = 128 'Ä'
0.00.092.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.488 I print_info: max token length = 1024
0.00.095.040 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.041 I load_tensors: offloading output layer to GPU
0.00.095.041 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.051 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.052 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.095.420 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.421 I llama_new_context_with_model: n_ctx         = 128
0.00.095.421 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.421 I llama_new_context_with_model: n_batch       = 128
0.00.095.422 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.422 I llama_new_context_with_model: flash_attn    = 0
0.00.095.422 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.423 I llama_new_context_with_model: freq_scale    = 1
0.00.095.423 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.423 I ggml_metal_init: allocating
0.00.095.426 I ggml_metal_init: found device: Apple M4
0.00.095.428 I ggml_metal_init: picking default device: Apple M4
0.00.096.008 I ggml_metal_init: using embedded metal library
0.00.098.502 I ggml_metal_init: GPU name:   Apple M4
0.00.098.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.504 I ggml_metal_init: simdgroup reduction   = true
0.00.098.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.504 I ggml_metal_init: has bfloat            = true
0.00.098.505 I ggml_metal_init: use bfloat            = true
0.00.098.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.443 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.831 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.835 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.863 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.840 I llama_new_context_with_model: graph nodes  = 967
0.00.109.840 I llama_new_context_with_model: graph splits = 2
0.00.109.841 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.841 I 
0.00.109.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.109.871 I compute_imatrix: tokenizing the input ..
0.00.116.573 I compute_imatrix: tokenization took 6.701 ms
0.00.116.574 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.795.303 I compute_imatrix: 1.68 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.798.204 I llama_perf_context_print:        load time =    1770.15 ms
0.01.798.205 I llama_perf_context_print: prompt eval time =    1678.04 ms /   128 tokens (   13.11 ms per token,    76.28 tokens per second)
0.01.798.205 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.798.206 I llama_perf_context_print:       total time =    1773.04 ms /   129 tokens
0.01.798.787 I ggml_metal_free: deallocating

real	0m1.987s
user	0m0.175s
sys	0m0.241s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4473 (22b31cd1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: n_vocab (hp)     = 50304
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12620a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12620a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12620aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12620b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12620ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12620bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12620c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12620cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12620d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12620d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12620dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12620dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12620eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12620f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12620fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1262101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1262108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126211010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126211730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126211f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126212620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126212d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126213460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126213d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126214420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1262146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126214cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126215960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126215ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126216160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126216600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1262168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126217150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126217690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126217950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126217df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126218290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126218730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126218bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126219070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126219510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1262199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126219e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12621a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12621a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12621abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12621b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12621baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12621c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12621c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12621cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12621d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12621d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12621df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12621e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12621ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12621f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12621f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12621f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126220140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126220400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1262208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126220d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1262211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126221680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126221b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126221fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126222460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126222900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126222da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126223240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1262236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126223b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1262240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126224620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126224b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1262250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126225610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126225b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1262260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126226600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126226b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1262270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1262275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126227b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126228090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1262285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126228b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126229080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1262295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126229b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12622a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12622a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12622ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12622b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12622b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12622bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12621b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12622bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12622c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12622cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12622d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12622d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12622dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12622e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12622e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12622ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12622f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12622f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12622fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126230190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1262306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126230c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1262310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126231570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126231a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126231eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126232350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1262327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126232c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126233130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1262335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126233a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126233f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1262343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126234850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126234cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126235190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126235630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126235ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126235f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126236410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1262368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126236d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1262371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126237690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126237b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126237fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126238470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126238910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126238db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126239250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1262396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126239b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12623a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12623a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12623a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12623ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12623b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12623b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12623bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12623c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12623c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12623c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12623ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12623d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12623d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12623dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12623e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12623e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12623ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12623eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12623f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12623f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12623fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126240150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1262405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126240a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126240f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1262413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126241870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126241d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1262421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126242650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126242af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126242f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126243430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1262438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126243d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126244210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1262446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126244b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126244ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126245490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126245930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126245dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126246270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126246710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126246bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126247050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1262474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126247990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126247e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126248380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1262488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126248e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126249370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126249630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126249c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12624a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12624a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12624b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12624b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12624b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12624bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12624c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12624cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12624d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12624d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12624d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12624e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12624e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12624ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12624f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12624f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12624fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126250130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126250680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126250bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126251120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126251670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126251bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126252110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126252660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126252bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126253100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126253650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126253ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1262540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126254640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126254b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1262550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126255630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126255b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1262560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126256620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126256b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1262570c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126257610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126257b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1262580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126258600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126258b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1262590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1262595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126259b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12625a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12625a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12625ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12625b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12625b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12625bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12625c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12625c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12625cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12625d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12625d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12625db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12625e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12625e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12625eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12625f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12625f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12625fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126260030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126260580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126260ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126260f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126261410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1262618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126261d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1262621f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126262690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126262b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126262fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126263470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126263910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126263db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126264250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1262646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126264b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126265030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126265580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126265ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1262663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126266ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126267200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1262674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126267cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126267f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126268580 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.145.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1168058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1168065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1168077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116807ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1168089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116809170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116809980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11680a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11680a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11680aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11680b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11680bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11680c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11680cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11680d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11680d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11680e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11680e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11680e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11680eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11680ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11680f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11680f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11680fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1168101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116810470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1168108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116810d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1168111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116811630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116811aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116811f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116812380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1168127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116812c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1168130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116813540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1168139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116813e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116814290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116814700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116814b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116814fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116815450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1168158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116815d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1168161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116816610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116816b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116817080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1168174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116817960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116818240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1168186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116818b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116818f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116819400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116819870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116819ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126249f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12624ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126268230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1262498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12624a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12621d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12621cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12621f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12624c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1262149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12621b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12621bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12621c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12621ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12621a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12621dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12621c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1262139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12620e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12621fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12622c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126267780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126216b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126216e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12624c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12624ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126214fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126215270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126215530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1262689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126268ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126268f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126269220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1262694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1262697a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126269a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126269d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126269fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12626a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12626a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12626a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12626aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12626ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12626b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12626b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12626b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12626b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12626bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12626be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12626c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12626c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12626c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12626c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12626cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12626cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12626d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12626d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12626d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12626d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12626dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12626df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12626e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12626e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12626e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12626ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12626ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12626efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12626f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12626f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12626f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12626faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12626fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126270020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1262702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1262705a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126270860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126270b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126270de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1262710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126271360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126271620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1262718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126271ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126271e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126272120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1262723e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1262726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126272960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126272c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126272ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1262731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126273460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126273720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1262739e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126273ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126273f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126274220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1262744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1262747a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126274a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126274d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126274fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1262752a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126275560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126275820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126275ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126275da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126276060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126276320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1262765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1262768a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126276b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126276e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1262770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1262773a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126277660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126277920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126277be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126277ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126278160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126278420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1262786e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1262789a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126278c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126278f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1262791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1262794a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126279760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126279a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126279ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126279fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12627a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12627a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12627a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12627aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12627ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12627b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12627b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12627b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12627b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12627bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12627bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12627c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12627c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12627c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12627cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12627ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12627d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12627d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12627d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12627d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12627dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12627df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12627e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12627e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12627e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12627ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12627ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12627efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12627f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12627f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12627f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12627fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12627fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126280030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1262802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1262805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126280870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126280b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126280df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1262810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126281370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126281630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1262818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126281bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126281e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126282130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1262823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1262826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126282970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126282c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126282ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1262831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126283470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126283730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1262839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126283cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126283f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126284230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1262844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1262847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126284a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126284d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126284ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1262852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126285570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126285830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126285af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126285db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126286070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126286330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1262865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1262868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126286b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126286e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1262870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1262873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126287670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126287930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126287bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126287eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126288170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126288430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1262886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1262889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126288c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126288f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1262891f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1262894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126289770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126289a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126289ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12628a310 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116808160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116804680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11680b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116819fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11681a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11681a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11681ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11681aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11681b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11681b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11681b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11681b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11681bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11681c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11681cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11681ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11681d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11681d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11681de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11681e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11681eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11681f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11681f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11681fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116820040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116820580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116820840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116820b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116820dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116821340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116821600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1168218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116821b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116821e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116822100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1168223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116822680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116822940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116822c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116822ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116823180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116823440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116823700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1168239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116823c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116823f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1168244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116824780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116824a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116824d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116824fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116825280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116825540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116825800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116825ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116825d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116826300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1168265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116826880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116826b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116826e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1168274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116827780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116827bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116828060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1168284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12628a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12628a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12628adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12628b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12628b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12628b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12628b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12628bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12628be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12628c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12628c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12628c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12628c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12628cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12628cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12628d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12628d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12628d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12628d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12628dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12628df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12628e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12628e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12628e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12628ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12628ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12628efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12628f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12628f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12628f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12628fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12628fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126290040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126290300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1262905c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126290880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126290b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126290e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1262910c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126291380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126291640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126291900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126291bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126291e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126292140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126292400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1262926c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126292980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126292c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126292f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1262931c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126293480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126293740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126293a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126293cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126293f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126294240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126294500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1262947c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126294a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126294d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126295000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1262952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126295580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126295840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126295b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126295dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126296080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126296340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126296600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1262968c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126296b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126296e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126297100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1262973c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126297680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126297940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126297c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126297ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126298180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126298440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126298700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1262989c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126298c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126298f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126299200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1262994c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126299780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126299a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126299d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126299fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12629a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12629a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12629a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12629aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12629ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12629b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12629b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12629b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12629b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12629bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12629be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12629c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12629c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12629c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12629c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12629cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12629ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12629d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12629d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12629d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12629d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12629dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12629df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12629e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12629e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12629e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12629ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12629ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12629ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12629f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12629f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12629f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12629fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12629fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1262a0000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1262a02c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1262a0580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1262a0840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1262a0b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1262a0dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1262a1080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1262a1340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1262a1600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1262a18c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1262a1b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1262a1e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1262a2100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1262a23c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1262a2680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1262a2c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1262a2f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1262a31d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1262a3490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1262a3750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1262a3a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1262a3cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1262a3f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1262a4250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1262a4510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1262a47d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1262a4a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1262a4d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1262a5010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1262a52d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1262a5590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1262a5850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1262a5b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1262a5dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1262a6090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1262a6350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1262a6610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1262a68d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1262a6b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1262a6e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1262a7110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1262a73d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1262a7690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1262a7950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1262a7c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1262a7ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1262a8190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1262a8450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1262a8710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1262a89d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1262a8c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1262a8f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1262a9210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1262a94d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1262a9790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1262a9a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1262a9d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1262a9fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1262aa290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1262aa550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1262aa810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1262aaad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1262aad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1262ab050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1262ab310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1262ab5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1262ab890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1262abb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1262abe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1262ac0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1262ac390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1262ac650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1262ac910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1262acbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1262ace90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1262ad150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1262ad410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1262ad6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1262ad990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1262adc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1262adf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1262ae1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1262ae490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1262ae750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1262aea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1262aecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1262aef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1262af250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1262af510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1262af9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1262b00d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1262b07f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1262b0ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1262b0d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1262b11e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1262b1650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.855s
user	0m0.295s
sys	0m0.314s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4473 (22b31cd1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: n_vocab (hp)     = 50304
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128e0b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128e0bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128e0c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128e0c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128e0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128e0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128e0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128e0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128e0e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128e0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128e0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128e0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128e0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128e105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128e10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128e11500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128e11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128e12340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128e12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128e13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128e13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128e14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128e14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128e15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128e15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128e15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128e16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128e16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128e171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128e17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128e17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128e17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128e18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128e189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128e18c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128e19120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128e195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128e19a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128e19f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128e1a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128e1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128e1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128e1b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128e1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128e1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128e1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128e1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128e1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128e1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128e1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128e1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128e1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128e1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128e1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128e1fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128e1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128e20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128e20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128e21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128e21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128e21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128e22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128e22510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128e229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128e22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128e232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128e23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128e23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128e240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128e24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128e24a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128e24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128e25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128e25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128e25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128e263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128e26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128e26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128e273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128e27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128e27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128e283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128e28920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128e28e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128e293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128e29910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128e29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128e2a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128e2a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128e2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128e2b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128e2b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128e2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128e2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128e2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128e2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128e1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128e2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128e2dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128e2e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128e2ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128e2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128e2f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128e2fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128e2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128e304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128e30a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128e30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128e314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128e31a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128e31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128e32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128e328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128e32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128e331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128e33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128e33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128e33fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128e34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128e34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128e34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128e35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128e35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128e36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128e364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128e36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128e36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128e372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128e37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128e37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128e38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128e38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128e38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128e39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128e397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128e39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128e3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128e3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128e3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128e3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128e3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128e3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128e3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128e3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128e3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128e3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128e3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128e3dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128e3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128e3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128e3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128e3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128e3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128e3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128e406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128e40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128e41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128e42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128e42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128e42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128e43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128e434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128e43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128e43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128e442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128e44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128e450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128e45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128e459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128e45e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128e46320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128e467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128e46c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128e47100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128e475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128e47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128e47ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128e48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128e48820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128e48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128e49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128e496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128e49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128e4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128e4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128e4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128e4af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128e4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128e4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128e4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128e4c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128e4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128e4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128e4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128e4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128e4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128e4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128e4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128e4f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128e4f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128e4ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128e50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128e509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128e50f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128e51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128e519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128e51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128e52450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128e529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128e52ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128e53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128e53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128e53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128e54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128e54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128e54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128e55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128e55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128e55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128e56410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128e56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128e56eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128e57400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128e57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128e57ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128e583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128e58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128e58e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128e593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128e59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128e59e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128e5a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128e5a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128e5ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128e5b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128e5b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128e5be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128e5c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128e5c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128e5ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128e5d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128e5d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128e5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128e5e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128e5e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128e5ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128e5f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128e5f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128e5fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128e60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128e608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128e60e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128e61360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128e618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128e61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128e622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128e62740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128e62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128e63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128e63520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128e639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128e63e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128e64300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128e647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128e64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128e650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128e65580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128e65a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128e65ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128e66360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128e668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128e66fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128e676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128e67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128e68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128e687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128e68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128e692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128e698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.629 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a0055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a0074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a007940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a008020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a008b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a009b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a00a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a00a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a00b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a00b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a00bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a00c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a00cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a00d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a00dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a00e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a00e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a00e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a00ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a00f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a00f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a00fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a00ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a0103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a010690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a0113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a011cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a0125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a012a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a012e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a0132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a013760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a0144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a015200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a015f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a0163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a016da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a0172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a017710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a018460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a0188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a018d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a0191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a019620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a019a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a01a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a01a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a01ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a01b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a01b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a01b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a01be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a01c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a01c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a01cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a01cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a01d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a01d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a01dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a01e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a01e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a01ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a01eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a01f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a01f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a01fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a0200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a020510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a020df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a0216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a021b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a021fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a022890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a022d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a0235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a023ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a024330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a0247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a024c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a0254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a025960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a026240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a0266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a026f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a028150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a0285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a028a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a029310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a029780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a029bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a02a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a02a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a02a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a02adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a02b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a02b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a02bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a02bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a02c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a02c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a02ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a02d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a02d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a02da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a02de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a02e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a02e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a02ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a02f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a02f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a02f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a02fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a030200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a030ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a030f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a0313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a031830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a032580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a0329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a0332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a033740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a034490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a034900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a0351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a035e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a0360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a036390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a036800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a036c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a0370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a037550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a0379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a037e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a0382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a038710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a038b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a038ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a039460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a0398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a039d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a03a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a03a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a03aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a03af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a03b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a03b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a03bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a03c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a03c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a03c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a03ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a03d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a03d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a03db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a03dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a03e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a03e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a03ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a03f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a03f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a03fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a040070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a0404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a040950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a041750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a041c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a0427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a042a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a043050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a043610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a043bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a044750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a044d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a0452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a045890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a045e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a046410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a0469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a046f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a047550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a047b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a0480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a048c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a049210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a0497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a049d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a04a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a04a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a04aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a04b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a04ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a04c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a04c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a04cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a04d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a04d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a04dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a04e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a04e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a04ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a04f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a04f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a04ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a050510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a050ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a051090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a051c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a0521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a052790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a052d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a053310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a0538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a053e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a054450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a054a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a054fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a055590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a056110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a0566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a056c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a057190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a057690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a057b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a058590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a058a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a058f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a059490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a059990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a059e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a05a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a05a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a05ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a05b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a05b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a05c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a05c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a05cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a05d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a05d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a05e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a05e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a05ea80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128f0dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128f0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128f0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128f0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128f0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128f0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128f0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128f0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128f0fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128f10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128f10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128f10c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128f117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128f12770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128f12e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128f135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128f13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128f143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128f14b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128f15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128f15960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128f16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128f167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128f16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128f17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128f17440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128f178b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128f17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128f18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128f18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128f18b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128f18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128f19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128f196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128f19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128f19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128f1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128f1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128f1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128f1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128f1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128f1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128f1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128f1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128f1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128f1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128f1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128f1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128f1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128f1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128f1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128f1e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128f1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128f1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128f1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128f1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128f1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128f202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128f20750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128f20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128f21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128f214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128f21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128f21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128f221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128f22660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128f22ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128f22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128f233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128f23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128f23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128f24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128f24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128f249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128f24e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128f252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128f25730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128f25ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128f26010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128f26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128f268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128f26d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128f271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128f27640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128f27ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128f27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128f28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128f28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128f290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128f29550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128f299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128f29e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128f2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128f2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128f2ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128f2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128f2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128f2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128f2bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128f2c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128f2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128f2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128f2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128f2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128f2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128f2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128f2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128f2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128f2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128f2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128f2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128f2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128f2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128f30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128f306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128f30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128f30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128f31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128f31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128f31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128f32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128f325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128f32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128f32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128f33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128f33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128f33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128f34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128f344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128f34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128f34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128f35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128f35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128f35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128f35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128f36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128f36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128f37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128f375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128f37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128f37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128f382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128f38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128f38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128f39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128f394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128f39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128f39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128f3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128f3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128f3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128f3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128f3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128f3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128f3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128f3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128f3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128f3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128f3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128f3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128f3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128f3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128f3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128f3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128f3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128f3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128f3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128f3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128f3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128f403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128f40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128f40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128f410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128f41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128f419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128f41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128f422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128f42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128f42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128f43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128f43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128f438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128f43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128f441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128f44630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128f44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128f44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128f45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128f457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128f45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128f460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128f46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128f469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128f46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128f47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128f47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128f47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128f47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128f48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128f488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128f48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128f491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128f49610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128f49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128f49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128f4a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128f4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128f4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128f4b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128f4b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128f4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128f4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128f4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128f4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128f4d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128f4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128f4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128f4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128f4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128f4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128f4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128f4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128f4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128f4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128f50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128f508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128f50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128f51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128f51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128f51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128f51ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128f52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128f527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128f52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128f530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128f53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128f53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128f53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128f54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128f546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128f54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128f54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128f55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128f55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128f55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128f56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128f565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128f56a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128f56ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128f57330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128f577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128f57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128f58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128f584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128f58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128f58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128f596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128f59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128f59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128f5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128f5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128f5ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128f5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128f5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128f5ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128f5bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128f5c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128f5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128f5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128f5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128f5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128f5d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128f5ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128f5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128f5eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128f5f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128f5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128f603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128f60ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128f60d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128f61200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128f61800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128f61e10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.920s
user	0m0.242s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
