Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.596s
user	0m0.894s
sys	0m1.261s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Built target build_info
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-barrier
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-arg-parser
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-autorelease
[ 62%] Built target test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-batched
[ 73%] Built target llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-bench
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-parallel
[ 80%] Built target llama-passkey
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-perplexity
[ 80%] Built target llama-cli
[ 80%] Built target llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 83%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tts
[ 91%] Built target llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.237s
user	0m6.607s
sys	0m11.016s

main: quantize time =  4324.88 ms
main:    total time =  4324.88 ms

main: quantize time =  3970.87 ms
main:    total time =  3970.87 ms

main: quantize time =  3490.27 ms
main:    total time =  3490.27 ms

main: quantize time =  3551.31 ms
main:    total time =  3551.31 ms

main: quantize time =  2090.08 ms
main:    total time =  2090.08 ms

main: quantize time =  5595.51 ms
main:    total time =  5595.51 ms

main: quantize time =  6010.89 ms
main:    total time =  6010.89 ms

main: quantize time =  6860.02 ms
main:    total time =  6860.02 ms

main: quantize time =  6352.58 ms
main:    total time =  6352.58 ms

main: quantize time =  4748.76 ms
main:    total time =  4748.76 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.179 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.356 I main: llama backend init
0.00.000.363 I main: load the model and apply lora adapter, if any
0.00.077.465 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.089.819 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.089.836 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.089.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.089.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.089.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.089.842 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.089.843 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.089.845 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.089.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.089.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.089.847 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.089.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.089.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.089.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.089.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.089.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.089.855 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.096.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.098.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.105.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.105.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.105.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.105.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.105.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.105.913 I llama_model_loader: - type  f32:  194 tensors
0.00.105.914 I llama_model_loader: - type  f16:   98 tensors
0.00.105.915 I print_info: file format = GGUF V3 (latest)
0.00.105.917 I print_info: file type   = all F32 (guessed)
0.00.105.919 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.124.506 I load: special tokens cache size = 25
0.00.134.902 I load: token to piece cache size = 0.2984 MB
0.00.134.905 I print_info: arch             = gptneox
0.00.134.906 I print_info: vocab_only       = 0
0.00.134.906 I print_info: n_ctx_train      = 2048
0.00.134.906 I print_info: n_embd           = 2048
0.00.134.906 I print_info: n_layer          = 24
0.00.134.911 I print_info: n_head           = 16
0.00.134.912 I print_info: n_head_kv        = 16
0.00.134.912 I print_info: n_rot            = 32
0.00.134.913 I print_info: n_swa            = 0
0.00.134.913 I print_info: n_embd_head_k    = 128
0.00.134.913 I print_info: n_embd_head_v    = 128
0.00.134.914 I print_info: n_gqa            = 1
0.00.134.915 I print_info: n_embd_k_gqa     = 2048
0.00.134.917 I print_info: n_embd_v_gqa     = 2048
0.00.134.918 I print_info: f_norm_eps       = 1.0e-05
0.00.134.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.134.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.134.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.134.920 I print_info: f_logit_scale    = 0.0e+00
0.00.134.921 I print_info: n_ff             = 8192
0.00.134.921 I print_info: n_expert         = 0
0.00.134.921 I print_info: n_expert_used    = 0
0.00.134.921 I print_info: causal attn      = 1
0.00.134.922 I print_info: pooling type     = 0
0.00.134.922 I print_info: rope type        = 2
0.00.134.922 I print_info: rope scaling     = linear
0.00.134.923 I print_info: freq_base_train  = 10000.0
0.00.134.924 I print_info: freq_scale_train = 1
0.00.134.924 I print_info: n_ctx_orig_yarn  = 2048
0.00.134.924 I print_info: rope_finetuned   = unknown
0.00.134.924 I print_info: ssm_d_conv       = 0
0.00.134.925 I print_info: ssm_d_inner      = 0
0.00.134.927 I print_info: ssm_d_state      = 0
0.00.134.927 I print_info: ssm_dt_rank      = 0
0.00.134.927 I print_info: ssm_dt_b_c_rms   = 0
0.00.134.927 I print_info: model type       = 1.4B
0.00.134.928 I print_info: model params     = 1.41 B
0.00.134.928 I print_info: general.name     = 1.4B
0.00.134.928 I print_info: vocab type       = BPE
0.00.134.928 I print_info: n_vocab          = 50304
0.00.134.930 I print_info: n_merges         = 50009
0.00.134.931 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.134.931 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.134.931 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.134.931 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.134.931 I print_info: LF token         = 187 'Ċ'
0.00.134.932 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.134.932 I print_info: max token length = 1024
0.00.134.933 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.195.080 I load_tensors: offloading 24 repeating layers to GPU
0.00.195.084 I load_tensors: offloading output layer to GPU
0.00.195.084 I load_tensors: offloaded 25/25 layers to GPU
0.00.195.110 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.195.112 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.195.580 I llama_init_from_model: n_seq_max     = 1
0.00.195.581 I llama_init_from_model: n_ctx         = 2048
0.00.195.583 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.195.583 I llama_init_from_model: n_batch       = 2048
0.00.195.583 I llama_init_from_model: n_ubatch      = 512
0.00.195.583 I llama_init_from_model: flash_attn    = 0
0.00.195.584 I llama_init_from_model: freq_base     = 10000.0
0.00.195.584 I llama_init_from_model: freq_scale    = 1
0.00.195.585 I ggml_metal_init: allocating
0.00.195.625 I ggml_metal_init: found device: Apple M4
0.00.195.631 I ggml_metal_init: picking default device: Apple M4
0.00.196.266 I ggml_metal_init: using embedded metal library
0.00.264.847 I ggml_metal_init: GPU name:   Apple M4
0.00.264.852 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.264.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.264.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.264.853 I ggml_metal_init: simdgroup reduction   = true
0.00.264.853 I ggml_metal_init: simdgroup matrix mul. = true
0.00.264.853 I ggml_metal_init: has residency sets    = true
0.00.264.858 I ggml_metal_init: has bfloat            = true
0.00.264.858 I ggml_metal_init: use bfloat            = true
0.00.264.859 I ggml_metal_init: hasUnifiedMemory      = true
0.00.264.861 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.223 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.425.215 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.425.223 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.425.269 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.428.979 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.428.981 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.428.982 I llama_init_from_model: graph nodes  = 967
0.00.428.982 I llama_init_from_model: graph splits = 2
0.00.428.989 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.429.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.429.105 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.725 I main: llama threadpool init, n_threads = 4
0.00.495.769 I 
0.00.495.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.802 I 
0.00.495.965 I sampler seed: 1234
0.00.495.969 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.495.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.495.994 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.495.995 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.322.776 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.02.322.777 I llama_perf_context_print:        load time =     417.42 ms
0.02.322.778 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.40 tokens per second)
0.02.322.778 I llama_perf_context_print:        eval time =    1780.38 ms /    63 runs   (   28.26 ms per token,    35.39 tokens per second)
0.02.322.779 I llama_perf_context_print:       total time =    1827.88 ms /    70 tokens
0.02.322.987 I ggml_metal_free: deallocating

real	0m2.607s
user	0m0.142s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.010.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.071 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.071 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.072 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.075 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.075 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.078 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.078 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.067 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.948 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.948 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.949 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.949 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.949 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.950 I llama_model_loader: - type  f32:  194 tensors
0.00.035.951 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.952 I print_info: file format = GGUF V3 (latest)
0.00.035.952 I print_info: file type   = Q8_0
0.00.035.953 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.370 I load: special tokens cache size = 25
0.00.051.751 I load: token to piece cache size = 0.2984 MB
0.00.051.755 I print_info: arch             = gptneox
0.00.051.755 I print_info: vocab_only       = 0
0.00.051.756 I print_info: n_ctx_train      = 2048
0.00.051.756 I print_info: n_embd           = 2048
0.00.051.756 I print_info: n_layer          = 24
0.00.051.762 I print_info: n_head           = 16
0.00.051.766 I print_info: n_head_kv        = 16
0.00.051.766 I print_info: n_rot            = 32
0.00.051.766 I print_info: n_swa            = 0
0.00.051.767 I print_info: n_embd_head_k    = 128
0.00.051.767 I print_info: n_embd_head_v    = 128
0.00.051.768 I print_info: n_gqa            = 1
0.00.051.768 I print_info: n_embd_k_gqa     = 2048
0.00.051.769 I print_info: n_embd_v_gqa     = 2048
0.00.051.770 I print_info: f_norm_eps       = 1.0e-05
0.00.051.770 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.771 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.771 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.772 I print_info: f_logit_scale    = 0.0e+00
0.00.051.773 I print_info: n_ff             = 8192
0.00.051.773 I print_info: n_expert         = 0
0.00.051.774 I print_info: n_expert_used    = 0
0.00.051.774 I print_info: causal attn      = 1
0.00.051.774 I print_info: pooling type     = 0
0.00.051.774 I print_info: rope type        = 2
0.00.051.774 I print_info: rope scaling     = linear
0.00.051.775 I print_info: freq_base_train  = 10000.0
0.00.051.775 I print_info: freq_scale_train = 1
0.00.051.776 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.776 I print_info: rope_finetuned   = unknown
0.00.051.776 I print_info: ssm_d_conv       = 0
0.00.051.776 I print_info: ssm_d_inner      = 0
0.00.051.776 I print_info: ssm_d_state      = 0
0.00.051.776 I print_info: ssm_dt_rank      = 0
0.00.051.777 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.778 I print_info: model type       = 1.4B
0.00.051.778 I print_info: model params     = 1.41 B
0.00.051.778 I print_info: general.name     = 1.4B
0.00.051.779 I print_info: vocab type       = BPE
0.00.051.779 I print_info: n_vocab          = 50304
0.00.051.779 I print_info: n_merges         = 50009
0.00.051.780 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.780 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.780 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.780 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.781 I print_info: LF token         = 187 'Ċ'
0.00.051.781 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.781 I print_info: max token length = 1024
0.00.051.782 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.022.445 I load_tensors: offloading 24 repeating layers to GPU
0.01.022.451 I load_tensors: offloading output layer to GPU
0.01.022.452 I load_tensors: offloaded 25/25 layers to GPU
0.01.022.477 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.022.478 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.023.367 I llama_init_from_model: n_seq_max     = 1
0.01.023.369 I llama_init_from_model: n_ctx         = 2048
0.01.023.369 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.023.370 I llama_init_from_model: n_batch       = 2048
0.01.023.370 I llama_init_from_model: n_ubatch      = 512
0.01.023.371 I llama_init_from_model: flash_attn    = 0
0.01.023.371 I llama_init_from_model: freq_base     = 10000.0
0.01.023.372 I llama_init_from_model: freq_scale    = 1
0.01.023.373 I ggml_metal_init: allocating
0.01.023.389 I ggml_metal_init: found device: Apple M4
0.01.023.398 I ggml_metal_init: picking default device: Apple M4
0.01.024.657 I ggml_metal_init: using embedded metal library
0.01.029.941 I ggml_metal_init: GPU name:   Apple M4
0.01.029.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.029.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.029.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.029.947 I ggml_metal_init: simdgroup reduction   = true
0.01.029.947 I ggml_metal_init: simdgroup matrix mul. = true
0.01.029.947 I ggml_metal_init: has residency sets    = true
0.01.029.947 I ggml_metal_init: has bfloat            = true
0.01.029.948 I ggml_metal_init: use bfloat            = true
0.01.029.948 I ggml_metal_init: hasUnifiedMemory      = true
0.01.029.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.046.894 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.095.669 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.095.675 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.095.710 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.100.466 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.100.468 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.100.468 I llama_init_from_model: graph nodes  = 967
0.01.100.468 I llama_init_from_model: graph splits = 2
0.01.100.473 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.100.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.100.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.148.261 I main: llama threadpool init, n_threads = 4
0.01.148.308 I 
0.01.148.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.148.332 I 
0.01.148.452 I sampler seed: 1234
0.01.148.457 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.148.476 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.148.477 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.148.477 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.299.599 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.02.299.600 I llama_perf_context_print:        load time =    1137.50 ms
0.02.299.601 I llama_perf_context_print: prompt eval time =      49.94 ms /     7 tokens (    7.13 ms per token,   140.16 tokens per second)
0.02.299.601 I llama_perf_context_print:        eval time =    1098.19 ms /    63 runs   (   17.43 ms per token,    57.37 tokens per second)
0.02.299.602 I llama_perf_context_print:       total time =    1152.03 ms /    70 tokens
0.02.299.853 I ggml_metal_free: deallocating

real	0m2.317s
user	0m0.111s
sys	0m0.267s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.011.547 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.740 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.508 I llama_model_loader: - type  f32:  194 tensors
0.00.027.509 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.510 I print_info: file format = GGUF V3 (latest)
0.00.027.515 I print_info: file type   = Q4_0
0.00.027.517 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.261 I load: special tokens cache size = 25
0.00.041.277 I load: token to piece cache size = 0.2984 MB
0.00.041.281 I print_info: arch             = gptneox
0.00.041.281 I print_info: vocab_only       = 0
0.00.041.281 I print_info: n_ctx_train      = 2048
0.00.041.282 I print_info: n_embd           = 2048
0.00.041.282 I print_info: n_layer          = 24
0.00.041.288 I print_info: n_head           = 16
0.00.041.290 I print_info: n_head_kv        = 16
0.00.041.290 I print_info: n_rot            = 32
0.00.041.290 I print_info: n_swa            = 0
0.00.041.291 I print_info: n_embd_head_k    = 128
0.00.041.292 I print_info: n_embd_head_v    = 128
0.00.041.293 I print_info: n_gqa            = 1
0.00.041.294 I print_info: n_embd_k_gqa     = 2048
0.00.041.295 I print_info: n_embd_v_gqa     = 2048
0.00.041.296 I print_info: f_norm_eps       = 1.0e-05
0.00.041.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.297 I print_info: f_logit_scale    = 0.0e+00
0.00.041.297 I print_info: n_ff             = 8192
0.00.041.298 I print_info: n_expert         = 0
0.00.041.298 I print_info: n_expert_used    = 0
0.00.041.298 I print_info: causal attn      = 1
0.00.041.298 I print_info: pooling type     = 0
0.00.041.298 I print_info: rope type        = 2
0.00.041.299 I print_info: rope scaling     = linear
0.00.041.299 I print_info: freq_base_train  = 10000.0
0.00.041.300 I print_info: freq_scale_train = 1
0.00.041.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.300 I print_info: rope_finetuned   = unknown
0.00.041.300 I print_info: ssm_d_conv       = 0
0.00.041.300 I print_info: ssm_d_inner      = 0
0.00.041.301 I print_info: ssm_d_state      = 0
0.00.041.302 I print_info: ssm_dt_rank      = 0
0.00.041.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.302 I print_info: model type       = 1.4B
0.00.041.302 I print_info: model params     = 1.41 B
0.00.041.302 I print_info: general.name     = 1.4B
0.00.041.303 I print_info: vocab type       = BPE
0.00.041.303 I print_info: n_vocab          = 50304
0.00.041.303 I print_info: n_merges         = 50009
0.00.041.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: LF token         = 187 'Ċ'
0.00.041.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.305 I print_info: max token length = 1024
0.00.041.305 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.611.374 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.397 I load_tensors: offloading output layer to GPU
0.00.611.398 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.431 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.611.432 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.612.657 I llama_init_from_model: n_seq_max     = 1
0.00.612.665 I llama_init_from_model: n_ctx         = 2048
0.00.612.666 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.612.667 I llama_init_from_model: n_batch       = 2048
0.00.612.667 I llama_init_from_model: n_ubatch      = 512
0.00.612.667 I llama_init_from_model: flash_attn    = 0
0.00.612.670 I llama_init_from_model: freq_base     = 10000.0
0.00.612.670 I llama_init_from_model: freq_scale    = 1
0.00.612.673 I ggml_metal_init: allocating
0.00.612.746 I ggml_metal_init: found device: Apple M4
0.00.612.760 I ggml_metal_init: picking default device: Apple M4
0.00.614.669 I ggml_metal_init: using embedded metal library
0.00.620.724 I ggml_metal_init: GPU name:   Apple M4
0.00.620.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.751 I ggml_metal_init: simdgroup reduction   = true
0.00.620.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.752 I ggml_metal_init: has residency sets    = true
0.00.620.752 I ggml_metal_init: has bfloat            = true
0.00.620.753 I ggml_metal_init: use bfloat            = true
0.00.620.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.050 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.725 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.700.736 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.700.775 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.706.355 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.706.358 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.706.358 I llama_init_from_model: graph nodes  = 967
0.00.706.358 I llama_init_from_model: graph splits = 2
0.00.706.363 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.706.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.706.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.154 I main: llama threadpool init, n_threads = 4
0.00.756.195 I 
0.00.756.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.223 I 
0.00.756.347 I sampler seed: 1234
0.00.756.351 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.394 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.397 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.397 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.479.649 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49237.17 tokens per second)
0.01.479.649 I llama_perf_context_print:        load time =     743.88 ms
0.01.479.650 I llama_perf_context_print: prompt eval time =      50.06 ms /     7 tokens (    7.15 ms per token,   139.83 tokens per second)
0.01.479.651 I llama_perf_context_print:        eval time =     670.43 ms /    63 runs   (   10.64 ms per token,    93.97 tokens per second)
0.01.479.652 I llama_perf_context_print:       total time =     724.22 ms /    70 tokens
0.01.479.883 I ggml_metal_free: deallocating

real	0m1.496s
user	0m0.114s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.261 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.768 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.769 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.006 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.009 I llama_model_loader: - type  f32:  194 tensors
0.00.026.009 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.009 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.010 I print_info: file format = GGUF V3 (latest)
0.00.026.010 I print_info: file type   = Q4_1
0.00.026.011 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.083 I load: special tokens cache size = 25
0.00.040.036 I load: token to piece cache size = 0.2984 MB
0.00.040.039 I print_info: arch             = gptneox
0.00.040.039 I print_info: vocab_only       = 0
0.00.040.040 I print_info: n_ctx_train      = 2048
0.00.040.040 I print_info: n_embd           = 2048
0.00.040.040 I print_info: n_layer          = 24
0.00.040.043 I print_info: n_head           = 16
0.00.040.044 I print_info: n_head_kv        = 16
0.00.040.044 I print_info: n_rot            = 32
0.00.040.044 I print_info: n_swa            = 0
0.00.040.045 I print_info: n_embd_head_k    = 128
0.00.040.045 I print_info: n_embd_head_v    = 128
0.00.040.045 I print_info: n_gqa            = 1
0.00.040.046 I print_info: n_embd_k_gqa     = 2048
0.00.040.049 I print_info: n_embd_v_gqa     = 2048
0.00.040.050 I print_info: f_norm_eps       = 1.0e-05
0.00.040.050 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.050 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.052 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.052 I print_info: f_logit_scale    = 0.0e+00
0.00.040.052 I print_info: n_ff             = 8192
0.00.040.053 I print_info: n_expert         = 0
0.00.040.053 I print_info: n_expert_used    = 0
0.00.040.053 I print_info: causal attn      = 1
0.00.040.053 I print_info: pooling type     = 0
0.00.040.054 I print_info: rope type        = 2
0.00.040.056 I print_info: rope scaling     = linear
0.00.040.056 I print_info: freq_base_train  = 10000.0
0.00.040.056 I print_info: freq_scale_train = 1
0.00.040.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.057 I print_info: rope_finetuned   = unknown
0.00.040.057 I print_info: ssm_d_conv       = 0
0.00.040.057 I print_info: ssm_d_inner      = 0
0.00.040.057 I print_info: ssm_d_state      = 0
0.00.040.057 I print_info: ssm_dt_rank      = 0
0.00.040.057 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.058 I print_info: model type       = 1.4B
0.00.040.058 I print_info: model params     = 1.41 B
0.00.040.058 I print_info: general.name     = 1.4B
0.00.040.063 I print_info: vocab type       = BPE
0.00.040.067 I print_info: n_vocab          = 50304
0.00.040.068 I print_info: n_merges         = 50009
0.00.040.068 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.068 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.068 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.069 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.069 I print_info: LF token         = 187 'Ċ'
0.00.040.070 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.070 I print_info: max token length = 1024
0.00.040.071 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.005 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.017 I load_tensors: offloading output layer to GPU
0.00.590.018 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.050 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.590.051 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.591.205 I llama_init_from_model: n_seq_max     = 1
0.00.591.212 I llama_init_from_model: n_ctx         = 2048
0.00.591.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.213 I llama_init_from_model: n_batch       = 2048
0.00.591.214 I llama_init_from_model: n_ubatch      = 512
0.00.591.214 I llama_init_from_model: flash_attn    = 0
0.00.591.216 I llama_init_from_model: freq_base     = 10000.0
0.00.591.220 I llama_init_from_model: freq_scale    = 1
0.00.591.222 I ggml_metal_init: allocating
0.00.591.301 I ggml_metal_init: found device: Apple M4
0.00.591.315 I ggml_metal_init: picking default device: Apple M4
0.00.593.254 I ggml_metal_init: using embedded metal library
0.00.599.165 I ggml_metal_init: GPU name:   Apple M4
0.00.599.170 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.173 I ggml_metal_init: simdgroup reduction   = true
0.00.599.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.173 I ggml_metal_init: has residency sets    = true
0.00.599.174 I ggml_metal_init: has bfloat            = true
0.00.599.174 I ggml_metal_init: use bfloat            = true
0.00.599.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.524 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.613 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.676.620 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.676.656 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.871 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.682.873 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.682.873 I llama_init_from_model: graph nodes  = 967
0.00.682.874 I llama_init_from_model: graph splits = 2
0.00.682.879 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.683.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.683.004 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.007 I main: llama threadpool init, n_threads = 4
0.00.731.049 I 
0.00.731.069 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.069 I 
0.00.731.187 I sampler seed: 1234
0.00.731.192 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.202 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.203 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.203 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.492.863 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.492.864 I llama_perf_context_print:        load time =     721.03 ms
0.01.492.865 I llama_perf_context_print: prompt eval time =      45.52 ms /     7 tokens (    6.50 ms per token,   153.79 tokens per second)
0.01.492.865 I llama_perf_context_print:        eval time =     713.40 ms /    63 runs   (   11.32 ms per token,    88.31 tokens per second)
0.01.492.866 I llama_perf_context_print:       total time =     762.57 ms /    70 tokens
0.01.493.073 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.111s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.379 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.235 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.235 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.241 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.072 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.854 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.857 I llama_model_loader: - type  f32:  194 tensors
0.00.026.857 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.857 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.858 I print_info: file format = GGUF V3 (latest)
0.00.026.858 I print_info: file type   = Q5_0
0.00.026.859 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.600 I load: special tokens cache size = 25
0.00.040.602 I load: token to piece cache size = 0.2984 MB
0.00.040.605 I print_info: arch             = gptneox
0.00.040.606 I print_info: vocab_only       = 0
0.00.040.606 I print_info: n_ctx_train      = 2048
0.00.040.606 I print_info: n_embd           = 2048
0.00.040.606 I print_info: n_layer          = 24
0.00.040.609 I print_info: n_head           = 16
0.00.040.610 I print_info: n_head_kv        = 16
0.00.040.610 I print_info: n_rot            = 32
0.00.040.610 I print_info: n_swa            = 0
0.00.040.610 I print_info: n_embd_head_k    = 128
0.00.040.611 I print_info: n_embd_head_v    = 128
0.00.040.611 I print_info: n_gqa            = 1
0.00.040.612 I print_info: n_embd_k_gqa     = 2048
0.00.040.613 I print_info: n_embd_v_gqa     = 2048
0.00.040.614 I print_info: f_norm_eps       = 1.0e-05
0.00.040.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.619 I print_info: f_logit_scale    = 0.0e+00
0.00.040.620 I print_info: n_ff             = 8192
0.00.040.622 I print_info: n_expert         = 0
0.00.040.622 I print_info: n_expert_used    = 0
0.00.040.623 I print_info: causal attn      = 1
0.00.040.623 I print_info: pooling type     = 0
0.00.040.624 I print_info: rope type        = 2
0.00.040.624 I print_info: rope scaling     = linear
0.00.040.624 I print_info: freq_base_train  = 10000.0
0.00.040.625 I print_info: freq_scale_train = 1
0.00.040.625 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.625 I print_info: rope_finetuned   = unknown
0.00.040.626 I print_info: ssm_d_conv       = 0
0.00.040.626 I print_info: ssm_d_inner      = 0
0.00.040.626 I print_info: ssm_d_state      = 0
0.00.040.626 I print_info: ssm_dt_rank      = 0
0.00.040.626 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.627 I print_info: model type       = 1.4B
0.00.040.628 I print_info: model params     = 1.41 B
0.00.040.631 I print_info: general.name     = 1.4B
0.00.040.631 I print_info: vocab type       = BPE
0.00.040.631 I print_info: n_vocab          = 50304
0.00.040.632 I print_info: n_merges         = 50009
0.00.040.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.632 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.632 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.633 I print_info: LF token         = 187 'Ċ'
0.00.040.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.634 I print_info: max token length = 1024
0.00.040.634 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.747 I load_tensors: offloading output layer to GPU
0.00.646.748 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.783 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.646.784 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.648.041 I llama_init_from_model: n_seq_max     = 1
0.00.648.048 I llama_init_from_model: n_ctx         = 2048
0.00.648.049 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.648.050 I llama_init_from_model: n_batch       = 2048
0.00.648.050 I llama_init_from_model: n_ubatch      = 512
0.00.648.051 I llama_init_from_model: flash_attn    = 0
0.00.648.053 I llama_init_from_model: freq_base     = 10000.0
0.00.648.057 I llama_init_from_model: freq_scale    = 1
0.00.648.059 I ggml_metal_init: allocating
0.00.648.138 I ggml_metal_init: found device: Apple M4
0.00.648.153 I ggml_metal_init: picking default device: Apple M4
0.00.650.118 I ggml_metal_init: using embedded metal library
0.00.655.734 I ggml_metal_init: GPU name:   Apple M4
0.00.655.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.743 I ggml_metal_init: simdgroup reduction   = true
0.00.655.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.743 I ggml_metal_init: has residency sets    = true
0.00.655.743 I ggml_metal_init: has bfloat            = true
0.00.655.744 I ggml_metal_init: use bfloat            = true
0.00.655.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.530 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.161 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.170 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.208 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.626 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.628 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.628 I llama_init_from_model: graph nodes  = 967
0.00.741.628 I llama_init_from_model: graph splits = 2
0.00.741.633 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.374 I main: llama threadpool init, n_threads = 4
0.00.791.418 I 
0.00.791.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.440 I 
0.00.791.566 I sampler seed: 1234
0.00.791.570 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.791.593 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.791.594 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.791.595 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.617.275 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.617.275 I llama_perf_context_print:        load time =     780.26 ms
0.01.617.276 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.02 tokens per second)
0.01.617.277 I llama_perf_context_print:        eval time =     779.66 ms /    63 runs   (   12.38 ms per token,    80.80 tokens per second)
0.01.617.277 I llama_perf_context_print:       total time =     826.64 ms /    70 tokens
0.01.617.506 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.112s
sys	0m0.227s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.811 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.814 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.814 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.818 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.291 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.292 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.292 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.293 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.293 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.293 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.294 I llama_model_loader: - type  f32:  194 tensors
0.00.026.294 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.294 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.295 I print_info: file format = GGUF V3 (latest)
0.00.026.295 I print_info: file type   = Q5_1
0.00.026.296 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.393 I load: special tokens cache size = 25
0.00.040.307 I load: token to piece cache size = 0.2984 MB
0.00.040.310 I print_info: arch             = gptneox
0.00.040.310 I print_info: vocab_only       = 0
0.00.040.311 I print_info: n_ctx_train      = 2048
0.00.040.311 I print_info: n_embd           = 2048
0.00.040.311 I print_info: n_layer          = 24
0.00.040.314 I print_info: n_head           = 16
0.00.040.315 I print_info: n_head_kv        = 16
0.00.040.316 I print_info: n_rot            = 32
0.00.040.316 I print_info: n_swa            = 0
0.00.040.316 I print_info: n_embd_head_k    = 128
0.00.040.318 I print_info: n_embd_head_v    = 128
0.00.040.319 I print_info: n_gqa            = 1
0.00.040.320 I print_info: n_embd_k_gqa     = 2048
0.00.040.320 I print_info: n_embd_v_gqa     = 2048
0.00.040.321 I print_info: f_norm_eps       = 1.0e-05
0.00.040.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.322 I print_info: f_logit_scale    = 0.0e+00
0.00.040.323 I print_info: n_ff             = 8192
0.00.040.325 I print_info: n_expert         = 0
0.00.040.325 I print_info: n_expert_used    = 0
0.00.040.325 I print_info: causal attn      = 1
0.00.040.325 I print_info: pooling type     = 0
0.00.040.326 I print_info: rope type        = 2
0.00.040.326 I print_info: rope scaling     = linear
0.00.040.327 I print_info: freq_base_train  = 10000.0
0.00.040.327 I print_info: freq_scale_train = 1
0.00.040.327 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.327 I print_info: rope_finetuned   = unknown
0.00.040.328 I print_info: ssm_d_conv       = 0
0.00.040.328 I print_info: ssm_d_inner      = 0
0.00.040.328 I print_info: ssm_d_state      = 0
0.00.040.329 I print_info: ssm_dt_rank      = 0
0.00.040.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.333 I print_info: model type       = 1.4B
0.00.040.333 I print_info: model params     = 1.41 B
0.00.040.333 I print_info: general.name     = 1.4B
0.00.040.333 I print_info: vocab type       = BPE
0.00.040.334 I print_info: n_vocab          = 50304
0.00.040.334 I print_info: n_merges         = 50009
0.00.040.334 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.334 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.334 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: LF token         = 187 'Ċ'
0.00.040.335 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: max token length = 1024
0.00.040.335 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.739.261 I load_tensors: offloading 24 repeating layers to GPU
0.00.739.284 I load_tensors: offloading output layer to GPU
0.00.739.285 I load_tensors: offloaded 25/25 layers to GPU
0.00.739.324 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.739.326 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.740.539 I llama_init_from_model: n_seq_max     = 1
0.00.740.550 I llama_init_from_model: n_ctx         = 2048
0.00.740.551 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.740.552 I llama_init_from_model: n_batch       = 2048
0.00.740.552 I llama_init_from_model: n_ubatch      = 512
0.00.740.552 I llama_init_from_model: flash_attn    = 0
0.00.740.554 I llama_init_from_model: freq_base     = 10000.0
0.00.740.560 I llama_init_from_model: freq_scale    = 1
0.00.740.562 I ggml_metal_init: allocating
0.00.740.646 I ggml_metal_init: found device: Apple M4
0.00.740.662 I ggml_metal_init: picking default device: Apple M4
0.00.742.632 I ggml_metal_init: using embedded metal library
0.00.749.192 I ggml_metal_init: GPU name:   Apple M4
0.00.749.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.749.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.749.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.749.197 I ggml_metal_init: simdgroup reduction   = true
0.00.749.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.749.198 I ggml_metal_init: has residency sets    = true
0.00.749.198 I ggml_metal_init: has bfloat            = true
0.00.749.198 I ggml_metal_init: use bfloat            = true
0.00.749.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.749.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.766.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.823.596 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.823.603 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.823.636 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.827.913 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.827.915 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.827.915 I llama_init_from_model: graph nodes  = 967
0.00.827.915 I llama_init_from_model: graph splits = 2
0.00.827.925 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.828.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.828.056 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.927 I main: llama threadpool init, n_threads = 4
0.00.881.974 I 
0.00.881.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.881.995 I 
0.00.882.115 I sampler seed: 1234
0.00.882.119 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.882.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.882.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.882.141 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.754.765 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.754.766 I llama_perf_context_print:        load time =     871.72 ms
0.01.754.767 I llama_perf_context_print: prompt eval time =      52.22 ms /     7 tokens (    7.46 ms per token,   134.04 tokens per second)
0.01.754.768 I llama_perf_context_print:        eval time =     817.44 ms /    63 runs   (   12.98 ms per token,    77.07 tokens per second)
0.01.754.769 I llama_perf_context_print:       total time =     873.54 ms /    70 tokens
0.01.755.024 I ggml_metal_free: deallocating

real	0m1.774s
user	0m0.111s
sys	0m0.224s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.048 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.697 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.713 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.261 I llama_model_loader: - type  f32:  194 tensors
0.00.033.261 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.262 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.262 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.263 I print_info: file format = GGUF V3 (latest)
0.00.033.263 I print_info: file type   = Q2_K - Medium
0.00.033.264 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.040.928 I load: special tokens cache size = 25
0.00.046.936 I load: token to piece cache size = 0.2984 MB
0.00.046.939 I print_info: arch             = gptneox
0.00.046.939 I print_info: vocab_only       = 0
0.00.046.939 I print_info: n_ctx_train      = 2048
0.00.046.939 I print_info: n_embd           = 2048
0.00.046.940 I print_info: n_layer          = 24
0.00.046.942 I print_info: n_head           = 16
0.00.046.943 I print_info: n_head_kv        = 16
0.00.046.943 I print_info: n_rot            = 32
0.00.046.943 I print_info: n_swa            = 0
0.00.046.944 I print_info: n_embd_head_k    = 128
0.00.046.944 I print_info: n_embd_head_v    = 128
0.00.046.944 I print_info: n_gqa            = 1
0.00.046.945 I print_info: n_embd_k_gqa     = 2048
0.00.046.946 I print_info: n_embd_v_gqa     = 2048
0.00.046.947 I print_info: f_norm_eps       = 1.0e-05
0.00.046.947 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.948 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.949 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.949 I print_info: f_logit_scale    = 0.0e+00
0.00.046.950 I print_info: n_ff             = 8192
0.00.046.950 I print_info: n_expert         = 0
0.00.046.950 I print_info: n_expert_used    = 0
0.00.046.950 I print_info: causal attn      = 1
0.00.046.952 I print_info: pooling type     = 0
0.00.046.954 I print_info: rope type        = 2
0.00.046.954 I print_info: rope scaling     = linear
0.00.046.954 I print_info: freq_base_train  = 10000.0
0.00.046.955 I print_info: freq_scale_train = 1
0.00.046.955 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.955 I print_info: rope_finetuned   = unknown
0.00.046.955 I print_info: ssm_d_conv       = 0
0.00.046.956 I print_info: ssm_d_inner      = 0
0.00.046.956 I print_info: ssm_d_state      = 0
0.00.046.956 I print_info: ssm_dt_rank      = 0
0.00.046.956 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.956 I print_info: model type       = 1.4B
0.00.046.957 I print_info: model params     = 1.41 B
0.00.046.957 I print_info: general.name     = 1.4B
0.00.046.957 I print_info: vocab type       = BPE
0.00.046.959 I print_info: n_vocab          = 50304
0.00.046.959 I print_info: n_merges         = 50009
0.00.046.959 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.959 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.960 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.960 I print_info: LF token         = 187 'Ċ'
0.00.046.960 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.960 I print_info: max token length = 1024
0.00.046.961 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.393.954 I load_tensors: offloading 24 repeating layers to GPU
0.00.393.966 I load_tensors: offloading output layer to GPU
0.00.393.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.394.000 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.394.001 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.395.406 I llama_init_from_model: n_seq_max     = 1
0.00.395.411 I llama_init_from_model: n_ctx         = 2048
0.00.395.412 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.395.413 I llama_init_from_model: n_batch       = 2048
0.00.395.413 I llama_init_from_model: n_ubatch      = 512
0.00.395.413 I llama_init_from_model: flash_attn    = 0
0.00.395.419 I llama_init_from_model: freq_base     = 10000.0
0.00.395.422 I llama_init_from_model: freq_scale    = 1
0.00.395.425 I ggml_metal_init: allocating
0.00.395.497 I ggml_metal_init: found device: Apple M4
0.00.395.510 I ggml_metal_init: picking default device: Apple M4
0.00.397.368 I ggml_metal_init: using embedded metal library
0.00.402.865 I ggml_metal_init: GPU name:   Apple M4
0.00.402.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.402.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.402.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.402.883 I ggml_metal_init: simdgroup reduction   = true
0.00.402.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.402.883 I ggml_metal_init: has residency sets    = true
0.00.402.884 I ggml_metal_init: has bfloat            = true
0.00.402.884 I ggml_metal_init: use bfloat            = true
0.00.402.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.402.889 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.424.349 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.480.646 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.480.659 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.480.731 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.485.751 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.485.753 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.485.754 I llama_init_from_model: graph nodes  = 967
0.00.485.754 I llama_init_from_model: graph splits = 2
0.00.485.759 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.485.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.485.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.692 I main: llama threadpool init, n_threads = 4
0.00.547.736 I 
0.00.547.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.761 I 
0.00.547.912 I sampler seed: 1234
0.00.547.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.547.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.547.937 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.547.937 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.233.429 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.233.430 I llama_perf_context_print:        load time =     536.93 ms
0.01.233.431 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.54 tokens per second)
0.01.233.431 I llama_perf_context_print:        eval time =     638.59 ms /    63 runs   (   10.14 ms per token,    98.65 tokens per second)
0.01.233.432 I llama_perf_context_print:       total time =     686.45 ms /    70 tokens
0.01.233.666 I ggml_metal_free: deallocating

real	0m1.253s
user	0m0.114s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.010.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.515 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.518 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.519 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.201 I llama_model_loader: - type  f32:  194 tensors
0.00.027.201 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.201 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.201 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.202 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.202 I print_info: file format = GGUF V3 (latest)
0.00.027.202 I print_info: file type   = Q3_K - Medium
0.00.027.203 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.370 I load: special tokens cache size = 25
0.00.041.492 I load: token to piece cache size = 0.2984 MB
0.00.041.495 I print_info: arch             = gptneox
0.00.041.496 I print_info: vocab_only       = 0
0.00.041.496 I print_info: n_ctx_train      = 2048
0.00.041.496 I print_info: n_embd           = 2048
0.00.041.496 I print_info: n_layer          = 24
0.00.041.499 I print_info: n_head           = 16
0.00.041.500 I print_info: n_head_kv        = 16
0.00.041.500 I print_info: n_rot            = 32
0.00.041.500 I print_info: n_swa            = 0
0.00.041.500 I print_info: n_embd_head_k    = 128
0.00.041.501 I print_info: n_embd_head_v    = 128
0.00.041.501 I print_info: n_gqa            = 1
0.00.041.505 I print_info: n_embd_k_gqa     = 2048
0.00.041.505 I print_info: n_embd_v_gqa     = 2048
0.00.041.506 I print_info: f_norm_eps       = 1.0e-05
0.00.041.507 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.509 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.509 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.509 I print_info: f_logit_scale    = 0.0e+00
0.00.041.510 I print_info: n_ff             = 8192
0.00.041.510 I print_info: n_expert         = 0
0.00.041.510 I print_info: n_expert_used    = 0
0.00.041.510 I print_info: causal attn      = 1
0.00.041.510 I print_info: pooling type     = 0
0.00.041.510 I print_info: rope type        = 2
0.00.041.511 I print_info: rope scaling     = linear
0.00.041.511 I print_info: freq_base_train  = 10000.0
0.00.041.511 I print_info: freq_scale_train = 1
0.00.041.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.512 I print_info: rope_finetuned   = unknown
0.00.041.512 I print_info: ssm_d_conv       = 0
0.00.041.512 I print_info: ssm_d_inner      = 0
0.00.041.512 I print_info: ssm_d_state      = 0
0.00.041.512 I print_info: ssm_dt_rank      = 0
0.00.041.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.513 I print_info: model type       = 1.4B
0.00.041.513 I print_info: model params     = 1.41 B
0.00.041.513 I print_info: general.name     = 1.4B
0.00.041.514 I print_info: vocab type       = BPE
0.00.041.514 I print_info: n_vocab          = 50304
0.00.041.514 I print_info: n_merges         = 50009
0.00.041.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.517 I print_info: LF token         = 187 'Ċ'
0.00.041.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.518 I print_info: max token length = 1024
0.00.041.518 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.177 I load_tensors: offloading output layer to GPU
0.00.451.177 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.218 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.227 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.452.769 I llama_init_from_model: n_seq_max     = 1
0.00.452.774 I llama_init_from_model: n_ctx         = 2048
0.00.452.775 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.452.775 I llama_init_from_model: n_batch       = 2048
0.00.452.776 I llama_init_from_model: n_ubatch      = 512
0.00.452.776 I llama_init_from_model: flash_attn    = 0
0.00.452.778 I llama_init_from_model: freq_base     = 10000.0
0.00.452.779 I llama_init_from_model: freq_scale    = 1
0.00.452.781 I ggml_metal_init: allocating
0.00.452.862 I ggml_metal_init: found device: Apple M4
0.00.452.876 I ggml_metal_init: picking default device: Apple M4
0.00.454.769 I ggml_metal_init: using embedded metal library
0.00.460.713 I ggml_metal_init: GPU name:   Apple M4
0.00.460.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.721 I ggml_metal_init: simdgroup reduction   = true
0.00.460.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.722 I ggml_metal_init: has residency sets    = true
0.00.460.722 I ggml_metal_init: has bfloat            = true
0.00.460.722 I ggml_metal_init: use bfloat            = true
0.00.460.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.771 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.535.606 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.535.643 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.539.893 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.539.895 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.539.895 I llama_init_from_model: graph nodes  = 967
0.00.539.895 I llama_init_from_model: graph splits = 2
0.00.539.900 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.540.025 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.540.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.506 I main: llama threadpool init, n_threads = 4
0.00.597.549 I 
0.00.597.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.571 I 
0.00.597.740 I sampler seed: 1234
0.00.597.744 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.597.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.597.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.597.770 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.343.225 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.343.227 I llama_perf_context_print:        load time =     585.91 ms
0.01.343.228 I llama_perf_context_print: prompt eval time =      50.03 ms /     7 tokens (    7.15 ms per token,   139.92 tokens per second)
0.01.343.229 I llama_perf_context_print:        eval time =     692.52 ms /    63 runs   (   10.99 ms per token,    90.97 tokens per second)
0.01.343.229 I llama_perf_context_print:       total time =     746.42 ms /    70 tokens
0.01.343.449 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.110s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.577 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.577 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.116 I llama_model_loader: - type  f32:  194 tensors
0.00.025.116 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.117 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.117 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.117 I print_info: file format = GGUF V3 (latest)
0.00.025.118 I print_info: file type   = Q4_K - Medium
0.00.025.119 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.828 I load: special tokens cache size = 25
0.00.038.772 I load: token to piece cache size = 0.2984 MB
0.00.038.775 I print_info: arch             = gptneox
0.00.038.775 I print_info: vocab_only       = 0
0.00.038.775 I print_info: n_ctx_train      = 2048
0.00.038.775 I print_info: n_embd           = 2048
0.00.038.775 I print_info: n_layer          = 24
0.00.038.778 I print_info: n_head           = 16
0.00.038.779 I print_info: n_head_kv        = 16
0.00.038.779 I print_info: n_rot            = 32
0.00.038.780 I print_info: n_swa            = 0
0.00.038.780 I print_info: n_embd_head_k    = 128
0.00.038.782 I print_info: n_embd_head_v    = 128
0.00.038.783 I print_info: n_gqa            = 1
0.00.038.783 I print_info: n_embd_k_gqa     = 2048
0.00.038.784 I print_info: n_embd_v_gqa     = 2048
0.00.038.785 I print_info: f_norm_eps       = 1.0e-05
0.00.038.785 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.785 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.785 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.785 I print_info: f_logit_scale    = 0.0e+00
0.00.038.786 I print_info: n_ff             = 8192
0.00.038.786 I print_info: n_expert         = 0
0.00.038.786 I print_info: n_expert_used    = 0
0.00.038.786 I print_info: causal attn      = 1
0.00.038.788 I print_info: pooling type     = 0
0.00.038.788 I print_info: rope type        = 2
0.00.038.790 I print_info: rope scaling     = linear
0.00.038.790 I print_info: freq_base_train  = 10000.0
0.00.038.791 I print_info: freq_scale_train = 1
0.00.038.791 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.791 I print_info: rope_finetuned   = unknown
0.00.038.791 I print_info: ssm_d_conv       = 0
0.00.038.791 I print_info: ssm_d_inner      = 0
0.00.038.792 I print_info: ssm_d_state      = 0
0.00.038.792 I print_info: ssm_dt_rank      = 0
0.00.038.792 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.792 I print_info: model type       = 1.4B
0.00.038.792 I print_info: model params     = 1.41 B
0.00.038.792 I print_info: general.name     = 1.4B
0.00.038.797 I print_info: vocab type       = BPE
0.00.038.797 I print_info: n_vocab          = 50304
0.00.038.797 I print_info: n_merges         = 50009
0.00.038.797 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.798 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.799 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.799 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.799 I print_info: LF token         = 187 'Ċ'
0.00.038.799 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.799 I print_info: max token length = 1024
0.00.038.800 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.542.649 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.665 I load_tensors: offloading output layer to GPU
0.00.542.665 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.701 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.708 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.544.264 I llama_init_from_model: n_seq_max     = 1
0.00.544.269 I llama_init_from_model: n_ctx         = 2048
0.00.544.269 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.544.270 I llama_init_from_model: n_batch       = 2048
0.00.544.270 I llama_init_from_model: n_ubatch      = 512
0.00.544.270 I llama_init_from_model: flash_attn    = 0
0.00.544.277 I llama_init_from_model: freq_base     = 10000.0
0.00.544.279 I llama_init_from_model: freq_scale    = 1
0.00.544.282 I ggml_metal_init: allocating
0.00.544.369 I ggml_metal_init: found device: Apple M4
0.00.544.382 I ggml_metal_init: picking default device: Apple M4
0.00.546.250 I ggml_metal_init: using embedded metal library
0.00.552.104 I ggml_metal_init: GPU name:   Apple M4
0.00.552.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.111 I ggml_metal_init: simdgroup reduction   = true
0.00.552.112 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.112 I ggml_metal_init: has residency sets    = true
0.00.552.112 I ggml_metal_init: has bfloat            = true
0.00.552.113 I ggml_metal_init: use bfloat            = true
0.00.552.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.123 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.832 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.819 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.623.824 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.623.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.028 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.628.029 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.628.030 I llama_init_from_model: graph nodes  = 967
0.00.628.030 I llama_init_from_model: graph splits = 2
0.00.628.036 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.628.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.628.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.289 I main: llama threadpool init, n_threads = 4
0.00.687.344 I 
0.00.687.366 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.368 I 
0.00.687.543 I sampler seed: 1234
0.00.687.548 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.566 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.566 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.566 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.446.232 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.446.232 I llama_perf_context_print:        load time =     677.68 ms
0.01.446.233 I llama_perf_context_print: prompt eval time =      55.45 ms /     7 tokens (    7.92 ms per token,   126.24 tokens per second)
0.01.446.234 I llama_perf_context_print:        eval time =     700.23 ms /    63 runs   (   11.11 ms per token,    89.97 tokens per second)
0.01.446.234 I llama_perf_context_print:       total time =     759.63 ms /    70 tokens
0.01.446.495 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.107s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.011.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.605 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.504 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.217 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.218 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.218 I llama_model_loader: - type  f32:  194 tensors
0.00.027.218 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.219 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.219 I print_info: file format = GGUF V3 (latest)
0.00.027.220 I print_info: file type   = Q5_K - Medium
0.00.027.221 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.383 I load: special tokens cache size = 25
0.00.041.351 I load: token to piece cache size = 0.2984 MB
0.00.041.353 I print_info: arch             = gptneox
0.00.041.354 I print_info: vocab_only       = 0
0.00.041.354 I print_info: n_ctx_train      = 2048
0.00.041.354 I print_info: n_embd           = 2048
0.00.041.354 I print_info: n_layer          = 24
0.00.041.357 I print_info: n_head           = 16
0.00.041.357 I print_info: n_head_kv        = 16
0.00.041.358 I print_info: n_rot            = 32
0.00.041.358 I print_info: n_swa            = 0
0.00.041.358 I print_info: n_embd_head_k    = 128
0.00.041.358 I print_info: n_embd_head_v    = 128
0.00.041.359 I print_info: n_gqa            = 1
0.00.041.360 I print_info: n_embd_k_gqa     = 2048
0.00.041.362 I print_info: n_embd_v_gqa     = 2048
0.00.041.363 I print_info: f_norm_eps       = 1.0e-05
0.00.041.363 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.363 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.365 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.365 I print_info: f_logit_scale    = 0.0e+00
0.00.041.366 I print_info: n_ff             = 8192
0.00.041.366 I print_info: n_expert         = 0
0.00.041.366 I print_info: n_expert_used    = 0
0.00.041.366 I print_info: causal attn      = 1
0.00.041.366 I print_info: pooling type     = 0
0.00.041.367 I print_info: rope type        = 2
0.00.041.367 I print_info: rope scaling     = linear
0.00.041.367 I print_info: freq_base_train  = 10000.0
0.00.041.369 I print_info: freq_scale_train = 1
0.00.041.369 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.369 I print_info: rope_finetuned   = unknown
0.00.041.369 I print_info: ssm_d_conv       = 0
0.00.041.369 I print_info: ssm_d_inner      = 0
0.00.041.370 I print_info: ssm_d_state      = 0
0.00.041.370 I print_info: ssm_dt_rank      = 0
0.00.041.370 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.370 I print_info: model type       = 1.4B
0.00.041.370 I print_info: model params     = 1.41 B
0.00.041.370 I print_info: general.name     = 1.4B
0.00.041.371 I print_info: vocab type       = BPE
0.00.041.371 I print_info: n_vocab          = 50304
0.00.041.371 I print_info: n_merges         = 50009
0.00.041.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.376 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.376 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.376 I print_info: LF token         = 187 'Ċ'
0.00.041.376 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.376 I print_info: max token length = 1024
0.00.041.377 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.492 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.510 I load_tensors: offloading output layer to GPU
0.00.612.511 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.541 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.612.542 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.614.076 I llama_init_from_model: n_seq_max     = 1
0.00.614.089 I llama_init_from_model: n_ctx         = 2048
0.00.614.089 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.614.090 I llama_init_from_model: n_batch       = 2048
0.00.614.090 I llama_init_from_model: n_ubatch      = 512
0.00.614.090 I llama_init_from_model: flash_attn    = 0
0.00.614.091 I llama_init_from_model: freq_base     = 10000.0
0.00.614.092 I llama_init_from_model: freq_scale    = 1
0.00.614.095 I ggml_metal_init: allocating
0.00.614.143 I ggml_metal_init: found device: Apple M4
0.00.614.160 I ggml_metal_init: picking default device: Apple M4
0.00.616.674 I ggml_metal_init: using embedded metal library
0.00.623.844 I ggml_metal_init: GPU name:   Apple M4
0.00.623.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.850 I ggml_metal_init: simdgroup reduction   = true
0.00.623.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.851 I ggml_metal_init: has residency sets    = true
0.00.623.851 I ggml_metal_init: has bfloat            = true
0.00.623.851 I ggml_metal_init: use bfloat            = true
0.00.623.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.854 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.669 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.786 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.701.792 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.701.826 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.706.206 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.706.207 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.706.208 I llama_init_from_model: graph nodes  = 967
0.00.706.208 I llama_init_from_model: graph splits = 2
0.00.706.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.706.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.706.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.316 I main: llama threadpool init, n_threads = 4
0.00.767.355 I 
0.00.767.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.373 I 
0.00.767.521 I sampler seed: 1234
0.00.767.525 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.558 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.561 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.561 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.610.667 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.610.668 I llama_perf_context_print:        load time =     755.55 ms
0.01.610.669 I llama_perf_context_print: prompt eval time =      51.52 ms /     7 tokens (    7.36 ms per token,   135.86 tokens per second)
0.01.610.670 I llama_perf_context_print:        eval time =     788.95 ms /    63 runs   (   12.52 ms per token,    79.85 tokens per second)
0.01.610.670 I llama_perf_context_print:       total time =     844.06 ms /    70 tokens
0.01.610.927 I ggml_metal_free: deallocating

real	0m1.630s
user	0m0.111s
sys	0m0.229s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.307 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.308 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.309 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.309 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.309 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.310 I llama_model_loader: - type  f32:  194 tensors
0.00.025.310 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.311 I print_info: file format = GGUF V3 (latest)
0.00.025.311 I print_info: file type   = Q6_K
0.00.025.312 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.469 I load: special tokens cache size = 25
0.00.039.564 I load: token to piece cache size = 0.2984 MB
0.00.039.567 I print_info: arch             = gptneox
0.00.039.567 I print_info: vocab_only       = 0
0.00.039.567 I print_info: n_ctx_train      = 2048
0.00.039.567 I print_info: n_embd           = 2048
0.00.039.567 I print_info: n_layer          = 24
0.00.039.570 I print_info: n_head           = 16
0.00.039.571 I print_info: n_head_kv        = 16
0.00.039.571 I print_info: n_rot            = 32
0.00.039.571 I print_info: n_swa            = 0
0.00.039.571 I print_info: n_embd_head_k    = 128
0.00.039.573 I print_info: n_embd_head_v    = 128
0.00.039.574 I print_info: n_gqa            = 1
0.00.039.574 I print_info: n_embd_k_gqa     = 2048
0.00.039.575 I print_info: n_embd_v_gqa     = 2048
0.00.039.576 I print_info: f_norm_eps       = 1.0e-05
0.00.039.576 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.576 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.577 I print_info: f_logit_scale    = 0.0e+00
0.00.039.577 I print_info: n_ff             = 8192
0.00.039.577 I print_info: n_expert         = 0
0.00.039.578 I print_info: n_expert_used    = 0
0.00.039.578 I print_info: causal attn      = 1
0.00.039.578 I print_info: pooling type     = 0
0.00.039.578 I print_info: rope type        = 2
0.00.039.578 I print_info: rope scaling     = linear
0.00.039.580 I print_info: freq_base_train  = 10000.0
0.00.039.581 I print_info: freq_scale_train = 1
0.00.039.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.581 I print_info: rope_finetuned   = unknown
0.00.039.581 I print_info: ssm_d_conv       = 0
0.00.039.581 I print_info: ssm_d_inner      = 0
0.00.039.582 I print_info: ssm_d_state      = 0
0.00.039.582 I print_info: ssm_dt_rank      = 0
0.00.039.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.582 I print_info: model type       = 1.4B
0.00.039.583 I print_info: model params     = 1.41 B
0.00.039.583 I print_info: general.name     = 1.4B
0.00.039.583 I print_info: vocab type       = BPE
0.00.039.583 I print_info: n_vocab          = 50304
0.00.039.584 I print_info: n_merges         = 50009
0.00.039.585 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.585 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: LF token         = 187 'Ċ'
0.00.039.586 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: max token length = 1024
0.00.039.587 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.659.050 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.054 I load_tensors: offloading output layer to GPU
0.00.659.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.076 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.659.078 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.660.391 I llama_init_from_model: n_seq_max     = 1
0.00.660.393 I llama_init_from_model: n_ctx         = 2048
0.00.660.394 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.394 I llama_init_from_model: n_batch       = 2048
0.00.660.395 I llama_init_from_model: n_ubatch      = 512
0.00.660.395 I llama_init_from_model: flash_attn    = 0
0.00.660.396 I llama_init_from_model: freq_base     = 10000.0
0.00.660.397 I llama_init_from_model: freq_scale    = 1
0.00.660.398 I ggml_metal_init: allocating
0.00.660.450 I ggml_metal_init: found device: Apple M4
0.00.660.460 I ggml_metal_init: picking default device: Apple M4
0.00.661.950 I ggml_metal_init: using embedded metal library
0.00.668.148 I ggml_metal_init: GPU name:   Apple M4
0.00.668.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.155 I ggml_metal_init: simdgroup reduction   = true
0.00.668.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.155 I ggml_metal_init: has residency sets    = true
0.00.668.155 I ggml_metal_init: has bfloat            = true
0.00.668.156 I ggml_metal_init: use bfloat            = true
0.00.668.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.834 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.574 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.585 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.621 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.002 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.004 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.005 I llama_init_from_model: graph nodes  = 967
0.00.741.005 I llama_init_from_model: graph splits = 2
0.00.741.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.133 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.133 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.697 I main: llama threadpool init, n_threads = 4
0.00.806.740 I 
0.00.806.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.764 I 
0.00.806.948 I sampler seed: 1234
0.00.806.953 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.963 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.964 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.964 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.677.187 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.677.188 I llama_perf_context_print:        load time =     797.13 ms
0.01.677.188 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.43 tokens per second)
0.01.677.189 I llama_perf_context_print:        eval time =     812.67 ms /    63 runs   (   12.90 ms per token,    77.52 tokens per second)
0.01.677.189 I llama_perf_context_print:       total time =     871.20 ms /    70 tokens
0.01.677.421 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.108s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.764 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.241 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.257 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.263 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.263 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.264 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.264 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.264 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.279 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.752 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.753 I llama_model_loader: - type  f32:  194 tensors
0.00.049.754 I llama_model_loader: - type  f16:   98 tensors
0.00.049.754 I print_info: file format = GGUF V3 (latest)
0.00.049.755 I print_info: file type   = all F32 (guessed)
0.00.049.757 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.061.583 I load: special tokens cache size = 25
0.00.069.064 I load: token to piece cache size = 0.2984 MB
0.00.069.067 I print_info: arch             = gptneox
0.00.069.067 I print_info: vocab_only       = 0
0.00.069.068 I print_info: n_ctx_train      = 2048
0.00.069.068 I print_info: n_embd           = 2048
0.00.069.068 I print_info: n_layer          = 24
0.00.069.072 I print_info: n_head           = 16
0.00.069.073 I print_info: n_head_kv        = 16
0.00.069.073 I print_info: n_rot            = 32
0.00.069.073 I print_info: n_swa            = 0
0.00.069.073 I print_info: n_embd_head_k    = 128
0.00.069.075 I print_info: n_embd_head_v    = 128
0.00.069.076 I print_info: n_gqa            = 1
0.00.069.077 I print_info: n_embd_k_gqa     = 2048
0.00.069.079 I print_info: n_embd_v_gqa     = 2048
0.00.069.079 I print_info: f_norm_eps       = 1.0e-05
0.00.069.080 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.080 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.080 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.080 I print_info: f_logit_scale    = 0.0e+00
0.00.069.081 I print_info: n_ff             = 8192
0.00.069.081 I print_info: n_expert         = 0
0.00.069.081 I print_info: n_expert_used    = 0
0.00.069.083 I print_info: causal attn      = 1
0.00.069.083 I print_info: pooling type     = 0
0.00.069.083 I print_info: rope type        = 2
0.00.069.083 I print_info: rope scaling     = linear
0.00.069.084 I print_info: freq_base_train  = 10000.0
0.00.069.084 I print_info: freq_scale_train = 1
0.00.069.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.084 I print_info: rope_finetuned   = unknown
0.00.069.085 I print_info: ssm_d_conv       = 0
0.00.069.085 I print_info: ssm_d_inner      = 0
0.00.069.085 I print_info: ssm_d_state      = 0
0.00.069.085 I print_info: ssm_dt_rank      = 0
0.00.069.085 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.085 I print_info: model type       = 1.4B
0.00.069.086 I print_info: model params     = 1.41 B
0.00.069.090 I print_info: general.name     = 1.4B
0.00.069.090 I print_info: vocab type       = BPE
0.00.069.091 I print_info: n_vocab          = 50304
0.00.069.094 I print_info: n_merges         = 50009
0.00.069.094 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.094 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.094 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.095 I print_info: LF token         = 187 'Ċ'
0.00.069.096 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.097 I print_info: max token length = 1024
0.00.069.097 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.470.950 I load_tensors: offloading 24 repeating layers to GPU
0.01.470.954 I load_tensors: offloading output layer to GPU
0.01.470.955 I load_tensors: offloaded 25/25 layers to GPU
0.01.470.983 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.470.985 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.471.795 I llama_init_from_model: n_seq_max     = 1
0.01.471.796 I llama_init_from_model: n_ctx         = 128
0.01.471.797 I llama_init_from_model: n_ctx_per_seq = 128
0.01.471.797 I llama_init_from_model: n_batch       = 128
0.01.471.797 I llama_init_from_model: n_ubatch      = 128
0.01.471.797 I llama_init_from_model: flash_attn    = 0
0.01.471.798 I llama_init_from_model: freq_base     = 10000.0
0.01.471.798 I llama_init_from_model: freq_scale    = 1
0.01.471.798 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.471.803 I ggml_metal_init: allocating
0.01.471.887 I ggml_metal_init: found device: Apple M4
0.01.471.894 I ggml_metal_init: picking default device: Apple M4
0.01.473.021 I ggml_metal_init: using embedded metal library
0.01.476.792 I ggml_metal_init: GPU name:   Apple M4
0.01.476.795 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.476.795 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.476.796 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.476.796 I ggml_metal_init: simdgroup reduction   = true
0.01.476.796 I ggml_metal_init: simdgroup matrix mul. = true
0.01.476.796 I ggml_metal_init: has residency sets    = true
0.01.476.797 I ggml_metal_init: has bfloat            = true
0.01.476.797 I ggml_metal_init: use bfloat            = true
0.01.476.797 I ggml_metal_init: hasUnifiedMemory      = true
0.01.476.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.487.537 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.489.200 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.489.203 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.489.226 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.490.888 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.490.889 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.490.889 I llama_init_from_model: graph nodes  = 967
0.01.490.890 I llama_init_from_model: graph splits = 2
0.01.490.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.490.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.525.737 I 
0.01.525.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.525.778 I perplexity: tokenizing the input ..
0.01.530.700 I perplexity: tokenization took 4.92 ms
0.01.530.704 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.649.081 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.650.618 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.650.649 I llama_perf_context_print:        load time =    1506.49 ms
0.01.650.650 I llama_perf_context_print: prompt eval time =     118.11 ms /   128 tokens (    0.92 ms per token,  1083.73 tokens per second)
0.01.650.651 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.650.651 I llama_perf_context_print:       total time =     124.91 ms /   129 tokens
0.01.651.044 I ggml_metal_free: deallocating

real	0m1.839s
user	0m0.095s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.918 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.920 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.920 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.921 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.921 I llama_model_loader: - type  f32:  194 tensors
0.00.025.922 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.922 I print_info: file format = GGUF V3 (latest)
0.00.025.923 I print_info: file type   = Q8_0
0.00.025.924 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.446 I load: special tokens cache size = 25
0.00.040.654 I load: token to piece cache size = 0.2984 MB
0.00.040.659 I print_info: arch             = gptneox
0.00.040.659 I print_info: vocab_only       = 0
0.00.040.659 I print_info: n_ctx_train      = 2048
0.00.040.659 I print_info: n_embd           = 2048
0.00.040.659 I print_info: n_layer          = 24
0.00.040.664 I print_info: n_head           = 16
0.00.040.665 I print_info: n_head_kv        = 16
0.00.040.665 I print_info: n_rot            = 32
0.00.040.665 I print_info: n_swa            = 0
0.00.040.665 I print_info: n_embd_head_k    = 128
0.00.040.665 I print_info: n_embd_head_v    = 128
0.00.040.666 I print_info: n_gqa            = 1
0.00.040.667 I print_info: n_embd_k_gqa     = 2048
0.00.040.667 I print_info: n_embd_v_gqa     = 2048
0.00.040.668 I print_info: f_norm_eps       = 1.0e-05
0.00.040.669 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.669 I print_info: f_logit_scale    = 0.0e+00
0.00.040.670 I print_info: n_ff             = 8192
0.00.040.670 I print_info: n_expert         = 0
0.00.040.670 I print_info: n_expert_used    = 0
0.00.040.670 I print_info: causal attn      = 1
0.00.040.670 I print_info: pooling type     = 0
0.00.040.671 I print_info: rope type        = 2
0.00.040.671 I print_info: rope scaling     = linear
0.00.040.673 I print_info: freq_base_train  = 10000.0
0.00.040.674 I print_info: freq_scale_train = 1
0.00.040.674 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.674 I print_info: rope_finetuned   = unknown
0.00.040.674 I print_info: ssm_d_conv       = 0
0.00.040.675 I print_info: ssm_d_inner      = 0
0.00.040.675 I print_info: ssm_d_state      = 0
0.00.040.675 I print_info: ssm_dt_rank      = 0
0.00.040.675 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.675 I print_info: model type       = 1.4B
0.00.040.675 I print_info: model params     = 1.41 B
0.00.040.676 I print_info: general.name     = 1.4B
0.00.040.676 I print_info: vocab type       = BPE
0.00.040.676 I print_info: n_vocab          = 50304
0.00.040.678 I print_info: n_merges         = 50009
0.00.040.678 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.678 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.678 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.678 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.679 I print_info: LF token         = 187 'Ċ'
0.00.040.679 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.679 I print_info: max token length = 1024
0.00.040.679 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.903.205 I load_tensors: offloading 24 repeating layers to GPU
0.00.903.212 I load_tensors: offloading output layer to GPU
0.00.903.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.903.239 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.903.240 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.904.447 I llama_init_from_model: n_seq_max     = 1
0.00.904.449 I llama_init_from_model: n_ctx         = 128
0.00.904.449 I llama_init_from_model: n_ctx_per_seq = 128
0.00.904.450 I llama_init_from_model: n_batch       = 128
0.00.904.450 I llama_init_from_model: n_ubatch      = 128
0.00.904.450 I llama_init_from_model: flash_attn    = 0
0.00.904.451 I llama_init_from_model: freq_base     = 10000.0
0.00.904.451 I llama_init_from_model: freq_scale    = 1
0.00.904.452 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.904.453 I ggml_metal_init: allocating
0.00.904.522 I ggml_metal_init: found device: Apple M4
0.00.904.532 I ggml_metal_init: picking default device: Apple M4
0.00.905.863 I ggml_metal_init: using embedded metal library
0.00.911.274 I ggml_metal_init: GPU name:   Apple M4
0.00.911.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.911.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.911.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.911.280 I ggml_metal_init: simdgroup reduction   = true
0.00.911.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.911.280 I ggml_metal_init: has residency sets    = true
0.00.911.280 I ggml_metal_init: has bfloat            = true
0.00.911.280 I ggml_metal_init: use bfloat            = true
0.00.911.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.911.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.926.286 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.929.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.929.642 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.929.688 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.932.814 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.932.815 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.932.816 I llama_init_from_model: graph nodes  = 967
0.00.932.816 I llama_init_from_model: graph splits = 2
0.00.932.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.932.819 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.959.095 I 
0.00.959.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.959.179 I perplexity: tokenizing the input ..
0.00.966.817 I perplexity: tokenization took 7.635 ms
0.00.966.830 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.104.762 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.106.281 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.106.302 I llama_perf_context_print:        load time =     949.11 ms
0.01.106.307 I llama_perf_context_print: prompt eval time =     137.05 ms /   128 tokens (    1.07 ms per token,   933.96 tokens per second)
0.01.106.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.106.309 I llama_perf_context_print:       total time =     147.21 ms /   129 tokens
0.01.106.629 I ggml_metal_free: deallocating

real	0m1.122s
user	0m0.078s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.278 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.923 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.925 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.925 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.926 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.926 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.926 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.927 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.928 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.932 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.932 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.933 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.935 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.740 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.639 I llama_model_loader: - type  f32:  194 tensors
0.00.025.640 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.641 I print_info: file format = GGUF V3 (latest)
0.00.025.641 I print_info: file type   = Q4_0
0.00.025.642 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.810 I load: special tokens cache size = 25
0.00.039.778 I load: token to piece cache size = 0.2984 MB
0.00.039.781 I print_info: arch             = gptneox
0.00.039.781 I print_info: vocab_only       = 0
0.00.039.782 I print_info: n_ctx_train      = 2048
0.00.039.782 I print_info: n_embd           = 2048
0.00.039.782 I print_info: n_layer          = 24
0.00.039.785 I print_info: n_head           = 16
0.00.039.786 I print_info: n_head_kv        = 16
0.00.039.786 I print_info: n_rot            = 32
0.00.039.786 I print_info: n_swa            = 0
0.00.039.786 I print_info: n_embd_head_k    = 128
0.00.039.789 I print_info: n_embd_head_v    = 128
0.00.039.789 I print_info: n_gqa            = 1
0.00.039.790 I print_info: n_embd_k_gqa     = 2048
0.00.039.791 I print_info: n_embd_v_gqa     = 2048
0.00.039.791 I print_info: f_norm_eps       = 1.0e-05
0.00.039.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.792 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.792 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.792 I print_info: f_logit_scale    = 0.0e+00
0.00.039.793 I print_info: n_ff             = 8192
0.00.039.793 I print_info: n_expert         = 0
0.00.039.794 I print_info: n_expert_used    = 0
0.00.039.794 I print_info: causal attn      = 1
0.00.039.794 I print_info: pooling type     = 0
0.00.039.794 I print_info: rope type        = 2
0.00.039.794 I print_info: rope scaling     = linear
0.00.039.795 I print_info: freq_base_train  = 10000.0
0.00.039.795 I print_info: freq_scale_train = 1
0.00.039.795 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.796 I print_info: rope_finetuned   = unknown
0.00.039.796 I print_info: ssm_d_conv       = 0
0.00.039.798 I print_info: ssm_d_inner      = 0
0.00.039.798 I print_info: ssm_d_state      = 0
0.00.039.798 I print_info: ssm_dt_rank      = 0
0.00.039.798 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.799 I print_info: model type       = 1.4B
0.00.039.799 I print_info: model params     = 1.41 B
0.00.039.799 I print_info: general.name     = 1.4B
0.00.039.800 I print_info: vocab type       = BPE
0.00.039.800 I print_info: n_vocab          = 50304
0.00.039.800 I print_info: n_merges         = 50009
0.00.039.800 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.800 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.801 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.804 I print_info: LF token         = 187 'Ċ'
0.00.039.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.810 I print_info: max token length = 1024
0.00.039.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.682 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.701 I load_tensors: offloading output layer to GPU
0.00.614.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.736 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.614.738 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.616.354 I llama_init_from_model: n_seq_max     = 1
0.00.616.360 I llama_init_from_model: n_ctx         = 128
0.00.616.361 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.361 I llama_init_from_model: n_batch       = 128
0.00.616.361 I llama_init_from_model: n_ubatch      = 128
0.00.616.362 I llama_init_from_model: flash_attn    = 0
0.00.616.365 I llama_init_from_model: freq_base     = 10000.0
0.00.616.365 I llama_init_from_model: freq_scale    = 1
0.00.616.368 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.371 I ggml_metal_init: allocating
0.00.616.449 I ggml_metal_init: found device: Apple M4
0.00.616.463 I ggml_metal_init: picking default device: Apple M4
0.00.618.251 I ggml_metal_init: using embedded metal library
0.00.623.652 I ggml_metal_init: GPU name:   Apple M4
0.00.623.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.672 I ggml_metal_init: simdgroup reduction   = true
0.00.623.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.672 I ggml_metal_init: has residency sets    = true
0.00.623.673 I ggml_metal_init: has bfloat            = true
0.00.623.673 I ggml_metal_init: use bfloat            = true
0.00.623.675 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.679 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.548 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.102 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.109 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.167 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.368 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.370 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.370 I llama_init_from_model: graph nodes  = 967
0.00.650.371 I llama_init_from_model: graph splits = 2
0.00.650.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.598 I 
0.00.681.678 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.686 I perplexity: tokenizing the input ..
0.00.689.137 I perplexity: tokenization took 7.447 ms
0.00.689.145 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.126 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.826.645 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.826.664 I llama_perf_context_print:        load time =     671.83 ms
0.00.826.664 I llama_perf_context_print: prompt eval time =     135.31 ms /   128 tokens (    1.06 ms per token,   946.00 tokens per second)
0.00.826.665 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.669 I llama_perf_context_print:       total time =     145.07 ms /   129 tokens
0.00.827.047 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.081s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.684 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.919 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.921 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.514 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.516 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.517 I llama_model_loader: - type  f32:  194 tensors
0.00.024.517 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.518 I print_info: file format = GGUF V3 (latest)
0.00.024.518 I print_info: file type   = Q4_1
0.00.024.520 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.566 I load: special tokens cache size = 25
0.00.038.528 I load: token to piece cache size = 0.2984 MB
0.00.038.530 I print_info: arch             = gptneox
0.00.038.531 I print_info: vocab_only       = 0
0.00.038.531 I print_info: n_ctx_train      = 2048
0.00.038.531 I print_info: n_embd           = 2048
0.00.038.531 I print_info: n_layer          = 24
0.00.038.535 I print_info: n_head           = 16
0.00.038.535 I print_info: n_head_kv        = 16
0.00.038.536 I print_info: n_rot            = 32
0.00.038.536 I print_info: n_swa            = 0
0.00.038.536 I print_info: n_embd_head_k    = 128
0.00.038.536 I print_info: n_embd_head_v    = 128
0.00.038.537 I print_info: n_gqa            = 1
0.00.038.537 I print_info: n_embd_k_gqa     = 2048
0.00.038.538 I print_info: n_embd_v_gqa     = 2048
0.00.038.539 I print_info: f_norm_eps       = 1.0e-05
0.00.038.539 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.539 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.539 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.540 I print_info: f_logit_scale    = 0.0e+00
0.00.038.540 I print_info: n_ff             = 8192
0.00.038.540 I print_info: n_expert         = 0
0.00.038.541 I print_info: n_expert_used    = 0
0.00.038.541 I print_info: causal attn      = 1
0.00.038.541 I print_info: pooling type     = 0
0.00.038.541 I print_info: rope type        = 2
0.00.038.541 I print_info: rope scaling     = linear
0.00.038.541 I print_info: freq_base_train  = 10000.0
0.00.038.544 I print_info: freq_scale_train = 1
0.00.038.545 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.545 I print_info: rope_finetuned   = unknown
0.00.038.545 I print_info: ssm_d_conv       = 0
0.00.038.545 I print_info: ssm_d_inner      = 0
0.00.038.545 I print_info: ssm_d_state      = 0
0.00.038.545 I print_info: ssm_dt_rank      = 0
0.00.038.545 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.546 I print_info: model type       = 1.4B
0.00.038.546 I print_info: model params     = 1.41 B
0.00.038.546 I print_info: general.name     = 1.4B
0.00.038.547 I print_info: vocab type       = BPE
0.00.038.547 I print_info: n_vocab          = 50304
0.00.038.547 I print_info: n_merges         = 50009
0.00.038.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.547 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.548 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.548 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.552 I print_info: LF token         = 187 'Ċ'
0.00.038.552 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.553 I print_info: max token length = 1024
0.00.038.553 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.576.032 I load_tensors: offloading 24 repeating layers to GPU
0.00.576.046 I load_tensors: offloading output layer to GPU
0.00.576.047 I load_tensors: offloaded 25/25 layers to GPU
0.00.576.085 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.576.087 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.577.722 I llama_init_from_model: n_seq_max     = 1
0.00.577.726 I llama_init_from_model: n_ctx         = 128
0.00.577.730 I llama_init_from_model: n_ctx_per_seq = 128
0.00.577.730 I llama_init_from_model: n_batch       = 128
0.00.577.731 I llama_init_from_model: n_ubatch      = 128
0.00.577.732 I llama_init_from_model: flash_attn    = 0
0.00.577.734 I llama_init_from_model: freq_base     = 10000.0
0.00.577.734 I llama_init_from_model: freq_scale    = 1
0.00.577.735 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.577.737 I ggml_metal_init: allocating
0.00.577.844 I ggml_metal_init: found device: Apple M4
0.00.577.859 I ggml_metal_init: picking default device: Apple M4
0.00.579.755 I ggml_metal_init: using embedded metal library
0.00.586.427 I ggml_metal_init: GPU name:   Apple M4
0.00.586.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.434 I ggml_metal_init: simdgroup reduction   = true
0.00.586.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.435 I ggml_metal_init: has residency sets    = true
0.00.586.435 I ggml_metal_init: has bfloat            = true
0.00.586.435 I ggml_metal_init: use bfloat            = true
0.00.586.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.604.773 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.258 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.608.262 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.608.305 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.611.432 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.611.434 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.611.434 I llama_init_from_model: graph nodes  = 967
0.00.611.435 I llama_init_from_model: graph splits = 2
0.00.611.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.611.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.255 I 
0.00.640.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.361 I perplexity: tokenizing the input ..
0.00.647.741 I perplexity: tokenization took 7.376 ms
0.00.647.748 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.040 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.780.573 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.780.599 I llama_perf_context_print:        load time =     631.56 ms
0.00.780.600 I llama_perf_context_print: prompt eval time =     130.40 ms /   128 tokens (    1.02 ms per token,   981.62 tokens per second)
0.00.780.600 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.601 I llama_perf_context_print:       total time =     140.35 ms /   129 tokens
0.00.781.017 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.082s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.055 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.070 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.715 I llama_model_loader: - type  f32:  194 tensors
0.00.025.715 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.716 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.716 I print_info: file format = GGUF V3 (latest)
0.00.025.717 I print_info: file type   = Q5_0
0.00.025.718 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.986 I load: special tokens cache size = 25
0.00.040.014 I load: token to piece cache size = 0.2984 MB
0.00.040.017 I print_info: arch             = gptneox
0.00.040.017 I print_info: vocab_only       = 0
0.00.040.017 I print_info: n_ctx_train      = 2048
0.00.040.018 I print_info: n_embd           = 2048
0.00.040.018 I print_info: n_layer          = 24
0.00.040.022 I print_info: n_head           = 16
0.00.040.023 I print_info: n_head_kv        = 16
0.00.040.023 I print_info: n_rot            = 32
0.00.040.024 I print_info: n_swa            = 0
0.00.040.024 I print_info: n_embd_head_k    = 128
0.00.040.025 I print_info: n_embd_head_v    = 128
0.00.040.025 I print_info: n_gqa            = 1
0.00.040.026 I print_info: n_embd_k_gqa     = 2048
0.00.040.027 I print_info: n_embd_v_gqa     = 2048
0.00.040.027 I print_info: f_norm_eps       = 1.0e-05
0.00.040.028 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.028 I print_info: f_logit_scale    = 0.0e+00
0.00.040.029 I print_info: n_ff             = 8192
0.00.040.029 I print_info: n_expert         = 0
0.00.040.029 I print_info: n_expert_used    = 0
0.00.040.029 I print_info: causal attn      = 1
0.00.040.029 I print_info: pooling type     = 0
0.00.040.029 I print_info: rope type        = 2
0.00.040.031 I print_info: rope scaling     = linear
0.00.040.032 I print_info: freq_base_train  = 10000.0
0.00.040.032 I print_info: freq_scale_train = 1
0.00.040.032 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.032 I print_info: rope_finetuned   = unknown
0.00.040.033 I print_info: ssm_d_conv       = 0
0.00.040.035 I print_info: ssm_d_inner      = 0
0.00.040.035 I print_info: ssm_d_state      = 0
0.00.040.035 I print_info: ssm_dt_rank      = 0
0.00.040.035 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.036 I print_info: model type       = 1.4B
0.00.040.036 I print_info: model params     = 1.41 B
0.00.040.036 I print_info: general.name     = 1.4B
0.00.040.037 I print_info: vocab type       = BPE
0.00.040.037 I print_info: n_vocab          = 50304
0.00.040.037 I print_info: n_merges         = 50009
0.00.040.038 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: LF token         = 187 'Ċ'
0.00.040.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.039 I print_info: max token length = 1024
0.00.040.040 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.040 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.054 I load_tensors: offloading output layer to GPU
0.00.630.055 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.088 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.630.095 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.631.578 I llama_init_from_model: n_seq_max     = 1
0.00.631.583 I llama_init_from_model: n_ctx         = 128
0.00.631.583 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.584 I llama_init_from_model: n_batch       = 128
0.00.631.584 I llama_init_from_model: n_ubatch      = 128
0.00.631.585 I llama_init_from_model: flash_attn    = 0
0.00.631.586 I llama_init_from_model: freq_base     = 10000.0
0.00.631.587 I llama_init_from_model: freq_scale    = 1
0.00.631.587 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.590 I ggml_metal_init: allocating
0.00.631.656 I ggml_metal_init: found device: Apple M4
0.00.631.669 I ggml_metal_init: picking default device: Apple M4
0.00.633.420 I ggml_metal_init: using embedded metal library
0.00.640.059 I ggml_metal_init: GPU name:   Apple M4
0.00.640.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.067 I ggml_metal_init: simdgroup reduction   = true
0.00.640.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.068 I ggml_metal_init: has residency sets    = true
0.00.640.068 I ggml_metal_init: has bfloat            = true
0.00.640.068 I ggml_metal_init: use bfloat            = true
0.00.640.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.222 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.691 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.661.698 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.771 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.998 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.000 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.001 I llama_init_from_model: graph nodes  = 967
0.00.665.001 I llama_init_from_model: graph splits = 2
0.00.665.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.181 I 
0.00.695.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.226 I perplexity: tokenizing the input ..
0.00.701.082 I perplexity: tokenization took 5.851 ms
0.00.701.093 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.477 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.840.088 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.840.111 I llama_perf_context_print:        load time =     685.22 ms
0.00.840.116 I llama_perf_context_print: prompt eval time =     136.97 ms /   128 tokens (    1.07 ms per token,   934.49 tokens per second)
0.00.840.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.119 I llama_perf_context_print:       total time =     144.93 ms /   129 tokens
0.00.840.493 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.079s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.521 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.521 I llama_model_loader: - type  f32:  194 tensors
0.00.024.522 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.522 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.523 I print_info: file format = GGUF V3 (latest)
0.00.024.523 I print_info: file type   = Q5_1
0.00.024.526 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.664 I load: special tokens cache size = 25
0.00.038.682 I load: token to piece cache size = 0.2984 MB
0.00.038.685 I print_info: arch             = gptneox
0.00.038.685 I print_info: vocab_only       = 0
0.00.038.685 I print_info: n_ctx_train      = 2048
0.00.038.685 I print_info: n_embd           = 2048
0.00.038.686 I print_info: n_layer          = 24
0.00.038.689 I print_info: n_head           = 16
0.00.038.690 I print_info: n_head_kv        = 16
0.00.038.690 I print_info: n_rot            = 32
0.00.038.690 I print_info: n_swa            = 0
0.00.038.690 I print_info: n_embd_head_k    = 128
0.00.038.692 I print_info: n_embd_head_v    = 128
0.00.038.693 I print_info: n_gqa            = 1
0.00.038.694 I print_info: n_embd_k_gqa     = 2048
0.00.038.694 I print_info: n_embd_v_gqa     = 2048
0.00.038.695 I print_info: f_norm_eps       = 1.0e-05
0.00.038.699 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.700 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.700 I print_info: f_logit_scale    = 0.0e+00
0.00.038.701 I print_info: n_ff             = 8192
0.00.038.701 I print_info: n_expert         = 0
0.00.038.701 I print_info: n_expert_used    = 0
0.00.038.701 I print_info: causal attn      = 1
0.00.038.701 I print_info: pooling type     = 0
0.00.038.701 I print_info: rope type        = 2
0.00.038.702 I print_info: rope scaling     = linear
0.00.038.704 I print_info: freq_base_train  = 10000.0
0.00.038.704 I print_info: freq_scale_train = 1
0.00.038.704 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.704 I print_info: rope_finetuned   = unknown
0.00.038.704 I print_info: ssm_d_conv       = 0
0.00.038.705 I print_info: ssm_d_inner      = 0
0.00.038.705 I print_info: ssm_d_state      = 0
0.00.038.705 I print_info: ssm_dt_rank      = 0
0.00.038.705 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.705 I print_info: model type       = 1.4B
0.00.038.710 I print_info: model params     = 1.41 B
0.00.038.712 I print_info: general.name     = 1.4B
0.00.038.712 I print_info: vocab type       = BPE
0.00.038.713 I print_info: n_vocab          = 50304
0.00.038.713 I print_info: n_merges         = 50009
0.00.038.713 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.713 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.714 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.715 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.715 I print_info: LF token         = 187 'Ċ'
0.00.038.715 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.716 I print_info: max token length = 1024
0.00.038.719 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.576.444 I load_tensors: offloading 24 repeating layers to GPU
0.00.576.459 I load_tensors: offloading output layer to GPU
0.00.576.460 I load_tensors: offloaded 25/25 layers to GPU
0.00.576.497 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.576.498 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.578.056 I llama_init_from_model: n_seq_max     = 1
0.00.578.060 I llama_init_from_model: n_ctx         = 128
0.00.578.060 I llama_init_from_model: n_ctx_per_seq = 128
0.00.578.061 I llama_init_from_model: n_batch       = 128
0.00.578.061 I llama_init_from_model: n_ubatch      = 128
0.00.578.062 I llama_init_from_model: flash_attn    = 0
0.00.578.063 I llama_init_from_model: freq_base     = 10000.0
0.00.578.063 I llama_init_from_model: freq_scale    = 1
0.00.578.064 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.578.066 I ggml_metal_init: allocating
0.00.578.089 I ggml_metal_init: found device: Apple M4
0.00.578.099 I ggml_metal_init: picking default device: Apple M4
0.00.579.449 I ggml_metal_init: using embedded metal library
0.00.585.806 I ggml_metal_init: GPU name:   Apple M4
0.00.585.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.585.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.585.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.585.811 I ggml_metal_init: simdgroup reduction   = true
0.00.585.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.585.812 I ggml_metal_init: has residency sets    = true
0.00.585.812 I ggml_metal_init: has bfloat            = true
0.00.585.812 I ggml_metal_init: use bfloat            = true
0.00.585.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.585.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.071 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.606.626 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.606.630 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.606.674 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.960 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.609.962 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.609.963 I llama_init_from_model: graph nodes  = 967
0.00.609.963 I llama_init_from_model: graph splits = 2
0.00.609.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.609.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.827 I 
0.00.642.905 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.912 I perplexity: tokenizing the input ..
0.00.650.390 I perplexity: tokenization took 7.475 ms
0.00.650.398 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.629 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.796.175 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.796.194 I llama_perf_context_print:        load time =     633.97 ms
0.00.796.195 I llama_perf_context_print: prompt eval time =     143.32 ms /   128 tokens (    1.12 ms per token,   893.12 tokens per second)
0.00.796.196 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.196 I llama_perf_context_print:       total time =     153.37 ms /   129 tokens
0.00.796.550 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.081s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.829 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.455 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.457 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.457 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.457 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.458 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.458 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.458 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.459 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.994 I llama_model_loader: - type  f32:  194 tensors
0.00.024.994 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.994 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.995 I print_info: file format = GGUF V3 (latest)
0.00.024.996 I print_info: file type   = Q2_K - Medium
0.00.024.996 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.717 I load: special tokens cache size = 25
0.00.038.658 I load: token to piece cache size = 0.2984 MB
0.00.038.661 I print_info: arch             = gptneox
0.00.038.661 I print_info: vocab_only       = 0
0.00.038.661 I print_info: n_ctx_train      = 2048
0.00.038.661 I print_info: n_embd           = 2048
0.00.038.662 I print_info: n_layer          = 24
0.00.038.665 I print_info: n_head           = 16
0.00.038.665 I print_info: n_head_kv        = 16
0.00.038.666 I print_info: n_rot            = 32
0.00.038.666 I print_info: n_swa            = 0
0.00.038.666 I print_info: n_embd_head_k    = 128
0.00.038.668 I print_info: n_embd_head_v    = 128
0.00.038.669 I print_info: n_gqa            = 1
0.00.038.670 I print_info: n_embd_k_gqa     = 2048
0.00.038.671 I print_info: n_embd_v_gqa     = 2048
0.00.038.671 I print_info: f_norm_eps       = 1.0e-05
0.00.038.672 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.672 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.672 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.672 I print_info: f_logit_scale    = 0.0e+00
0.00.038.673 I print_info: n_ff             = 8192
0.00.038.673 I print_info: n_expert         = 0
0.00.038.673 I print_info: n_expert_used    = 0
0.00.038.673 I print_info: causal attn      = 1
0.00.038.673 I print_info: pooling type     = 0
0.00.038.673 I print_info: rope type        = 2
0.00.038.674 I print_info: rope scaling     = linear
0.00.038.674 I print_info: freq_base_train  = 10000.0
0.00.038.678 I print_info: freq_scale_train = 1
0.00.038.678 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.678 I print_info: rope_finetuned   = unknown
0.00.038.678 I print_info: ssm_d_conv       = 0
0.00.038.679 I print_info: ssm_d_inner      = 0
0.00.038.679 I print_info: ssm_d_state      = 0
0.00.038.679 I print_info: ssm_dt_rank      = 0
0.00.038.679 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.679 I print_info: model type       = 1.4B
0.00.038.680 I print_info: model params     = 1.41 B
0.00.038.680 I print_info: general.name     = 1.4B
0.00.038.680 I print_info: vocab type       = BPE
0.00.038.680 I print_info: n_vocab          = 50304
0.00.038.681 I print_info: n_merges         = 50009
0.00.038.681 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.681 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.681 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.681 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.682 I print_info: LF token         = 187 'Ċ'
0.00.038.682 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.682 I print_info: max token length = 1024
0.00.038.682 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.389.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.389.369 I load_tensors: offloading output layer to GPU
0.00.389.370 I load_tensors: offloaded 25/25 layers to GPU
0.00.389.403 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.389.408 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.390.986 I llama_init_from_model: n_seq_max     = 1
0.00.390.992 I llama_init_from_model: n_ctx         = 128
0.00.390.993 I llama_init_from_model: n_ctx_per_seq = 128
0.00.390.993 I llama_init_from_model: n_batch       = 128
0.00.390.994 I llama_init_from_model: n_ubatch      = 128
0.00.390.994 I llama_init_from_model: flash_attn    = 0
0.00.390.997 I llama_init_from_model: freq_base     = 10000.0
0.00.390.997 I llama_init_from_model: freq_scale    = 1
0.00.390.998 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.391.004 I ggml_metal_init: allocating
0.00.391.071 I ggml_metal_init: found device: Apple M4
0.00.391.085 I ggml_metal_init: picking default device: Apple M4
0.00.392.810 I ggml_metal_init: using embedded metal library
0.00.398.334 I ggml_metal_init: GPU name:   Apple M4
0.00.398.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.398.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.398.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.398.344 I ggml_metal_init: simdgroup reduction   = true
0.00.398.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.398.345 I ggml_metal_init: has residency sets    = true
0.00.398.345 I ggml_metal_init: has bfloat            = true
0.00.398.345 I ggml_metal_init: use bfloat            = true
0.00.398.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.398.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.419.341 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.422.840 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.422.851 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.422.902 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.426.288 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.426.290 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.426.290 I llama_init_from_model: graph nodes  = 967
0.00.426.290 I llama_init_from_model: graph splits = 2
0.00.426.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.426.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.992 I 
0.00.460.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.460.081 I perplexity: tokenizing the input ..
0.00.466.749 I perplexity: tokenization took 6.667 ms
0.00.466.754 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.609.447 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.610.998 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.611.024 I llama_perf_context_print:        load time =     450.16 ms
0.00.611.025 I llama_perf_context_print: prompt eval time =     142.46 ms /   128 tokens (    1.11 ms per token,   898.52 tokens per second)
0.00.611.026 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.026 I llama_perf_context_print:       total time =     151.03 ms /   129 tokens
0.00.611.407 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.081s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.689 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.396 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.398 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.399 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.399 I llama_model_loader: - type  f32:  194 tensors
0.00.024.400 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.400 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.400 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.400 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.401 I print_info: file format = GGUF V3 (latest)
0.00.024.401 I print_info: file type   = Q3_K - Medium
0.00.024.404 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.125 I load: special tokens cache size = 25
0.00.038.134 I load: token to piece cache size = 0.2984 MB
0.00.038.137 I print_info: arch             = gptneox
0.00.038.137 I print_info: vocab_only       = 0
0.00.038.137 I print_info: n_ctx_train      = 2048
0.00.038.138 I print_info: n_embd           = 2048
0.00.038.138 I print_info: n_layer          = 24
0.00.038.141 I print_info: n_head           = 16
0.00.038.141 I print_info: n_head_kv        = 16
0.00.038.142 I print_info: n_rot            = 32
0.00.038.144 I print_info: n_swa            = 0
0.00.038.144 I print_info: n_embd_head_k    = 128
0.00.038.144 I print_info: n_embd_head_v    = 128
0.00.038.145 I print_info: n_gqa            = 1
0.00.038.145 I print_info: n_embd_k_gqa     = 2048
0.00.038.150 I print_info: n_embd_v_gqa     = 2048
0.00.038.151 I print_info: f_norm_eps       = 1.0e-05
0.00.038.151 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.151 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.152 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.153 I print_info: f_logit_scale    = 0.0e+00
0.00.038.155 I print_info: n_ff             = 8192
0.00.038.155 I print_info: n_expert         = 0
0.00.038.155 I print_info: n_expert_used    = 0
0.00.038.155 I print_info: causal attn      = 1
0.00.038.156 I print_info: pooling type     = 0
0.00.038.156 I print_info: rope type        = 2
0.00.038.156 I print_info: rope scaling     = linear
0.00.038.156 I print_info: freq_base_train  = 10000.0
0.00.038.157 I print_info: freq_scale_train = 1
0.00.038.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.157 I print_info: rope_finetuned   = unknown
0.00.038.157 I print_info: ssm_d_conv       = 0
0.00.038.158 I print_info: ssm_d_inner      = 0
0.00.038.158 I print_info: ssm_d_state      = 0
0.00.038.158 I print_info: ssm_dt_rank      = 0
0.00.038.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.158 I print_info: model type       = 1.4B
0.00.038.158 I print_info: model params     = 1.41 B
0.00.038.159 I print_info: general.name     = 1.4B
0.00.038.159 I print_info: vocab type       = BPE
0.00.038.159 I print_info: n_vocab          = 50304
0.00.038.161 I print_info: n_merges         = 50009
0.00.038.161 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.161 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.161 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.161 I print_info: LF token         = 187 'Ċ'
0.00.038.162 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.162 I print_info: max token length = 1024
0.00.038.162 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.436 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.450 I load_tensors: offloading output layer to GPU
0.00.436.451 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.484 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.485 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.437.741 I llama_init_from_model: n_seq_max     = 1
0.00.437.751 I llama_init_from_model: n_ctx         = 128
0.00.437.752 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.752 I llama_init_from_model: n_batch       = 128
0.00.437.753 I llama_init_from_model: n_ubatch      = 128
0.00.437.753 I llama_init_from_model: flash_attn    = 0
0.00.437.755 I llama_init_from_model: freq_base     = 10000.0
0.00.437.756 I llama_init_from_model: freq_scale    = 1
0.00.437.756 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.760 I ggml_metal_init: allocating
0.00.437.825 I ggml_metal_init: found device: Apple M4
0.00.437.839 I ggml_metal_init: picking default device: Apple M4
0.00.439.959 I ggml_metal_init: using embedded metal library
0.00.445.647 I ggml_metal_init: GPU name:   Apple M4
0.00.445.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.655 I ggml_metal_init: simdgroup reduction   = true
0.00.445.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.656 I ggml_metal_init: has residency sets    = true
0.00.445.656 I ggml_metal_init: has bfloat            = true
0.00.445.657 I ggml_metal_init: use bfloat            = true
0.00.445.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.762 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.333 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.469.337 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.469.379 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.997 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.472.999 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.472.999 I llama_init_from_model: graph nodes  = 967
0.00.473.000 I llama_init_from_model: graph splits = 2
0.00.473.003 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.473.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.182 I 
0.00.502.261 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.270 I perplexity: tokenizing the input ..
0.00.509.174 I perplexity: tokenization took 6.902 ms
0.00.509.180 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.757 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.650.368 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.650.387 I llama_perf_context_print:        load time =     493.32 ms
0.00.650.388 I llama_perf_context_print: prompt eval time =     138.68 ms /   128 tokens (    1.08 ms per token,   922.96 tokens per second)
0.00.650.389 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.650.389 I llama_perf_context_print:       total time =     148.21 ms /   129 tokens
0.00.650.743 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.081s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.848 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.524 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.413 I llama_model_loader: - type  f32:  194 tensors
0.00.024.414 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.414 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.414 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.415 I print_info: file format = GGUF V3 (latest)
0.00.024.415 I print_info: file type   = Q4_K - Medium
0.00.024.417 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.727 I load: special tokens cache size = 25
0.00.038.703 I load: token to piece cache size = 0.2984 MB
0.00.038.707 I print_info: arch             = gptneox
0.00.038.707 I print_info: vocab_only       = 0
0.00.038.708 I print_info: n_ctx_train      = 2048
0.00.038.708 I print_info: n_embd           = 2048
0.00.038.708 I print_info: n_layer          = 24
0.00.038.712 I print_info: n_head           = 16
0.00.038.713 I print_info: n_head_kv        = 16
0.00.038.713 I print_info: n_rot            = 32
0.00.038.713 I print_info: n_swa            = 0
0.00.038.714 I print_info: n_embd_head_k    = 128
0.00.038.714 I print_info: n_embd_head_v    = 128
0.00.038.715 I print_info: n_gqa            = 1
0.00.038.716 I print_info: n_embd_k_gqa     = 2048
0.00.038.717 I print_info: n_embd_v_gqa     = 2048
0.00.038.717 I print_info: f_norm_eps       = 1.0e-05
0.00.038.717 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.718 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.718 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.718 I print_info: f_logit_scale    = 0.0e+00
0.00.038.719 I print_info: n_ff             = 8192
0.00.038.719 I print_info: n_expert         = 0
0.00.038.719 I print_info: n_expert_used    = 0
0.00.038.719 I print_info: causal attn      = 1
0.00.038.719 I print_info: pooling type     = 0
0.00.038.719 I print_info: rope type        = 2
0.00.038.720 I print_info: rope scaling     = linear
0.00.038.720 I print_info: freq_base_train  = 10000.0
0.00.038.720 I print_info: freq_scale_train = 1
0.00.038.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.721 I print_info: rope_finetuned   = unknown
0.00.038.723 I print_info: ssm_d_conv       = 0
0.00.038.723 I print_info: ssm_d_inner      = 0
0.00.038.723 I print_info: ssm_d_state      = 0
0.00.038.723 I print_info: ssm_dt_rank      = 0
0.00.038.723 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.724 I print_info: model type       = 1.4B
0.00.038.724 I print_info: model params     = 1.41 B
0.00.038.724 I print_info: general.name     = 1.4B
0.00.038.725 I print_info: vocab type       = BPE
0.00.038.725 I print_info: n_vocab          = 50304
0.00.038.725 I print_info: n_merges         = 50009
0.00.038.725 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.725 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: LF token         = 187 'Ċ'
0.00.038.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.730 I print_info: max token length = 1024
0.00.038.732 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.522.610 I load_tensors: offloading 24 repeating layers to GPU
0.00.522.625 I load_tensors: offloading output layer to GPU
0.00.522.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.522.661 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.522.663 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.524.014 I llama_init_from_model: n_seq_max     = 1
0.00.524.020 I llama_init_from_model: n_ctx         = 128
0.00.524.021 I llama_init_from_model: n_ctx_per_seq = 128
0.00.524.022 I llama_init_from_model: n_batch       = 128
0.00.524.022 I llama_init_from_model: n_ubatch      = 128
0.00.524.022 I llama_init_from_model: flash_attn    = 0
0.00.524.024 I llama_init_from_model: freq_base     = 10000.0
0.00.524.024 I llama_init_from_model: freq_scale    = 1
0.00.524.025 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.524.040 I ggml_metal_init: allocating
0.00.524.122 I ggml_metal_init: found device: Apple M4
0.00.524.136 I ggml_metal_init: picking default device: Apple M4
0.00.525.929 I ggml_metal_init: using embedded metal library
0.00.532.371 I ggml_metal_init: GPU name:   Apple M4
0.00.532.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.532.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.532.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.532.380 I ggml_metal_init: simdgroup reduction   = true
0.00.532.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.532.381 I ggml_metal_init: has residency sets    = true
0.00.532.381 I ggml_metal_init: has bfloat            = true
0.00.532.381 I ggml_metal_init: use bfloat            = true
0.00.532.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.532.384 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.550.507 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.554.126 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.554.130 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.554.172 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.557.396 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.557.398 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.557.399 I llama_init_from_model: graph nodes  = 967
0.00.557.399 I llama_init_from_model: graph splits = 2
0.00.557.402 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.557.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.501 I 
0.00.588.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.585 I perplexity: tokenizing the input ..
0.00.596.263 I perplexity: tokenization took 7.674 ms
0.00.596.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.132 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.745.646 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.745.671 I llama_perf_context_print:        load time =     579.64 ms
0.00.745.672 I llama_perf_context_print: prompt eval time =     146.96 ms /   128 tokens (    1.15 ms per token,   870.98 tokens per second)
0.00.745.673 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.673 I llama_perf_context_print:       total time =     157.17 ms /   129 tokens
0.00.746.049 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.081s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.993 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.720 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.480 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.260 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.260 I llama_model_loader: - type  f32:  194 tensors
0.00.025.261 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.261 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.261 I print_info: file format = GGUF V3 (latest)
0.00.025.262 I print_info: file type   = Q5_K - Medium
0.00.025.263 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.300 I load: special tokens cache size = 25
0.00.039.008 I load: token to piece cache size = 0.2984 MB
0.00.039.011 I print_info: arch             = gptneox
0.00.039.011 I print_info: vocab_only       = 0
0.00.039.011 I print_info: n_ctx_train      = 2048
0.00.039.012 I print_info: n_embd           = 2048
0.00.039.012 I print_info: n_layer          = 24
0.00.039.015 I print_info: n_head           = 16
0.00.039.015 I print_info: n_head_kv        = 16
0.00.039.016 I print_info: n_rot            = 32
0.00.039.016 I print_info: n_swa            = 0
0.00.039.016 I print_info: n_embd_head_k    = 128
0.00.039.016 I print_info: n_embd_head_v    = 128
0.00.039.017 I print_info: n_gqa            = 1
0.00.039.018 I print_info: n_embd_k_gqa     = 2048
0.00.039.018 I print_info: n_embd_v_gqa     = 2048
0.00.039.019 I print_info: f_norm_eps       = 1.0e-05
0.00.039.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.020 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.020 I print_info: f_logit_scale    = 0.0e+00
0.00.039.021 I print_info: n_ff             = 8192
0.00.039.021 I print_info: n_expert         = 0
0.00.039.021 I print_info: n_expert_used    = 0
0.00.039.021 I print_info: causal attn      = 1
0.00.039.021 I print_info: pooling type     = 0
0.00.039.021 I print_info: rope type        = 2
0.00.039.022 I print_info: rope scaling     = linear
0.00.039.022 I print_info: freq_base_train  = 10000.0
0.00.039.022 I print_info: freq_scale_train = 1
0.00.039.023 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.023 I print_info: rope_finetuned   = unknown
0.00.039.023 I print_info: ssm_d_conv       = 0
0.00.039.023 I print_info: ssm_d_inner      = 0
0.00.039.023 I print_info: ssm_d_state      = 0
0.00.039.023 I print_info: ssm_dt_rank      = 0
0.00.039.024 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.024 I print_info: model type       = 1.4B
0.00.039.024 I print_info: model params     = 1.41 B
0.00.039.024 I print_info: general.name     = 1.4B
0.00.039.025 I print_info: vocab type       = BPE
0.00.039.025 I print_info: n_vocab          = 50304
0.00.039.025 I print_info: n_merges         = 50009
0.00.039.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.027 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: LF token         = 187 'Ċ'
0.00.039.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: max token length = 1024
0.00.039.029 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.138 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.156 I load_tensors: offloading output layer to GPU
0.00.606.156 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.190 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.606.192 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.607.745 I llama_init_from_model: n_seq_max     = 1
0.00.607.751 I llama_init_from_model: n_ctx         = 128
0.00.607.752 I llama_init_from_model: n_ctx_per_seq = 128
0.00.607.752 I llama_init_from_model: n_batch       = 128
0.00.607.753 I llama_init_from_model: n_ubatch      = 128
0.00.607.753 I llama_init_from_model: flash_attn    = 0
0.00.607.755 I llama_init_from_model: freq_base     = 10000.0
0.00.607.756 I llama_init_from_model: freq_scale    = 1
0.00.607.756 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.607.759 I ggml_metal_init: allocating
0.00.607.827 I ggml_metal_init: found device: Apple M4
0.00.607.841 I ggml_metal_init: picking default device: Apple M4
0.00.609.627 I ggml_metal_init: using embedded metal library
0.00.616.084 I ggml_metal_init: GPU name:   Apple M4
0.00.616.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.090 I ggml_metal_init: simdgroup reduction   = true
0.00.616.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.090 I ggml_metal_init: has residency sets    = true
0.00.616.090 I ggml_metal_init: has bfloat            = true
0.00.616.091 I ggml_metal_init: use bfloat            = true
0.00.616.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.521 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.002 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.008 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.344 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.640.346 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.640.346 I llama_init_from_model: graph nodes  = 967
0.00.640.347 I llama_init_from_model: graph splits = 2
0.00.640.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.640.350 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.757 I 
0.00.676.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.846 I perplexity: tokenizing the input ..
0.00.683.387 I perplexity: tokenization took 6.538 ms
0.00.683.391 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.746 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.834.273 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.834.302 I llama_perf_context_print:        load time =     666.75 ms
0.00.834.303 I llama_perf_context_print: prompt eval time =     148.95 ms /   128 tokens (    1.16 ms per token,   859.34 tokens per second)
0.00.834.303 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.304 I llama_perf_context_print:       total time =     157.55 ms /   129 tokens
0.00.834.699 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.079s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.415 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.071 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.822 I llama_model_loader: - type  f32:  194 tensors
0.00.024.823 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.823 I print_info: file format = GGUF V3 (latest)
0.00.024.824 I print_info: file type   = Q6_K
0.00.024.825 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.501 I load: special tokens cache size = 25
0.00.038.579 I load: token to piece cache size = 0.2984 MB
0.00.038.582 I print_info: arch             = gptneox
0.00.038.582 I print_info: vocab_only       = 0
0.00.038.583 I print_info: n_ctx_train      = 2048
0.00.038.583 I print_info: n_embd           = 2048
0.00.038.583 I print_info: n_layer          = 24
0.00.038.586 I print_info: n_head           = 16
0.00.038.587 I print_info: n_head_kv        = 16
0.00.038.587 I print_info: n_rot            = 32
0.00.038.587 I print_info: n_swa            = 0
0.00.038.588 I print_info: n_embd_head_k    = 128
0.00.038.588 I print_info: n_embd_head_v    = 128
0.00.038.589 I print_info: n_gqa            = 1
0.00.038.590 I print_info: n_embd_k_gqa     = 2048
0.00.038.590 I print_info: n_embd_v_gqa     = 2048
0.00.038.591 I print_info: f_norm_eps       = 1.0e-05
0.00.038.591 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.592 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.592 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.592 I print_info: f_logit_scale    = 0.0e+00
0.00.038.592 I print_info: n_ff             = 8192
0.00.038.593 I print_info: n_expert         = 0
0.00.038.593 I print_info: n_expert_used    = 0
0.00.038.593 I print_info: causal attn      = 1
0.00.038.593 I print_info: pooling type     = 0
0.00.038.593 I print_info: rope type        = 2
0.00.038.593 I print_info: rope scaling     = linear
0.00.038.594 I print_info: freq_base_train  = 10000.0
0.00.038.594 I print_info: freq_scale_train = 1
0.00.038.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.595 I print_info: rope_finetuned   = unknown
0.00.038.595 I print_info: ssm_d_conv       = 0
0.00.038.595 I print_info: ssm_d_inner      = 0
0.00.038.595 I print_info: ssm_d_state      = 0
0.00.038.595 I print_info: ssm_dt_rank      = 0
0.00.038.595 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.596 I print_info: model type       = 1.4B
0.00.038.597 I print_info: model params     = 1.41 B
0.00.038.597 I print_info: general.name     = 1.4B
0.00.038.598 I print_info: vocab type       = BPE
0.00.038.600 I print_info: n_vocab          = 50304
0.00.038.600 I print_info: n_merges         = 50009
0.00.038.600 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: LF token         = 187 'Ċ'
0.00.038.601 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.601 I print_info: max token length = 1024
0.00.038.602 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.844 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.847 I load_tensors: offloading output layer to GPU
0.00.637.848 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.873 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.637.875 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.639.184 I llama_init_from_model: n_seq_max     = 1
0.00.639.186 I llama_init_from_model: n_ctx         = 128
0.00.639.187 I llama_init_from_model: n_ctx_per_seq = 128
0.00.639.190 I llama_init_from_model: n_batch       = 128
0.00.639.191 I llama_init_from_model: n_ubatch      = 128
0.00.639.191 I llama_init_from_model: flash_attn    = 0
0.00.639.192 I llama_init_from_model: freq_base     = 10000.0
0.00.639.193 I llama_init_from_model: freq_scale    = 1
0.00.639.194 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.639.197 I ggml_metal_init: allocating
0.00.639.230 I ggml_metal_init: found device: Apple M4
0.00.639.242 I ggml_metal_init: picking default device: Apple M4
0.00.640.685 I ggml_metal_init: using embedded metal library
0.00.646.915 I ggml_metal_init: GPU name:   Apple M4
0.00.646.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.920 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.921 I ggml_metal_init: simdgroup reduction   = true
0.00.646.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.921 I ggml_metal_init: has residency sets    = true
0.00.646.922 I ggml_metal_init: has bfloat            = true
0.00.646.922 I ggml_metal_init: use bfloat            = true
0.00.646.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.924 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.533 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.667.094 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.667.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.434 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.670.436 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.670.436 I llama_init_from_model: graph nodes  = 967
0.00.670.437 I llama_init_from_model: graph splits = 2
0.00.670.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.670.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.945 I 
0.00.703.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.026 I perplexity: tokenizing the input ..
0.00.708.938 I perplexity: tokenization took 5.91 ms
0.00.708.942 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.243 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.849.977 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.849.997 I llama_perf_context_print:        load time =     693.52 ms
0.00.849.998 I llama_perf_context_print: prompt eval time =     139.07 ms /   128 tokens (    1.09 ms per token,   920.39 tokens per second)
0.00.849.998 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.999 I llama_perf_context_print:       total time =     147.06 ms /   129 tokens
0.00.850.325 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.077s
sys	0m0.160s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4659 (225bbbfa) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.616 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.378 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.392 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.400 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.535 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.456 I llama_model_loader: - type  f32:  194 tensors
0.00.040.456 I llama_model_loader: - type  f16:   98 tensors
0.00.040.457 I print_info: file format = GGUF V3 (latest)
0.00.040.458 I print_info: file type   = all F32 (guessed)
0.00.040.459 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.048.827 I load: special tokens cache size = 25
0.00.054.951 I load: token to piece cache size = 0.2984 MB
0.00.054.955 I print_info: arch             = gptneox
0.00.054.956 I print_info: vocab_only       = 0
0.00.054.956 I print_info: n_ctx_train      = 2048
0.00.054.956 I print_info: n_embd           = 2048
0.00.054.956 I print_info: n_layer          = 24
0.00.054.961 I print_info: n_head           = 16
0.00.054.962 I print_info: n_head_kv        = 16
0.00.054.962 I print_info: n_rot            = 32
0.00.054.962 I print_info: n_swa            = 0
0.00.054.962 I print_info: n_embd_head_k    = 128
0.00.054.962 I print_info: n_embd_head_v    = 128
0.00.054.963 I print_info: n_gqa            = 1
0.00.054.964 I print_info: n_embd_k_gqa     = 2048
0.00.054.965 I print_info: n_embd_v_gqa     = 2048
0.00.054.965 I print_info: f_norm_eps       = 1.0e-05
0.00.054.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.966 I print_info: f_logit_scale    = 0.0e+00
0.00.054.967 I print_info: n_ff             = 8192
0.00.054.967 I print_info: n_expert         = 0
0.00.054.967 I print_info: n_expert_used    = 0
0.00.054.967 I print_info: causal attn      = 1
0.00.054.967 I print_info: pooling type     = 0
0.00.054.968 I print_info: rope type        = 2
0.00.054.968 I print_info: rope scaling     = linear
0.00.054.968 I print_info: freq_base_train  = 10000.0
0.00.054.968 I print_info: freq_scale_train = 1
0.00.054.969 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.969 I print_info: rope_finetuned   = unknown
0.00.054.969 I print_info: ssm_d_conv       = 0
0.00.054.969 I print_info: ssm_d_inner      = 0
0.00.054.969 I print_info: ssm_d_state      = 0
0.00.054.969 I print_info: ssm_dt_rank      = 0
0.00.054.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.970 I print_info: model type       = 1.4B
0.00.054.970 I print_info: model params     = 1.41 B
0.00.054.970 I print_info: general.name     = 1.4B
0.00.054.971 I print_info: vocab type       = BPE
0.00.054.971 I print_info: n_vocab          = 50304
0.00.054.971 I print_info: n_merges         = 50009
0.00.054.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.975 I print_info: LF token         = 187 'Ċ'
0.00.054.975 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.976 I print_info: max token length = 1024
0.00.054.976 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.468.277 I load_tensors: offloading 24 repeating layers to GPU
0.01.468.287 I load_tensors: offloading output layer to GPU
0.01.468.288 I load_tensors: offloaded 25/25 layers to GPU
0.01.468.312 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.468.314 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.469.303 I llama_init_from_model: n_seq_max     = 1
0.01.469.305 I llama_init_from_model: n_ctx         = 128
0.01.469.306 I llama_init_from_model: n_ctx_per_seq = 128
0.01.469.306 I llama_init_from_model: n_batch       = 128
0.01.469.306 I llama_init_from_model: n_ubatch      = 128
0.01.469.306 I llama_init_from_model: flash_attn    = 0
0.01.469.307 I llama_init_from_model: freq_base     = 10000.0
0.01.469.308 I llama_init_from_model: freq_scale    = 1
0.01.469.308 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.469.309 I ggml_metal_init: allocating
0.01.469.371 I ggml_metal_init: found device: Apple M4
0.01.469.379 I ggml_metal_init: picking default device: Apple M4
0.01.470.521 I ggml_metal_init: using embedded metal library
0.01.474.470 I ggml_metal_init: GPU name:   Apple M4
0.01.474.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.474.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.474.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.474.473 I ggml_metal_init: simdgroup reduction   = true
0.01.474.474 I ggml_metal_init: simdgroup matrix mul. = true
0.01.474.474 I ggml_metal_init: has residency sets    = true
0.01.474.474 I ggml_metal_init: has bfloat            = true
0.01.474.474 I ggml_metal_init: use bfloat            = true
0.01.474.475 I ggml_metal_init: hasUnifiedMemory      = true
0.01.474.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.486.306 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.488.013 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.488.016 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.488.044 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.489.889 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.489.891 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.489.892 I llama_init_from_model: graph nodes  = 967
0.01.489.892 I llama_init_from_model: graph splits = 2
0.01.489.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.489.893 I 
0.01.489.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.489.940 I compute_imatrix: tokenizing the input ..
0.01.494.238 I compute_imatrix: tokenization took 4.297 ms
0.01.494.240 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.757.377 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.760.063 I llama_perf_context_print:        load time =    1738.76 ms
0.01.760.064 I llama_perf_context_print: prompt eval time =     261.35 ms /   128 tokens (    2.04 ms per token,   489.77 tokens per second)
0.01.760.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.760.066 I llama_perf_context_print:       total time =    1741.44 ms /   129 tokens
0.01.760.666 I ggml_metal_free: deallocating

real	0m1.944s
user	0m0.107s
sys	0m0.237s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4659 (225bbbfa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ee05060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ee08230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ee087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ee08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ee09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ee098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ee09ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ee0a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ee0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ee0af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ee0b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ee0b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ee0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ee0cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ee0d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ee0db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ee0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ee0e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ee0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ee0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ee0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ee10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ee10d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ee11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ee11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ee12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ee12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ee13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ee137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ee13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ee13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ee141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ee14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ee14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ee15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ee15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ee15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ee16060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ee16500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ee169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ee16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ee172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ee17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ee17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ee17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ee184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ee18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ee19420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ee19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ee1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ee1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ee1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ee1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ee1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ee1c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ee1c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ee1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ee1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ee1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ee1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ee1dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ee1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ee1e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ee1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ee1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ee1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ee1f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ee1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ee20230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ee206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ee20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ee21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ee214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ee21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ee21f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ee224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ee229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ee22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ee23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ee239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ee23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ee24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ee249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ee24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ee25470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ee259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ee25f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ee26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ee269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ee26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ee27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ee279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ee27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ee28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ee28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ee28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ee29430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ee19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ee298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ee04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ee046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ee04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ee04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ee053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ee05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ee05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ee06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ee065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ee06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ee06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ee07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ee07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ee07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ee08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ee084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ee08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ee08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ee09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ee09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ee09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ee09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ee0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ee0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ee0b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ee0b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ee0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ee0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ee0c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ee0c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ee0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ee0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ee0d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ee0d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ee0ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ee0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ee0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ee0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ee0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ee0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ee0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ee0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ee10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ee105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ee10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ee10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ee112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ee11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ee11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ee12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ee124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ee12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ee12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ee13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ee13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ee13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ee13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ee143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ee14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ee14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ee15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ee15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ee159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ee15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ee162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ee16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ee16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ee17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ee17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ee17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ee17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ee181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ee18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ee18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ee18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ee193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ee19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ee19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ee1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ee1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ee1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ee1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ee1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ee1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ee1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ee1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ee1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ee1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ee1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ee1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ee1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ee1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ee1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ee1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ee1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ee1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ee1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ee1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ee1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ee1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ee20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ee20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ee20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ee20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ee21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ee21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ee21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ee22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ee226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ee22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ee23110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ee23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ee23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ee24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ee24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ee25080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ee25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ee25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ee261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ee26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ee26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ee27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ee278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ee27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ee28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ee28a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ee28fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ee29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ee29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ee2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ee2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ee2ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ee2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ee2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ee2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ee2b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ee2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ee2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ee2c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ee2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ee2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ee2d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ee2d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ee2dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ee2e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ee2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ee2eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ee2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ee2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ee2faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ee2fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ee30540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ee30a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ee30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ee31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ee31a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ee31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ee32520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ee32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ee32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ee33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ee33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ee33fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ee34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ee34a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ee34fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ee354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ee35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ee35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ee36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ee36820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ee36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ee37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ee37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ee37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ee37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ee383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ee38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ee38d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ee391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ee39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ee39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ee39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ee3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ee3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ee3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ee3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ee3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ee3c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ee3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ee3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ee3d4f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.746.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ee084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ee08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ee1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ee3d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ee1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ee1cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ee122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ee18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ee196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ee19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ee187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ee181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ee1a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ee0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ee144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ee14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ee29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ee128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ee12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ee12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ee3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ee3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ee3ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ee3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ee3e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ee3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ee3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ee3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ee3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ee3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ee3f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ee3f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ee3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ee3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ee3ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ee40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ee40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ee40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ee40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ee40d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ee41050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ee41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ee415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ee41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ee41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ee41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ee420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ee42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ee42650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ee42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ee42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ee42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ee43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ee43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ee436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ee43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ee43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ee43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ee441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ee44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ee44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ee44a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ee44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ee44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ee45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ee45510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ee457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ee45a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ee45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ee46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ee462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ee46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ee46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ee46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ee46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ee47090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ee47350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ee47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ee478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ee47b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ee480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ee48630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ee48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ee490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ee49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ee49b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ee4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ee4a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ee4ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ee4b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ee4b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ee4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ee4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ee4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ee4cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ee4d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ee4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ee4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ee4e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ee4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ee4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ee4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ee4f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ee4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ee50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ee505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ee50b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ee51050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ee515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ee51af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ee52040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ee52590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ee52ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ee52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ee53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ee538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ee53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ee54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ee546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ee54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ee54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ee55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ee55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ee55dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ee56260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ee56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ee56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ee57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ee574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ee57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ee57e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ee582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ee58760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ee58c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ee590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ee59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ee599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ee59e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ee5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ee5a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ee5ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ee5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ee5b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ee5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ee5bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ee5c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ee5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ee5ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ee5d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ee5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ee5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ee5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ee5e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ee5e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ee5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ee5f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ee5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ee5fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ee60090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ee605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ee60ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ee60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ee614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ee619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ee61ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ee62400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ee62910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ee62d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ee631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ee63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ee63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ee63f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ee643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ee64820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ee64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ee65100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ee65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ee659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ee65e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ee662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ee66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ee66ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ee67010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ee67480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ee678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ee67d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ee681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ee68640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ee68ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ee68f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ee69390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ee69800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ee69c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ee6a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ee6a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ee6a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ee6ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ee6b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ee6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ee6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ee6bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ee6c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ee6c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ee6cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ee6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ee6d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ee6da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ee6e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ee6e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ee6e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ee6f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ee6f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ee6f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ee6fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ee702a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ee70710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ee70b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ee70ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ee71460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ee718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ee71d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ee721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ee72620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ee72a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ee72f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ee73370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ee737e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ee73c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ee740c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ee74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ee749a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ee74e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ee75280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ee756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ee75b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ee75fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ee76440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ee768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ee76d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ee77190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ee77600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ee77a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ee77ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ee78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ee787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ee78c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ee790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ee79510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ee79980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ee79df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ee7a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ee7a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ee7ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ee7afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ee7b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ee7b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ee7bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ee7c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ee7c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ee7ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ee7cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ee7d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ee7d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ee7dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ee7e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ee7e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ee7e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ee7edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ee7f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ee7f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ee7fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ee7ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ee80400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ee80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ee80ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ee81150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ee815c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ee81a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ee81ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ee82310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ee82780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ee82bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ee83060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ee83ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ee841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ee84910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ee85030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ee852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ee85760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ee85d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ee86370 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f8046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f8058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f8065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f807a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f808580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f808d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f809540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f80a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f80aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f80b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f80b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f80c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f80c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f80ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f80d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f80dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f80df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f80e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f80e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f80eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f80ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f80f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f80f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f80fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f810030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f8104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f810910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f810d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f8111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f811660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f811ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f811f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f8123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f812820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f812c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f813100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f813570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f8139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f813e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f8142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f814730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f814ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f815010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f815480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f8158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f815d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f8161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f816740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f816c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f8170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f817520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f817990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f817e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f8186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f818b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f818fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f819430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f8198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f819d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f81a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f81a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f81aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f81aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f81b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f81b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f81bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f81c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f81c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f81c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f81cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f81d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f81d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f81db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f81dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f81e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f81e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f81ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f81f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f81f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f81fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f81feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f820320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f820790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f820c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f821070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f8214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f821950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f821dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f822230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f8226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f822b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f822f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f8233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f823c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f823f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f8243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f824820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f824c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f825100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f825570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f8259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f825e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f8262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f826730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f826ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f827010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f827480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f8278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f827d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f8281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f828640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f828ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f828f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f829390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f829800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f829c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f82a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f82a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f82a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f82ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f82b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f82b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f82bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f82bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f82c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f82c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f82cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f82d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f82d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f82da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f82df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f82e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f82e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f82ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f82f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f82f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f82f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f82fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f830280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f8306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f830b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f830fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f831440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f8318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f831d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f832190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f832600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f832a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f832ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f833350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f8337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f833c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f8340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f834510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f834980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f834df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f835260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f8356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f835b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f835fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f836420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f836d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f837170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f8375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f837a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f837ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f838330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f8387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f838c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f839080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f8394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f839960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f839dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f83a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f83a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f83ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f83af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f83b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f83b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f83bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f83c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f83c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f83ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f83cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f83d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f83d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f83dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f83e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f83e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f83e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f83edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f83f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f83f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f83fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f83ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f8403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f840850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f840cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f841130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f841cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f841f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f842230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f8426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f842b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f842f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f8433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f843860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f843cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f844140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f8445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f844a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f844e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f845300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f845770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f845be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f846050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f8464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f846930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f846da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f847210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f847680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f847af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f847f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f8483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f848840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f848cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f849120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f849590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f849a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f849e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f84a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f84a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f84abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f84b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f84b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f84b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f84bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f84c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f84c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f84cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f84cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f84d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f84d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f84dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f84e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f84e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f84e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f84ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f84f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f84f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f84fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f850010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f850480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f8508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f850d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f8511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f851640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f851ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f851f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f852390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f852800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f852c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f8530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f853550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f8539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f853e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f8542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f854710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f854b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f854ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f855460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f8558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f856340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f856a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f857180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f8578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f857b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f857fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f8585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f858be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.792s
user	0m0.280s
sys	0m0.334s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4659 (225bbbfa)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f70b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f70b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f70bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f70c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f70c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f70cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f70d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f70d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f70df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f70e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f70e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f70ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f70f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f7100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f7108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f711000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f711720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f711e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f712560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f713b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f714290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f714b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f715250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f715510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f715b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f716790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f716cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f717430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f7176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f7184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f718c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f7190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f719560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f719a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f719ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f71a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f71a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f71ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f71b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f71b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f71b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f71c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f71c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f71cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f71d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f71db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f71e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f71e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f71ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f71f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f71fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f71feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f720170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f720780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f720f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f7216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f7224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f722950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f723290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f723730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f723bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f724070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f724510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f7249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f724f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f7259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f725ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f726440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f726990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f726ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f727430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f727980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f727ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f728970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f728ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f729410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f729960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f729eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f72a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f72a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f72aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f72b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f72b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f72be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f72c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f72c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f71c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f72cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f72d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f72daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f72dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f72e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f72ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f72efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f72f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f72fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f72ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f730520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f730a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f731510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f731a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f731f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f7323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f732840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f733620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f733ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f733f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f734400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f7348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f734d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f7351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f735680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f735b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f735fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f736900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f736da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f737240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f7376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f737b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f738020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f7384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f738960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f7392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f739be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f73a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f73a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f73a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f73ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f73b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f73b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f73bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f73c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f73c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f73ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f73cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f73d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f73d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f73dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f73e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f73e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f73ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f73ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f73f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f73f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f73fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f7401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f740640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f740ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f740f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f741420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f7418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f741d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f742200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f7426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f742b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f742fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f743480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f743920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f743dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f744260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f744700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f744ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f745040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f7454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f745980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f745e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f7462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f746760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f746c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f7470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f747540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f7479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f747e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f748320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f7487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f748c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f7491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f749700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f749c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f74a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f74a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f74aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f74b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f74b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f74be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f74c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f74c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f74cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f74d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f74d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f74de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f74e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f74e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f74ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f74f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f74fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f74ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f7504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f750a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f750f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f7514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f751a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f751f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f7524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f7529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f752f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f753490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f7539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f753f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f754480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f7549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f754f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f755470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f7559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f755f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f756460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f7569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f756f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f757450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f7579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f757ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f758440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f758990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f758ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f759430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f759980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f759ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f75a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f75a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f75aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f75b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f75b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f75beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f75c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f75c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f75cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f75d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f75d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f75de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f75e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f75e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f75ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f75f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f75f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f75fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f7603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f760910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f760e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f7613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f761900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f761da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f762240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f7626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f762b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f763020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f7634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f763960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f763e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f7642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f764740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f764be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f765080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f765520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f7659c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f765e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f7663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f766ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f7671f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f767910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f768030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f7682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f768ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f768da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f7693b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f704d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f7051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f705630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f705aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f705f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f706380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f7067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f706c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f7070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f707540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f7079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f7080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f708bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f709370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f709b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f70a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f70a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f70b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f70b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f70bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f70c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f70cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f70d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f70dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f70e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f70e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f70e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f70ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f70f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f70f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f70fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f70ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f7103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f710670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f710ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f710f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f7113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f711830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f711ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f712110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f7129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f712e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f7132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f713740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f713bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f714020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f714490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f714900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f714d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f7151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f715650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f715f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f7163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f716810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f716d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f717280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f7176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f717b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f718440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f7188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f718d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f719190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f719600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f719a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f719ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f71a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f71a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f71ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f71b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f71b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f71b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f71bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f71c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f71c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f71cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f71cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f71d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f71d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f71dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f71e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f71e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f71ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f71eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f71f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f71f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f71fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f720080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f7204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f720960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f720dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f7216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f721b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f721f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f722400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f722870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f722ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f723150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f7235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f723a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f723ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f724310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f724780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f724bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f725060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f7254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f725940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f725db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f726690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f726b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f726f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f7273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f727850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f727cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f728130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f7285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f728a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f728e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f7292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f729760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f729bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f72a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f72a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f72a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f72ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f72b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f72b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f72bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f72bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f72c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f72c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f72cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f72d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f72d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f72d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f72de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f72e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f72e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f72ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f72f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f72f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f72f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f72fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f7301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f730650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f730ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f730f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f7313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f731c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f7320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f732560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f7329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f732e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f7332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f733720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f733b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f734000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f734470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f7348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f7351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f735df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f7360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f736370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f7367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f736c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f7370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f737530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f7379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f737e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f738280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f7386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f738b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f738fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f739440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f7398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f739d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f73a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f73a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f73aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f73aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f73b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f73b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f73bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f73c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f73c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f73c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f73cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f73d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f73d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f73db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f73dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f73e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f73e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f73ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f73f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f73f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f73fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f7404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f740930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f741210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f741730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f741c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f7427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f743030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f7435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f744170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f744cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f7452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f745870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f7463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f7469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f746f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f747530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f7480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f748670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f748c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f7491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f7497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f749d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f74a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f74a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f74aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f74b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f74ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f74bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f74c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f74cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f74d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f74d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f74dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f74e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f74edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f74f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f74f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f74ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f7504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f750ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f751070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f751630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f7521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f752770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f752d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f7532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f7538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f753e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f754430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f7549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f754fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f755570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f755b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f7566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f756c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f757670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f757b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f758070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f758570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f758a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f758f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f759470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f759970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f75a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f75a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f75ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f75b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f75b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f75c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f75c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f75cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f75d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f75d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f75e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f75e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f75ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f606430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f6068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f606d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f607180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f6075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f607a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f607ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f608340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f6087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f608c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f609090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f6097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f60a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f60aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f60b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f60b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f60c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f60c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f60cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f60d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f60dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f60e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f60ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f60f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f60f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f60fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f60ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f6103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f610850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f610cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f611130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f611660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f611ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f611d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f612200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f612670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f612ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f612f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f6133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f613830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f613ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f614110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f614580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f6149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f614e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f6152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f615740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f615bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f616490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f616900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f616d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f6171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f617ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f6184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f6189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f618e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f619280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f6196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f619b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f619fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f61a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f61a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f61ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f61b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f61b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f61ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f61bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f61c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f61c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f61cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f61d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f61d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f61d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f61ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f61e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f61e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f61eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f61efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f61f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f61f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f61fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f620170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f6205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f620a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f620ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f621330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f6217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f621c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f622080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f6224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f622960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f622dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f623240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f6236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f623b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f623f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f624400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f624870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f624ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f625150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f6259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f626110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f626580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f6269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f626e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f6272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f627740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f627bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f628020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f628490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f628d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f6291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f629650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f629f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f62a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f62a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f62ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f62b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f62b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f62b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f62be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f62c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f62c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f62cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f62d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f62d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f62d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f62dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f62e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f62e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f62eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f62ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f62f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f62f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f62fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f6300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f630540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f6309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f631290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f631700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f631b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f631fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f632450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f6328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f632d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f6331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f633a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f633ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f634360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f6347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f634c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f6350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f635520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f635990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f635e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f6366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f636b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f636fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f637430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f6378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f637d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f638180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f6385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f638a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f638ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f6397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f639c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f63a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f63a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f63a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f63ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f63b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f63b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f63bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f63bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f63c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f63c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f63ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f63d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f63d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f63da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f63deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f63e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f63e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f63ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f63f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f63f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f63f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f63fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f640230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f6406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f640b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f640f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f6413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f641cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f642140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f6425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f642a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f642e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f643cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f644870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f644ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f645150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f6455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f645a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f645ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f646310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f646780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f647060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f6474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f647940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f647db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f648220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f648690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f648b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f648f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f6493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f649850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f649cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f64a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f64a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f64aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f64ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f64b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f64b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f64bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f64c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f64c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f64c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f64cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f64d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f64d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f64dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f64df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f64e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f64e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f64eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f64f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f64f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f64f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f64fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f6502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f650740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f650bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f651020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f651490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f651900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f651d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f6521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f652650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f652ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f652f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f6533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f653c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f6540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f654560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f6549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f654e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f6552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f655720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f655b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f656000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f656470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f6568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f656d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f6571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f6580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f6587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f658ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f659600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f6598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f659d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f65a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f65a940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.957s
user	0m0.235s
sys	0m0.183s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
