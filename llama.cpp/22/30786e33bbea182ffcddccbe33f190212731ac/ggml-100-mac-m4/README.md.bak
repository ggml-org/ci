### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.48 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.31 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.35 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.23 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  177.23 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.95 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   26.14 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.39 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 220.33 sec*proc (27 tests)

Total Test time (real) = 220.34 sec

real	3m40.370s
user	7m28.593s
sys	0m6.332s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.23 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.37 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.24 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.29 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.48 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.05 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.23 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.60 sec*proc (27 tests)

Total Test time (real) =  51.61 sec

real	0m51.622s
user	1m11.224s
sys	0m5.656s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.168 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.636 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.840 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.850 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.851 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.852 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.853 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.854 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.855 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.855 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.856 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.857 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.860 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.860 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.861 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.862 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.862 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.863 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.863 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.118 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.120 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.121 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.121 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.122 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.122 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.123 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.123 I llama_model_loader: - type  f32:  124 tensors
0.00.032.124 I llama_model_loader: - type  f16:   73 tensors
0.00.036.545 I llm_load_vocab: special tokens cache size = 5
0.00.038.690 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.694 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.694 I llm_load_print_meta: arch             = bert
0.00.038.695 I llm_load_print_meta: vocab type       = WPM
0.00.038.695 I llm_load_print_meta: n_vocab          = 30522
0.00.038.695 I llm_load_print_meta: n_merges         = 0
0.00.038.696 I llm_load_print_meta: vocab_only       = 0
0.00.038.696 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.696 I llm_load_print_meta: n_embd           = 384
0.00.038.696 I llm_load_print_meta: n_layer          = 12
0.00.038.713 I llm_load_print_meta: n_head           = 12
0.00.038.717 I llm_load_print_meta: n_head_kv        = 12
0.00.038.718 I llm_load_print_meta: n_rot            = 32
0.00.038.718 I llm_load_print_meta: n_swa            = 0
0.00.038.718 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.718 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.719 I llm_load_print_meta: n_gqa            = 1
0.00.038.720 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.726 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.727 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.728 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.729 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.729 I llm_load_print_meta: n_ff             = 1536
0.00.038.730 I llm_load_print_meta: n_expert         = 0
0.00.038.730 I llm_load_print_meta: n_expert_used    = 0
0.00.038.730 I llm_load_print_meta: causal attn      = 0
0.00.038.730 I llm_load_print_meta: pooling type     = 2
0.00.038.731 I llm_load_print_meta: rope type        = 2
0.00.038.731 I llm_load_print_meta: rope scaling     = linear
0.00.038.731 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.732 I llm_load_print_meta: freq_scale_train = 1
0.00.038.732 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.732 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.735 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.735 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.735 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.735 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.735 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.736 I llm_load_print_meta: model type       = 33M
0.00.038.736 I llm_load_print_meta: model ftype      = F16
0.00.038.737 I llm_load_print_meta: model params     = 33.21 M
0.00.038.738 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.738 I llm_load_print_meta: general.name     = Bge Small
0.00.038.739 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.739 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.739 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.739 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.740 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.740 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.741 I llm_load_print_meta: max token length = 21
0.00.040.906 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.040.906 I llm_load_tensors: offloading output layer to GPU
0.00.040.911 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.040.939 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.941 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.502 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.503 I llama_new_context_with_model: n_ctx         = 512
0.00.041.503 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.504 I llama_new_context_with_model: n_batch       = 2048
0.00.041.504 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.504 I llama_new_context_with_model: flash_attn    = 0
0.00.041.505 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.505 I llama_new_context_with_model: freq_scale    = 1
0.00.041.506 I ggml_metal_init: allocating
0.00.041.510 I ggml_metal_init: found device: Apple M4
0.00.041.513 I ggml_metal_init: picking default device: Apple M4
0.00.042.414 I ggml_metal_init: using embedded metal library
0.00.046.743 I ggml_metal_init: GPU name:   Apple M4
0.00.046.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.747 I ggml_metal_init: simdgroup reduction   = true
0.00.046.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.747 I ggml_metal_init: has bfloat            = true
0.00.046.747 I ggml_metal_init: use bfloat            = true
0.00.046.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.749 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.654 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.656 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.657 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.060.445 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.060.446 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.060.447 I llama_new_context_with_model: graph nodes  = 429
0.00.060.447 I llama_new_context_with_model: graph splits = 2
0.00.060.468 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.993 I 
0.00.067.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.687 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.432 I llama_perf_context_print:        load time =      45.35 ms
0.00.072.434 I llama_perf_context_print: prompt eval time =       4.60 ms /     9 tokens (    0.51 ms per token,  1956.10 tokens per second)
0.00.072.435 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.435 I llama_perf_context_print:       total time =       5.44 ms /    10 tokens
0.00.072.566 I ggml_metal_free: deallocating

real	0m0.273s
user	0m0.050s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.032 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.229 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.196 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.199 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.200 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.201 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.201 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.202 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.202 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.203 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.203 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.203 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.204 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.206 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.206 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.207 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.207 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.207 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.207 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.208 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.219 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.221 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.221 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.221 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.222 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.222 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.222 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.222 I llama_model_loader: - type  f32:  124 tensors
0.00.014.223 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.577 I llm_load_vocab: special tokens cache size = 5
0.00.017.819 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.822 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.822 I llm_load_print_meta: arch             = bert
0.00.017.823 I llm_load_print_meta: vocab type       = WPM
0.00.017.823 I llm_load_print_meta: n_vocab          = 30522
0.00.017.823 I llm_load_print_meta: n_merges         = 0
0.00.017.823 I llm_load_print_meta: vocab_only       = 0
0.00.017.823 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.824 I llm_load_print_meta: n_embd           = 384
0.00.017.824 I llm_load_print_meta: n_layer          = 12
0.00.017.833 I llm_load_print_meta: n_head           = 12
0.00.017.833 I llm_load_print_meta: n_head_kv        = 12
0.00.017.834 I llm_load_print_meta: n_rot            = 32
0.00.017.834 I llm_load_print_meta: n_swa            = 0
0.00.017.834 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.836 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.837 I llm_load_print_meta: n_gqa            = 1
0.00.017.837 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.838 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.838 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.839 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.839 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.839 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.839 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.840 I llm_load_print_meta: n_ff             = 1536
0.00.017.840 I llm_load_print_meta: n_expert         = 0
0.00.017.840 I llm_load_print_meta: n_expert_used    = 0
0.00.017.840 I llm_load_print_meta: causal attn      = 0
0.00.017.840 I llm_load_print_meta: pooling type     = 2
0.00.017.840 I llm_load_print_meta: rope type        = 2
0.00.017.841 I llm_load_print_meta: rope scaling     = linear
0.00.017.841 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.841 I llm_load_print_meta: freq_scale_train = 1
0.00.017.841 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.841 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.842 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.842 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.842 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.842 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.842 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.842 I llm_load_print_meta: model type       = 33M
0.00.017.842 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.843 I llm_load_print_meta: model params     = 33.21 M
0.00.017.843 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.843 I llm_load_print_meta: general.name     = Bge Small
0.00.017.844 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.844 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.844 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.844 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.844 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.845 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.845 I llm_load_print_meta: max token length = 21
0.00.019.170 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.170 I llm_load_tensors: offloading output layer to GPU
0.00.019.170 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.178 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.179 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.534 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.535 I llama_new_context_with_model: n_ctx         = 512
0.00.019.535 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.535 I llama_new_context_with_model: n_batch       = 2048
0.00.019.535 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.536 I llama_new_context_with_model: flash_attn    = 0
0.00.019.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.536 I llama_new_context_with_model: freq_scale    = 1
0.00.019.537 I ggml_metal_init: allocating
0.00.019.540 I ggml_metal_init: found device: Apple M4
0.00.019.542 I ggml_metal_init: picking default device: Apple M4
0.00.020.139 I ggml_metal_init: using embedded metal library
0.00.022.479 I ggml_metal_init: GPU name:   Apple M4
0.00.022.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.481 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.482 I ggml_metal_init: simdgroup reduction   = true
0.00.022.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.482 I ggml_metal_init: has bfloat            = true
0.00.022.483 I ggml_metal_init: use bfloat            = true
0.00.022.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.276 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.278 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.280 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.865 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.867 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.867 I llama_new_context_with_model: graph nodes  = 429
0.00.033.867 I llama_new_context_with_model: graph splits = 2
0.00.033.875 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.175 I 
0.00.038.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.744 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.962 I llama_perf_context_print:        load time =      28.94 ms
0.00.042.963 I llama_perf_context_print: prompt eval time =       4.08 ms /     9 tokens (    0.45 ms per token,  2204.80 tokens per second)
0.00.042.964 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.965 I llama_perf_context_print:       total time =       4.79 ms /    10 tokens
0.00.043.163 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.142 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.513 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.963 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.968 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.971 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.973 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.974 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.975 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.976 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.977 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.980 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.981 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.981 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.984 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.985 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.986 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.769 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.989 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.990 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.991 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.991 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.991 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.992 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.992 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.992 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.993 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.993 I llama_model_loader: - type  f32:   41 tensors
0.00.050.993 I llama_model_loader: - type  f16:   29 tensors
0.00.069.486 W llm_load_vocab: empty token at index 5
0.00.074.126 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.372 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.400 I llm_load_vocab: special tokens cache size = 5
0.00.334.814 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.334.821 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.334.821 I llm_load_print_meta: arch             = jina-bert-v2
0.00.334.823 I llm_load_print_meta: vocab type       = BPE
0.00.334.823 I llm_load_print_meta: n_vocab          = 61056
0.00.334.824 I llm_load_print_meta: n_merges         = 39382
0.00.334.824 I llm_load_print_meta: vocab_only       = 0
0.00.334.824 I llm_load_print_meta: n_ctx_train      = 8192
0.00.334.824 I llm_load_print_meta: n_embd           = 384
0.00.334.824 I llm_load_print_meta: n_layer          = 4
0.00.334.864 I llm_load_print_meta: n_head           = 12
0.00.334.866 I llm_load_print_meta: n_head_kv        = 12
0.00.334.866 I llm_load_print_meta: n_rot            = 32
0.00.334.866 I llm_load_print_meta: n_swa            = 0
0.00.334.866 I llm_load_print_meta: n_embd_head_k    = 32
0.00.334.866 I llm_load_print_meta: n_embd_head_v    = 32
0.00.334.867 I llm_load_print_meta: n_gqa            = 1
0.00.334.868 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.334.868 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.334.869 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.334.870 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.334.870 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.334.870 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.334.870 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.334.871 I llm_load_print_meta: n_ff             = 1536
0.00.334.871 I llm_load_print_meta: n_expert         = 0
0.00.334.871 I llm_load_print_meta: n_expert_used    = 0
0.00.334.872 I llm_load_print_meta: causal attn      = 0
0.00.334.872 I llm_load_print_meta: pooling type     = -1
0.00.334.872 I llm_load_print_meta: rope type        = -1
0.00.334.872 I llm_load_print_meta: rope scaling     = linear
0.00.334.873 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.334.873 I llm_load_print_meta: freq_scale_train = 1
0.00.334.873 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.334.874 I llm_load_print_meta: rope_finetuned   = unknown
0.00.334.874 I llm_load_print_meta: ssm_d_conv       = 0
0.00.334.874 I llm_load_print_meta: ssm_d_inner      = 0
0.00.334.874 I llm_load_print_meta: ssm_d_state      = 0
0.00.334.874 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.334.875 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.334.875 I llm_load_print_meta: model type       = 33M
0.00.334.876 I llm_load_print_meta: model ftype      = F16
0.00.334.876 I llm_load_print_meta: model params     = 32.90 M
0.00.334.877 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.334.877 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.334.878 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.334.878 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.334.878 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.334.878 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.334.878 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.334.879 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.334.879 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.334.879 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.334.879 I llm_load_print_meta: max token length = 45
0.00.336.296 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.336.296 I llm_load_tensors: offloading output layer to GPU
0.00.336.296 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.336.322 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.323 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.266 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.267 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.267 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.267 I llama_new_context_with_model: n_batch       = 2048
0.00.337.267 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.268 I llama_new_context_with_model: flash_attn    = 0
0.00.337.268 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.268 I llama_new_context_with_model: freq_scale    = 1
0.00.337.269 I ggml_metal_init: allocating
0.00.337.278 I ggml_metal_init: found device: Apple M4
0.00.337.281 I ggml_metal_init: picking default device: Apple M4
0.00.338.072 I ggml_metal_init: using embedded metal library
0.00.340.912 I ggml_metal_init: GPU name:   Apple M4
0.00.340.914 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.340.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.340.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.340.915 I ggml_metal_init: simdgroup reduction   = true
0.00.340.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.340.915 I ggml_metal_init: has bfloat            = true
0.00.340.915 I ggml_metal_init: use bfloat            = true
0.00.340.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.340.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.003 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.353.008 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.353.010 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.353.550 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.353.551 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.353.551 I llama_new_context_with_model: graph nodes  = 154
0.00.353.551 I llama_new_context_with_model: graph splits = 2
0.00.353.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.759 I 
0.00.365.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.950 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.951 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.957 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.957 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.961 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.961 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.528 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.364 I llama_perf_context_print:        load time =     341.24 ms
0.00.370.365 I llama_perf_context_print: prompt eval time =       3.83 ms /    62 tokens (    0.06 ms per token, 16196.45 tokens per second)
0.00.370.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.366 I llama_perf_context_print:       total time =       4.61 ms /    63 tokens
0.00.370.549 I ggml_metal_free: deallocating

real	0m1.068s
user	0m0.341s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.072 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.160 I main: llama backend init
0.00.000.165 I main: load the model and apply lora adapter, if any
0.00.049.383 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.060.905 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.940 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.941 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.947 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.077.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.612 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.614 I llama_model_loader: - type  f32:  194 tensors
0.00.077.615 I llama_model_loader: - type  f16:   98 tensors
0.00.106.240 I llm_load_vocab: special tokens cache size = 25
0.00.115.447 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.115.451 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.115.451 I llm_load_print_meta: arch             = gptneox
0.00.115.452 I llm_load_print_meta: vocab type       = BPE
0.00.115.452 I llm_load_print_meta: n_vocab          = 50304
0.00.115.452 I llm_load_print_meta: n_merges         = 50009
0.00.115.452 I llm_load_print_meta: vocab_only       = 0
0.00.115.453 I llm_load_print_meta: n_ctx_train      = 2048
0.00.115.453 I llm_load_print_meta: n_embd           = 2048
0.00.115.453 I llm_load_print_meta: n_layer          = 24
0.00.115.468 I llm_load_print_meta: n_head           = 16
0.00.115.468 I llm_load_print_meta: n_head_kv        = 16
0.00.115.469 I llm_load_print_meta: n_rot            = 32
0.00.115.469 I llm_load_print_meta: n_swa            = 0
0.00.115.472 I llm_load_print_meta: n_embd_head_k    = 128
0.00.115.472 I llm_load_print_meta: n_embd_head_v    = 128
0.00.115.473 I llm_load_print_meta: n_gqa            = 1
0.00.115.474 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.115.474 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.115.475 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.115.475 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.115.476 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.115.477 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.115.478 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.115.479 I llm_load_print_meta: n_ff             = 8192
0.00.115.479 I llm_load_print_meta: n_expert         = 0
0.00.115.480 I llm_load_print_meta: n_expert_used    = 0
0.00.115.480 I llm_load_print_meta: causal attn      = 1
0.00.115.485 I llm_load_print_meta: pooling type     = 0
0.00.115.485 I llm_load_print_meta: rope type        = 2
0.00.115.485 I llm_load_print_meta: rope scaling     = linear
0.00.115.486 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.115.486 I llm_load_print_meta: freq_scale_train = 1
0.00.115.488 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.115.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.115.488 I llm_load_print_meta: ssm_d_conv       = 0
0.00.115.488 I llm_load_print_meta: ssm_d_inner      = 0
0.00.115.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.115.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.115.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.115.489 I llm_load_print_meta: model type       = 1.4B
0.00.115.490 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.115.490 I llm_load_print_meta: model params     = 1.41 B
0.00.115.491 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.115.491 I llm_load_print_meta: general.name     = 1.4B
0.00.115.491 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.115.492 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.115.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.115.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.115.492 I llm_load_print_meta: LF token         = 128 ''
0.00.115.493 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.115.493 I llm_load_print_meta: max token length = 1024
0.00.117.711 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.117.711 I llm_load_tensors: offloading output layer to GPU
0.00.117.711 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.117.731 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.117.733 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.118.799 I llama_new_context_with_model: n_seq_max     = 1
0.00.118.801 I llama_new_context_with_model: n_ctx         = 2048
0.00.118.801 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.118.801 I llama_new_context_with_model: n_batch       = 2048
0.00.118.801 I llama_new_context_with_model: n_ubatch      = 512
0.00.118.802 I llama_new_context_with_model: flash_attn    = 0
0.00.118.802 I llama_new_context_with_model: freq_base     = 10000.0
0.00.118.803 I llama_new_context_with_model: freq_scale    = 1
0.00.118.803 I ggml_metal_init: allocating
0.00.118.807 I ggml_metal_init: found device: Apple M4
0.00.118.809 I ggml_metal_init: picking default device: Apple M4
0.00.119.572 I ggml_metal_init: using embedded metal library
0.00.129.469 I ggml_metal_init: GPU name:   Apple M4
0.00.129.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.129.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.129.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.129.474 I ggml_metal_init: simdgroup reduction   = true
0.00.129.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.129.474 I ggml_metal_init: has bfloat            = true
0.00.129.474 I ggml_metal_init: use bfloat            = true
0.00.129.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.129.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.118 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.174.124 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.174.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.175.084 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.175.085 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.175.086 I llama_new_context_with_model: graph nodes  = 967
0.00.175.086 I llama_new_context_with_model: graph splits = 2
0.00.175.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.175.252 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.175.253 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.253.590 I main: llama threadpool init, n_threads = 4
0.00.253.622 I 
0.00.253.659 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.253.660 I 
0.00.253.740 I sampler seed: 1234
0.00.253.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.253.780 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.253.782 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.253.782 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.101.610 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.02.101.610 I llama_perf_context_print:        load time =     204.19 ms
0.02.101.611 I llama_perf_context_print: prompt eval time =      43.97 ms /     7 tokens (    6.28 ms per token,   159.20 tokens per second)
0.02.101.612 I llama_perf_context_print:        eval time =    1800.92 ms /    63 runs   (   28.59 ms per token,    34.98 tokens per second)
0.02.101.612 I llama_perf_context_print:       total time =    1848.02 ms /    70 tokens
0.02.101.795 I ggml_metal_free: deallocating

real	0m2.466s
user	0m0.139s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.488 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.488 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.832 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.855 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.856 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.857 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.858 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.858 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.862 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.185 I llama_model_loader: - type  f32:  194 tensors
0.00.059.185 I llama_model_loader: - type  f16:   98 tensors
0.00.089.529 I llm_load_vocab: special tokens cache size = 25
0.00.096.598 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.601 I llm_load_print_meta: arch             = gptneox
0.00.096.601 I llm_load_print_meta: vocab type       = BPE
0.00.096.601 I llm_load_print_meta: n_vocab          = 50304
0.00.096.601 I llm_load_print_meta: n_merges         = 50009
0.00.096.601 I llm_load_print_meta: vocab_only       = 0
0.00.096.602 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.602 I llm_load_print_meta: n_embd           = 2048
0.00.096.602 I llm_load_print_meta: n_layer          = 24
0.00.096.616 I llm_load_print_meta: n_head           = 16
0.00.096.617 I llm_load_print_meta: n_head_kv        = 16
0.00.096.617 I llm_load_print_meta: n_rot            = 32
0.00.096.617 I llm_load_print_meta: n_swa            = 0
0.00.096.617 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.617 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.618 I llm_load_print_meta: n_gqa            = 1
0.00.096.618 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.619 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.619 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.620 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.620 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.620 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.620 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.621 I llm_load_print_meta: n_ff             = 8192
0.00.096.621 I llm_load_print_meta: n_expert         = 0
0.00.096.622 I llm_load_print_meta: n_expert_used    = 0
0.00.096.622 I llm_load_print_meta: causal attn      = 1
0.00.096.622 I llm_load_print_meta: pooling type     = 0
0.00.096.622 I llm_load_print_meta: rope type        = 2
0.00.096.622 I llm_load_print_meta: rope scaling     = linear
0.00.096.623 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.623 I llm_load_print_meta: freq_scale_train = 1
0.00.096.623 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.623 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.624 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.624 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.624 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.624 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.624 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.625 I llm_load_print_meta: model type       = 1.4B
0.00.096.625 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.096.626 I llm_load_print_meta: model params     = 1.41 B
0.00.096.626 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.096.626 I llm_load_print_meta: general.name     = 1.4B
0.00.096.627 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.627 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.627 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.627 I llm_load_print_meta: LF token         = 128 ''
0.00.096.628 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.628 I llm_load_print_meta: max token length = 1024
0.00.099.174 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.099.174 I llm_load_tensors: offloading output layer to GPU
0.00.099.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.099.185 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.186 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.100.102 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.103 I llama_new_context_with_model: n_ctx         = 128
0.00.100.103 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.100.103 I llama_new_context_with_model: n_batch       = 128
0.00.100.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.100.104 I llama_new_context_with_model: flash_attn    = 0
0.00.100.104 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.105 I llama_new_context_with_model: freq_scale    = 1
0.00.100.105 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.100.105 I ggml_metal_init: allocating
0.00.100.113 I ggml_metal_init: found device: Apple M4
0.00.100.115 I ggml_metal_init: picking default device: Apple M4
0.00.100.715 I ggml_metal_init: using embedded metal library
0.00.103.325 I ggml_metal_init: GPU name:   Apple M4
0.00.103.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.328 I ggml_metal_init: simdgroup reduction   = true
0.00.103.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.328 I ggml_metal_init: has bfloat            = true
0.00.103.328 I ggml_metal_init: use bfloat            = true
0.00.103.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.587 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.114.589 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.114.603 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.509 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.115.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.115.510 I llama_new_context_with_model: graph nodes  = 967
0.00.115.510 I llama_new_context_with_model: graph splits = 2
0.00.115.522 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.115.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.994.778 I 
0.00.994.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.994.935 I perplexity: tokenizing the input ..
0.01.006.904 I perplexity: tokenization took 11.967 ms
0.01.006.910 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.128.549 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.130.609 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.130.632 I llama_perf_context_print:        load time =     969.27 ms
0.01.130.634 I llama_perf_context_print: prompt eval time =     121.25 ms /   128 tokens (    0.95 ms per token,  1055.69 tokens per second)
0.01.130.636 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.130.637 I llama_perf_context_print:       total time =     135.86 ms /   129 tokens
0.01.131.594 I ggml_metal_free: deallocating

real	0m1.487s
user	0m0.128s
sys	0m0.194s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.816 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.818 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.819 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.769 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.882 I llama_model_loader: - type  f32:  194 tensors
0.00.032.883 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.034 I llm_load_vocab: special tokens cache size = 25
0.00.059.990 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.994 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.994 I llm_load_print_meta: arch             = gptneox
0.00.059.995 I llm_load_print_meta: vocab type       = BPE
0.00.059.995 I llm_load_print_meta: n_vocab          = 50304
0.00.059.995 I llm_load_print_meta: n_merges         = 50009
0.00.059.997 I llm_load_print_meta: vocab_only       = 0
0.00.059.997 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.998 I llm_load_print_meta: n_embd           = 2048
0.00.059.998 I llm_load_print_meta: n_layer          = 24
0.00.060.018 I llm_load_print_meta: n_head           = 16
0.00.060.019 I llm_load_print_meta: n_head_kv        = 16
0.00.060.019 I llm_load_print_meta: n_rot            = 32
0.00.060.020 I llm_load_print_meta: n_swa            = 0
0.00.060.020 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.020 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.020 I llm_load_print_meta: n_gqa            = 1
0.00.060.021 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.022 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.022 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.022 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.023 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.023 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.023 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.024 I llm_load_print_meta: n_ff             = 8192
0.00.060.024 I llm_load_print_meta: n_expert         = 0
0.00.060.024 I llm_load_print_meta: n_expert_used    = 0
0.00.060.024 I llm_load_print_meta: causal attn      = 1
0.00.060.024 I llm_load_print_meta: pooling type     = 0
0.00.060.024 I llm_load_print_meta: rope type        = 2
0.00.060.027 I llm_load_print_meta: rope scaling     = linear
0.00.060.027 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.027 I llm_load_print_meta: freq_scale_train = 1
0.00.060.028 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.028 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.028 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.028 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.028 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.028 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.028 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.029 I llm_load_print_meta: model type       = 1.4B
0.00.060.033 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.034 I llm_load_print_meta: model params     = 1.41 B
0.00.060.034 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.035 I llm_load_print_meta: general.name     = 1.4B
0.00.060.035 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.035 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.037 I llm_load_print_meta: LF token         = 128 ''
0.00.060.038 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.038 I llm_load_print_meta: max token length = 1024
0.00.062.478 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.478 I llm_load_tensors: offloading output layer to GPU
0.00.062.478 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.489 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.491 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.448 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.449 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.449 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.449 I llama_new_context_with_model: n_batch       = 2048
0.00.063.449 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.450 I llama_new_context_with_model: flash_attn    = 0
0.00.063.450 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.450 I llama_new_context_with_model: freq_scale    = 1
0.00.063.451 I ggml_metal_init: allocating
0.00.063.456 I ggml_metal_init: found device: Apple M4
0.00.063.458 I ggml_metal_init: picking default device: Apple M4
0.00.064.168 I ggml_metal_init: using embedded metal library
0.00.066.743 I ggml_metal_init: GPU name:   Apple M4
0.00.066.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.745 I ggml_metal_init: simdgroup reduction   = true
0.00.066.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.746 I ggml_metal_init: has bfloat            = true
0.00.066.746 I ggml_metal_init: use bfloat            = true
0.00.066.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.418 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.434 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.457 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.618 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.620 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.621 I llama_new_context_with_model: graph nodes  = 967
0.00.102.621 I llama_new_context_with_model: graph splits = 2
0.00.102.649 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.792 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.298.733 I main: llama threadpool init, n_threads = 4
0.01.298.771 I 
0.01.298.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.298.800 I 
0.01.299.034 I sampler seed: 1234
0.01.299.039 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.299.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.299.060 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.299.060 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.391.820 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.02.391.820 I llama_perf_context_print:        load time =    1288.96 ms
0.02.391.821 I llama_perf_context_print: prompt eval time =      46.04 ms /     7 tokens (    6.58 ms per token,   152.04 tokens per second)
0.02.391.823 I llama_perf_context_print:        eval time =    1043.77 ms /    63 runs   (   16.57 ms per token,    60.36 tokens per second)
0.02.391.823 I llama_perf_context_print:       total time =    1093.09 ms /    70 tokens
0.02.392.017 I ggml_metal_free: deallocating

real	0m2.411s
user	0m0.113s
sys	0m0.232s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.135 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.816 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.724 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.429 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.023 I llama_model_loader: - type  f32:  194 tensors
0.00.034.024 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.516 I llm_load_vocab: special tokens cache size = 25
0.00.066.882 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.886 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.886 I llm_load_print_meta: arch             = gptneox
0.00.066.887 I llm_load_print_meta: vocab type       = BPE
0.00.066.887 I llm_load_print_meta: n_vocab          = 50304
0.00.066.887 I llm_load_print_meta: n_merges         = 50009
0.00.066.887 I llm_load_print_meta: vocab_only       = 0
0.00.066.887 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.887 I llm_load_print_meta: n_embd           = 2048
0.00.066.889 I llm_load_print_meta: n_layer          = 24
0.00.066.905 I llm_load_print_meta: n_head           = 16
0.00.066.906 I llm_load_print_meta: n_head_kv        = 16
0.00.066.906 I llm_load_print_meta: n_rot            = 32
0.00.066.906 I llm_load_print_meta: n_swa            = 0
0.00.066.906 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.906 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.907 I llm_load_print_meta: n_gqa            = 1
0.00.066.908 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.908 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.909 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.909 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.909 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.909 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.910 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.911 I llm_load_print_meta: n_ff             = 8192
0.00.066.911 I llm_load_print_meta: n_expert         = 0
0.00.066.911 I llm_load_print_meta: n_expert_used    = 0
0.00.066.911 I llm_load_print_meta: causal attn      = 1
0.00.066.912 I llm_load_print_meta: pooling type     = 0
0.00.066.912 I llm_load_print_meta: rope type        = 2
0.00.066.912 I llm_load_print_meta: rope scaling     = linear
0.00.066.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.913 I llm_load_print_meta: freq_scale_train = 1
0.00.066.913 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.913 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.913 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.913 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.914 I llm_load_print_meta: model type       = 1.4B
0.00.066.914 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.914 I llm_load_print_meta: model params     = 1.41 B
0.00.066.915 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.915 I llm_load_print_meta: general.name     = 1.4B
0.00.066.915 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.915 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.916 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.916 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.916 I llm_load_print_meta: LF token         = 128 ''
0.00.066.916 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.916 I llm_load_print_meta: max token length = 1024
0.00.069.328 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.329 I llm_load_tensors: offloading output layer to GPU
0.00.069.329 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.340 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.341 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.324 I llama_new_context_with_model: n_ctx         = 128
0.00.070.324 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.325 I llama_new_context_with_model: n_batch       = 128
0.00.070.325 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.325 I llama_new_context_with_model: flash_attn    = 0
0.00.070.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.326 I llama_new_context_with_model: freq_scale    = 1
0.00.070.326 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.326 I ggml_metal_init: allocating
0.00.070.329 I ggml_metal_init: found device: Apple M4
0.00.070.331 I ggml_metal_init: picking default device: Apple M4
0.00.070.968 I ggml_metal_init: using embedded metal library
0.00.073.591 I ggml_metal_init: GPU name:   Apple M4
0.00.073.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.593 I ggml_metal_init: simdgroup reduction   = true
0.00.073.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.594 I ggml_metal_init: has bfloat            = true
0.00.073.594 I ggml_metal_init: use bfloat            = true
0.00.073.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.202 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.205 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.219 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.223 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.224 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.224 I llama_new_context_with_model: graph nodes  = 967
0.00.085.224 I llama_new_context_with_model: graph splits = 2
0.00.085.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.916.877 I 
0.00.916.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.916.918 I perplexity: tokenizing the input ..
0.00.924.970 I perplexity: tokenization took 8.05 ms
0.00.924.974 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.049.452 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.050.602 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.050.619 I llama_perf_context_print:        load time =     905.06 ms
0.01.050.620 I llama_perf_context_print: prompt eval time =     124.25 ms /   128 tokens (    0.97 ms per token,  1030.19 tokens per second)
0.01.050.620 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.050.621 I llama_perf_context_print:       total time =     133.74 ms /   129 tokens
0.01.051.102 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.095s
sys	0m0.153s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.017.058 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.248 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.249 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.250 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.250 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.250 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.251 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.251 I llama_model_loader: - type  f32:  194 tensors
0.00.034.252 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.252 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.185 I llm_load_vocab: special tokens cache size = 25
0.00.069.831 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.839 I llm_load_print_meta: arch             = gptneox
0.00.069.840 I llm_load_print_meta: vocab type       = BPE
0.00.069.840 I llm_load_print_meta: n_vocab          = 50304
0.00.069.840 I llm_load_print_meta: n_merges         = 50009
0.00.069.841 I llm_load_print_meta: vocab_only       = 0
0.00.069.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.841 I llm_load_print_meta: n_embd           = 2048
0.00.069.841 I llm_load_print_meta: n_layer          = 24
0.00.069.857 I llm_load_print_meta: n_head           = 16
0.00.069.858 I llm_load_print_meta: n_head_kv        = 16
0.00.069.858 I llm_load_print_meta: n_rot            = 32
0.00.069.858 I llm_load_print_meta: n_swa            = 0
0.00.069.859 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.859 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.860 I llm_load_print_meta: n_gqa            = 1
0.00.069.861 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.861 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.862 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.863 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.863 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.863 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.863 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.864 I llm_load_print_meta: n_ff             = 8192
0.00.069.864 I llm_load_print_meta: n_expert         = 0
0.00.069.864 I llm_load_print_meta: n_expert_used    = 0
0.00.069.865 I llm_load_print_meta: causal attn      = 1
0.00.069.865 I llm_load_print_meta: pooling type     = 0
0.00.069.866 I llm_load_print_meta: rope type        = 2
0.00.069.866 I llm_load_print_meta: rope scaling     = linear
0.00.069.866 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.867 I llm_load_print_meta: freq_scale_train = 1
0.00.069.867 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.867 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.867 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.867 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.867 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.868 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.868 I llm_load_print_meta: model type       = 1.4B
0.00.069.868 I llm_load_print_meta: model ftype      = Q4_0
0.00.069.869 I llm_load_print_meta: model params     = 1.41 B
0.00.069.869 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.069.870 I llm_load_print_meta: general.name     = 1.4B
0.00.069.870 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.870 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.870 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.870 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.871 I llm_load_print_meta: LF token         = 128 ''
0.00.069.871 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.871 I llm_load_print_meta: max token length = 1024
0.00.072.609 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.609 I llm_load_tensors: offloading output layer to GPU
0.00.072.609 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.622 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.623 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.073.843 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.844 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.844 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.844 I llama_new_context_with_model: n_batch       = 2048
0.00.073.845 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.845 I llama_new_context_with_model: flash_attn    = 0
0.00.073.845 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.845 I llama_new_context_with_model: freq_scale    = 1
0.00.073.846 I ggml_metal_init: allocating
0.00.073.849 I ggml_metal_init: found device: Apple M4
0.00.073.852 I ggml_metal_init: picking default device: Apple M4
0.00.074.749 I ggml_metal_init: using embedded metal library
0.00.078.357 I ggml_metal_init: GPU name:   Apple M4
0.00.078.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.361 I ggml_metal_init: simdgroup reduction   = true
0.00.078.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.362 I ggml_metal_init: has bfloat            = true
0.00.078.362 I ggml_metal_init: use bfloat            = true
0.00.078.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.118.634 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.644 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.669 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.851 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.853 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.853 I llama_new_context_with_model: graph nodes  = 967
0.00.119.854 I llama_new_context_with_model: graph splits = 2
0.00.119.882 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.023 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.732 I main: llama threadpool init, n_threads = 4
0.00.768.771 I 
0.00.768.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.799 I 
0.00.769.018 I sampler seed: 1234
0.00.769.022 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.075 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.077 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.077 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.445.331 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47082.23 tokens per second)
0.01.445.331 I llama_perf_context_print:        load time =     751.67 ms
0.01.445.333 I llama_perf_context_print: prompt eval time =      44.43 ms /     7 tokens (    6.35 ms per token,   157.57 tokens per second)
0.01.445.333 I llama_perf_context_print:        eval time =     628.98 ms /    63 runs   (    9.98 ms per token,   100.16 tokens per second)
0.01.445.334 I llama_perf_context_print:       total time =     676.60 ms /    70 tokens
0.01.445.556 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.126s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.706 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.221 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.231 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.231 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.233 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.233 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.233 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.235 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.995 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.777 I llama_model_loader: - type  f32:  194 tensors
0.00.023.778 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.104 I llm_load_vocab: special tokens cache size = 25
0.00.050.026 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.028 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.029 I llm_load_print_meta: arch             = gptneox
0.00.050.029 I llm_load_print_meta: vocab type       = BPE
0.00.050.029 I llm_load_print_meta: n_vocab          = 50304
0.00.050.030 I llm_load_print_meta: n_merges         = 50009
0.00.050.030 I llm_load_print_meta: vocab_only       = 0
0.00.050.030 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.030 I llm_load_print_meta: n_embd           = 2048
0.00.050.030 I llm_load_print_meta: n_layer          = 24
0.00.050.044 I llm_load_print_meta: n_head           = 16
0.00.050.045 I llm_load_print_meta: n_head_kv        = 16
0.00.050.047 I llm_load_print_meta: n_rot            = 32
0.00.050.047 I llm_load_print_meta: n_swa            = 0
0.00.050.048 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.048 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.048 I llm_load_print_meta: n_gqa            = 1
0.00.050.049 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.050 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.051 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.051 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.051 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.052 I llm_load_print_meta: n_ff             = 8192
0.00.050.052 I llm_load_print_meta: n_expert         = 0
0.00.050.053 I llm_load_print_meta: n_expert_used    = 0
0.00.050.053 I llm_load_print_meta: causal attn      = 1
0.00.050.053 I llm_load_print_meta: pooling type     = 0
0.00.050.053 I llm_load_print_meta: rope type        = 2
0.00.050.054 I llm_load_print_meta: rope scaling     = linear
0.00.050.054 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.054 I llm_load_print_meta: freq_scale_train = 1
0.00.050.054 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.055 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.055 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.055 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.055 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.055 I llm_load_print_meta: model type       = 1.4B
0.00.050.059 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.059 I llm_load_print_meta: model params     = 1.41 B
0.00.050.059 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.060 I llm_load_print_meta: general.name     = 1.4B
0.00.050.060 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.061 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.062 I llm_load_print_meta: LF token         = 128 ''
0.00.050.062 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.062 I llm_load_print_meta: max token length = 1024
0.00.051.661 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.661 I llm_load_tensors: offloading output layer to GPU
0.00.051.661 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.671 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.672 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.493 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.494 I llama_new_context_with_model: n_ctx         = 128
0.00.052.494 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.494 I llama_new_context_with_model: n_batch       = 128
0.00.052.494 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.495 I llama_new_context_with_model: flash_attn    = 0
0.00.052.495 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.495 I llama_new_context_with_model: freq_scale    = 1
0.00.052.496 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.496 I ggml_metal_init: allocating
0.00.052.499 I ggml_metal_init: found device: Apple M4
0.00.052.501 I ggml_metal_init: picking default device: Apple M4
0.00.053.078 I ggml_metal_init: using embedded metal library
0.00.055.427 I ggml_metal_init: GPU name:   Apple M4
0.00.055.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.430 I ggml_metal_init: simdgroup reduction   = true
0.00.055.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.430 I ggml_metal_init: has bfloat            = true
0.00.055.430 I ggml_metal_init: use bfloat            = true
0.00.055.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.469 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.471 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.484 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.342 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.343 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.344 I llama_new_context_with_model: graph nodes  = 967
0.00.067.344 I llama_new_context_with_model: graph splits = 2
0.00.067.357 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.020 I 
0.00.633.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.068 I perplexity: tokenizing the input ..
0.00.641.042 I perplexity: tokenization took 7.972 ms
0.00.641.045 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.623 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.764.763 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.764.780 I llama_perf_context_print:        load time =     623.31 ms
0.00.764.781 I llama_perf_context_print: prompt eval time =     122.35 ms /   128 tokens (    0.96 ms per token,  1046.20 tokens per second)
0.00.764.782 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.782 I llama_perf_context_print:       total time =     131.76 ms /   129 tokens
0.00.765.273 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.078s
sys	0m0.114s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.194 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.219 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.227 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.227 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.228 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.231 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.231 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.444 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.528 I llama_model_loader: - type  f32:  194 tensors
0.00.034.528 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.011 I llm_load_vocab: special tokens cache size = 25
0.00.062.023 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.025 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.026 I llm_load_print_meta: arch             = gptneox
0.00.062.026 I llm_load_print_meta: vocab type       = BPE
0.00.062.026 I llm_load_print_meta: n_vocab          = 50304
0.00.062.026 I llm_load_print_meta: n_merges         = 50009
0.00.062.027 I llm_load_print_meta: vocab_only       = 0
0.00.062.027 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.027 I llm_load_print_meta: n_embd           = 2048
0.00.062.027 I llm_load_print_meta: n_layer          = 24
0.00.062.041 I llm_load_print_meta: n_head           = 16
0.00.062.043 I llm_load_print_meta: n_head_kv        = 16
0.00.062.043 I llm_load_print_meta: n_rot            = 32
0.00.062.043 I llm_load_print_meta: n_swa            = 0
0.00.062.043 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.044 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.044 I llm_load_print_meta: n_gqa            = 1
0.00.062.045 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.058 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.059 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.059 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.059 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.059 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.060 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.061 I llm_load_print_meta: n_ff             = 8192
0.00.062.062 I llm_load_print_meta: n_expert         = 0
0.00.062.062 I llm_load_print_meta: n_expert_used    = 0
0.00.062.064 I llm_load_print_meta: causal attn      = 1
0.00.062.065 I llm_load_print_meta: pooling type     = 0
0.00.062.065 I llm_load_print_meta: rope type        = 2
0.00.062.065 I llm_load_print_meta: rope scaling     = linear
0.00.062.066 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.066 I llm_load_print_meta: freq_scale_train = 1
0.00.062.066 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.069 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.070 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.070 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.070 I llm_load_print_meta: model type       = 1.4B
0.00.062.070 I llm_load_print_meta: model ftype      = Q4_1
0.00.062.071 I llm_load_print_meta: model params     = 1.41 B
0.00.062.071 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.062.071 I llm_load_print_meta: general.name     = 1.4B
0.00.062.072 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.073 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.073 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.073 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.074 I llm_load_print_meta: LF token         = 128 ''
0.00.062.074 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.074 I llm_load_print_meta: max token length = 1024
0.00.064.062 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.063 I llm_load_tensors: offloading output layer to GPU
0.00.064.063 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.073 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.064.074 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.064.978 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.979 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.979 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.979 I llama_new_context_with_model: n_batch       = 2048
0.00.064.980 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.980 I llama_new_context_with_model: flash_attn    = 0
0.00.064.980 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.980 I llama_new_context_with_model: freq_scale    = 1
0.00.064.981 I ggml_metal_init: allocating
0.00.064.984 I ggml_metal_init: found device: Apple M4
0.00.064.986 I ggml_metal_init: picking default device: Apple M4
0.00.065.591 I ggml_metal_init: using embedded metal library
0.00.067.995 I ggml_metal_init: GPU name:   Apple M4
0.00.067.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.996 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.997 I ggml_metal_init: simdgroup reduction   = true
0.00.067.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.997 I ggml_metal_init: has bfloat            = true
0.00.067.999 I ggml_metal_init: use bfloat            = true
0.00.067.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.694 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.699 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.717 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.746 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.747 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.748 I llama_new_context_with_model: graph nodes  = 967
0.00.098.748 I llama_new_context_with_model: graph splits = 2
0.00.098.774 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.891 I main: llama threadpool init, n_threads = 4
0.00.814.924 I 
0.00.814.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.958 I 
0.00.815.177 I sampler seed: 1234
0.00.815.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.197 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.197 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.197 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.545.845 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62610.23 tokens per second)
0.01.545.845 I llama_perf_context_print:        load time =     805.69 ms
0.01.545.846 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.37 tokens per second)
0.01.545.847 I llama_perf_context_print:        eval time =     684.13 ms /    63 runs   (   10.86 ms per token,    92.09 tokens per second)
0.01.545.847 I llama_perf_context_print:       total time =     730.96 ms /    70 tokens
0.01.546.050 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.112s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.803 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.437 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.255 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.152 I llama_model_loader: - type  f32:  194 tensors
0.00.023.152 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.152 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.207 I llm_load_vocab: special tokens cache size = 25
0.00.050.235 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.238 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.238 I llm_load_print_meta: arch             = gptneox
0.00.050.238 I llm_load_print_meta: vocab type       = BPE
0.00.050.239 I llm_load_print_meta: n_vocab          = 50304
0.00.050.239 I llm_load_print_meta: n_merges         = 50009
0.00.050.239 I llm_load_print_meta: vocab_only       = 0
0.00.050.239 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.239 I llm_load_print_meta: n_embd           = 2048
0.00.050.239 I llm_load_print_meta: n_layer          = 24
0.00.050.254 I llm_load_print_meta: n_head           = 16
0.00.050.254 I llm_load_print_meta: n_head_kv        = 16
0.00.050.255 I llm_load_print_meta: n_rot            = 32
0.00.050.255 I llm_load_print_meta: n_swa            = 0
0.00.050.255 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.255 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.256 I llm_load_print_meta: n_gqa            = 1
0.00.050.258 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.258 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.259 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.259 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.260 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.260 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.260 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.260 I llm_load_print_meta: n_ff             = 8192
0.00.050.261 I llm_load_print_meta: n_expert         = 0
0.00.050.261 I llm_load_print_meta: n_expert_used    = 0
0.00.050.261 I llm_load_print_meta: causal attn      = 1
0.00.050.261 I llm_load_print_meta: pooling type     = 0
0.00.050.261 I llm_load_print_meta: rope type        = 2
0.00.050.263 I llm_load_print_meta: rope scaling     = linear
0.00.050.263 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.263 I llm_load_print_meta: freq_scale_train = 1
0.00.050.263 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.264 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.264 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.264 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.264 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.265 I llm_load_print_meta: model type       = 1.4B
0.00.050.265 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.265 I llm_load_print_meta: model params     = 1.41 B
0.00.050.267 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.267 I llm_load_print_meta: general.name     = 1.4B
0.00.050.267 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.267 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.267 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.267 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.268 I llm_load_print_meta: LF token         = 128 ''
0.00.050.268 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.268 I llm_load_print_meta: max token length = 1024
0.00.052.288 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.288 I llm_load_tensors: offloading output layer to GPU
0.00.052.288 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.299 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.300 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.206 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.207 I llama_new_context_with_model: n_ctx         = 128
0.00.053.208 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.208 I llama_new_context_with_model: n_batch       = 128
0.00.053.208 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.208 I llama_new_context_with_model: flash_attn    = 0
0.00.053.208 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.209 I llama_new_context_with_model: freq_scale    = 1
0.00.053.209 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.210 I ggml_metal_init: allocating
0.00.053.213 I ggml_metal_init: found device: Apple M4
0.00.053.215 I ggml_metal_init: picking default device: Apple M4
0.00.053.799 I ggml_metal_init: using embedded metal library
0.00.056.177 I ggml_metal_init: GPU name:   Apple M4
0.00.056.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.179 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.179 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.180 I ggml_metal_init: simdgroup reduction   = true
0.00.056.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.180 I ggml_metal_init: has bfloat            = true
0.00.056.180 I ggml_metal_init: use bfloat            = true
0.00.056.181 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.445 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.447 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.463 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.384 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.385 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.385 I llama_new_context_with_model: graph nodes  = 967
0.00.068.385 I llama_new_context_with_model: graph splits = 2
0.00.068.398 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.341 I 
0.00.670.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.408 I perplexity: tokenizing the input ..
0.00.678.154 I perplexity: tokenization took 7.744 ms
0.00.678.157 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.890 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.802.142 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.802.161 I llama_perf_context_print:        load time =     661.53 ms
0.00.802.163 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.03 tokens per second)
0.00.802.164 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.165 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.802.666 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.104s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.345 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.696 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.602 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.620 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.621 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.621 I llama_model_loader: - type  f32:  194 tensors
0.00.024.622 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.622 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.701 I llm_load_vocab: special tokens cache size = 25
0.00.051.551 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.553 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.554 I llm_load_print_meta: arch             = gptneox
0.00.051.554 I llm_load_print_meta: vocab type       = BPE
0.00.051.554 I llm_load_print_meta: n_vocab          = 50304
0.00.051.555 I llm_load_print_meta: n_merges         = 50009
0.00.051.555 I llm_load_print_meta: vocab_only       = 0
0.00.051.555 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.555 I llm_load_print_meta: n_embd           = 2048
0.00.051.555 I llm_load_print_meta: n_layer          = 24
0.00.051.569 I llm_load_print_meta: n_head           = 16
0.00.051.571 I llm_load_print_meta: n_head_kv        = 16
0.00.051.571 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.571 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.571 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.572 I llm_load_print_meta: n_gqa            = 1
0.00.051.575 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.575 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.576 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.576 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.576 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.577 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.578 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.578 I llm_load_print_meta: n_ff             = 8192
0.00.051.578 I llm_load_print_meta: n_expert         = 0
0.00.051.579 I llm_load_print_meta: n_expert_used    = 0
0.00.051.579 I llm_load_print_meta: causal attn      = 1
0.00.051.579 I llm_load_print_meta: pooling type     = 0
0.00.051.579 I llm_load_print_meta: rope type        = 2
0.00.051.579 I llm_load_print_meta: rope scaling     = linear
0.00.051.579 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.580 I llm_load_print_meta: freq_scale_train = 1
0.00.051.580 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.580 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.580 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.580 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.580 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.581 I llm_load_print_meta: model type       = 1.4B
0.00.051.581 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.581 I llm_load_print_meta: model params     = 1.41 B
0.00.051.582 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.582 I llm_load_print_meta: general.name     = 1.4B
0.00.051.582 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: LF token         = 128 ''
0.00.051.584 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.585 I llm_load_print_meta: max token length = 1024
0.00.053.605 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.605 I llm_load_tensors: offloading output layer to GPU
0.00.053.605 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.616 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.617 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.507 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.508 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.508 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.509 I llama_new_context_with_model: n_batch       = 2048
0.00.054.509 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.509 I llama_new_context_with_model: flash_attn    = 0
0.00.054.509 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.510 I llama_new_context_with_model: freq_scale    = 1
0.00.054.510 I ggml_metal_init: allocating
0.00.054.513 I ggml_metal_init: found device: Apple M4
0.00.054.515 I ggml_metal_init: picking default device: Apple M4
0.00.055.123 I ggml_metal_init: using embedded metal library
0.00.057.490 I ggml_metal_init: GPU name:   Apple M4
0.00.057.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.492 I ggml_metal_init: simdgroup reduction   = true
0.00.057.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.492 I ggml_metal_init: has bfloat            = true
0.00.057.492 I ggml_metal_init: use bfloat            = true
0.00.057.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.056 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.063 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.087 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.172 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.174 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.174 I llama_new_context_with_model: graph nodes  = 967
0.00.089.175 I llama_new_context_with_model: graph splits = 2
0.00.089.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.667 I main: llama threadpool init, n_threads = 4
0.00.781.704 I 
0.00.781.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.739 I 
0.00.781.957 I sampler seed: 1234
0.00.781.963 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.979 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.979 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.979 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.571.716 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.01.571.716 I llama_perf_context_print:        load time =     772.32 ms
0.01.571.717 I llama_perf_context_print: prompt eval time =      43.17 ms /     7 tokens (    6.17 ms per token,   162.16 tokens per second)
0.01.571.718 I llama_perf_context_print:        eval time =     743.60 ms /    63 runs   (   11.80 ms per token,    84.72 tokens per second)
0.01.571.718 I llama_perf_context_print:       total time =     790.05 ms /    70 tokens
0.01.571.933 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.111s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.891 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.502 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.281 I llama_model_loader: - type  f32:  194 tensors
0.00.024.281 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.282 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.604 I llm_load_vocab: special tokens cache size = 25
0.00.050.594 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.597 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.597 I llm_load_print_meta: arch             = gptneox
0.00.050.597 I llm_load_print_meta: vocab type       = BPE
0.00.050.598 I llm_load_print_meta: n_vocab          = 50304
0.00.050.598 I llm_load_print_meta: n_merges         = 50009
0.00.050.598 I llm_load_print_meta: vocab_only       = 0
0.00.050.598 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.598 I llm_load_print_meta: n_embd           = 2048
0.00.050.599 I llm_load_print_meta: n_layer          = 24
0.00.050.613 I llm_load_print_meta: n_head           = 16
0.00.050.614 I llm_load_print_meta: n_head_kv        = 16
0.00.050.614 I llm_load_print_meta: n_rot            = 32
0.00.050.614 I llm_load_print_meta: n_swa            = 0
0.00.050.615 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.615 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.615 I llm_load_print_meta: n_gqa            = 1
0.00.050.616 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.617 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.617 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.618 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.618 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.618 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.618 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.619 I llm_load_print_meta: n_ff             = 8192
0.00.050.619 I llm_load_print_meta: n_expert         = 0
0.00.050.619 I llm_load_print_meta: n_expert_used    = 0
0.00.050.619 I llm_load_print_meta: causal attn      = 1
0.00.050.619 I llm_load_print_meta: pooling type     = 0
0.00.050.620 I llm_load_print_meta: rope type        = 2
0.00.050.620 I llm_load_print_meta: rope scaling     = linear
0.00.050.620 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.620 I llm_load_print_meta: freq_scale_train = 1
0.00.050.621 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.621 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.621 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.621 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.621 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.621 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.621 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.622 I llm_load_print_meta: model type       = 1.4B
0.00.050.624 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.625 I llm_load_print_meta: model params     = 1.41 B
0.00.050.625 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.625 I llm_load_print_meta: general.name     = 1.4B
0.00.050.625 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.626 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.626 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: LF token         = 128 ''
0.00.050.631 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: max token length = 1024
0.00.052.623 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.623 I llm_load_tensors: offloading output layer to GPU
0.00.052.624 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.634 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.635 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.534 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.534 I llama_new_context_with_model: n_ctx         = 128
0.00.053.535 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.535 I llama_new_context_with_model: n_batch       = 128
0.00.053.535 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.535 I llama_new_context_with_model: flash_attn    = 0
0.00.053.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.536 I llama_new_context_with_model: freq_scale    = 1
0.00.053.536 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.537 I ggml_metal_init: allocating
0.00.053.540 I ggml_metal_init: found device: Apple M4
0.00.053.542 I ggml_metal_init: picking default device: Apple M4
0.00.054.113 I ggml_metal_init: using embedded metal library
0.00.056.454 I ggml_metal_init: GPU name:   Apple M4
0.00.056.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.457 I ggml_metal_init: simdgroup reduction   = true
0.00.056.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.457 I ggml_metal_init: has bfloat            = true
0.00.056.457 I ggml_metal_init: use bfloat            = true
0.00.056.457 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.351 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.374 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.290 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.291 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.291 I llama_new_context_with_model: graph nodes  = 967
0.00.068.292 I llama_new_context_with_model: graph splits = 2
0.00.068.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.826 I 
0.00.690.890 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.913 I perplexity: tokenizing the input ..
0.00.699.234 I perplexity: tokenization took 8.319 ms
0.00.699.237 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.227 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.835.496 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.835.516 I llama_perf_context_print:        load time =     680.93 ms
0.00.835.520 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.80 tokens per second)
0.00.835.521 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.522 I llama_perf_context_print:       total time =     144.69 ms /   129 tokens
0.00.836.072 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.078s
sys	0m0.108s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.830 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.135 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.140 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.006 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.008 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.009 I llama_model_loader: - type  f32:  194 tensors
0.00.024.009 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.010 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.185 I llm_load_vocab: special tokens cache size = 25
0.00.051.167 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.170 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.170 I llm_load_print_meta: arch             = gptneox
0.00.051.171 I llm_load_print_meta: vocab type       = BPE
0.00.051.171 I llm_load_print_meta: n_vocab          = 50304
0.00.051.171 I llm_load_print_meta: n_merges         = 50009
0.00.051.171 I llm_load_print_meta: vocab_only       = 0
0.00.051.172 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.172 I llm_load_print_meta: n_embd           = 2048
0.00.051.172 I llm_load_print_meta: n_layer          = 24
0.00.051.181 I llm_load_print_meta: n_head           = 16
0.00.051.183 I llm_load_print_meta: n_head_kv        = 16
0.00.051.183 I llm_load_print_meta: n_rot            = 32
0.00.051.183 I llm_load_print_meta: n_swa            = 0
0.00.051.183 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.183 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.184 I llm_load_print_meta: n_gqa            = 1
0.00.051.185 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.187 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.188 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.188 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.188 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.188 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.189 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.189 I llm_load_print_meta: n_ff             = 8192
0.00.051.190 I llm_load_print_meta: n_expert         = 0
0.00.051.190 I llm_load_print_meta: n_expert_used    = 0
0.00.051.191 I llm_load_print_meta: causal attn      = 1
0.00.051.192 I llm_load_print_meta: pooling type     = 0
0.00.051.193 I llm_load_print_meta: rope type        = 2
0.00.051.193 I llm_load_print_meta: rope scaling     = linear
0.00.051.193 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.193 I llm_load_print_meta: freq_scale_train = 1
0.00.051.193 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.197 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.198 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.198 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.198 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.198 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.198 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.199 I llm_load_print_meta: model type       = 1.4B
0.00.051.200 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.200 I llm_load_print_meta: model params     = 1.41 B
0.00.051.201 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.202 I llm_load_print_meta: general.name     = 1.4B
0.00.051.202 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.202 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.202 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.203 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.203 I llm_load_print_meta: LF token         = 128 ''
0.00.051.203 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.203 I llm_load_print_meta: max token length = 1024
0.00.052.993 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.994 I llm_load_tensors: offloading output layer to GPU
0.00.052.994 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.000 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.000 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.892 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.892 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.893 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.893 I llama_new_context_with_model: n_batch       = 2048
0.00.053.893 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.893 I llama_new_context_with_model: flash_attn    = 0
0.00.053.894 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.894 I llama_new_context_with_model: freq_scale    = 1
0.00.053.894 I ggml_metal_init: allocating
0.00.053.897 I ggml_metal_init: found device: Apple M4
0.00.053.899 I ggml_metal_init: picking default device: Apple M4
0.00.054.485 I ggml_metal_init: using embedded metal library
0.00.056.840 I ggml_metal_init: GPU name:   Apple M4
0.00.056.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.842 I ggml_metal_init: simdgroup reduction   = true
0.00.056.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.844 I ggml_metal_init: has bfloat            = true
0.00.056.844 I ggml_metal_init: use bfloat            = true
0.00.056.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.020 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.027 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.044 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.045 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.045 I llama_new_context_with_model: graph nodes  = 967
0.00.087.045 I llama_new_context_with_model: graph splits = 2
0.00.087.070 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.123 I main: llama threadpool init, n_threads = 4
0.00.692.168 I 
0.00.692.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.212 I 
0.00.692.457 I sampler seed: 1234
0.00.692.461 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.498 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.498 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.535.537 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.01.535.538 I llama_perf_context_print:        load time =     683.29 ms
0.01.535.538 I llama_perf_context_print: prompt eval time =      46.25 ms /     7 tokens (    6.61 ms per token,   151.34 tokens per second)
0.01.535.539 I llama_perf_context_print:        eval time =     793.80 ms /    63 runs   (   12.60 ms per token,    79.37 tokens per second)
0.01.535.539 I llama_perf_context_print:       total time =     843.42 ms /    70 tokens
0.01.535.732 I ggml_metal_free: deallocating

real	0m1.555s
user	0m0.110s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.936 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.761 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.762 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.762 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.623 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.453 I llama_model_loader: - type  f32:  194 tensors
0.00.023.454 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.454 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.557 I llm_load_vocab: special tokens cache size = 25
0.00.050.495 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.498 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.498 I llm_load_print_meta: arch             = gptneox
0.00.050.498 I llm_load_print_meta: vocab type       = BPE
0.00.050.499 I llm_load_print_meta: n_vocab          = 50304
0.00.050.499 I llm_load_print_meta: n_merges         = 50009
0.00.050.499 I llm_load_print_meta: vocab_only       = 0
0.00.050.499 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.499 I llm_load_print_meta: n_embd           = 2048
0.00.050.499 I llm_load_print_meta: n_layer          = 24
0.00.050.514 I llm_load_print_meta: n_head           = 16
0.00.050.515 I llm_load_print_meta: n_head_kv        = 16
0.00.050.515 I llm_load_print_meta: n_rot            = 32
0.00.050.515 I llm_load_print_meta: n_swa            = 0
0.00.050.515 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.516 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.516 I llm_load_print_meta: n_gqa            = 1
0.00.050.517 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.518 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.518 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.519 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.519 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.519 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.519 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.520 I llm_load_print_meta: n_ff             = 8192
0.00.050.520 I llm_load_print_meta: n_expert         = 0
0.00.050.521 I llm_load_print_meta: n_expert_used    = 0
0.00.050.521 I llm_load_print_meta: causal attn      = 1
0.00.050.521 I llm_load_print_meta: pooling type     = 0
0.00.050.521 I llm_load_print_meta: rope type        = 2
0.00.050.521 I llm_load_print_meta: rope scaling     = linear
0.00.050.523 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.524 I llm_load_print_meta: freq_scale_train = 1
0.00.050.525 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.525 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.525 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.525 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.525 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.525 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.525 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.526 I llm_load_print_meta: model type       = 1.4B
0.00.050.526 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.526 I llm_load_print_meta: model params     = 1.41 B
0.00.050.527 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.527 I llm_load_print_meta: general.name     = 1.4B
0.00.050.527 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.527 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: LF token         = 128 ''
0.00.050.528 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: max token length = 1024
0.00.052.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.617 I llm_load_tensors: offloading output layer to GPU
0.00.052.617 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.628 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.629 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.562 I llama_new_context_with_model: n_ctx         = 128
0.00.053.562 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.562 I llama_new_context_with_model: n_batch       = 128
0.00.053.562 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.563 I llama_new_context_with_model: flash_attn    = 0
0.00.053.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.563 I llama_new_context_with_model: freq_scale    = 1
0.00.053.564 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.564 I ggml_metal_init: allocating
0.00.053.567 I ggml_metal_init: found device: Apple M4
0.00.053.569 I ggml_metal_init: picking default device: Apple M4
0.00.054.164 I ggml_metal_init: using embedded metal library
0.00.056.486 I ggml_metal_init: GPU name:   Apple M4
0.00.056.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.488 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.489 I ggml_metal_init: simdgroup reduction   = true
0.00.056.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.489 I ggml_metal_init: has bfloat            = true
0.00.056.489 I ggml_metal_init: use bfloat            = true
0.00.056.491 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.492 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.574 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.576 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.589 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.535 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.536 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.537 I llama_new_context_with_model: graph nodes  = 967
0.00.068.537 I llama_new_context_with_model: graph splits = 2
0.00.068.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.775 I 
0.00.620.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.833 I perplexity: tokenizing the input ..
0.00.627.873 I perplexity: tokenization took 7.038 ms
0.00.627.876 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.659 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.763.061 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.763.077 I llama_perf_context_print:        load time =     611.84 ms
0.00.763.078 I llama_perf_context_print: prompt eval time =     133.56 ms /   128 tokens (    1.04 ms per token,   958.41 tokens per second)
0.00.763.079 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.081 I llama_perf_context_print:       total time =     142.30 ms /   129 tokens
0.00.763.427 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.078s
sys	0m0.107s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.404 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.034 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.035 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.036 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.037 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.037 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.867 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.869 I llama_model_loader: - type  f32:  194 tensors
0.00.023.869 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.870 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.870 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.017 I llm_load_vocab: special tokens cache size = 25
0.00.051.169 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.175 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.175 I llm_load_print_meta: arch             = gptneox
0.00.051.175 I llm_load_print_meta: vocab type       = BPE
0.00.051.176 I llm_load_print_meta: n_vocab          = 50304
0.00.051.176 I llm_load_print_meta: n_merges         = 50009
0.00.051.176 I llm_load_print_meta: vocab_only       = 0
0.00.051.188 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.190 I llm_load_print_meta: n_embd           = 2048
0.00.051.191 I llm_load_print_meta: n_layer          = 24
0.00.051.204 I llm_load_print_meta: n_head           = 16
0.00.051.205 I llm_load_print_meta: n_head_kv        = 16
0.00.051.205 I llm_load_print_meta: n_rot            = 32
0.00.051.205 I llm_load_print_meta: n_swa            = 0
0.00.051.205 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.205 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.206 I llm_load_print_meta: n_gqa            = 1
0.00.051.207 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.207 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.210 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.211 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.212 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.214 I llm_load_print_meta: n_ff             = 8192
0.00.051.214 I llm_load_print_meta: n_expert         = 0
0.00.051.216 I llm_load_print_meta: n_expert_used    = 0
0.00.051.217 I llm_load_print_meta: causal attn      = 1
0.00.051.217 I llm_load_print_meta: pooling type     = 0
0.00.051.217 I llm_load_print_meta: rope type        = 2
0.00.051.217 I llm_load_print_meta: rope scaling     = linear
0.00.051.218 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.218 I llm_load_print_meta: freq_scale_train = 1
0.00.051.218 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.219 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.219 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.219 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.220 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.220 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.220 I llm_load_print_meta: model type       = 1.4B
0.00.051.220 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.221 I llm_load_print_meta: model params     = 1.41 B
0.00.051.221 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.221 I llm_load_print_meta: general.name     = 1.4B
0.00.051.224 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.224 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.224 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.224 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.225 I llm_load_print_meta: LF token         = 128 ''
0.00.051.228 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.228 I llm_load_print_meta: max token length = 1024
0.00.053.172 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.172 I llm_load_tensors: offloading output layer to GPU
0.00.053.172 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.178 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.178 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.240 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.241 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.242 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.242 I llama_new_context_with_model: n_batch       = 2048
0.00.054.242 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.242 I llama_new_context_with_model: flash_attn    = 0
0.00.054.243 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.243 I llama_new_context_with_model: freq_scale    = 1
0.00.054.243 I ggml_metal_init: allocating
0.00.054.246 I ggml_metal_init: found device: Apple M4
0.00.054.248 I ggml_metal_init: picking default device: Apple M4
0.00.054.856 I ggml_metal_init: using embedded metal library
0.00.057.191 I ggml_metal_init: GPU name:   Apple M4
0.00.057.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.193 I ggml_metal_init: simdgroup reduction   = true
0.00.057.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.195 I ggml_metal_init: has bfloat            = true
0.00.057.195 I ggml_metal_init: use bfloat            = true
0.00.057.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.469 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.474 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.554 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.555 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.555 I llama_new_context_with_model: graph nodes  = 967
0.00.088.556 I llama_new_context_with_model: graph splits = 2
0.00.088.581 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.458.425 I main: llama threadpool init, n_threads = 4
0.00.458.462 I 
0.00.458.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.458.495 I 
0.00.458.762 I sampler seed: 1234
0.00.458.767 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.458.783 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.458.786 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.458.786 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.123.507 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.123.507 I llama_perf_context_print:        load time =     449.02 ms
0.01.123.508 I llama_perf_context_print: prompt eval time =      35.61 ms /     7 tokens (    5.09 ms per token,   196.59 tokens per second)
0.01.123.509 I llama_perf_context_print:        eval time =     626.85 ms /    63 runs   (    9.95 ms per token,   100.50 tokens per second)
0.01.123.509 I llama_perf_context_print:       total time =     665.08 ms /    70 tokens
0.01.123.719 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.111s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.166 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.931 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.943 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.944 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.946 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.947 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.759 I llama_model_loader: - type  f32:  194 tensors
0.00.025.759 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.760 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.296 I llm_load_vocab: special tokens cache size = 25
0.00.053.327 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.331 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.332 I llm_load_print_meta: arch             = gptneox
0.00.053.332 I llm_load_print_meta: vocab type       = BPE
0.00.053.332 I llm_load_print_meta: n_vocab          = 50304
0.00.053.332 I llm_load_print_meta: n_merges         = 50009
0.00.053.333 I llm_load_print_meta: vocab_only       = 0
0.00.053.333 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.333 I llm_load_print_meta: n_embd           = 2048
0.00.053.333 I llm_load_print_meta: n_layer          = 24
0.00.053.349 I llm_load_print_meta: n_head           = 16
0.00.053.351 I llm_load_print_meta: n_head_kv        = 16
0.00.053.351 I llm_load_print_meta: n_rot            = 32
0.00.053.351 I llm_load_print_meta: n_swa            = 0
0.00.053.351 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.351 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.352 I llm_load_print_meta: n_gqa            = 1
0.00.053.352 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.353 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.353 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.355 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.355 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.355 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.356 I llm_load_print_meta: n_ff             = 8192
0.00.053.356 I llm_load_print_meta: n_expert         = 0
0.00.053.356 I llm_load_print_meta: n_expert_used    = 0
0.00.053.356 I llm_load_print_meta: causal attn      = 1
0.00.053.357 I llm_load_print_meta: pooling type     = 0
0.00.053.357 I llm_load_print_meta: rope type        = 2
0.00.053.357 I llm_load_print_meta: rope scaling     = linear
0.00.053.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.358 I llm_load_print_meta: freq_scale_train = 1
0.00.053.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.358 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.358 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.358 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.358 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.361 I llm_load_print_meta: model type       = 1.4B
0.00.053.361 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.361 I llm_load_print_meta: model params     = 1.41 B
0.00.053.362 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.362 I llm_load_print_meta: general.name     = 1.4B
0.00.053.362 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.364 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.364 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.364 I llm_load_print_meta: LF token         = 128 ''
0.00.053.365 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.365 I llm_load_print_meta: max token length = 1024
0.00.055.380 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.380 I llm_load_tensors: offloading output layer to GPU
0.00.055.380 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.391 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.392 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.327 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.328 I llama_new_context_with_model: n_ctx         = 128
0.00.056.328 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.329 I llama_new_context_with_model: n_batch       = 128
0.00.056.329 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.329 I llama_new_context_with_model: flash_attn    = 0
0.00.056.329 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.330 I llama_new_context_with_model: freq_scale    = 1
0.00.056.330 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.330 I ggml_metal_init: allocating
0.00.056.334 I ggml_metal_init: found device: Apple M4
0.00.056.337 I ggml_metal_init: picking default device: Apple M4
0.00.056.932 I ggml_metal_init: using embedded metal library
0.00.059.779 I ggml_metal_init: GPU name:   Apple M4
0.00.059.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.786 I ggml_metal_init: simdgroup reduction   = true
0.00.059.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.786 I ggml_metal_init: has bfloat            = true
0.00.059.786 I ggml_metal_init: use bfloat            = true
0.00.059.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.905 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.908 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.925 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.907 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.908 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.909 I llama_new_context_with_model: graph nodes  = 967
0.00.071.909 I llama_new_context_with_model: graph splits = 2
0.00.071.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.432 I 
0.00.385.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.481 I perplexity: tokenizing the input ..
0.00.392.948 I perplexity: tokenization took 7.466 ms
0.00.392.955 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.524.850 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.526.375 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.526.389 I llama_perf_context_print:        load time =     376.26 ms
0.00.526.390 I llama_perf_context_print: prompt eval time =     131.64 ms /   128 tokens (    1.03 ms per token,   972.33 tokens per second)
0.00.526.391 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.526.391 I llama_perf_context_print:       total time =     140.96 ms /   129 tokens
0.00.526.726 I ggml_metal_free: deallocating

real	0m0.543s
user	0m0.081s
sys	0m0.061s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.318 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.857 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.860 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.860 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.861 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.863 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.724 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.704 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.704 I llama_model_loader: - type  f32:  194 tensors
0.00.023.705 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.705 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.705 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.706 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.951 I llm_load_vocab: special tokens cache size = 25
0.00.051.024 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.027 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.027 I llm_load_print_meta: arch             = gptneox
0.00.051.028 I llm_load_print_meta: vocab type       = BPE
0.00.051.028 I llm_load_print_meta: n_vocab          = 50304
0.00.051.028 I llm_load_print_meta: n_merges         = 50009
0.00.051.028 I llm_load_print_meta: vocab_only       = 0
0.00.051.028 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.028 I llm_load_print_meta: n_embd           = 2048
0.00.051.029 I llm_load_print_meta: n_layer          = 24
0.00.051.038 I llm_load_print_meta: n_head           = 16
0.00.051.039 I llm_load_print_meta: n_head_kv        = 16
0.00.051.039 I llm_load_print_meta: n_rot            = 32
0.00.051.039 I llm_load_print_meta: n_swa            = 0
0.00.051.040 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.040 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.040 I llm_load_print_meta: n_gqa            = 1
0.00.051.041 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.042 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.042 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.043 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.043 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.043 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.044 I llm_load_print_meta: n_ff             = 8192
0.00.051.044 I llm_load_print_meta: n_expert         = 0
0.00.051.044 I llm_load_print_meta: n_expert_used    = 0
0.00.051.045 I llm_load_print_meta: causal attn      = 1
0.00.051.045 I llm_load_print_meta: pooling type     = 0
0.00.051.045 I llm_load_print_meta: rope type        = 2
0.00.051.045 I llm_load_print_meta: rope scaling     = linear
0.00.051.047 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.048 I llm_load_print_meta: freq_scale_train = 1
0.00.051.048 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.048 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.048 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.048 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.048 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.049 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.049 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.049 I llm_load_print_meta: model type       = 1.4B
0.00.051.049 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.050 I llm_load_print_meta: model params     = 1.41 B
0.00.051.051 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.051 I llm_load_print_meta: general.name     = 1.4B
0.00.051.051 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.052 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.052 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.052 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.052 I llm_load_print_meta: LF token         = 128 ''
0.00.051.052 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.053 I llm_load_print_meta: max token length = 1024
0.00.052.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.843 I llm_load_tensors: offloading output layer to GPU
0.00.052.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.849 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.849 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.782 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.782 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.783 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.783 I llama_new_context_with_model: n_batch       = 2048
0.00.053.783 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.783 I llama_new_context_with_model: flash_attn    = 0
0.00.053.783 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.784 I llama_new_context_with_model: freq_scale    = 1
0.00.053.784 I ggml_metal_init: allocating
0.00.053.791 I ggml_metal_init: found device: Apple M4
0.00.053.793 I ggml_metal_init: picking default device: Apple M4
0.00.054.367 I ggml_metal_init: using embedded metal library
0.00.056.735 I ggml_metal_init: GPU name:   Apple M4
0.00.056.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.737 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.738 I ggml_metal_init: simdgroup reduction   = true
0.00.056.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.738 I ggml_metal_init: has bfloat            = true
0.00.056.738 I ggml_metal_init: use bfloat            = true
0.00.056.739 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.671 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.683 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.704 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.706 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.708 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.709 I llama_new_context_with_model: graph nodes  = 967
0.00.088.709 I llama_new_context_with_model: graph splits = 2
0.00.088.735 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.950 I main: llama threadpool init, n_threads = 4
0.00.537.995 I 
0.00.538.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.051 I 
0.00.538.289 I sampler seed: 1234
0.00.538.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.538.309 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.538.310 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.538.310 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.284.593 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.284.593 I llama_perf_context_print:        load time =     528.63 ms
0.01.284.596 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.07 tokens per second)
0.01.284.597 I llama_perf_context_print:        eval time =     702.78 ms /    63 runs   (   11.16 ms per token,    89.64 tokens per second)
0.01.284.597 I llama_perf_context_print:       total time =     746.65 ms /    70 tokens
0.01.284.813 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.111s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.509 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.512 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.514 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.515 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.253 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.104 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.106 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.106 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.107 I llama_model_loader: - type  f32:  194 tensors
0.00.023.107 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.108 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.108 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.108 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.591 I llm_load_vocab: special tokens cache size = 25
0.00.049.562 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.565 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.565 I llm_load_print_meta: arch             = gptneox
0.00.049.565 I llm_load_print_meta: vocab type       = BPE
0.00.049.565 I llm_load_print_meta: n_vocab          = 50304
0.00.049.566 I llm_load_print_meta: n_merges         = 50009
0.00.049.566 I llm_load_print_meta: vocab_only       = 0
0.00.049.566 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.566 I llm_load_print_meta: n_embd           = 2048
0.00.049.566 I llm_load_print_meta: n_layer          = 24
0.00.049.581 I llm_load_print_meta: n_head           = 16
0.00.049.583 I llm_load_print_meta: n_head_kv        = 16
0.00.049.583 I llm_load_print_meta: n_rot            = 32
0.00.049.583 I llm_load_print_meta: n_swa            = 0
0.00.049.583 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.584 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.584 I llm_load_print_meta: n_gqa            = 1
0.00.049.585 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.587 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.587 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.587 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.587 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.588 I llm_load_print_meta: n_ff             = 8192
0.00.049.588 I llm_load_print_meta: n_expert         = 0
0.00.049.588 I llm_load_print_meta: n_expert_used    = 0
0.00.049.588 I llm_load_print_meta: causal attn      = 1
0.00.049.588 I llm_load_print_meta: pooling type     = 0
0.00.049.588 I llm_load_print_meta: rope type        = 2
0.00.049.589 I llm_load_print_meta: rope scaling     = linear
0.00.049.589 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.589 I llm_load_print_meta: freq_scale_train = 1
0.00.049.589 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.590 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.590 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.590 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.590 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.590 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.590 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.590 I llm_load_print_meta: model type       = 1.4B
0.00.049.591 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.591 I llm_load_print_meta: model params     = 1.41 B
0.00.049.592 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.594 I llm_load_print_meta: general.name     = 1.4B
0.00.049.594 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.594 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.594 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.594 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.595 I llm_load_print_meta: LF token         = 128 ''
0.00.049.595 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.595 I llm_load_print_meta: max token length = 1024
0.00.051.579 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.579 I llm_load_tensors: offloading output layer to GPU
0.00.051.579 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.590 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.591 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.471 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.472 I llama_new_context_with_model: n_ctx         = 128
0.00.052.472 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.472 I llama_new_context_with_model: n_batch       = 128
0.00.052.472 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.473 I llama_new_context_with_model: flash_attn    = 0
0.00.052.473 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.473 I llama_new_context_with_model: freq_scale    = 1
0.00.052.474 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.474 I ggml_metal_init: allocating
0.00.052.480 I ggml_metal_init: found device: Apple M4
0.00.052.484 I ggml_metal_init: picking default device: Apple M4
0.00.053.057 I ggml_metal_init: using embedded metal library
0.00.055.432 I ggml_metal_init: GPU name:   Apple M4
0.00.055.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.435 I ggml_metal_init: simdgroup reduction   = true
0.00.055.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.435 I ggml_metal_init: has bfloat            = true
0.00.055.435 I ggml_metal_init: use bfloat            = true
0.00.055.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.801 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.803 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.820 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.684 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.685 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.686 I llama_new_context_with_model: graph nodes  = 967
0.00.067.686 I llama_new_context_with_model: graph splits = 2
0.00.067.699 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.822 I 
0.00.477.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.878 I perplexity: tokenizing the input ..
0.00.485.549 I perplexity: tokenization took 7.669 ms
0.00.485.552 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.169 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.619.430 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.619.441 I llama_perf_context_print:        load time =     469.15 ms
0.00.619.442 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.84 tokens per second)
0.00.619.442 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.443 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.619.808 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.078s
sys	0m0.084s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.319 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.323 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.325 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.327 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.330 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.091 I llama_model_loader: - type  f32:  194 tensors
0.00.024.091 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.091 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.092 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.605 I llm_load_vocab: special tokens cache size = 25
0.00.050.553 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.555 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.556 I llm_load_print_meta: arch             = gptneox
0.00.050.556 I llm_load_print_meta: vocab type       = BPE
0.00.050.556 I llm_load_print_meta: n_vocab          = 50304
0.00.050.556 I llm_load_print_meta: n_merges         = 50009
0.00.050.557 I llm_load_print_meta: vocab_only       = 0
0.00.050.557 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.557 I llm_load_print_meta: n_embd           = 2048
0.00.050.557 I llm_load_print_meta: n_layer          = 24
0.00.050.571 I llm_load_print_meta: n_head           = 16
0.00.050.573 I llm_load_print_meta: n_head_kv        = 16
0.00.050.573 I llm_load_print_meta: n_rot            = 32
0.00.050.573 I llm_load_print_meta: n_swa            = 0
0.00.050.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.574 I llm_load_print_meta: n_gqa            = 1
0.00.050.575 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.576 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.577 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.577 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.577 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.578 I llm_load_print_meta: n_ff             = 8192
0.00.050.578 I llm_load_print_meta: n_expert         = 0
0.00.050.578 I llm_load_print_meta: n_expert_used    = 0
0.00.050.578 I llm_load_print_meta: causal attn      = 1
0.00.050.578 I llm_load_print_meta: pooling type     = 0
0.00.050.579 I llm_load_print_meta: rope type        = 2
0.00.050.580 I llm_load_print_meta: rope scaling     = linear
0.00.050.580 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.580 I llm_load_print_meta: freq_scale_train = 1
0.00.050.580 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.580 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.581 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.581 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.581 I llm_load_print_meta: model type       = 1.4B
0.00.050.582 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.582 I llm_load_print_meta: model params     = 1.41 B
0.00.050.583 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.583 I llm_load_print_meta: general.name     = 1.4B
0.00.050.583 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.583 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.584 I llm_load_print_meta: LF token         = 128 ''
0.00.050.584 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.584 I llm_load_print_meta: max token length = 1024
0.00.052.222 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.222 I llm_load_tensors: offloading output layer to GPU
0.00.052.222 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.232 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.233 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.051 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.051 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.051 I llama_new_context_with_model: n_batch       = 2048
0.00.053.051 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.051 I llama_new_context_with_model: flash_attn    = 0
0.00.053.052 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.052 I llama_new_context_with_model: freq_scale    = 1
0.00.053.053 I ggml_metal_init: allocating
0.00.053.060 I ggml_metal_init: found device: Apple M4
0.00.053.062 I ggml_metal_init: picking default device: Apple M4
0.00.053.631 I ggml_metal_init: using embedded metal library
0.00.055.968 I ggml_metal_init: GPU name:   Apple M4
0.00.055.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.971 I ggml_metal_init: simdgroup reduction   = true
0.00.055.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.971 I ggml_metal_init: has bfloat            = true
0.00.055.972 I ggml_metal_init: use bfloat            = true
0.00.055.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.666 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.674 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.692 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.680 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.681 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.682 I llama_new_context_with_model: graph nodes  = 967
0.00.085.682 I llama_new_context_with_model: graph splits = 2
0.00.085.706 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.929 I main: llama threadpool init, n_threads = 4
0.00.622.973 I 
0.00.623.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.011 I 
0.00.623.258 I sampler seed: 1234
0.00.623.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.623.297 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.623.298 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.623.298 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.383.574 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.383.574 I llama_perf_context_print:        load time =     612.98 ms
0.01.383.575 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.46 tokens per second)
0.01.383.575 I llama_perf_context_print:        eval time =     710.13 ms /    63 runs   (   11.27 ms per token,    88.72 tokens per second)
0.01.383.580 I llama_perf_context_print:       total time =     760.65 ms /    70 tokens
0.01.383.780 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.511 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.091 I llama_model_loader: - type  f32:  194 tensors
0.00.024.091 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.091 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.092 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.557 I llm_load_vocab: special tokens cache size = 25
0.00.050.535 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.538 I llm_load_print_meta: arch             = gptneox
0.00.050.539 I llm_load_print_meta: vocab type       = BPE
0.00.050.539 I llm_load_print_meta: n_vocab          = 50304
0.00.050.539 I llm_load_print_meta: n_merges         = 50009
0.00.050.539 I llm_load_print_meta: vocab_only       = 0
0.00.050.540 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.540 I llm_load_print_meta: n_embd           = 2048
0.00.050.540 I llm_load_print_meta: n_layer          = 24
0.00.050.555 I llm_load_print_meta: n_head           = 16
0.00.050.556 I llm_load_print_meta: n_head_kv        = 16
0.00.050.556 I llm_load_print_meta: n_rot            = 32
0.00.050.556 I llm_load_print_meta: n_swa            = 0
0.00.050.556 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.557 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.557 I llm_load_print_meta: n_gqa            = 1
0.00.050.558 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.559 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.561 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.562 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.562 I llm_load_print_meta: n_ff             = 8192
0.00.050.562 I llm_load_print_meta: n_expert         = 0
0.00.050.563 I llm_load_print_meta: n_expert_used    = 0
0.00.050.563 I llm_load_print_meta: causal attn      = 1
0.00.050.563 I llm_load_print_meta: pooling type     = 0
0.00.050.563 I llm_load_print_meta: rope type        = 2
0.00.050.564 I llm_load_print_meta: rope scaling     = linear
0.00.050.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.566 I llm_load_print_meta: freq_scale_train = 1
0.00.050.566 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.566 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.566 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.566 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.567 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.567 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.567 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.567 I llm_load_print_meta: model type       = 1.4B
0.00.050.567 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.571 I llm_load_print_meta: model params     = 1.41 B
0.00.050.572 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.572 I llm_load_print_meta: general.name     = 1.4B
0.00.050.572 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: LF token         = 128 ''
0.00.050.573 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: max token length = 1024
0.00.052.593 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.593 I llm_load_tensors: offloading output layer to GPU
0.00.052.593 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.603 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.604 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.563 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.564 I llama_new_context_with_model: n_ctx         = 128
0.00.053.564 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.564 I llama_new_context_with_model: n_batch       = 128
0.00.053.564 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.564 I llama_new_context_with_model: flash_attn    = 0
0.00.053.565 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.565 I llama_new_context_with_model: freq_scale    = 1
0.00.053.565 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.566 I ggml_metal_init: allocating
0.00.053.569 I ggml_metal_init: found device: Apple M4
0.00.053.571 I ggml_metal_init: picking default device: Apple M4
0.00.054.150 I ggml_metal_init: using embedded metal library
0.00.056.487 I ggml_metal_init: GPU name:   Apple M4
0.00.056.489 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.490 I ggml_metal_init: simdgroup reduction   = true
0.00.056.490 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.490 I ggml_metal_init: has bfloat            = true
0.00.056.492 I ggml_metal_init: use bfloat            = true
0.00.056.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.429 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.431 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.445 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.398 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.399 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.399 I llama_new_context_with_model: graph nodes  = 967
0.00.068.399 I llama_new_context_with_model: graph splits = 2
0.00.068.412 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.413 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.560.233 I 
0.00.560.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.560.282 I perplexity: tokenizing the input ..
0.00.568.193 I perplexity: tokenization took 7.909 ms
0.00.568.196 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.564 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.703.761 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.703.776 I llama_perf_context_print:        load time =     550.72 ms
0.00.703.778 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.20 tokens per second)
0.00.703.779 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.779 I llama_perf_context_print:       total time =     143.54 ms /   129 tokens
0.00.704.269 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.078s
sys	0m0.100s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.867 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.257 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.258 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.259 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.262 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.266 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.148 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.149 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.149 I llama_model_loader: - type  f32:  194 tensors
0.00.024.150 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.150 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.436 I llm_load_vocab: special tokens cache size = 25
0.00.051.204 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.206 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.207 I llm_load_print_meta: arch             = gptneox
0.00.051.207 I llm_load_print_meta: vocab type       = BPE
0.00.051.207 I llm_load_print_meta: n_vocab          = 50304
0.00.051.207 I llm_load_print_meta: n_merges         = 50009
0.00.051.208 I llm_load_print_meta: vocab_only       = 0
0.00.051.208 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.208 I llm_load_print_meta: n_embd           = 2048
0.00.051.208 I llm_load_print_meta: n_layer          = 24
0.00.051.223 I llm_load_print_meta: n_head           = 16
0.00.051.223 I llm_load_print_meta: n_head_kv        = 16
0.00.051.224 I llm_load_print_meta: n_rot            = 32
0.00.051.224 I llm_load_print_meta: n_swa            = 0
0.00.051.224 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.224 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.225 I llm_load_print_meta: n_gqa            = 1
0.00.051.226 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.226 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.229 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.229 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.230 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.230 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.230 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.230 I llm_load_print_meta: n_ff             = 8192
0.00.051.231 I llm_load_print_meta: n_expert         = 0
0.00.051.231 I llm_load_print_meta: n_expert_used    = 0
0.00.051.231 I llm_load_print_meta: causal attn      = 1
0.00.051.231 I llm_load_print_meta: pooling type     = 0
0.00.051.231 I llm_load_print_meta: rope type        = 2
0.00.051.231 I llm_load_print_meta: rope scaling     = linear
0.00.051.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.232 I llm_load_print_meta: freq_scale_train = 1
0.00.051.233 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.234 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.234 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.234 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.234 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.235 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.235 I llm_load_print_meta: model type       = 1.4B
0.00.051.236 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.236 I llm_load_print_meta: model params     = 1.41 B
0.00.051.236 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.237 I llm_load_print_meta: general.name     = 1.4B
0.00.051.237 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.237 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.237 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.237 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.238 I llm_load_print_meta: LF token         = 128 ''
0.00.051.238 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.238 I llm_load_print_meta: max token length = 1024
0.00.053.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.343 I llm_load_tensors: offloading output layer to GPU
0.00.053.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.354 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.355 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.264 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.265 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.265 I llama_new_context_with_model: n_batch       = 2048
0.00.054.265 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.265 I llama_new_context_with_model: flash_attn    = 0
0.00.054.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.266 I llama_new_context_with_model: freq_scale    = 1
0.00.054.266 I ggml_metal_init: allocating
0.00.054.271 I ggml_metal_init: found device: Apple M4
0.00.054.275 I ggml_metal_init: picking default device: Apple M4
0.00.054.873 I ggml_metal_init: using embedded metal library
0.00.057.186 I ggml_metal_init: GPU name:   Apple M4
0.00.057.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.188 I ggml_metal_init: simdgroup reduction   = true
0.00.057.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.189 I ggml_metal_init: has bfloat            = true
0.00.057.189 I ggml_metal_init: use bfloat            = true
0.00.057.189 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.312 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.320 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.338 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.391 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.392 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.392 I llama_new_context_with_model: graph nodes  = 967
0.00.087.393 I llama_new_context_with_model: graph splits = 2
0.00.087.416 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.558 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.558 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.513 I main: llama threadpool init, n_threads = 4
0.00.683.545 I 
0.00.683.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.575 I 
0.00.683.722 I sampler seed: 1234
0.00.683.727 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.768 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.771 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.534.100 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 63055.06 tokens per second)
0.01.534.101 I llama_perf_context_print:        load time =     674.64 ms
0.01.534.102 I llama_perf_context_print: prompt eval time =      51.46 ms /     7 tokens (    7.35 ms per token,   136.03 tokens per second)
0.01.534.102 I llama_perf_context_print:        eval time =     796.00 ms /    63 runs   (   12.63 ms per token,    79.15 tokens per second)
0.01.534.102 I llama_perf_context_print:       total time =     850.59 ms /    70 tokens
0.01.534.289 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.110s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.838 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.832 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.835 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.751 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.829 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.725 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.726 I llama_model_loader: - type  f32:  194 tensors
0.00.023.726 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.727 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.726 I llm_load_vocab: special tokens cache size = 25
0.00.050.823 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.825 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.826 I llm_load_print_meta: arch             = gptneox
0.00.050.826 I llm_load_print_meta: vocab type       = BPE
0.00.050.826 I llm_load_print_meta: n_vocab          = 50304
0.00.050.826 I llm_load_print_meta: n_merges         = 50009
0.00.050.826 I llm_load_print_meta: vocab_only       = 0
0.00.050.827 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.827 I llm_load_print_meta: n_embd           = 2048
0.00.050.827 I llm_load_print_meta: n_layer          = 24
0.00.050.841 I llm_load_print_meta: n_head           = 16
0.00.050.842 I llm_load_print_meta: n_head_kv        = 16
0.00.050.842 I llm_load_print_meta: n_rot            = 32
0.00.050.843 I llm_load_print_meta: n_swa            = 0
0.00.050.843 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.843 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.844 I llm_load_print_meta: n_gqa            = 1
0.00.050.844 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.845 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.846 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.846 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.849 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.849 I llm_load_print_meta: n_ff             = 8192
0.00.050.849 I llm_load_print_meta: n_expert         = 0
0.00.050.849 I llm_load_print_meta: n_expert_used    = 0
0.00.050.853 I llm_load_print_meta: causal attn      = 1
0.00.050.853 I llm_load_print_meta: pooling type     = 0
0.00.050.853 I llm_load_print_meta: rope type        = 2
0.00.050.853 I llm_load_print_meta: rope scaling     = linear
0.00.050.854 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.854 I llm_load_print_meta: freq_scale_train = 1
0.00.050.854 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.854 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.855 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.856 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.856 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.856 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.856 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.857 I llm_load_print_meta: model type       = 1.4B
0.00.050.857 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.857 I llm_load_print_meta: model params     = 1.41 B
0.00.050.858 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.858 I llm_load_print_meta: general.name     = 1.4B
0.00.050.858 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.858 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.859 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.859 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.859 I llm_load_print_meta: LF token         = 128 ''
0.00.050.859 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.859 I llm_load_print_meta: max token length = 1024
0.00.052.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.881 I llm_load_tensors: offloading output layer to GPU
0.00.052.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.892 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.893 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.780 I llama_new_context_with_model: n_ctx         = 128
0.00.053.780 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.780 I llama_new_context_with_model: n_batch       = 128
0.00.053.780 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.780 I llama_new_context_with_model: flash_attn    = 0
0.00.053.781 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.781 I llama_new_context_with_model: freq_scale    = 1
0.00.053.781 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.782 I ggml_metal_init: allocating
0.00.053.785 I ggml_metal_init: found device: Apple M4
0.00.053.787 I ggml_metal_init: picking default device: Apple M4
0.00.054.354 I ggml_metal_init: using embedded metal library
0.00.056.724 I ggml_metal_init: GPU name:   Apple M4
0.00.056.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.726 I ggml_metal_init: simdgroup reduction   = true
0.00.056.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.726 I ggml_metal_init: has bfloat            = true
0.00.056.727 I ggml_metal_init: use bfloat            = true
0.00.056.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.765 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.767 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.783 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.773 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.774 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.775 I llama_new_context_with_model: graph nodes  = 967
0.00.068.775 I llama_new_context_with_model: graph splits = 2
0.00.068.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.178 I 
0.00.649.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.237 I perplexity: tokenizing the input ..
0.00.657.173 I perplexity: tokenization took 7.933 ms
0.00.657.176 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.313 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.503 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.514 I llama_perf_context_print:        load time =     640.34 ms
0.00.798.516 I llama_perf_context_print: prompt eval time =     139.91 ms /   128 tokens (    1.09 ms per token,   914.87 tokens per second)
0.00.798.517 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.517 I llama_perf_context_print:       total time =     149.34 ms /   129 tokens
0.00.799.090 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.079s
sys	0m0.119s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.454 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.326 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.329 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.331 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.331 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.332 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.363 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.229 I llama_model_loader: - type  f32:  194 tensors
0.00.025.229 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.562 I llm_load_vocab: special tokens cache size = 25
0.00.052.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.617 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.617 I llm_load_print_meta: arch             = gptneox
0.00.052.617 I llm_load_print_meta: vocab type       = BPE
0.00.052.617 I llm_load_print_meta: n_vocab          = 50304
0.00.052.618 I llm_load_print_meta: n_merges         = 50009
0.00.052.618 I llm_load_print_meta: vocab_only       = 0
0.00.052.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.618 I llm_load_print_meta: n_embd           = 2048
0.00.052.618 I llm_load_print_meta: n_layer          = 24
0.00.052.633 I llm_load_print_meta: n_head           = 16
0.00.052.635 I llm_load_print_meta: n_head_kv        = 16
0.00.052.636 I llm_load_print_meta: n_rot            = 32
0.00.052.636 I llm_load_print_meta: n_swa            = 0
0.00.052.636 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.636 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.638 I llm_load_print_meta: n_gqa            = 1
0.00.052.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.639 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.640 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.640 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.640 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.640 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.641 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.641 I llm_load_print_meta: n_ff             = 8192
0.00.052.641 I llm_load_print_meta: n_expert         = 0
0.00.052.641 I llm_load_print_meta: n_expert_used    = 0
0.00.052.641 I llm_load_print_meta: causal attn      = 1
0.00.052.641 I llm_load_print_meta: pooling type     = 0
0.00.052.642 I llm_load_print_meta: rope type        = 2
0.00.052.642 I llm_load_print_meta: rope scaling     = linear
0.00.052.642 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.642 I llm_load_print_meta: freq_scale_train = 1
0.00.052.643 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.643 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.643 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.643 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.643 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.643 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.643 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.644 I llm_load_print_meta: model type       = 1.4B
0.00.052.644 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.644 I llm_load_print_meta: model params     = 1.41 B
0.00.052.645 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.645 I llm_load_print_meta: general.name     = 1.4B
0.00.052.645 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.646 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.646 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.646 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.646 I llm_load_print_meta: LF token         = 128 ''
0.00.052.647 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.648 I llm_load_print_meta: max token length = 1024
0.00.054.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.712 I llm_load_tensors: offloading output layer to GPU
0.00.054.712 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.722 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.724 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.630 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.631 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.631 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.631 I llama_new_context_with_model: n_batch       = 2048
0.00.055.631 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.631 I llama_new_context_with_model: flash_attn    = 0
0.00.055.632 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.632 I llama_new_context_with_model: freq_scale    = 1
0.00.055.632 I ggml_metal_init: allocating
0.00.055.635 I ggml_metal_init: found device: Apple M4
0.00.055.637 I ggml_metal_init: picking default device: Apple M4
0.00.056.221 I ggml_metal_init: using embedded metal library
0.00.058.534 I ggml_metal_init: GPU name:   Apple M4
0.00.058.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.536 I ggml_metal_init: simdgroup reduction   = true
0.00.058.537 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.537 I ggml_metal_init: has bfloat            = true
0.00.058.537 I ggml_metal_init: use bfloat            = true
0.00.058.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.849 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.860 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.876 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.900 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.901 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.902 I llama_new_context_with_model: graph nodes  = 967
0.00.088.902 I llama_new_context_with_model: graph splits = 2
0.00.088.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.057 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.197 I main: llama threadpool init, n_threads = 4
0.00.754.238 I 
0.00.754.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.268 I 
0.00.754.541 I sampler seed: 1234
0.00.754.547 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.564 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.564 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.564 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.634.408 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.634.408 I llama_perf_context_print:        load time =     744.74 ms
0.01.634.409 I llama_perf_context_print: prompt eval time =      54.07 ms /     7 tokens (    7.72 ms per token,   129.46 tokens per second)
0.01.634.410 I llama_perf_context_print:        eval time =     822.89 ms /    63 runs   (   13.06 ms per token,    76.56 tokens per second)
0.01.634.410 I llama_perf_context_print:       total time =     880.21 ms /    70 tokens
0.01.634.611 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4345 (2230786e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.243 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.943 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.952 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.955 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.956 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.956 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.959 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.698 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.699 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.699 I llama_model_loader: - type  f32:  194 tensors
0.00.023.700 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.022 I llm_load_vocab: special tokens cache size = 25
0.00.049.864 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.866 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.867 I llm_load_print_meta: arch             = gptneox
0.00.049.867 I llm_load_print_meta: vocab type       = BPE
0.00.049.867 I llm_load_print_meta: n_vocab          = 50304
0.00.049.867 I llm_load_print_meta: n_merges         = 50009
0.00.049.868 I llm_load_print_meta: vocab_only       = 0
0.00.049.868 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.868 I llm_load_print_meta: n_embd           = 2048
0.00.049.868 I llm_load_print_meta: n_layer          = 24
0.00.049.882 I llm_load_print_meta: n_head           = 16
0.00.049.883 I llm_load_print_meta: n_head_kv        = 16
0.00.049.883 I llm_load_print_meta: n_rot            = 32
0.00.049.883 I llm_load_print_meta: n_swa            = 0
0.00.049.883 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.883 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.884 I llm_load_print_meta: n_gqa            = 1
0.00.049.885 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.885 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.886 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.886 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.886 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.889 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.889 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.889 I llm_load_print_meta: n_ff             = 8192
0.00.049.889 I llm_load_print_meta: n_expert         = 0
0.00.049.889 I llm_load_print_meta: n_expert_used    = 0
0.00.049.890 I llm_load_print_meta: causal attn      = 1
0.00.049.891 I llm_load_print_meta: pooling type     = 0
0.00.049.891 I llm_load_print_meta: rope type        = 2
0.00.049.891 I llm_load_print_meta: rope scaling     = linear
0.00.049.891 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.892 I llm_load_print_meta: freq_scale_train = 1
0.00.049.892 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.892 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.892 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.892 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.892 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.892 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.892 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.896 I llm_load_print_meta: model type       = 1.4B
0.00.049.896 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.896 I llm_load_print_meta: model params     = 1.41 B
0.00.049.897 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.897 I llm_load_print_meta: general.name     = 1.4B
0.00.049.897 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.897 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.897 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.897 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.898 I llm_load_print_meta: LF token         = 128 ''
0.00.049.899 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.899 I llm_load_print_meta: max token length = 1024
0.00.051.913 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.913 I llm_load_tensors: offloading output layer to GPU
0.00.051.914 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.924 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.926 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.176 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.177 I llama_new_context_with_model: n_ctx         = 128
0.00.053.177 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.177 I llama_new_context_with_model: n_batch       = 128
0.00.053.177 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.178 I llama_new_context_with_model: flash_attn    = 0
0.00.053.178 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.178 I llama_new_context_with_model: freq_scale    = 1
0.00.053.179 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.179 I ggml_metal_init: allocating
0.00.053.184 I ggml_metal_init: found device: Apple M4
0.00.053.187 I ggml_metal_init: picking default device: Apple M4
0.00.053.764 I ggml_metal_init: using embedded metal library
0.00.056.062 I ggml_metal_init: GPU name:   Apple M4
0.00.056.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.065 I ggml_metal_init: simdgroup reduction   = true
0.00.056.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.065 I ggml_metal_init: has bfloat            = true
0.00.056.065 I ggml_metal_init: use bfloat            = true
0.00.056.065 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.066 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.763 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.766 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.779 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.657 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.658 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.659 I llama_new_context_with_model: graph nodes  = 967
0.00.067.659 I llama_new_context_with_model: graph splits = 2
0.00.067.672 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.278.622 I 
0.00.278.663 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.278.676 I perplexity: tokenizing the input ..
0.00.286.510 I perplexity: tokenization took 7.832 ms
0.00.286.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.426.870 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.428.120 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.428.142 I llama_perf_context_print:        load time =     269.37 ms
0.00.428.142 I llama_perf_context_print: prompt eval time =     140.06 ms /   128 tokens (    1.09 ms per token,   913.89 tokens per second)
0.00.428.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.428.144 I llama_perf_context_print:       total time =     149.52 ms /   129 tokens
0.00.428.642 I ggml_metal_free: deallocating

real	0m0.444s
user	0m0.078s
sys	0m0.056s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4345 (2230786e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d90a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d90a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d90aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d90b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d90ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d90bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d90c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d90cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d90d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d90d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d90daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d90dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d90eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d90f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d90fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d9101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d910910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d911030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d911750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d911f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d912640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d912d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d913480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d913d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d914440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d914700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d914d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d915980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d915ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d916180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d916620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d9168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d917170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d9176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d917970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d917e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d9182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d918750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d918bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d919090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d919530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d9199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d919e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d91a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d91a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d91abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d91b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d91bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d91c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d91c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d91cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d91d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d91d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d91df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d91e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d91ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d91f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d91f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d91f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d920160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d920420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d9208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d920d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d921200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d9216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d921b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d921fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d922480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d922920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d922dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d923260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d923700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d923ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d9240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d924640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d924b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d9250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d925630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d925b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d9260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d926620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d926b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d9270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d927610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d927b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d9280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d928600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d928b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d9290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d9295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d929b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d92a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d92a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d92ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d92b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d92b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d92bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d91b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d92bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d92c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d92cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d92d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d92d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d92dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d92e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d92e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d92ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d92f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d92f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d92fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d9301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d930700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d930c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d9310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d931590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d931a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d931ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d932370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d932810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d932cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d933150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d9335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d933a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d933f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d9343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d934870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d934d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d9351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d935650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d935af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d935f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d936430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d9368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d936d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d937210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d9376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d937b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d937ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d938490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d938930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d938dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d939270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d939710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d939bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d93a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d93a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d93a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d93ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d93b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d93b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d93bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d93c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d93c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d93c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d93ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d93d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d93d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d93dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d93e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d93e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d93ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d93eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d93f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d93f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d93fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d940170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d940610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d940ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d940f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d9413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d941890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d941d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d9421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d942670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d942b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d942fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d943450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d9438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d943d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d944230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d9446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d944b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d945010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d9454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d945950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d945df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d946290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d946730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d946bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d947070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d947510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d9479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d947e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d9483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d9488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d948e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d949390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d949650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d949c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d94a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d94a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d94b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d94b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d94b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d94bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d94c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d94cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d94d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d94d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d94d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d94e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d94e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d94ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d94f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d94f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d94fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d950150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d9506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d950bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d951140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d951690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d951be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d952130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d952680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d952bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d953120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d953670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d953bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d954110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d954660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d954bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d955100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d955650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d955ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d9560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d956640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d956b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d9570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d957630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d957b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d9580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d958620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d958b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d9590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d959610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d959b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d95a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d95a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d95ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d95b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d95b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d95bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d95c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d95c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d95cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d95d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d95d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d95db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d95e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d95e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d95eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d95f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d95f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d95fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d960050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d9605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d960af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d960f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d961430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d9618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d961d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d962210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d9626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d962b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d962ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d963490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d963930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d963dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d964270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d964710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d964bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d965050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d9655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d965cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d9663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d966b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d967220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d9674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d967cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d967f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d9685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.144.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f306100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f306570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f3069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f306e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f3072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f307730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f307ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f304080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f3044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f304960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f308010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f3085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f309120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f3098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f30a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f30a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f30af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f30b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f30bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f30c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f30cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f30d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f30da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f30e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f30e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f30eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f30f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f30f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f30fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f3105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f310a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f310d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f3115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f311ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f311da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f312240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f3126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f312b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f313020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f3134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f313960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f313e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f3142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f314740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f314a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f315010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f315620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f315c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f316240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f316850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f316e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f317470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f317a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f318090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f318880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f318d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f3191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f319480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f319a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f31a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f31a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f31abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f31b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f31b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f31b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f31be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f31c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f31c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f31cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f31d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f31d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f31da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f31dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f31e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f31e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f31ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f31f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f31f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f31fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f3203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f320920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f320e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f3213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f321910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f321e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f3223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f322900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f322e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f3233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f3238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f323e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f324390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f3248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f324e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f325380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f3258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f325e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f326370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f3268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f326e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f327360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f3278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f327e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f328350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f3288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f328df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f329340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f329890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f329de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f32a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f32a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f32add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f32b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f32b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f32bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f32c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f32c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f32ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f32cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f32d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f32d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f32dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f32e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f32e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f32eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f32ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f32f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f32f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f32fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f3301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f330660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f330b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f330fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f331440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f3318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f331d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f332220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f3326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f332b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f333000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f3334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f333940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f333de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f334280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f334720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f334bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f335060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f335500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f3359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f335e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f3362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f336780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f336c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f3370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f337560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f337a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f337ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f338340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f3387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f338c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f339120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f3395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f339a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f339f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f33a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f33a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f33ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f33b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f33b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f33bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f33bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f33c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f33c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f33cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f33d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f33d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f33db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f33dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f33e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f33e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f33eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f33f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f33f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f33fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f340020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f3404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f340960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f340e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f3412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f341740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f341be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f342080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f342520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f342a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f342fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f343510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f343a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f343d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f344330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f344940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f344f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f345740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f345be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f345ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f3464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f346ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f3472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f347750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f347bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f348090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f348840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f348d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f3492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f349d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f34a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f34a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f34ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f34b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f34b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f34bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f34c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f34c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f34cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f34d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f34d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f34dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f34e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f34e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f34ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f34f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f34f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f34fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f350270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f3507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f350d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f351260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f3517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f351d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f352250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f3527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f352cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f353240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f353790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f353ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f354230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f354780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f354cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f355220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f355770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f355cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f356210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f356760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f356cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f357200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f357750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f357ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f3581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f358740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f358c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f3591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f359730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f359c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f35a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f35a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f35ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f35b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f35b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f35bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f35bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f35c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f35c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f35cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f35d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f35d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f35db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f35e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f35e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f35e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f35ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f35f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f35f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f35fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f360390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f360ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f3611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f3618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f361bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f3623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f362660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f362c70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f7078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f7083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f70a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f70b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f70b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f70bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f70c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f70cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f70d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f70db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f70de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f70e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f70ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f70f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f70fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f7107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f7110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f7119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f7138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f7141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f7157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f7160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f7185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f71a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f71a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f71a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f71adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f71b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f71b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f71bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f71c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f71ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f71d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f71d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f71da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f71de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f71e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f71ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f71f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f71f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f71f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f71fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f7229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f7232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f723bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f724490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f724900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f724d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f7251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f725650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f725ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f725f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f7263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f726810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f726c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f7270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f727560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f7279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f727e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f7282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f728b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f729000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f729470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f7298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f729d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f72a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f72a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f72aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f72af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f72b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f72b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f72bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f72c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f72c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f72c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f72ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f72d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f72d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f72db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f72dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f72e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f72e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f72ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f72f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f72f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f72fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f72fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f730360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f7307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f730c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f7310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f731520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f731990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f731e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f732270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f7326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f732b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f732fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f733430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f7338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f734180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f7345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f734ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f735340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f7357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f735c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f736500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f736970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f7376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f737b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f737fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f738410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f738880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f738cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f739160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f7395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f739a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f739eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f73a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f73a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f73ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f73b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f73b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f73b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f73bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f73c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f73c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f73cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f73cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f73d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f73d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f73dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f73e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f73e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f73ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f73ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f73f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f73f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f73fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f7405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f740a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f741a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f741f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f742400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f742870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f742ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f743150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f7435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f743a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f743ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f744310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f744bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f745060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f7454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f745940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f745db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f746b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f746f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f7473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f747850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f747cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f7485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f7492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f749760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f749bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f74a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f74a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f74a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f74b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f74b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f74b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f74be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f74c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f74c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f74cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f74cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f74d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f74d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f74dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f74e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f74e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f74ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f74ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f74f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f74f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f74fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f7500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f7509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f750e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f751280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f7516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f751b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f751fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f7528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f752d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f753190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f753600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f753ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f754350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f7547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f754c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f7550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f755510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f755980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f7563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f756b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f757230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f757950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f758680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f758c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.835s
user	0m0.293s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4345 (2230786e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153807640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153807d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153808300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1538088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153808e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153809410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1538099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153809f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15380a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15380aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15380af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15380b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15380bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15380c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15380cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15380d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15380dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15380e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15380eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15380f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15380fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153810190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1538108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153811150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153811870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153811b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153812db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1538132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1538135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153813a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153813d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1538145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153814ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153814da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153815240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1538156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153815b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1538164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153816960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153816e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1538172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153817740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153817a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153818010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153818620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153818f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153819550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15381a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15381a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15381ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15381b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15381bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15381c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15381c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15381c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15381cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15381d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15381d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15381dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15381e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15381e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15381ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15381ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15381f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15381f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15381fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1538201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153820690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153820b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153820fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153821520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153821a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153821fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153822510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153822a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153822fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153823500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153823fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1538244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153824a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153824f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1538254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153825a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153825f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1538264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153826a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153826f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1538274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153827a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153827f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1538284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153828a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153828f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153818c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1538293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153829b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15382a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15382a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15382ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15382b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15382b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15382bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15382c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15382c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15382cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15382d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15382d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15382db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15382e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15382e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15382e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15382ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15382f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15382f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15382fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1538300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153830580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153830a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153830ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153831360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153831800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153831ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153832140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1538325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153832a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153832f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1538333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153833860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153833d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1538341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153834640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153834ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153834f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153835420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1538358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153835d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153836200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1538366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153836b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153836fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153837480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153837920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153837dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153838260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153838700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153838ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153839040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1538394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153839980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15383a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15383a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15383ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15383b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15383b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15383b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15383be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15383c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15383c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15383cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15383d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15383d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15383da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15383dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15383e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15383e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15383ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15383f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15383f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15383faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15383ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1538403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153840880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153840d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1538411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153841660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153841fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153842440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1538428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153842d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153843220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1538436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153843b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153844000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1538444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153844940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153844de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153845280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1538457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153845d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153846270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1538467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153846a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153847090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1538476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153847cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1538484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153848940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153848c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153849210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153849820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15384a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15384a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15384a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15384adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15384b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15384baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15384c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15384c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15384cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15384d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15384d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15384dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15384e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15384e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15384eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15384f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15384f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15384fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153850000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153850550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153850aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153850ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153851540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153851a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153851fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153852530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153852a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153852fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153853520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153853a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153853fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153854510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153854a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153854fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153855500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153855a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153855fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1538564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153856a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1538574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153857a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153857f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1538584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153858a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153858f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1538594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153859a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153859f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15385a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15385aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15385af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15385b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15385b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15385bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15385c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15385c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15385cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15385d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15385d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15385df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15385e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15385e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15385ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15385f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15385f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15385fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15385ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153860420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1538608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153860d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153861200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1538616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153861b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153861fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153862480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1538629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1538630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153863810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153863f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153864650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153864910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153865100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1538653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1538659d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.090.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153829be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15382a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15382a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15382a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15382ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15382b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15382b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15382baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15382bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15382c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15382c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15382ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15382d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15382de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15382e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15382ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15382f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15382fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153830230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153830bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1538312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153831990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153832080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153832770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153832e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1538332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153833740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153833bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153834020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153834490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153834900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153834d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1538351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1538354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153835910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153835d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1538361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153836660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153836ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153836f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1538373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153837820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153837c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153838100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153838570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1538389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153838e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1538392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153839730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153839ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15383a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15383a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15383a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15383ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15383b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15383b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15383bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15383bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15383c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15383c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15383cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15383d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15383d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15383d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15383de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15383e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15383e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15383eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15383eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15383f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15383f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15383fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1538401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153840620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153840a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153840f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153841370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1538417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153841c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1538420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153842530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1538429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153842e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153843280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1538436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153843b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153843fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153844440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1538448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153845190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153845600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153845a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153845ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153846350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1538467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153846c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1538470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153847510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153847980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153847df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153848260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1538486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153848b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153848fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153849420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153849890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153849d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15384a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15384a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15384aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15384aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15384b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15384b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15384bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15384c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15384c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15384c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15384cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15384d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15384d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15384db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15384df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15384e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15384e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15384ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15384f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15384f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15384fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15384fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153850780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153850bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153851060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1538514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153851940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153851db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153852220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153852690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153852b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153852f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1538533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153853850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153853cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153854130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1538545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153854a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153854e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1538552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153855760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153855bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153856040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1538564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153856920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153856d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153857200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153857670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153857ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153857f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1538583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153858830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153858ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153859110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153859580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1538599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153859e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15385a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15385a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15385abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15385b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15385b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15385b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15385bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15385c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15385c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15385cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15385cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15385d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15385d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15385dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15385e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15385e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15385e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15385ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15385f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15385f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15385fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153860000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153860470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1538608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153860d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1538611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153861630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153861aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153861f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153862380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1538627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153862c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1538630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153863540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1538639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153863e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153864290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153864700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153864fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153865450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1538658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1538086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153808150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153807640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153821f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1538223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153822810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153822c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1538230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153823560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1538239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153823e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1538242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153824720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153824b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153825000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153825470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1538258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153825d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1538261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153826630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153826aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153826f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153827380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1538277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153827c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1538280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153828540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1538289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153828e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153814ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153814f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1538153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153815820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153815c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153816100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153816570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1538169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153816e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1538172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153817730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153817ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153818010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153818480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1538188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153818d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1538191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153819640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153819ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153819f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15381a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15381a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15381ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15381b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15381b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15381b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15381be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15381c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15381c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15381cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15381cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15381d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15381d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15381dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15381e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15381e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15381ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15381ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15381f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15381f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15381fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1538200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153820530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1538209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153820e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153821280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153821970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153813710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153813e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1538144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15380ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15380b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15380b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15380b8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1526093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152609850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152609cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15260a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15260a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15260aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15260ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15260b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15260b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15260bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15260c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15260c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15260d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15260d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15260e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15260e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15260f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15260f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15260fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152610650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1526122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1526129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152612cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152612f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1526133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152613850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152613cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152614130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152614660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152614ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152615200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152615670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152615f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1526163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152616830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152616ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152617580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1526179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152617e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1526182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152618740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152618bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152619020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152619490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152619d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15261a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15261a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15261aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15261af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15261b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15261b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15261be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15261c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15261c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15261cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15261cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15261d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15261d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15261dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15261e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15261e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15261ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15261eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15261f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15261f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15261fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1526200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152620510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152620980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152620df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1526216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152622890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152622d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152623170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1526235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152623a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152623ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152624330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1526247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152624c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152625080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1526254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152625960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152625dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152626240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1526266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152626b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152626f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152627400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152627ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152628150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1526285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152628a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152628ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152629310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152629780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152629bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15262a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15262a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15262a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15262adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15262b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15262b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15262bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15262bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15262c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15262c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15262ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15262d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15262d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15262da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15262de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15262e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15262e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15262ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15262f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15262f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15262f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15262fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152630200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152630670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152630ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152630f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1526313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152631830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152631ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152632110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152632580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1526329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152632e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1526332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152633740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152634020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152634d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1526351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152635f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1526363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152636810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152636c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1526370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152637560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1526379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152637e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1526382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152638720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152638b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152639000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152639470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1526398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152639d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15263a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15263a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15263aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15263af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15263b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15263b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15263bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15263c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15263c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15263c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15263ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15263d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15263d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15263db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15263dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15263e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15263e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15263ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15263f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15263f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15263fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15263fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152640360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1526407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152640c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1526410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152641520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1526426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152642b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152642fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152643430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1526438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152643d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152644180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1526445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152644a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152644ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1526458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152645d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152646890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152646b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152646e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152647280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1526476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152647b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152647fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152648440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1526488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152648d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152649190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152649600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152649ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15264a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15264a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15264ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15264b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15264b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15264b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15264bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15264c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15264c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15264cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15264cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15264d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15264d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15264dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15264e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15264e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15264ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15264eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15264f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15264f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152650110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1526503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152650cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152651120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152651590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152651a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1526522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152652750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152652bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152653030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1526534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152653910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152653d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1526541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152654ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152654f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1526553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152655820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152655c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152656100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152656570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1526569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152656e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1526572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152657730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152657ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152658010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152658480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1526588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152658d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1526591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152659640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152659ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152659f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15265a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15265a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15265b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15265b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15265c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15265c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15265ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15265cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15265d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15265db10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.941s
user	0m0.247s
sys	0m0.153s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.59 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.18 sec*proc (2 tests)

Total Test time (real) =   1.19 sec
        1.22 real         0.74 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.27 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.56 real         0.15 user         0.05 sys
```
