### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.37 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.23 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.68 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.36 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.97 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.05 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.85 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.68 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.09 sec*proc (29 tests)

Total Test time (real) = 165.10 sec

real	2m45.111s
user	4m38.203s
sys	0m5.692s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.78 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.25 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.43 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.40 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.03 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.37 sec*proc (29 tests)

Total Test time (real) =  48.39 sec

real	0m48.399s
user	0m54.371s
sys	0m5.232s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.127 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.362 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.366 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.368 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.369 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.369 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.370 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.371 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.371 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.372 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.372 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.372 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.375 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.375 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.375 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.376 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.376 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.376 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.377 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.022.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.023.004 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.005 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.023.006 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.023.006 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.023.006 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.023.007 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.023.007 I llama_model_loader: - type  f32:  124 tensors
0.00.023.007 I llama_model_loader: - type  f16:   73 tensors
0.00.023.008 I print_info: file format = GGUF V3 (latest)
0.00.023.009 I print_info: file type   = F16
0.00.023.010 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.025.331 I load: special tokens cache size = 5
0.00.026.516 I load: token to piece cache size = 0.2032 MB
0.00.026.539 I print_info: arch             = bert
0.00.026.539 I print_info: vocab_only       = 0
0.00.026.540 I print_info: n_ctx_train      = 512
0.00.026.540 I print_info: n_embd           = 384
0.00.026.540 I print_info: n_layer          = 12
0.00.026.543 I print_info: n_head           = 12
0.00.026.544 I print_info: n_head_kv        = 12
0.00.026.544 I print_info: n_rot            = 32
0.00.026.544 I print_info: n_swa            = 0
0.00.026.544 I print_info: n_embd_head_k    = 32
0.00.026.544 I print_info: n_embd_head_v    = 32
0.00.026.545 I print_info: n_gqa            = 1
0.00.026.546 I print_info: n_embd_k_gqa     = 384
0.00.026.546 I print_info: n_embd_v_gqa     = 384
0.00.026.547 I print_info: f_norm_eps       = 1.0e-12
0.00.026.547 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.026.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.026.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.026.548 I print_info: f_logit_scale    = 0.0e+00
0.00.026.549 I print_info: n_ff             = 1536
0.00.026.549 I print_info: n_expert         = 0
0.00.026.550 I print_info: n_expert_used    = 0
0.00.026.550 I print_info: causal attn      = 0
0.00.026.550 I print_info: pooling type     = 2
0.00.026.550 I print_info: rope type        = 2
0.00.026.553 I print_info: rope scaling     = linear
0.00.026.554 I print_info: freq_base_train  = 10000.0
0.00.026.554 I print_info: freq_scale_train = 1
0.00.026.554 I print_info: n_ctx_orig_yarn  = 512
0.00.026.555 I print_info: rope_finetuned   = unknown
0.00.026.555 I print_info: ssm_d_conv       = 0
0.00.026.555 I print_info: ssm_d_inner      = 0
0.00.026.555 I print_info: ssm_d_state      = 0
0.00.026.555 I print_info: ssm_dt_rank      = 0
0.00.026.555 I print_info: ssm_dt_b_c_rms   = 0
0.00.026.555 I print_info: model type       = 33M
0.00.026.556 I print_info: model params     = 33.21 M
0.00.026.556 I print_info: general.name     = Bge Small
0.00.026.557 I print_info: vocab type       = WPM
0.00.026.557 I print_info: n_vocab          = 30522
0.00.026.557 I print_info: n_merges         = 0
0.00.026.557 I print_info: BOS token        = 101 '[CLS]'
0.00.026.558 I print_info: UNK token        = 100 '[UNK]'
0.00.026.559 I print_info: SEP token        = 102 '[SEP]'
0.00.026.560 I print_info: PAD token        = 0 '[PAD]'
0.00.026.560 I print_info: MASK token       = 103 '[MASK]'
0.00.026.560 I print_info: LF token         = 0 '[PAD]'
0.00.026.560 I print_info: max token length = 21
0.00.026.561 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.028.628 I load_tensors: offloading 12 repeating layers to GPU
0.00.028.629 I load_tensors: offloading output layer to GPU
0.00.028.629 I load_tensors: offloaded 13/13 layers to GPU
0.00.028.649 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.028.651 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.028.872 I llama_init_from_model: n_seq_max     = 1
0.00.028.873 I llama_init_from_model: n_ctx         = 512
0.00.028.873 I llama_init_from_model: n_ctx_per_seq = 512
0.00.028.873 I llama_init_from_model: n_batch       = 2048
0.00.028.873 I llama_init_from_model: n_ubatch      = 2048
0.00.028.874 I llama_init_from_model: flash_attn    = 0
0.00.028.874 I llama_init_from_model: freq_base     = 10000.0
0.00.028.874 I llama_init_from_model: freq_scale    = 1
0.00.028.875 I ggml_metal_init: allocating
0.00.028.884 I ggml_metal_init: found device: Apple M4
0.00.028.890 I ggml_metal_init: picking default device: Apple M4
0.00.029.375 I ggml_metal_init: using embedded metal library
0.00.031.933 I ggml_metal_init: GPU name:   Apple M4
0.00.031.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.031.936 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.031.936 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.031.937 I ggml_metal_init: simdgroup reduction   = true
0.00.031.937 I ggml_metal_init: simdgroup matrix mul. = true
0.00.031.937 I ggml_metal_init: has residency sets    = true
0.00.031.937 I ggml_metal_init: has bfloat            = true
0.00.031.937 I ggml_metal_init: use bfloat            = true
0.00.031.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.031.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.042.589 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.043.179 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.043.181 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.043.183 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.044.187 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.044.188 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.044.189 I llama_init_from_model: graph nodes  = 429
0.00.044.189 I llama_init_from_model: graph splits = 2
0.00.044.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.044.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.048.620 I 
0.00.048.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.183 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.053.536 I llama_perf_context_print:        load time =      30.67 ms
0.00.053.537 I llama_perf_context_print: prompt eval time =       4.22 ms /     9 tokens (    0.47 ms per token,  2130.18 tokens per second)
0.00.053.538 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.053.539 I llama_perf_context_print:       total time =       4.92 ms /    10 tokens
0.00.053.755 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.035s
sys	0m0.026s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.049 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.980 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.365 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.370 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.371 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.372 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.372 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.373 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.373 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.374 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.374 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.375 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.377 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.378 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.378 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.378 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.379 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.379 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.460 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.053 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.054 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.054 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.055 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.055 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.055 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.056 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.056 I llama_model_loader: - type  f32:  124 tensors
0.00.014.056 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.057 I print_info: file format = GGUF V3 (latest)
0.00.014.058 I print_info: file type   = Q8_0
0.00.014.059 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.332 I load: special tokens cache size = 5
0.00.017.589 I load: token to piece cache size = 0.2032 MB
0.00.017.599 I print_info: arch             = bert
0.00.017.600 I print_info: vocab_only       = 0
0.00.017.600 I print_info: n_ctx_train      = 512
0.00.017.600 I print_info: n_embd           = 384
0.00.017.601 I print_info: n_layer          = 12
0.00.017.603 I print_info: n_head           = 12
0.00.017.604 I print_info: n_head_kv        = 12
0.00.017.604 I print_info: n_rot            = 32
0.00.017.604 I print_info: n_swa            = 0
0.00.017.607 I print_info: n_embd_head_k    = 32
0.00.017.607 I print_info: n_embd_head_v    = 32
0.00.017.608 I print_info: n_gqa            = 1
0.00.017.609 I print_info: n_embd_k_gqa     = 384
0.00.017.609 I print_info: n_embd_v_gqa     = 384
0.00.017.610 I print_info: f_norm_eps       = 1.0e-12
0.00.017.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.611 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.611 I print_info: f_logit_scale    = 0.0e+00
0.00.017.612 I print_info: n_ff             = 1536
0.00.017.612 I print_info: n_expert         = 0
0.00.017.612 I print_info: n_expert_used    = 0
0.00.017.612 I print_info: causal attn      = 0
0.00.017.612 I print_info: pooling type     = 2
0.00.017.612 I print_info: rope type        = 2
0.00.017.613 I print_info: rope scaling     = linear
0.00.017.613 I print_info: freq_base_train  = 10000.0
0.00.017.613 I print_info: freq_scale_train = 1
0.00.017.613 I print_info: n_ctx_orig_yarn  = 512
0.00.017.614 I print_info: rope_finetuned   = unknown
0.00.017.614 I print_info: ssm_d_conv       = 0
0.00.017.614 I print_info: ssm_d_inner      = 0
0.00.017.614 I print_info: ssm_d_state      = 0
0.00.017.614 I print_info: ssm_dt_rank      = 0
0.00.017.614 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.615 I print_info: model type       = 33M
0.00.017.615 I print_info: model params     = 33.21 M
0.00.017.615 I print_info: general.name     = Bge Small
0.00.017.616 I print_info: vocab type       = WPM
0.00.017.616 I print_info: n_vocab          = 30522
0.00.017.616 I print_info: n_merges         = 0
0.00.017.616 I print_info: BOS token        = 101 '[CLS]'
0.00.017.617 I print_info: UNK token        = 100 '[UNK]'
0.00.017.617 I print_info: SEP token        = 102 '[SEP]'
0.00.017.617 I print_info: PAD token        = 0 '[PAD]'
0.00.017.617 I print_info: MASK token       = 103 '[MASK]'
0.00.017.617 I print_info: LF token         = 0 '[PAD]'
0.00.017.618 I print_info: max token length = 21
0.00.017.618 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.301 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.302 I load_tensors: offloading output layer to GPU
0.00.019.302 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.308 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.309 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.484 I llama_init_from_model: n_seq_max     = 1
0.00.019.485 I llama_init_from_model: n_ctx         = 512
0.00.019.485 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.486 I llama_init_from_model: n_batch       = 2048
0.00.019.486 I llama_init_from_model: n_ubatch      = 2048
0.00.019.486 I llama_init_from_model: flash_attn    = 0
0.00.019.486 I llama_init_from_model: freq_base     = 10000.0
0.00.019.487 I llama_init_from_model: freq_scale    = 1
0.00.019.487 I ggml_metal_init: allocating
0.00.019.491 I ggml_metal_init: found device: Apple M4
0.00.019.496 I ggml_metal_init: picking default device: Apple M4
0.00.019.921 I ggml_metal_init: using embedded metal library
0.00.022.312 I ggml_metal_init: GPU name:   Apple M4
0.00.022.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.314 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.315 I ggml_metal_init: simdgroup reduction   = true
0.00.022.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.315 I ggml_metal_init: has residency sets    = true
0.00.022.315 I ggml_metal_init: has bfloat            = true
0.00.022.315 I ggml_metal_init: use bfloat            = true
0.00.022.316 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.317 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.068 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.668 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.670 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.673 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.648 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.649 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.649 I llama_init_from_model: graph nodes  = 429
0.00.034.649 I llama_init_from_model: graph splits = 2
0.00.034.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.651 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.768 I 
0.00.038.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.302 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.710 I llama_perf_context_print:        load time =      29.78 ms
0.00.043.711 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2105.26 tokens per second)
0.00.043.712 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.713 I llama_perf_context_print:       total time =       4.94 ms /    10 tokens
0.00.043.879 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.265 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.498 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.083 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.090 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.091 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.092 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.093 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.094 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.095 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.095 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.096 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.096 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.100 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.100 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.101 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.101 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.102 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.485 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.485 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.486 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.486 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.486 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.487 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.487 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.487 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.488 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.488 I llama_model_loader: - type  f32:   40 tensors
0.00.048.488 I llama_model_loader: - type  f16:   30 tensors
0.00.048.489 I print_info: file format = GGUF V3 (latest)
0.00.048.490 I print_info: file type   = F16
0.00.048.491 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.690 W load: empty token at index 5
0.00.057.629 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.048 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.083 I load: special tokens cache size = 5
0.00.319.311 I load: token to piece cache size = 1.5060 MB
0.00.319.342 I print_info: arch             = jina-bert-v2
0.00.319.343 I print_info: vocab_only       = 0
0.00.319.343 I print_info: n_ctx_train      = 8192
0.00.319.343 I print_info: n_embd           = 384
0.00.319.344 I print_info: n_layer          = 4
0.00.319.350 I print_info: n_head           = 12
0.00.319.350 I print_info: n_head_kv        = 12
0.00.319.350 I print_info: n_rot            = 32
0.00.319.350 I print_info: n_swa            = 0
0.00.319.351 I print_info: n_embd_head_k    = 32
0.00.319.351 I print_info: n_embd_head_v    = 32
0.00.319.351 I print_info: n_gqa            = 1
0.00.319.352 I print_info: n_embd_k_gqa     = 384
0.00.319.352 I print_info: n_embd_v_gqa     = 384
0.00.319.355 I print_info: f_norm_eps       = 1.0e-12
0.00.319.358 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.319.358 I print_info: f_clamp_kqv      = 0.0e+00
0.00.319.359 I print_info: f_max_alibi_bias = 8.0e+00
0.00.319.359 I print_info: f_logit_scale    = 0.0e+00
0.00.319.359 I print_info: n_ff             = 1536
0.00.319.360 I print_info: n_expert         = 0
0.00.319.360 I print_info: n_expert_used    = 0
0.00.319.360 I print_info: causal attn      = 0
0.00.319.360 I print_info: pooling type     = -1
0.00.319.360 I print_info: rope type        = -1
0.00.319.360 I print_info: rope scaling     = linear
0.00.319.361 I print_info: freq_base_train  = 10000.0
0.00.319.361 I print_info: freq_scale_train = 1
0.00.319.361 I print_info: n_ctx_orig_yarn  = 8192
0.00.319.362 I print_info: rope_finetuned   = unknown
0.00.319.362 I print_info: ssm_d_conv       = 0
0.00.319.362 I print_info: ssm_d_inner      = 0
0.00.319.362 I print_info: ssm_d_state      = 0
0.00.319.362 I print_info: ssm_dt_rank      = 0
0.00.319.364 I print_info: ssm_dt_b_c_rms   = 0
0.00.319.364 I print_info: model type       = 33M
0.00.319.364 I print_info: model params     = 32.90 M
0.00.319.365 I print_info: general.name     = Jina Bert Implementation
0.00.319.366 I print_info: vocab type       = BPE
0.00.319.366 I print_info: n_vocab          = 61056
0.00.319.366 I print_info: n_merges         = 39382
0.00.319.366 I print_info: BOS token        = 0 '<s>'
0.00.319.367 I print_info: EOS token        = 2 '</s>'
0.00.319.372 I print_info: UNK token        = 3 '<unk>'
0.00.319.374 I print_info: SEP token        = 2 '</s>'
0.00.319.374 I print_info: PAD token        = 1 '<pad>'
0.00.319.374 I print_info: MASK token       = 4 '<mask>'
0.00.319.374 I print_info: EOG token        = 2 '</s>'
0.00.319.374 I print_info: max token length = 45
0.00.319.376 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.294 I load_tensors: offloading 4 repeating layers to GPU
0.00.321.295 I load_tensors: offloading output layer to GPU
0.00.321.296 I load_tensors: offloaded 5/5 layers to GPU
0.00.321.318 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.321.319 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.321.627 I llama_init_from_model: n_seq_max     = 1
0.00.321.628 I llama_init_from_model: n_ctx         = 8192
0.00.321.628 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.321.628 I llama_init_from_model: n_batch       = 2048
0.00.321.628 I llama_init_from_model: n_ubatch      = 2048
0.00.321.629 I llama_init_from_model: flash_attn    = 0
0.00.321.629 I llama_init_from_model: freq_base     = 10000.0
0.00.321.629 I llama_init_from_model: freq_scale    = 1
0.00.321.630 I ggml_metal_init: allocating
0.00.321.633 I ggml_metal_init: found device: Apple M4
0.00.321.636 I ggml_metal_init: picking default device: Apple M4
0.00.322.337 I ggml_metal_init: using embedded metal library
0.00.325.141 I ggml_metal_init: GPU name:   Apple M4
0.00.325.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.325.143 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.325.144 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.325.144 I ggml_metal_init: simdgroup reduction   = true
0.00.325.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.325.144 I ggml_metal_init: has residency sets    = true
0.00.325.144 I ggml_metal_init: has bfloat            = true
0.00.325.144 I ggml_metal_init: use bfloat            = true
0.00.325.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.325.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.334.782 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.337.696 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.337.698 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.337.699 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.343.842 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.343.844 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.343.844 I llama_init_from_model: graph nodes  = 154
0.00.343.844 I llama_init_from_model: graph splits = 2
0.00.343.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.343.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.350.028 I 
0.00.350.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.350.152 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.350.153 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.350.156 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.350.156 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.350.162 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.350.163 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.350.636 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.354.334 I llama_perf_context_print:        load time =     327.52 ms
0.00.354.335 I llama_perf_context_print: prompt eval time =       3.69 ms /    62 tokens (    0.06 ms per token, 16815.84 tokens per second)
0.00.354.336 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.354.338 I llama_perf_context_print:       total time =       4.30 ms /    63 tokens
0.00.354.581 I ggml_metal_free: deallocating

real	0m1.136s
user	0m0.326s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.095 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.214 I main: llama backend init
0.00.000.220 I main: load the model and apply lora adapter, if any
0.00.100.473 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.112.844 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.112.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.112.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.112.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.112.860 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.112.860 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.112.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.112.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.112.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.112.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.112.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.112.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.112.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.112.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.112.872 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.112.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.112.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.119.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.122.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.129.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.129.288 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.129.288 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.129.289 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.129.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.129.290 I llama_model_loader: - type  f32:  194 tensors
0.00.129.300 I llama_model_loader: - type  f16:   98 tensors
0.00.129.301 I print_info: file format = GGUF V3 (latest)
0.00.129.302 I print_info: file type   = all F32 (guessed)
0.00.129.304 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.138.788 I load: special tokens cache size = 25
0.00.145.267 I load: token to piece cache size = 0.2984 MB
0.00.145.288 I print_info: arch             = gptneox
0.00.145.288 I print_info: vocab_only       = 0
0.00.145.289 I print_info: n_ctx_train      = 2048
0.00.145.289 I print_info: n_embd           = 2048
0.00.145.289 I print_info: n_layer          = 24
0.00.145.294 I print_info: n_head           = 16
0.00.145.294 I print_info: n_head_kv        = 16
0.00.145.294 I print_info: n_rot            = 32
0.00.145.295 I print_info: n_swa            = 0
0.00.145.295 I print_info: n_embd_head_k    = 128
0.00.145.295 I print_info: n_embd_head_v    = 128
0.00.145.296 I print_info: n_gqa            = 1
0.00.145.296 I print_info: n_embd_k_gqa     = 2048
0.00.145.297 I print_info: n_embd_v_gqa     = 2048
0.00.145.298 I print_info: f_norm_eps       = 1.0e-05
0.00.145.298 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.145.298 I print_info: f_clamp_kqv      = 0.0e+00
0.00.145.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.145.298 I print_info: f_logit_scale    = 0.0e+00
0.00.145.299 I print_info: n_ff             = 8192
0.00.145.299 I print_info: n_expert         = 0
0.00.145.299 I print_info: n_expert_used    = 0
0.00.145.299 I print_info: causal attn      = 1
0.00.145.299 I print_info: pooling type     = 0
0.00.145.300 I print_info: rope type        = 2
0.00.145.300 I print_info: rope scaling     = linear
0.00.145.300 I print_info: freq_base_train  = 10000.0
0.00.145.301 I print_info: freq_scale_train = 1
0.00.145.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.145.303 I print_info: rope_finetuned   = unknown
0.00.145.303 I print_info: ssm_d_conv       = 0
0.00.145.303 I print_info: ssm_d_inner      = 0
0.00.145.304 I print_info: ssm_d_state      = 0
0.00.145.304 I print_info: ssm_dt_rank      = 0
0.00.145.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.145.304 I print_info: model type       = 1.4B
0.00.145.304 I print_info: model params     = 1.41 B
0.00.145.305 I print_info: general.name     = 1.4B
0.00.145.305 I print_info: vocab type       = BPE
0.00.145.305 I print_info: n_vocab          = 50304
0.00.145.305 I print_info: n_merges         = 50009
0.00.145.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.145.306 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.145.306 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.145.306 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.145.307 I print_info: LF token         = 187 ''
0.00.145.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.145.308 I print_info: max token length = 1024
0.00.145.308 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.195.880 I load_tensors: offloading 24 repeating layers to GPU
0.00.195.882 I load_tensors: offloading output layer to GPU
0.00.195.883 I load_tensors: offloaded 25/25 layers to GPU
0.00.195.911 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.195.912 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.196.551 I llama_init_from_model: n_seq_max     = 1
0.00.196.551 I llama_init_from_model: n_ctx         = 2048
0.00.196.552 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.196.552 I llama_init_from_model: n_batch       = 2048
0.00.196.552 I llama_init_from_model: n_ubatch      = 512
0.00.196.552 I llama_init_from_model: flash_attn    = 0
0.00.196.553 I llama_init_from_model: freq_base     = 10000.0
0.00.196.553 I llama_init_from_model: freq_scale    = 1
0.00.196.555 I ggml_metal_init: allocating
0.00.196.620 I ggml_metal_init: found device: Apple M4
0.00.196.627 I ggml_metal_init: picking default device: Apple M4
0.00.197.271 I ggml_metal_init: using embedded metal library
0.00.243.728 I ggml_metal_init: GPU name:   Apple M4
0.00.243.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.243.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.243.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.243.733 I ggml_metal_init: simdgroup reduction   = true
0.00.243.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.243.733 I ggml_metal_init: has residency sets    = true
0.00.243.733 I ggml_metal_init: has bfloat            = true
0.00.243.733 I ggml_metal_init: use bfloat            = true
0.00.243.734 I ggml_metal_init: hasUnifiedMemory      = true
0.00.243.735 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.299.625 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.328.860 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.328.866 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.328.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.332.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.332.444 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.332.444 I llama_init_from_model: graph nodes  = 967
0.00.332.444 I llama_init_from_model: graph splits = 2
0.00.332.451 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.332.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.332.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.400.268 I main: llama threadpool init, n_threads = 4
0.00.400.306 I 
0.00.400.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.400.339 I 
0.00.400.487 I sampler seed: 1234
0.00.400.492 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.400.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.400.527 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.400.527 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.226.972 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.02.226.972 I llama_perf_context_print:        load time =     298.89 ms
0.02.226.973 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.28 tokens per second)
0.02.226.974 I llama_perf_context_print:        eval time =    1779.84 ms /    63 runs   (   28.25 ms per token,    35.40 tokens per second)
0.02.226.974 I llama_perf_context_print:       total time =    1827.60 ms /    70 tokens
0.02.227.232 I ggml_metal_free: deallocating

real	0m2.573s
user	0m0.121s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.529 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.559 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.092 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.111 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.111 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.114 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.115 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.116 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.116 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.117 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.118 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.121 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.121 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.122 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.863 I llama_model_loader: - type  f32:  194 tensors
0.00.056.863 I llama_model_loader: - type  f16:   98 tensors
0.00.056.864 I print_info: file format = GGUF V3 (latest)
0.00.056.865 I print_info: file type   = all F32 (guessed)
0.00.056.866 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.747 I load: special tokens cache size = 25
0.00.077.073 I load: token to piece cache size = 0.2984 MB
0.00.077.088 I print_info: arch             = gptneox
0.00.077.089 I print_info: vocab_only       = 0
0.00.077.090 I print_info: n_ctx_train      = 2048
0.00.077.090 I print_info: n_embd           = 2048
0.00.077.090 I print_info: n_layer          = 24
0.00.077.092 I print_info: n_head           = 16
0.00.077.093 I print_info: n_head_kv        = 16
0.00.077.093 I print_info: n_rot            = 32
0.00.077.093 I print_info: n_swa            = 0
0.00.077.094 I print_info: n_embd_head_k    = 128
0.00.077.094 I print_info: n_embd_head_v    = 128
0.00.077.095 I print_info: n_gqa            = 1
0.00.077.095 I print_info: n_embd_k_gqa     = 2048
0.00.077.096 I print_info: n_embd_v_gqa     = 2048
0.00.077.096 I print_info: f_norm_eps       = 1.0e-05
0.00.077.097 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.097 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.097 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.097 I print_info: f_logit_scale    = 0.0e+00
0.00.077.098 I print_info: n_ff             = 8192
0.00.077.098 I print_info: n_expert         = 0
0.00.077.098 I print_info: n_expert_used    = 0
0.00.077.098 I print_info: causal attn      = 1
0.00.077.098 I print_info: pooling type     = 0
0.00.077.099 I print_info: rope type        = 2
0.00.077.104 I print_info: rope scaling     = linear
0.00.077.104 I print_info: freq_base_train  = 10000.0
0.00.077.104 I print_info: freq_scale_train = 1
0.00.077.105 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.105 I print_info: rope_finetuned   = unknown
0.00.077.106 I print_info: ssm_d_conv       = 0
0.00.077.106 I print_info: ssm_d_inner      = 0
0.00.077.106 I print_info: ssm_d_state      = 0
0.00.077.107 I print_info: ssm_dt_rank      = 0
0.00.077.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.107 I print_info: model type       = 1.4B
0.00.077.107 I print_info: model params     = 1.41 B
0.00.077.107 I print_info: general.name     = 1.4B
0.00.077.108 I print_info: vocab type       = BPE
0.00.077.108 I print_info: n_vocab          = 50304
0.00.077.108 I print_info: n_merges         = 50009
0.00.077.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.109 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.110 I print_info: LF token         = 187 ''
0.00.077.110 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.110 I print_info: max token length = 1024
0.00.077.110 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.417.901 I load_tensors: offloading 24 repeating layers to GPU
0.01.417.905 I load_tensors: offloading output layer to GPU
0.01.417.905 I load_tensors: offloaded 25/25 layers to GPU
0.01.417.930 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.417.932 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.418.927 I llama_init_from_model: n_seq_max     = 1
0.01.418.928 I llama_init_from_model: n_ctx         = 128
0.01.418.928 I llama_init_from_model: n_ctx_per_seq = 128
0.01.418.929 I llama_init_from_model: n_batch       = 128
0.01.418.929 I llama_init_from_model: n_ubatch      = 128
0.01.418.929 I llama_init_from_model: flash_attn    = 0
0.01.418.929 I llama_init_from_model: freq_base     = 10000.0
0.01.418.930 I llama_init_from_model: freq_scale    = 1
0.01.418.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.418.931 I ggml_metal_init: allocating
0.01.418.983 I ggml_metal_init: found device: Apple M4
0.01.418.989 I ggml_metal_init: picking default device: Apple M4
0.01.419.937 I ggml_metal_init: using embedded metal library
0.01.423.897 I ggml_metal_init: GPU name:   Apple M4
0.01.423.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.423.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.423.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.423.901 I ggml_metal_init: simdgroup reduction   = true
0.01.423.901 I ggml_metal_init: simdgroup matrix mul. = true
0.01.423.901 I ggml_metal_init: has residency sets    = true
0.01.423.901 I ggml_metal_init: has bfloat            = true
0.01.423.901 I ggml_metal_init: use bfloat            = true
0.01.423.902 I ggml_metal_init: hasUnifiedMemory      = true
0.01.423.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.434.678 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.436.406 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.436.408 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.436.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.438.030 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.438.032 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.438.032 I llama_init_from_model: graph nodes  = 967
0.01.438.032 I llama_init_from_model: graph splits = 2
0.01.438.034 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.438.034 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.472.589 I 
0.01.472.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.472.661 I perplexity: tokenizing the input ..
0.01.477.454 I perplexity: tokenization took 4.79 ms
0.01.477.457 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.608.845 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.610.261 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.610.278 I llama_perf_context_print:        load time =    1449.02 ms
0.01.610.279 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.44 tokens per second)
0.01.610.279 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.610.280 I llama_perf_context_print:       total time =     137.69 ms /   129 tokens
0.01.610.659 I ggml_metal_free: deallocating

real	0m1.824s
user	0m0.098s
sys	0m0.255s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.329 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.381 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.394 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.395 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.397 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.397 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.398 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.398 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.399 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.936 I llama_model_loader: - type  f32:  194 tensors
0.00.026.936 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.937 I print_info: file format = GGUF V3 (latest)
0.00.026.937 I print_info: file type   = Q8_0
0.00.026.938 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.031 I load: special tokens cache size = 25
0.00.041.725 I load: token to piece cache size = 0.2984 MB
0.00.041.746 I print_info: arch             = gptneox
0.00.041.747 I print_info: vocab_only       = 0
0.00.041.747 I print_info: n_ctx_train      = 2048
0.00.041.747 I print_info: n_embd           = 2048
0.00.041.747 I print_info: n_layer          = 24
0.00.041.753 I print_info: n_head           = 16
0.00.041.754 I print_info: n_head_kv        = 16
0.00.041.754 I print_info: n_rot            = 32
0.00.041.754 I print_info: n_swa            = 0
0.00.041.754 I print_info: n_embd_head_k    = 128
0.00.041.754 I print_info: n_embd_head_v    = 128
0.00.041.755 I print_info: n_gqa            = 1
0.00.041.755 I print_info: n_embd_k_gqa     = 2048
0.00.041.756 I print_info: n_embd_v_gqa     = 2048
0.00.041.757 I print_info: f_norm_eps       = 1.0e-05
0.00.041.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.757 I print_info: f_logit_scale    = 0.0e+00
0.00.041.758 I print_info: n_ff             = 8192
0.00.041.758 I print_info: n_expert         = 0
0.00.041.759 I print_info: n_expert_used    = 0
0.00.041.759 I print_info: causal attn      = 1
0.00.041.759 I print_info: pooling type     = 0
0.00.041.759 I print_info: rope type        = 2
0.00.041.759 I print_info: rope scaling     = linear
0.00.041.760 I print_info: freq_base_train  = 10000.0
0.00.041.760 I print_info: freq_scale_train = 1
0.00.041.760 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.760 I print_info: rope_finetuned   = unknown
0.00.041.760 I print_info: ssm_d_conv       = 0
0.00.041.761 I print_info: ssm_d_inner      = 0
0.00.041.761 I print_info: ssm_d_state      = 0
0.00.041.761 I print_info: ssm_dt_rank      = 0
0.00.041.761 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.761 I print_info: model type       = 1.4B
0.00.041.763 I print_info: model params     = 1.41 B
0.00.041.767 I print_info: general.name     = 1.4B
0.00.041.768 I print_info: vocab type       = BPE
0.00.041.769 I print_info: n_vocab          = 50304
0.00.041.769 I print_info: n_merges         = 50009
0.00.041.769 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.771 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.771 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.771 I print_info: LF token         = 187 ''
0.00.041.771 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.771 I print_info: max token length = 1024
0.00.041.772 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.909.551 I load_tensors: offloading 24 repeating layers to GPU
0.00.909.556 I load_tensors: offloading output layer to GPU
0.00.909.557 I load_tensors: offloaded 25/25 layers to GPU
0.00.909.577 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.909.579 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.910.425 I llama_init_from_model: n_seq_max     = 1
0.00.910.427 I llama_init_from_model: n_ctx         = 2048
0.00.910.427 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.910.428 I llama_init_from_model: n_batch       = 2048
0.00.910.428 I llama_init_from_model: n_ubatch      = 512
0.00.910.429 I llama_init_from_model: flash_attn    = 0
0.00.910.430 I llama_init_from_model: freq_base     = 10000.0
0.00.910.430 I llama_init_from_model: freq_scale    = 1
0.00.910.431 I ggml_metal_init: allocating
0.00.910.449 I ggml_metal_init: found device: Apple M4
0.00.910.457 I ggml_metal_init: picking default device: Apple M4
0.00.911.657 I ggml_metal_init: using embedded metal library
0.00.917.238 I ggml_metal_init: GPU name:   Apple M4
0.00.917.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.917.242 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.917.243 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.917.243 I ggml_metal_init: simdgroup reduction   = true
0.00.917.244 I ggml_metal_init: simdgroup matrix mul. = true
0.00.917.244 I ggml_metal_init: has residency sets    = true
0.00.917.244 I ggml_metal_init: has bfloat            = true
0.00.917.244 I ggml_metal_init: use bfloat            = true
0.00.917.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.917.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.932.939 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.984.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.984.297 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.984.319 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.989.448 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.989.450 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.989.450 I llama_init_from_model: graph nodes  = 967
0.00.989.450 I llama_init_from_model: graph splits = 2
0.00.989.453 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.989.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.989.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.043.701 I main: llama threadpool init, n_threads = 4
0.01.043.751 I 
0.01.043.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.043.774 I 
0.01.043.925 I sampler seed: 1234
0.01.043.931 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.043.971 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.043.974 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.043.974 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.140.307 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.02.140.309 I llama_perf_context_print:        load time =    1032.66 ms
0.02.140.309 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.20 tokens per second)
0.02.140.310 I llama_perf_context_print:        eval time =    1044.21 ms /    63 runs   (   16.57 ms per token,    60.33 tokens per second)
0.02.140.312 I llama_perf_context_print:       total time =    1097.31 ms /    70 tokens
0.02.140.574 I ggml_metal_free: deallocating

real	0m2.161s
user	0m0.108s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.502 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.264 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.268 I llama_model_loader: - type  f32:  194 tensors
0.00.025.268 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.269 I print_info: file format = GGUF V3 (latest)
0.00.025.269 I print_info: file type   = Q8_0
0.00.025.270 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.229 I load: special tokens cache size = 25
0.00.039.334 I load: token to piece cache size = 0.2984 MB
0.00.039.354 I print_info: arch             = gptneox
0.00.039.355 I print_info: vocab_only       = 0
0.00.039.355 I print_info: n_ctx_train      = 2048
0.00.039.356 I print_info: n_embd           = 2048
0.00.039.356 I print_info: n_layer          = 24
0.00.039.360 I print_info: n_head           = 16
0.00.039.361 I print_info: n_head_kv        = 16
0.00.039.361 I print_info: n_rot            = 32
0.00.039.361 I print_info: n_swa            = 0
0.00.039.361 I print_info: n_embd_head_k    = 128
0.00.039.361 I print_info: n_embd_head_v    = 128
0.00.039.362 I print_info: n_gqa            = 1
0.00.039.362 I print_info: n_embd_k_gqa     = 2048
0.00.039.363 I print_info: n_embd_v_gqa     = 2048
0.00.039.364 I print_info: f_norm_eps       = 1.0e-05
0.00.039.366 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.366 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.366 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.366 I print_info: f_logit_scale    = 0.0e+00
0.00.039.367 I print_info: n_ff             = 8192
0.00.039.367 I print_info: n_expert         = 0
0.00.039.367 I print_info: n_expert_used    = 0
0.00.039.367 I print_info: causal attn      = 1
0.00.039.369 I print_info: pooling type     = 0
0.00.039.369 I print_info: rope type        = 2
0.00.039.369 I print_info: rope scaling     = linear
0.00.039.370 I print_info: freq_base_train  = 10000.0
0.00.039.370 I print_info: freq_scale_train = 1
0.00.039.370 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.370 I print_info: rope_finetuned   = unknown
0.00.039.370 I print_info: ssm_d_conv       = 0
0.00.039.370 I print_info: ssm_d_inner      = 0
0.00.039.371 I print_info: ssm_d_state      = 0
0.00.039.371 I print_info: ssm_dt_rank      = 0
0.00.039.371 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.375 I print_info: model type       = 1.4B
0.00.039.376 I print_info: model params     = 1.41 B
0.00.039.376 I print_info: general.name     = 1.4B
0.00.039.377 I print_info: vocab type       = BPE
0.00.039.377 I print_info: n_vocab          = 50304
0.00.039.377 I print_info: n_merges         = 50009
0.00.039.378 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.378 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.378 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.378 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.378 I print_info: LF token         = 187 ''
0.00.039.379 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.379 I print_info: max token length = 1024
0.00.039.379 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.789.073 I load_tensors: offloading 24 repeating layers to GPU
0.00.789.080 I load_tensors: offloading output layer to GPU
0.00.789.080 I load_tensors: offloaded 25/25 layers to GPU
0.00.789.114 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.789.116 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.790.568 I llama_init_from_model: n_seq_max     = 1
0.00.790.570 I llama_init_from_model: n_ctx         = 128
0.00.790.570 I llama_init_from_model: n_ctx_per_seq = 128
0.00.790.571 I llama_init_from_model: n_batch       = 128
0.00.790.571 I llama_init_from_model: n_ubatch      = 128
0.00.790.571 I llama_init_from_model: flash_attn    = 0
0.00.790.572 I llama_init_from_model: freq_base     = 10000.0
0.00.790.572 I llama_init_from_model: freq_scale    = 1
0.00.790.573 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.790.577 I ggml_metal_init: allocating
0.00.790.688 I ggml_metal_init: found device: Apple M4
0.00.790.699 I ggml_metal_init: picking default device: Apple M4
0.00.791.910 I ggml_metal_init: using embedded metal library
0.00.797.253 I ggml_metal_init: GPU name:   Apple M4
0.00.797.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.797.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.797.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.797.258 I ggml_metal_init: simdgroup reduction   = true
0.00.797.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.797.259 I ggml_metal_init: has residency sets    = true
0.00.797.259 I ggml_metal_init: has bfloat            = true
0.00.797.259 I ggml_metal_init: use bfloat            = true
0.00.797.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.797.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.814.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.816.472 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.816.479 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.816.502 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.818.727 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.818.729 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.818.729 I llama_init_from_model: graph nodes  = 967
0.00.818.729 I llama_init_from_model: graph splits = 2
0.00.818.731 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.818.732 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.844.795 I 
0.00.844.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.844.873 I perplexity: tokenizing the input ..
0.00.851.553 I perplexity: tokenization took 6.677 ms
0.00.851.559 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.989.766 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.991.107 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.991.124 I llama_perf_context_print:        load time =     835.29 ms
0.00.991.125 I llama_perf_context_print: prompt eval time =     137.81 ms /   128 tokens (    1.08 ms per token,   928.83 tokens per second)
0.00.991.126 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.991.126 I llama_perf_context_print:       total time =     146.33 ms /   129 tokens
0.00.991.537 I ggml_metal_free: deallocating

real	0m1.006s
user	0m0.077s
sys	0m0.175s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.031 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.727 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.732 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.732 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.733 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.734 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.740 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.426 I llama_model_loader: - type  f32:  194 tensors
0.00.027.426 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.427 I print_info: file format = GGUF V3 (latest)
0.00.027.428 I print_info: file type   = Q4_0
0.00.027.431 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.846 I load: special tokens cache size = 25
0.00.042.354 I load: token to piece cache size = 0.2984 MB
0.00.042.369 I print_info: arch             = gptneox
0.00.042.370 I print_info: vocab_only       = 0
0.00.042.370 I print_info: n_ctx_train      = 2048
0.00.042.370 I print_info: n_embd           = 2048
0.00.042.370 I print_info: n_layer          = 24
0.00.042.375 I print_info: n_head           = 16
0.00.042.376 I print_info: n_head_kv        = 16
0.00.042.376 I print_info: n_rot            = 32
0.00.042.378 I print_info: n_swa            = 0
0.00.042.378 I print_info: n_embd_head_k    = 128
0.00.042.379 I print_info: n_embd_head_v    = 128
0.00.042.380 I print_info: n_gqa            = 1
0.00.042.381 I print_info: n_embd_k_gqa     = 2048
0.00.042.381 I print_info: n_embd_v_gqa     = 2048
0.00.042.382 I print_info: f_norm_eps       = 1.0e-05
0.00.042.383 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.383 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.383 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.383 I print_info: f_logit_scale    = 0.0e+00
0.00.042.384 I print_info: n_ff             = 8192
0.00.042.384 I print_info: n_expert         = 0
0.00.042.385 I print_info: n_expert_used    = 0
0.00.042.385 I print_info: causal attn      = 1
0.00.042.385 I print_info: pooling type     = 0
0.00.042.385 I print_info: rope type        = 2
0.00.042.385 I print_info: rope scaling     = linear
0.00.042.386 I print_info: freq_base_train  = 10000.0
0.00.042.386 I print_info: freq_scale_train = 1
0.00.042.386 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.386 I print_info: rope_finetuned   = unknown
0.00.042.386 I print_info: ssm_d_conv       = 0
0.00.042.387 I print_info: ssm_d_inner      = 0
0.00.042.387 I print_info: ssm_d_state      = 0
0.00.042.387 I print_info: ssm_dt_rank      = 0
0.00.042.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.387 I print_info: model type       = 1.4B
0.00.042.391 I print_info: model params     = 1.41 B
0.00.042.391 I print_info: general.name     = 1.4B
0.00.042.395 I print_info: vocab type       = BPE
0.00.042.395 I print_info: n_vocab          = 50304
0.00.042.395 I print_info: n_merges         = 50009
0.00.042.395 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.396 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.397 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.397 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.397 I print_info: LF token         = 187 ''
0.00.042.397 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.398 I print_info: max token length = 1024
0.00.042.399 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.507.692 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.710 I load_tensors: offloading output layer to GPU
0.00.507.711 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.744 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.507.746 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.509.406 I llama_init_from_model: n_seq_max     = 1
0.00.509.408 I llama_init_from_model: n_ctx         = 2048
0.00.509.409 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.509.410 I llama_init_from_model: n_batch       = 2048
0.00.509.410 I llama_init_from_model: n_ubatch      = 512
0.00.509.411 I llama_init_from_model: flash_attn    = 0
0.00.509.413 I llama_init_from_model: freq_base     = 10000.0
0.00.509.413 I llama_init_from_model: freq_scale    = 1
0.00.509.416 I ggml_metal_init: allocating
0.00.509.489 I ggml_metal_init: found device: Apple M4
0.00.509.502 I ggml_metal_init: picking default device: Apple M4
0.00.511.131 I ggml_metal_init: using embedded metal library
0.00.517.781 I ggml_metal_init: GPU name:   Apple M4
0.00.517.786 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.517.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.517.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.517.788 I ggml_metal_init: simdgroup reduction   = true
0.00.517.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.517.789 I ggml_metal_init: has residency sets    = true
0.00.517.789 I ggml_metal_init: has bfloat            = true
0.00.517.790 I ggml_metal_init: use bfloat            = true
0.00.517.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.517.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.536.336 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.593.867 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.593.874 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.593.896 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.614 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.598.616 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.598.616 I llama_init_from_model: graph nodes  = 967
0.00.598.617 I llama_init_from_model: graph splits = 2
0.00.598.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.598.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.598.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.378 I main: llama threadpool init, n_threads = 4
0.00.657.423 I 
0.00.657.445 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.445 I 
0.00.657.612 I sampler seed: 1234
0.00.657.617 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.657.632 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.657.634 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.657.634 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.337.268 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49374.13 tokens per second)
0.01.337.269 I llama_perf_context_print:        load time =     645.57 ms
0.01.337.270 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.26 tokens per second)
0.01.337.271 I llama_perf_context_print:        eval time =     627.56 ms /    63 runs   (    9.96 ms per token,   100.39 tokens per second)
0.01.337.271 I llama_perf_context_print:       total time =     680.66 ms /    70 tokens
0.01.337.573 I ggml_metal_free: deallocating

real	0m1.357s
user	0m0.110s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.075 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.269 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.276 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.279 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.282 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.837 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.837 I llama_model_loader: - type  f32:  194 tensors
0.00.025.838 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.838 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.839 I print_info: file format = GGUF V3 (latest)
0.00.025.839 I print_info: file type   = Q4_0
0.00.025.841 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.923 I load: special tokens cache size = 25
0.00.040.244 I load: token to piece cache size = 0.2984 MB
0.00.040.262 I print_info: arch             = gptneox
0.00.040.263 I print_info: vocab_only       = 0
0.00.040.263 I print_info: n_ctx_train      = 2048
0.00.040.263 I print_info: n_embd           = 2048
0.00.040.263 I print_info: n_layer          = 24
0.00.040.267 I print_info: n_head           = 16
0.00.040.268 I print_info: n_head_kv        = 16
0.00.040.268 I print_info: n_rot            = 32
0.00.040.268 I print_info: n_swa            = 0
0.00.040.268 I print_info: n_embd_head_k    = 128
0.00.040.268 I print_info: n_embd_head_v    = 128
0.00.040.269 I print_info: n_gqa            = 1
0.00.040.269 I print_info: n_embd_k_gqa     = 2048
0.00.040.270 I print_info: n_embd_v_gqa     = 2048
0.00.040.270 I print_info: f_norm_eps       = 1.0e-05
0.00.040.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.272 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.272 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.273 I print_info: f_logit_scale    = 0.0e+00
0.00.040.273 I print_info: n_ff             = 8192
0.00.040.274 I print_info: n_expert         = 0
0.00.040.274 I print_info: n_expert_used    = 0
0.00.040.274 I print_info: causal attn      = 1
0.00.040.274 I print_info: pooling type     = 0
0.00.040.274 I print_info: rope type        = 2
0.00.040.274 I print_info: rope scaling     = linear
0.00.040.275 I print_info: freq_base_train  = 10000.0
0.00.040.275 I print_info: freq_scale_train = 1
0.00.040.275 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.275 I print_info: rope_finetuned   = unknown
0.00.040.275 I print_info: ssm_d_conv       = 0
0.00.040.276 I print_info: ssm_d_inner      = 0
0.00.040.276 I print_info: ssm_d_state      = 0
0.00.040.276 I print_info: ssm_dt_rank      = 0
0.00.040.276 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.276 I print_info: model type       = 1.4B
0.00.040.276 I print_info: model params     = 1.41 B
0.00.040.276 I print_info: general.name     = 1.4B
0.00.040.277 I print_info: vocab type       = BPE
0.00.040.277 I print_info: n_vocab          = 50304
0.00.040.277 I print_info: n_merges         = 50009
0.00.040.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.278 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.278 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.278 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.278 I print_info: LF token         = 187 ''
0.00.040.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.279 I print_info: max token length = 1024
0.00.040.279 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.491.807 I load_tensors: offloading 24 repeating layers to GPU
0.00.491.820 I load_tensors: offloading output layer to GPU
0.00.491.821 I load_tensors: offloaded 25/25 layers to GPU
0.00.491.855 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.491.856 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.493.472 I llama_init_from_model: n_seq_max     = 1
0.00.493.475 I llama_init_from_model: n_ctx         = 128
0.00.493.475 I llama_init_from_model: n_ctx_per_seq = 128
0.00.493.476 I llama_init_from_model: n_batch       = 128
0.00.493.476 I llama_init_from_model: n_ubatch      = 128
0.00.493.477 I llama_init_from_model: flash_attn    = 0
0.00.493.479 I llama_init_from_model: freq_base     = 10000.0
0.00.493.480 I llama_init_from_model: freq_scale    = 1
0.00.493.480 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.493.483 I ggml_metal_init: allocating
0.00.493.573 I ggml_metal_init: found device: Apple M4
0.00.493.587 I ggml_metal_init: picking default device: Apple M4
0.00.495.155 I ggml_metal_init: using embedded metal library
0.00.500.712 I ggml_metal_init: GPU name:   Apple M4
0.00.500.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.500.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.500.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.500.723 I ggml_metal_init: simdgroup reduction   = true
0.00.500.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.500.724 I ggml_metal_init: has residency sets    = true
0.00.500.724 I ggml_metal_init: has bfloat            = true
0.00.500.724 I ggml_metal_init: use bfloat            = true
0.00.500.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.500.729 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.521.451 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.180 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.525.184 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.525.211 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.588 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.528.590 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.528.591 I llama_init_from_model: graph nodes  = 967
0.00.528.591 I llama_init_from_model: graph splits = 2
0.00.528.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.528.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.052 I 
0.00.554.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.173 I perplexity: tokenizing the input ..
0.00.561.663 I perplexity: tokenization took 7.489 ms
0.00.561.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.697.206 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.698.553 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.698.567 I llama_perf_context_print:        load time =     543.97 ms
0.00.698.568 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.23 tokens per second)
0.00.698.568 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.698.569 I llama_perf_context_print:       total time =     144.52 ms /   129 tokens
0.00.698.953 I ggml_metal_free: deallocating

real	0m0.715s
user	0m0.081s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.566 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.560 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.854 I llama_model_loader: - type  f32:  194 tensors
0.00.025.855 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.855 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.856 I print_info: file format = GGUF V3 (latest)
0.00.025.856 I print_info: file type   = Q4_1
0.00.025.857 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.728 I load: special tokens cache size = 25
0.00.040.010 I load: token to piece cache size = 0.2984 MB
0.00.040.024 I print_info: arch             = gptneox
0.00.040.025 I print_info: vocab_only       = 0
0.00.040.025 I print_info: n_ctx_train      = 2048
0.00.040.026 I print_info: n_embd           = 2048
0.00.040.026 I print_info: n_layer          = 24
0.00.040.029 I print_info: n_head           = 16
0.00.040.029 I print_info: n_head_kv        = 16
0.00.040.030 I print_info: n_rot            = 32
0.00.040.030 I print_info: n_swa            = 0
0.00.040.030 I print_info: n_embd_head_k    = 128
0.00.040.030 I print_info: n_embd_head_v    = 128
0.00.040.031 I print_info: n_gqa            = 1
0.00.040.031 I print_info: n_embd_k_gqa     = 2048
0.00.040.033 I print_info: n_embd_v_gqa     = 2048
0.00.040.034 I print_info: f_norm_eps       = 1.0e-05
0.00.040.034 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.036 I print_info: f_logit_scale    = 0.0e+00
0.00.040.036 I print_info: n_ff             = 8192
0.00.040.037 I print_info: n_expert         = 0
0.00.040.037 I print_info: n_expert_used    = 0
0.00.040.037 I print_info: causal attn      = 1
0.00.040.037 I print_info: pooling type     = 0
0.00.040.038 I print_info: rope type        = 2
0.00.040.040 I print_info: rope scaling     = linear
0.00.040.040 I print_info: freq_base_train  = 10000.0
0.00.040.040 I print_info: freq_scale_train = 1
0.00.040.040 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.041 I print_info: rope_finetuned   = unknown
0.00.040.041 I print_info: ssm_d_conv       = 0
0.00.040.041 I print_info: ssm_d_inner      = 0
0.00.040.041 I print_info: ssm_d_state      = 0
0.00.040.041 I print_info: ssm_dt_rank      = 0
0.00.040.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.041 I print_info: model type       = 1.4B
0.00.040.042 I print_info: model params     = 1.41 B
0.00.040.042 I print_info: general.name     = 1.4B
0.00.040.042 I print_info: vocab type       = BPE
0.00.040.042 I print_info: n_vocab          = 50304
0.00.040.043 I print_info: n_merges         = 50009
0.00.040.043 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.046 I print_info: LF token         = 187 ''
0.00.040.047 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.047 I print_info: max token length = 1024
0.00.040.047 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.322 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.337 I load_tensors: offloading output layer to GPU
0.00.603.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.373 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.603.375 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.604.891 I llama_init_from_model: n_seq_max     = 1
0.00.604.893 I llama_init_from_model: n_ctx         = 2048
0.00.604.894 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.604.895 I llama_init_from_model: n_batch       = 2048
0.00.604.895 I llama_init_from_model: n_ubatch      = 512
0.00.604.896 I llama_init_from_model: flash_attn    = 0
0.00.604.898 I llama_init_from_model: freq_base     = 10000.0
0.00.604.899 I llama_init_from_model: freq_scale    = 1
0.00.604.901 I ggml_metal_init: allocating
0.00.604.984 I ggml_metal_init: found device: Apple M4
0.00.604.997 I ggml_metal_init: picking default device: Apple M4
0.00.606.605 I ggml_metal_init: using embedded metal library
0.00.613.401 I ggml_metal_init: GPU name:   Apple M4
0.00.613.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.407 I ggml_metal_init: simdgroup reduction   = true
0.00.613.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.407 I ggml_metal_init: has residency sets    = true
0.00.613.408 I ggml_metal_init: has bfloat            = true
0.00.613.408 I ggml_metal_init: use bfloat            = true
0.00.613.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.687.247 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.687.269 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.692.434 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.692.438 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.692.438 I llama_init_from_model: graph nodes  = 967
0.00.692.438 I llama_init_from_model: graph splits = 2
0.00.692.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.692.578 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.692.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.795 I main: llama threadpool init, n_threads = 4
0.00.750.847 I 
0.00.750.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.870 I 
0.00.751.017 I sampler seed: 1234
0.00.751.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.037 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.037 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.037 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.479.455 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.479.455 I llama_perf_context_print:        load time =     740.46 ms
0.01.479.456 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.37 tokens per second)
0.01.479.458 I llama_perf_context_print:        eval time =     676.41 ms /    63 runs   (   10.74 ms per token,    93.14 tokens per second)
0.01.479.459 I llama_perf_context_print:       total time =     729.43 ms /    70 tokens
0.01.479.695 I ggml_metal_free: deallocating

real	0m1.498s
user	0m0.109s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.918 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.992 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.994 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.994 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.997 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.594 I llama_model_loader: - type  f32:  194 tensors
0.00.024.594 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.595 I print_info: file format = GGUF V3 (latest)
0.00.024.596 I print_info: file type   = Q4_1
0.00.024.597 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.005 I load: special tokens cache size = 25
0.00.039.515 I load: token to piece cache size = 0.2984 MB
0.00.039.532 I print_info: arch             = gptneox
0.00.039.533 I print_info: vocab_only       = 0
0.00.039.533 I print_info: n_ctx_train      = 2048
0.00.039.534 I print_info: n_embd           = 2048
0.00.039.534 I print_info: n_layer          = 24
0.00.039.538 I print_info: n_head           = 16
0.00.039.539 I print_info: n_head_kv        = 16
0.00.039.539 I print_info: n_rot            = 32
0.00.039.539 I print_info: n_swa            = 0
0.00.039.539 I print_info: n_embd_head_k    = 128
0.00.039.539 I print_info: n_embd_head_v    = 128
0.00.039.540 I print_info: n_gqa            = 1
0.00.039.540 I print_info: n_embd_k_gqa     = 2048
0.00.039.544 I print_info: n_embd_v_gqa     = 2048
0.00.039.545 I print_info: f_norm_eps       = 1.0e-05
0.00.039.545 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.545 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.545 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.547 I print_info: f_logit_scale    = 0.0e+00
0.00.039.547 I print_info: n_ff             = 8192
0.00.039.548 I print_info: n_expert         = 0
0.00.039.548 I print_info: n_expert_used    = 0
0.00.039.548 I print_info: causal attn      = 1
0.00.039.548 I print_info: pooling type     = 0
0.00.039.549 I print_info: rope type        = 2
0.00.039.550 I print_info: rope scaling     = linear
0.00.039.550 I print_info: freq_base_train  = 10000.0
0.00.039.550 I print_info: freq_scale_train = 1
0.00.039.550 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.551 I print_info: rope_finetuned   = unknown
0.00.039.551 I print_info: ssm_d_conv       = 0
0.00.039.551 I print_info: ssm_d_inner      = 0
0.00.039.551 I print_info: ssm_d_state      = 0
0.00.039.551 I print_info: ssm_dt_rank      = 0
0.00.039.551 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.551 I print_info: model type       = 1.4B
0.00.039.552 I print_info: model params     = 1.41 B
0.00.039.552 I print_info: general.name     = 1.4B
0.00.039.553 I print_info: vocab type       = BPE
0.00.039.553 I print_info: n_vocab          = 50304
0.00.039.553 I print_info: n_merges         = 50009
0.00.039.553 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.553 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.554 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.554 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.554 I print_info: LF token         = 187 ''
0.00.039.554 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.554 I print_info: max token length = 1024
0.00.039.555 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.579.315 I load_tensors: offloading 24 repeating layers to GPU
0.00.579.329 I load_tensors: offloading output layer to GPU
0.00.579.330 I load_tensors: offloaded 25/25 layers to GPU
0.00.579.362 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.579.364 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.581.037 I llama_init_from_model: n_seq_max     = 1
0.00.581.041 I llama_init_from_model: n_ctx         = 128
0.00.581.042 I llama_init_from_model: n_ctx_per_seq = 128
0.00.581.042 I llama_init_from_model: n_batch       = 128
0.00.581.042 I llama_init_from_model: n_ubatch      = 128
0.00.581.043 I llama_init_from_model: flash_attn    = 0
0.00.581.044 I llama_init_from_model: freq_base     = 10000.0
0.00.581.045 I llama_init_from_model: freq_scale    = 1
0.00.581.045 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.581.048 I ggml_metal_init: allocating
0.00.581.129 I ggml_metal_init: found device: Apple M4
0.00.581.144 I ggml_metal_init: picking default device: Apple M4
0.00.582.700 I ggml_metal_init: using embedded metal library
0.00.589.411 I ggml_metal_init: GPU name:   Apple M4
0.00.589.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.589.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.589.422 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.589.423 I ggml_metal_init: simdgroup reduction   = true
0.00.589.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.589.424 I ggml_metal_init: has residency sets    = true
0.00.589.424 I ggml_metal_init: has bfloat            = true
0.00.589.424 I ggml_metal_init: use bfloat            = true
0.00.589.426 I ggml_metal_init: hasUnifiedMemory      = true
0.00.589.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.011 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.555 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.611.559 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.611.596 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.615.039 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.615.040 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.615.041 I llama_init_from_model: graph nodes  = 967
0.00.615.041 I llama_init_from_model: graph splits = 2
0.00.615.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.615.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.378 I 
0.00.640.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.494 I perplexity: tokenizing the input ..
0.00.647.802 I perplexity: tokenization took 7.303 ms
0.00.647.812 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.725 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.784.138 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.784.157 I llama_perf_context_print:        load time =     631.45 ms
0.00.784.162 I llama_perf_context_print: prompt eval time =     133.95 ms /   128 tokens (    1.05 ms per token,   955.57 tokens per second)
0.00.784.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.165 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.784.546 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.080s
sys	0m0.116s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.060 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.960 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.966 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.734 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.747 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.482 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.483 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.484 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.484 I llama_model_loader: - type  f32:  194 tensors
0.00.026.484 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.485 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.485 I print_info: file format = GGUF V3 (latest)
0.00.026.486 I print_info: file type   = Q5_0
0.00.026.487 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.338 I load: special tokens cache size = 25
0.00.040.785 I load: token to piece cache size = 0.2984 MB
0.00.040.799 I print_info: arch             = gptneox
0.00.040.800 I print_info: vocab_only       = 0
0.00.040.800 I print_info: n_ctx_train      = 2048
0.00.040.800 I print_info: n_embd           = 2048
0.00.040.801 I print_info: n_layer          = 24
0.00.040.803 I print_info: n_head           = 16
0.00.040.804 I print_info: n_head_kv        = 16
0.00.040.804 I print_info: n_rot            = 32
0.00.040.804 I print_info: n_swa            = 0
0.00.040.804 I print_info: n_embd_head_k    = 128
0.00.040.804 I print_info: n_embd_head_v    = 128
0.00.040.805 I print_info: n_gqa            = 1
0.00.040.806 I print_info: n_embd_k_gqa     = 2048
0.00.040.806 I print_info: n_embd_v_gqa     = 2048
0.00.040.807 I print_info: f_norm_eps       = 1.0e-05
0.00.040.807 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.808 I print_info: f_logit_scale    = 0.0e+00
0.00.040.810 I print_info: n_ff             = 8192
0.00.040.810 I print_info: n_expert         = 0
0.00.040.811 I print_info: n_expert_used    = 0
0.00.040.811 I print_info: causal attn      = 1
0.00.040.811 I print_info: pooling type     = 0
0.00.040.811 I print_info: rope type        = 2
0.00.040.811 I print_info: rope scaling     = linear
0.00.040.812 I print_info: freq_base_train  = 10000.0
0.00.040.812 I print_info: freq_scale_train = 1
0.00.040.812 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.812 I print_info: rope_finetuned   = unknown
0.00.040.812 I print_info: ssm_d_conv       = 0
0.00.040.812 I print_info: ssm_d_inner      = 0
0.00.040.812 I print_info: ssm_d_state      = 0
0.00.040.814 I print_info: ssm_dt_rank      = 0
0.00.040.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.814 I print_info: model type       = 1.4B
0.00.040.814 I print_info: model params     = 1.41 B
0.00.040.815 I print_info: general.name     = 1.4B
0.00.040.815 I print_info: vocab type       = BPE
0.00.040.815 I print_info: n_vocab          = 50304
0.00.040.815 I print_info: n_merges         = 50009
0.00.040.816 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: LF token         = 187 ''
0.00.040.817 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.818 I print_info: max token length = 1024
0.00.040.818 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.682.890 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.907 I load_tensors: offloading output layer to GPU
0.00.682.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.942 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.682.943 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.684.571 I llama_init_from_model: n_seq_max     = 1
0.00.684.578 I llama_init_from_model: n_ctx         = 2048
0.00.684.578 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.684.579 I llama_init_from_model: n_batch       = 2048
0.00.684.579 I llama_init_from_model: n_ubatch      = 512
0.00.684.579 I llama_init_from_model: flash_attn    = 0
0.00.684.581 I llama_init_from_model: freq_base     = 10000.0
0.00.684.582 I llama_init_from_model: freq_scale    = 1
0.00.684.585 I ggml_metal_init: allocating
0.00.684.630 I ggml_metal_init: found device: Apple M4
0.00.684.640 I ggml_metal_init: picking default device: Apple M4
0.00.686.556 I ggml_metal_init: using embedded metal library
0.00.693.513 I ggml_metal_init: GPU name:   Apple M4
0.00.693.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.521 I ggml_metal_init: simdgroup reduction   = true
0.00.693.522 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.522 I ggml_metal_init: has residency sets    = true
0.00.693.522 I ggml_metal_init: has bfloat            = true
0.00.693.522 I ggml_metal_init: use bfloat            = true
0.00.693.523 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.689 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.771.323 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.771.332 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.771.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.776.013 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.776.015 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.776.015 I llama_init_from_model: graph nodes  = 967
0.00.776.015 I llama_init_from_model: graph splits = 2
0.00.776.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.776.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.776.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.837.744 I main: llama threadpool init, n_threads = 4
0.00.837.793 I 
0.00.837.813 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.837.814 I 
0.00.837.976 I sampler seed: 1234
0.00.837.980 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.837.997 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.837.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.837.997 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.628.187 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.628.188 I llama_perf_context_print:        load time =     826.90 ms
0.01.628.189 I llama_perf_context_print: prompt eval time =      52.88 ms /     7 tokens (    7.55 ms per token,   132.38 tokens per second)
0.01.628.189 I llama_perf_context_print:        eval time =     734.37 ms /    63 runs   (   11.66 ms per token,    85.79 tokens per second)
0.01.628.190 I llama_perf_context_print:       total time =     791.23 ms /    70 tokens
0.01.628.477 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.325 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.655 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.656 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.656 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.473 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.384 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.386 I llama_model_loader: - type  f32:  194 tensors
0.00.025.386 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.386 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.387 I print_info: file format = GGUF V3 (latest)
0.00.025.388 I print_info: file type   = Q5_0
0.00.025.389 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.834 I load: special tokens cache size = 25
0.00.040.006 I load: token to piece cache size = 0.2984 MB
0.00.040.021 I print_info: arch             = gptneox
0.00.040.022 I print_info: vocab_only       = 0
0.00.040.023 I print_info: n_ctx_train      = 2048
0.00.040.023 I print_info: n_embd           = 2048
0.00.040.023 I print_info: n_layer          = 24
0.00.040.027 I print_info: n_head           = 16
0.00.040.028 I print_info: n_head_kv        = 16
0.00.040.028 I print_info: n_rot            = 32
0.00.040.028 I print_info: n_swa            = 0
0.00.040.028 I print_info: n_embd_head_k    = 128
0.00.040.028 I print_info: n_embd_head_v    = 128
0.00.040.029 I print_info: n_gqa            = 1
0.00.040.030 I print_info: n_embd_k_gqa     = 2048
0.00.040.030 I print_info: n_embd_v_gqa     = 2048
0.00.040.031 I print_info: f_norm_eps       = 1.0e-05
0.00.040.031 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.031 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.031 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.031 I print_info: f_logit_scale    = 0.0e+00
0.00.040.032 I print_info: n_ff             = 8192
0.00.040.033 I print_info: n_expert         = 0
0.00.040.034 I print_info: n_expert_used    = 0
0.00.040.034 I print_info: causal attn      = 1
0.00.040.034 I print_info: pooling type     = 0
0.00.040.034 I print_info: rope type        = 2
0.00.040.034 I print_info: rope scaling     = linear
0.00.040.035 I print_info: freq_base_train  = 10000.0
0.00.040.035 I print_info: freq_scale_train = 1
0.00.040.035 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.035 I print_info: rope_finetuned   = unknown
0.00.040.036 I print_info: ssm_d_conv       = 0
0.00.040.036 I print_info: ssm_d_inner      = 0
0.00.040.036 I print_info: ssm_d_state      = 0
0.00.040.036 I print_info: ssm_dt_rank      = 0
0.00.040.036 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.036 I print_info: model type       = 1.4B
0.00.040.036 I print_info: model params     = 1.41 B
0.00.040.037 I print_info: general.name     = 1.4B
0.00.040.037 I print_info: vocab type       = BPE
0.00.040.037 I print_info: n_vocab          = 50304
0.00.040.037 I print_info: n_merges         = 50009
0.00.040.038 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: LF token         = 187 ''
0.00.040.040 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.040 I print_info: max token length = 1024
0.00.040.040 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.719.683 I load_tensors: offloading 24 repeating layers to GPU
0.00.719.698 I load_tensors: offloading output layer to GPU
0.00.719.699 I load_tensors: offloaded 25/25 layers to GPU
0.00.719.735 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.719.738 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.721.426 I llama_init_from_model: n_seq_max     = 1
0.00.721.430 I llama_init_from_model: n_ctx         = 128
0.00.721.431 I llama_init_from_model: n_ctx_per_seq = 128
0.00.721.431 I llama_init_from_model: n_batch       = 128
0.00.721.432 I llama_init_from_model: n_ubatch      = 128
0.00.721.432 I llama_init_from_model: flash_attn    = 0
0.00.721.434 I llama_init_from_model: freq_base     = 10000.0
0.00.721.435 I llama_init_from_model: freq_scale    = 1
0.00.721.435 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.721.437 I ggml_metal_init: allocating
0.00.721.519 I ggml_metal_init: found device: Apple M4
0.00.721.532 I ggml_metal_init: picking default device: Apple M4
0.00.723.014 I ggml_metal_init: using embedded metal library
0.00.729.550 I ggml_metal_init: GPU name:   Apple M4
0.00.729.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.729.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.729.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.729.558 I ggml_metal_init: simdgroup reduction   = true
0.00.729.558 I ggml_metal_init: simdgroup matrix mul. = true
0.00.729.558 I ggml_metal_init: has residency sets    = true
0.00.729.558 I ggml_metal_init: has bfloat            = true
0.00.729.559 I ggml_metal_init: use bfloat            = true
0.00.729.560 I ggml_metal_init: hasUnifiedMemory      = true
0.00.729.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.536 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.060 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.751.069 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.751.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.754.316 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.754.318 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.754.318 I llama_init_from_model: graph nodes  = 967
0.00.754.319 I llama_init_from_model: graph splits = 2
0.00.754.322 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.754.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.329 I 
0.00.787.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.455 I perplexity: tokenizing the input ..
0.00.794.506 I perplexity: tokenization took 7.048 ms
0.00.794.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.943.708 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.945.038 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.945.060 I llama_perf_context_print:        load time =     778.00 ms
0.00.945.061 I llama_perf_context_print: prompt eval time =     148.33 ms /   128 tokens (    1.16 ms per token,   862.95 tokens per second)
0.00.945.061 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.945.061 I llama_perf_context_print:       total time =     157.73 ms /   129 tokens
0.00.945.430 I ggml_metal_free: deallocating

real	0m0.962s
user	0m0.081s
sys	0m0.152s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.274 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.805 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.808 I llama_model_loader: - type  f32:  194 tensors
0.00.025.809 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.809 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.810 I print_info: file format = GGUF V3 (latest)
0.00.025.811 I print_info: file type   = Q5_1
0.00.025.812 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.017 I load: special tokens cache size = 25
0.00.040.591 I load: token to piece cache size = 0.2984 MB
0.00.040.605 I print_info: arch             = gptneox
0.00.040.606 I print_info: vocab_only       = 0
0.00.040.606 I print_info: n_ctx_train      = 2048
0.00.040.606 I print_info: n_embd           = 2048
0.00.040.607 I print_info: n_layer          = 24
0.00.040.609 I print_info: n_head           = 16
0.00.040.610 I print_info: n_head_kv        = 16
0.00.040.610 I print_info: n_rot            = 32
0.00.040.610 I print_info: n_swa            = 0
0.00.040.611 I print_info: n_embd_head_k    = 128
0.00.040.611 I print_info: n_embd_head_v    = 128
0.00.040.613 I print_info: n_gqa            = 1
0.00.040.614 I print_info: n_embd_k_gqa     = 2048
0.00.040.616 I print_info: n_embd_v_gqa     = 2048
0.00.040.616 I print_info: f_norm_eps       = 1.0e-05
0.00.040.617 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.617 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.617 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.617 I print_info: f_logit_scale    = 0.0e+00
0.00.040.618 I print_info: n_ff             = 8192
0.00.040.618 I print_info: n_expert         = 0
0.00.040.618 I print_info: n_expert_used    = 0
0.00.040.618 I print_info: causal attn      = 1
0.00.040.618 I print_info: pooling type     = 0
0.00.040.619 I print_info: rope type        = 2
0.00.040.619 I print_info: rope scaling     = linear
0.00.040.620 I print_info: freq_base_train  = 10000.0
0.00.040.620 I print_info: freq_scale_train = 1
0.00.040.620 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.620 I print_info: rope_finetuned   = unknown
0.00.040.621 I print_info: ssm_d_conv       = 0
0.00.040.621 I print_info: ssm_d_inner      = 0
0.00.040.621 I print_info: ssm_d_state      = 0
0.00.040.622 I print_info: ssm_dt_rank      = 0
0.00.040.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.622 I print_info: model type       = 1.4B
0.00.040.622 I print_info: model params     = 1.41 B
0.00.040.622 I print_info: general.name     = 1.4B
0.00.040.623 I print_info: vocab type       = BPE
0.00.040.623 I print_info: n_vocab          = 50304
0.00.040.623 I print_info: n_merges         = 50009
0.00.040.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.623 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.624 I print_info: LF token         = 187 ''
0.00.040.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.624 I print_info: max token length = 1024
0.00.040.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.020 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.034 I load_tensors: offloading output layer to GPU
0.00.648.035 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.067 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.648.069 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.649.711 I llama_init_from_model: n_seq_max     = 1
0.00.649.715 I llama_init_from_model: n_ctx         = 2048
0.00.649.716 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.716 I llama_init_from_model: n_batch       = 2048
0.00.649.717 I llama_init_from_model: n_ubatch      = 512
0.00.649.717 I llama_init_from_model: flash_attn    = 0
0.00.649.720 I llama_init_from_model: freq_base     = 10000.0
0.00.649.720 I llama_init_from_model: freq_scale    = 1
0.00.649.723 I ggml_metal_init: allocating
0.00.649.797 I ggml_metal_init: found device: Apple M4
0.00.649.810 I ggml_metal_init: picking default device: Apple M4
0.00.651.377 I ggml_metal_init: using embedded metal library
0.00.658.012 I ggml_metal_init: GPU name:   Apple M4
0.00.658.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.017 I ggml_metal_init: simdgroup reduction   = true
0.00.658.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.018 I ggml_metal_init: has residency sets    = true
0.00.658.018 I ggml_metal_init: has bfloat            = true
0.00.658.018 I ggml_metal_init: use bfloat            = true
0.00.658.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.174 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.308 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.314 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.337 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.599 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.601 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.601 I llama_init_from_model: graph nodes  = 967
0.00.734.601 I llama_init_from_model: graph splits = 2
0.00.734.608 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.741 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.741 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.547 I main: llama threadpool init, n_threads = 4
0.00.794.594 I 
0.00.794.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.615 I 
0.00.794.772 I sampler seed: 1234
0.00.794.777 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.821 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.824 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.651.917 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.651.917 I llama_perf_context_print:        load time =     785.06 ms
0.01.651.918 I llama_perf_context_print: prompt eval time =      50.65 ms /     7 tokens (    7.24 ms per token,   138.22 tokens per second)
0.01.651.919 I llama_perf_context_print:        eval time =     803.71 ms /    63 runs   (   12.76 ms per token,    78.39 tokens per second)
0.01.651.919 I llama_perf_context_print:       total time =     858.08 ms /    70 tokens
0.01.652.187 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.111s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.877 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.037.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.886 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.887 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.888 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.889 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.891 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.891 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.891 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.762 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.763 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.764 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.764 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.764 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.765 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.046.765 I llama_model_loader: - type  f32:  194 tensors
0.00.046.765 I llama_model_loader: - type q5_1:   97 tensors
0.00.046.766 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.767 I print_info: file format = GGUF V3 (latest)
0.00.046.768 I print_info: file type   = Q5_1
0.00.046.768 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.055.167 I load: special tokens cache size = 25
0.00.061.680 I load: token to piece cache size = 0.2984 MB
0.00.061.698 I print_info: arch             = gptneox
0.00.061.699 I print_info: vocab_only       = 0
0.00.061.699 I print_info: n_ctx_train      = 2048
0.00.061.700 I print_info: n_embd           = 2048
0.00.061.700 I print_info: n_layer          = 24
0.00.061.704 I print_info: n_head           = 16
0.00.061.704 I print_info: n_head_kv        = 16
0.00.061.704 I print_info: n_rot            = 32
0.00.061.705 I print_info: n_swa            = 0
0.00.061.705 I print_info: n_embd_head_k    = 128
0.00.061.705 I print_info: n_embd_head_v    = 128
0.00.061.705 I print_info: n_gqa            = 1
0.00.061.706 I print_info: n_embd_k_gqa     = 2048
0.00.061.707 I print_info: n_embd_v_gqa     = 2048
0.00.061.707 I print_info: f_norm_eps       = 1.0e-05
0.00.061.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.711 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.711 I print_info: f_logit_scale    = 0.0e+00
0.00.061.711 I print_info: n_ff             = 8192
0.00.061.712 I print_info: n_expert         = 0
0.00.061.712 I print_info: n_expert_used    = 0
0.00.061.712 I print_info: causal attn      = 1
0.00.061.712 I print_info: pooling type     = 0
0.00.061.712 I print_info: rope type        = 2
0.00.061.712 I print_info: rope scaling     = linear
0.00.061.713 I print_info: freq_base_train  = 10000.0
0.00.061.713 I print_info: freq_scale_train = 1
0.00.061.713 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.713 I print_info: rope_finetuned   = unknown
0.00.061.713 I print_info: ssm_d_conv       = 0
0.00.061.713 I print_info: ssm_d_inner      = 0
0.00.061.715 I print_info: ssm_d_state      = 0
0.00.061.715 I print_info: ssm_dt_rank      = 0
0.00.061.715 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.716 I print_info: model type       = 1.4B
0.00.061.716 I print_info: model params     = 1.41 B
0.00.061.716 I print_info: general.name     = 1.4B
0.00.061.716 I print_info: vocab type       = BPE
0.00.061.717 I print_info: n_vocab          = 50304
0.00.061.717 I print_info: n_merges         = 50009
0.00.061.717 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.717 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.717 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.718 I print_info: LF token         = 187 ''
0.00.061.718 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.718 I print_info: max token length = 1024
0.00.061.719 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.169 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.190 I load_tensors: offloading output layer to GPU
0.00.668.191 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.240 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.668.242 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.669.400 I llama_init_from_model: n_seq_max     = 1
0.00.669.403 I llama_init_from_model: n_ctx         = 128
0.00.669.403 I llama_init_from_model: n_ctx_per_seq = 128
0.00.669.404 I llama_init_from_model: n_batch       = 128
0.00.669.404 I llama_init_from_model: n_ubatch      = 128
0.00.669.405 I llama_init_from_model: flash_attn    = 0
0.00.669.407 I llama_init_from_model: freq_base     = 10000.0
0.00.669.407 I llama_init_from_model: freq_scale    = 1
0.00.669.408 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.669.411 I ggml_metal_init: allocating
0.00.669.537 I ggml_metal_init: found device: Apple M4
0.00.669.552 I ggml_metal_init: picking default device: Apple M4
0.00.671.198 I ggml_metal_init: using embedded metal library
0.00.677.784 I ggml_metal_init: GPU name:   Apple M4
0.00.677.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.677.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.677.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.677.790 I ggml_metal_init: simdgroup reduction   = true
0.00.677.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.677.791 I ggml_metal_init: has residency sets    = true
0.00.677.791 I ggml_metal_init: has bfloat            = true
0.00.677.791 I ggml_metal_init: use bfloat            = true
0.00.677.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.677.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.609 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.698.614 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.698.640 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.702.105 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.702.107 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.702.108 I llama_init_from_model: graph nodes  = 967
0.00.702.108 I llama_init_from_model: graph splits = 2
0.00.702.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.702.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.532 I 
0.00.730.663 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.689 I perplexity: tokenizing the input ..
0.00.738.115 I perplexity: tokenization took 7.423 ms
0.00.738.121 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.873.734 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.875.066 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.875.079 I llama_perf_context_print:        load time =     720.66 ms
0.00.875.080 I llama_perf_context_print: prompt eval time =     134.70 ms /   128 tokens (    1.05 ms per token,   950.24 tokens per second)
0.00.875.080 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.875.081 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.875.452 I ggml_metal_free: deallocating

real	0m0.891s
user	0m0.081s
sys	0m0.146s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.550 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.555 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.994 I llama_model_loader: - type  f32:  194 tensors
0.00.024.994 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.995 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.996 I print_info: file format = GGUF V3 (latest)
0.00.024.996 I print_info: file type   = Q2_K - Medium
0.00.024.998 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.963 I load: special tokens cache size = 25
0.00.039.303 I load: token to piece cache size = 0.2984 MB
0.00.039.320 I print_info: arch             = gptneox
0.00.039.321 I print_info: vocab_only       = 0
0.00.039.321 I print_info: n_ctx_train      = 2048
0.00.039.322 I print_info: n_embd           = 2048
0.00.039.322 I print_info: n_layer          = 24
0.00.039.328 I print_info: n_head           = 16
0.00.039.329 I print_info: n_head_kv        = 16
0.00.039.329 I print_info: n_rot            = 32
0.00.039.329 I print_info: n_swa            = 0
0.00.039.329 I print_info: n_embd_head_k    = 128
0.00.039.329 I print_info: n_embd_head_v    = 128
0.00.039.330 I print_info: n_gqa            = 1
0.00.039.330 I print_info: n_embd_k_gqa     = 2048
0.00.039.331 I print_info: n_embd_v_gqa     = 2048
0.00.039.331 I print_info: f_norm_eps       = 1.0e-05
0.00.039.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.332 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.335 I print_info: f_logit_scale    = 0.0e+00
0.00.039.336 I print_info: n_ff             = 8192
0.00.039.336 I print_info: n_expert         = 0
0.00.039.336 I print_info: n_expert_used    = 0
0.00.039.336 I print_info: causal attn      = 1
0.00.039.336 I print_info: pooling type     = 0
0.00.039.337 I print_info: rope type        = 2
0.00.039.337 I print_info: rope scaling     = linear
0.00.039.337 I print_info: freq_base_train  = 10000.0
0.00.039.337 I print_info: freq_scale_train = 1
0.00.039.337 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.338 I print_info: rope_finetuned   = unknown
0.00.039.338 I print_info: ssm_d_conv       = 0
0.00.039.338 I print_info: ssm_d_inner      = 0
0.00.039.338 I print_info: ssm_d_state      = 0
0.00.039.338 I print_info: ssm_dt_rank      = 0
0.00.039.338 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.339 I print_info: model type       = 1.4B
0.00.039.339 I print_info: model params     = 1.41 B
0.00.039.339 I print_info: general.name     = 1.4B
0.00.039.340 I print_info: vocab type       = BPE
0.00.039.340 I print_info: n_vocab          = 50304
0.00.039.340 I print_info: n_merges         = 50009
0.00.039.340 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.340 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: LF token         = 187 ''
0.00.039.341 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: max token length = 1024
0.00.039.342 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.361.266 I load_tensors: offloading 24 repeating layers to GPU
0.00.361.281 I load_tensors: offloading output layer to GPU
0.00.361.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.361.319 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.361.324 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.362.940 I llama_init_from_model: n_seq_max     = 1
0.00.362.947 I llama_init_from_model: n_ctx         = 2048
0.00.362.947 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.362.948 I llama_init_from_model: n_batch       = 2048
0.00.362.948 I llama_init_from_model: n_ubatch      = 512
0.00.362.949 I llama_init_from_model: flash_attn    = 0
0.00.362.951 I llama_init_from_model: freq_base     = 10000.0
0.00.362.951 I llama_init_from_model: freq_scale    = 1
0.00.362.953 I ggml_metal_init: allocating
0.00.363.094 I ggml_metal_init: found device: Apple M4
0.00.363.108 I ggml_metal_init: picking default device: Apple M4
0.00.364.782 I ggml_metal_init: using embedded metal library
0.00.370.150 I ggml_metal_init: GPU name:   Apple M4
0.00.370.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.370.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.370.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.370.167 I ggml_metal_init: simdgroup reduction   = true
0.00.370.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.370.167 I ggml_metal_init: has residency sets    = true
0.00.370.167 I ggml_metal_init: has bfloat            = true
0.00.370.168 I ggml_metal_init: use bfloat            = true
0.00.370.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.370.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.391.970 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.447.384 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.447.390 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.447.415 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.451.622 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.451.624 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.451.625 I llama_init_from_model: graph nodes  = 967
0.00.451.625 I llama_init_from_model: graph splits = 2
0.00.451.630 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.451.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.451.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.375 I main: llama threadpool init, n_threads = 4
0.00.510.426 I 
0.00.510.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.448 I 
0.00.510.631 I sampler seed: 1234
0.00.510.636 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.651 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.653 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.653 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.199.902 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.199.903 I llama_perf_context_print:        load time =     499.81 ms
0.01.199.904 I llama_perf_context_print: prompt eval time =      44.56 ms /     7 tokens (    6.37 ms per token,   157.11 tokens per second)
0.01.199.905 I llama_perf_context_print:        eval time =     641.92 ms /    63 runs   (   10.19 ms per token,    98.14 tokens per second)
0.01.199.906 I llama_perf_context_print:       total time =     690.24 ms /    70 tokens
0.01.200.116 I ggml_metal_free: deallocating

real	0m1.219s
user	0m0.112s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.323 I llama_model_loader: - type  f32:  194 tensors
0.00.024.323 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.324 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.324 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.324 I print_info: file format = GGUF V3 (latest)
0.00.024.325 I print_info: file type   = Q2_K - Medium
0.00.024.326 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.720 I load: special tokens cache size = 25
0.00.039.122 I load: token to piece cache size = 0.2984 MB
0.00.039.140 I print_info: arch             = gptneox
0.00.039.140 I print_info: vocab_only       = 0
0.00.039.141 I print_info: n_ctx_train      = 2048
0.00.039.141 I print_info: n_embd           = 2048
0.00.039.141 I print_info: n_layer          = 24
0.00.039.145 I print_info: n_head           = 16
0.00.039.146 I print_info: n_head_kv        = 16
0.00.039.146 I print_info: n_rot            = 32
0.00.039.146 I print_info: n_swa            = 0
0.00.039.146 I print_info: n_embd_head_k    = 128
0.00.039.146 I print_info: n_embd_head_v    = 128
0.00.039.147 I print_info: n_gqa            = 1
0.00.039.147 I print_info: n_embd_k_gqa     = 2048
0.00.039.148 I print_info: n_embd_v_gqa     = 2048
0.00.039.149 I print_info: f_norm_eps       = 1.0e-05
0.00.039.149 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.149 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.149 I print_info: f_logit_scale    = 0.0e+00
0.00.039.150 I print_info: n_ff             = 8192
0.00.039.150 I print_info: n_expert         = 0
0.00.039.150 I print_info: n_expert_used    = 0
0.00.039.150 I print_info: causal attn      = 1
0.00.039.150 I print_info: pooling type     = 0
0.00.039.151 I print_info: rope type        = 2
0.00.039.151 I print_info: rope scaling     = linear
0.00.039.151 I print_info: freq_base_train  = 10000.0
0.00.039.152 I print_info: freq_scale_train = 1
0.00.039.153 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.153 I print_info: rope_finetuned   = unknown
0.00.039.153 I print_info: ssm_d_conv       = 0
0.00.039.154 I print_info: ssm_d_inner      = 0
0.00.039.154 I print_info: ssm_d_state      = 0
0.00.039.154 I print_info: ssm_dt_rank      = 0
0.00.039.154 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.154 I print_info: model type       = 1.4B
0.00.039.155 I print_info: model params     = 1.41 B
0.00.039.155 I print_info: general.name     = 1.4B
0.00.039.155 I print_info: vocab type       = BPE
0.00.039.155 I print_info: n_vocab          = 50304
0.00.039.155 I print_info: n_merges         = 50009
0.00.039.156 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.156 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.158 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.159 I print_info: LF token         = 187 ''
0.00.039.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.159 I print_info: max token length = 1024
0.00.039.159 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.370.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.370.513 I load_tensors: offloading output layer to GPU
0.00.370.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.370.546 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.370.547 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.372.181 I llama_init_from_model: n_seq_max     = 1
0.00.372.184 I llama_init_from_model: n_ctx         = 128
0.00.372.185 I llama_init_from_model: n_ctx_per_seq = 128
0.00.372.185 I llama_init_from_model: n_batch       = 128
0.00.372.186 I llama_init_from_model: n_ubatch      = 128
0.00.372.186 I llama_init_from_model: flash_attn    = 0
0.00.372.189 I llama_init_from_model: freq_base     = 10000.0
0.00.372.189 I llama_init_from_model: freq_scale    = 1
0.00.372.190 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.372.192 I ggml_metal_init: allocating
0.00.372.295 I ggml_metal_init: found device: Apple M4
0.00.372.310 I ggml_metal_init: picking default device: Apple M4
0.00.373.879 I ggml_metal_init: using embedded metal library
0.00.379.366 I ggml_metal_init: GPU name:   Apple M4
0.00.379.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.379.380 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.379.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.379.381 I ggml_metal_init: simdgroup reduction   = true
0.00.379.381 I ggml_metal_init: simdgroup matrix mul. = true
0.00.379.382 I ggml_metal_init: has residency sets    = true
0.00.379.382 I ggml_metal_init: has bfloat            = true
0.00.379.382 I ggml_metal_init: use bfloat            = true
0.00.379.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.379.388 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.402.171 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.406.035 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.406.041 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.406.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.409.403 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.409.405 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.409.406 I llama_init_from_model: graph nodes  = 967
0.00.409.406 I llama_init_from_model: graph splits = 2
0.00.409.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.409.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.435 I 
0.00.439.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.556 I perplexity: tokenizing the input ..
0.00.446.516 I perplexity: tokenization took 6.955 ms
0.00.446.523 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.226 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.580.599 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.580.613 I llama_perf_context_print:        load time =     430.65 ms
0.00.580.614 I llama_perf_context_print: prompt eval time =     131.81 ms /   128 tokens (    1.03 ms per token,   971.10 tokens per second)
0.00.580.615 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.580.615 I llama_perf_context_print:       total time =     141.18 ms /   129 tokens
0.00.580.981 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.082s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.685 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.182 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.193 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.194 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.196 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.936 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.670 I llama_model_loader: - type  f32:  194 tensors
0.00.024.670 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.670 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.671 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.671 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.671 I print_info: file format = GGUF V3 (latest)
0.00.024.672 I print_info: file type   = Q3_K - Medium
0.00.024.673 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.427 I load: special tokens cache size = 25
0.00.038.529 I load: token to piece cache size = 0.2984 MB
0.00.038.543 I print_info: arch             = gptneox
0.00.038.544 I print_info: vocab_only       = 0
0.00.038.544 I print_info: n_ctx_train      = 2048
0.00.038.545 I print_info: n_embd           = 2048
0.00.038.545 I print_info: n_layer          = 24
0.00.038.547 I print_info: n_head           = 16
0.00.038.548 I print_info: n_head_kv        = 16
0.00.038.548 I print_info: n_rot            = 32
0.00.038.548 I print_info: n_swa            = 0
0.00.038.549 I print_info: n_embd_head_k    = 128
0.00.038.549 I print_info: n_embd_head_v    = 128
0.00.038.550 I print_info: n_gqa            = 1
0.00.038.550 I print_info: n_embd_k_gqa     = 2048
0.00.038.551 I print_info: n_embd_v_gqa     = 2048
0.00.038.552 I print_info: f_norm_eps       = 1.0e-05
0.00.038.552 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.552 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.552 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.553 I print_info: f_logit_scale    = 0.0e+00
0.00.038.553 I print_info: n_ff             = 8192
0.00.038.555 I print_info: n_expert         = 0
0.00.038.556 I print_info: n_expert_used    = 0
0.00.038.556 I print_info: causal attn      = 1
0.00.038.556 I print_info: pooling type     = 0
0.00.038.556 I print_info: rope type        = 2
0.00.038.556 I print_info: rope scaling     = linear
0.00.038.556 I print_info: freq_base_train  = 10000.0
0.00.038.557 I print_info: freq_scale_train = 1
0.00.038.557 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.557 I print_info: rope_finetuned   = unknown
0.00.038.557 I print_info: ssm_d_conv       = 0
0.00.038.557 I print_info: ssm_d_inner      = 0
0.00.038.557 I print_info: ssm_d_state      = 0
0.00.038.557 I print_info: ssm_dt_rank      = 0
0.00.038.557 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.558 I print_info: model type       = 1.4B
0.00.038.558 I print_info: model params     = 1.41 B
0.00.038.558 I print_info: general.name     = 1.4B
0.00.038.558 I print_info: vocab type       = BPE
0.00.038.559 I print_info: n_vocab          = 50304
0.00.038.559 I print_info: n_merges         = 50009
0.00.038.559 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.559 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.559 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.559 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.560 I print_info: LF token         = 187 ''
0.00.038.560 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.560 I print_info: max token length = 1024
0.00.038.564 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.610 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.626 I load_tensors: offloading output layer to GPU
0.00.446.627 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.660 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.661 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.450 I llama_init_from_model: n_seq_max     = 1
0.00.448.453 I llama_init_from_model: n_ctx         = 2048
0.00.448.454 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.448.454 I llama_init_from_model: n_batch       = 2048
0.00.448.454 I llama_init_from_model: n_ubatch      = 512
0.00.448.455 I llama_init_from_model: flash_attn    = 0
0.00.448.457 I llama_init_from_model: freq_base     = 10000.0
0.00.448.457 I llama_init_from_model: freq_scale    = 1
0.00.448.460 I ggml_metal_init: allocating
0.00.448.567 I ggml_metal_init: found device: Apple M4
0.00.448.582 I ggml_metal_init: picking default device: Apple M4
0.00.450.232 I ggml_metal_init: using embedded metal library
0.00.456.941 I ggml_metal_init: GPU name:   Apple M4
0.00.456.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.950 I ggml_metal_init: simdgroup reduction   = true
0.00.456.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.951 I ggml_metal_init: has residency sets    = true
0.00.456.951 I ggml_metal_init: has bfloat            = true
0.00.456.952 I ggml_metal_init: use bfloat            = true
0.00.456.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.952 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.519 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.526 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.550 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.537.120 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.537.121 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.537.122 I llama_init_from_model: graph nodes  = 967
0.00.537.122 I llama_init_from_model: graph splits = 2
0.00.537.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.537.252 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.537.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.811 I main: llama threadpool init, n_threads = 4
0.00.592.862 I 
0.00.592.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.883 I 
0.00.593.054 I sampler seed: 1234
0.00.593.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.593.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.593.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.593.074 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.338.022 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.338.023 I llama_perf_context_print:        load time =     582.41 ms
0.01.338.023 I llama_perf_context_print: prompt eval time =      49.66 ms /     7 tokens (    7.09 ms per token,   140.96 tokens per second)
0.01.338.024 I llama_perf_context_print:        eval time =     692.41 ms /    63 runs   (   10.99 ms per token,    90.99 tokens per second)
0.01.338.024 I llama_perf_context_print:       total time =     745.92 ms /    70 tokens
0.01.338.257 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.109s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.113 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.113 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.114 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.115 I llama_model_loader: - type  f32:  194 tensors
0.00.024.115 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.115 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.116 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.116 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.121 I print_info: file format = GGUF V3 (latest)
0.00.024.122 I print_info: file type   = Q3_K - Medium
0.00.024.123 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.039 I load: special tokens cache size = 25
0.00.038.450 I load: token to piece cache size = 0.2984 MB
0.00.038.468 I print_info: arch             = gptneox
0.00.038.469 I print_info: vocab_only       = 0
0.00.038.469 I print_info: n_ctx_train      = 2048
0.00.038.469 I print_info: n_embd           = 2048
0.00.038.469 I print_info: n_layer          = 24
0.00.038.473 I print_info: n_head           = 16
0.00.038.474 I print_info: n_head_kv        = 16
0.00.038.474 I print_info: n_rot            = 32
0.00.038.474 I print_info: n_swa            = 0
0.00.038.474 I print_info: n_embd_head_k    = 128
0.00.038.477 I print_info: n_embd_head_v    = 128
0.00.038.478 I print_info: n_gqa            = 1
0.00.038.479 I print_info: n_embd_k_gqa     = 2048
0.00.038.479 I print_info: n_embd_v_gqa     = 2048
0.00.038.480 I print_info: f_norm_eps       = 1.0e-05
0.00.038.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.481 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.481 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.481 I print_info: f_logit_scale    = 0.0e+00
0.00.038.481 I print_info: n_ff             = 8192
0.00.038.482 I print_info: n_expert         = 0
0.00.038.482 I print_info: n_expert_used    = 0
0.00.038.482 I print_info: causal attn      = 1
0.00.038.482 I print_info: pooling type     = 0
0.00.038.482 I print_info: rope type        = 2
0.00.038.482 I print_info: rope scaling     = linear
0.00.038.483 I print_info: freq_base_train  = 10000.0
0.00.038.483 I print_info: freq_scale_train = 1
0.00.038.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.483 I print_info: rope_finetuned   = unknown
0.00.038.483 I print_info: ssm_d_conv       = 0
0.00.038.484 I print_info: ssm_d_inner      = 0
0.00.038.484 I print_info: ssm_d_state      = 0
0.00.038.484 I print_info: ssm_dt_rank      = 0
0.00.038.484 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.484 I print_info: model type       = 1.4B
0.00.038.484 I print_info: model params     = 1.41 B
0.00.038.485 I print_info: general.name     = 1.4B
0.00.038.485 I print_info: vocab type       = BPE
0.00.038.485 I print_info: n_vocab          = 50304
0.00.038.485 I print_info: n_merges         = 50009
0.00.038.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: LF token         = 187 ''
0.00.038.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.487 I print_info: max token length = 1024
0.00.038.487 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.449.575 I load_tensors: offloading 24 repeating layers to GPU
0.00.449.590 I load_tensors: offloading output layer to GPU
0.00.449.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.449.625 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.449.626 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.451.327 I llama_init_from_model: n_seq_max     = 1
0.00.451.330 I llama_init_from_model: n_ctx         = 128
0.00.451.331 I llama_init_from_model: n_ctx_per_seq = 128
0.00.451.331 I llama_init_from_model: n_batch       = 128
0.00.451.331 I llama_init_from_model: n_ubatch      = 128
0.00.451.332 I llama_init_from_model: flash_attn    = 0
0.00.451.335 I llama_init_from_model: freq_base     = 10000.0
0.00.451.335 I llama_init_from_model: freq_scale    = 1
0.00.451.336 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.451.338 I ggml_metal_init: allocating
0.00.451.462 I ggml_metal_init: found device: Apple M4
0.00.451.479 I ggml_metal_init: picking default device: Apple M4
0.00.453.133 I ggml_metal_init: using embedded metal library
0.00.458.543 I ggml_metal_init: GPU name:   Apple M4
0.00.458.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.458.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.458.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.458.555 I ggml_metal_init: simdgroup reduction   = true
0.00.458.555 I ggml_metal_init: simdgroup matrix mul. = true
0.00.458.555 I ggml_metal_init: has residency sets    = true
0.00.458.556 I ggml_metal_init: has bfloat            = true
0.00.458.556 I ggml_metal_init: use bfloat            = true
0.00.458.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.458.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.478.631 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.482.233 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.482.238 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.482.270 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.485.850 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.485.852 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.485.853 I llama_init_from_model: graph nodes  = 967
0.00.485.853 I llama_init_from_model: graph splits = 2
0.00.485.857 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.485.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.736 I 
0.00.518.824 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.852 I perplexity: tokenizing the input ..
0.00.525.988 I perplexity: tokenization took 7.134 ms
0.00.525.994 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.671.970 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.673.308 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.673.324 I llama_perf_context_print:        load time =     509.67 ms
0.00.673.325 I llama_perf_context_print: prompt eval time =     145.01 ms /   128 tokens (    1.13 ms per token,   882.71 tokens per second)
0.00.673.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.673.326 I llama_perf_context_print:       total time =     154.59 ms /   129 tokens
0.00.673.683 I ggml_metal_free: deallocating

real	0m0.688s
user	0m0.080s
sys	0m0.122s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.503 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.503 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.505 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.320 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.086 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.088 I llama_model_loader: - type  f32:  194 tensors
0.00.024.089 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.089 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.089 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.090 I print_info: file format = GGUF V3 (latest)
0.00.024.090 I print_info: file type   = Q4_K - Medium
0.00.024.091 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.243 I load: special tokens cache size = 25
0.00.038.630 I load: token to piece cache size = 0.2984 MB
0.00.038.643 I print_info: arch             = gptneox
0.00.038.644 I print_info: vocab_only       = 0
0.00.038.645 I print_info: n_ctx_train      = 2048
0.00.038.645 I print_info: n_embd           = 2048
0.00.038.645 I print_info: n_layer          = 24
0.00.038.648 I print_info: n_head           = 16
0.00.038.648 I print_info: n_head_kv        = 16
0.00.038.649 I print_info: n_rot            = 32
0.00.038.650 I print_info: n_swa            = 0
0.00.038.650 I print_info: n_embd_head_k    = 128
0.00.038.650 I print_info: n_embd_head_v    = 128
0.00.038.651 I print_info: n_gqa            = 1
0.00.038.652 I print_info: n_embd_k_gqa     = 2048
0.00.038.652 I print_info: n_embd_v_gqa     = 2048
0.00.038.653 I print_info: f_norm_eps       = 1.0e-05
0.00.038.653 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.657 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.657 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.659 I print_info: f_logit_scale    = 0.0e+00
0.00.038.659 I print_info: n_ff             = 8192
0.00.038.660 I print_info: n_expert         = 0
0.00.038.660 I print_info: n_expert_used    = 0
0.00.038.660 I print_info: causal attn      = 1
0.00.038.660 I print_info: pooling type     = 0
0.00.038.660 I print_info: rope type        = 2
0.00.038.660 I print_info: rope scaling     = linear
0.00.038.661 I print_info: freq_base_train  = 10000.0
0.00.038.661 I print_info: freq_scale_train = 1
0.00.038.661 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.661 I print_info: rope_finetuned   = unknown
0.00.038.661 I print_info: ssm_d_conv       = 0
0.00.038.662 I print_info: ssm_d_inner      = 0
0.00.038.662 I print_info: ssm_d_state      = 0
0.00.038.662 I print_info: ssm_dt_rank      = 0
0.00.038.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.662 I print_info: model type       = 1.4B
0.00.038.665 I print_info: model params     = 1.41 B
0.00.038.665 I print_info: general.name     = 1.4B
0.00.038.666 I print_info: vocab type       = BPE
0.00.038.666 I print_info: n_vocab          = 50304
0.00.038.666 I print_info: n_merges         = 50009
0.00.038.667 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.667 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.667 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.667 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.671 I print_info: LF token         = 187 ''
0.00.038.671 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.671 I print_info: max token length = 1024
0.00.038.672 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.541.957 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.974 I load_tensors: offloading output layer to GPU
0.00.541.975 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.010 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.011 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.543.666 I llama_init_from_model: n_seq_max     = 1
0.00.543.670 I llama_init_from_model: n_ctx         = 2048
0.00.543.670 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.543.671 I llama_init_from_model: n_batch       = 2048
0.00.543.671 I llama_init_from_model: n_ubatch      = 512
0.00.543.671 I llama_init_from_model: flash_attn    = 0
0.00.543.674 I llama_init_from_model: freq_base     = 10000.0
0.00.543.674 I llama_init_from_model: freq_scale    = 1
0.00.543.678 I ggml_metal_init: allocating
0.00.543.795 I ggml_metal_init: found device: Apple M4
0.00.543.809 I ggml_metal_init: picking default device: Apple M4
0.00.545.442 I ggml_metal_init: using embedded metal library
0.00.552.173 I ggml_metal_init: GPU name:   Apple M4
0.00.552.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.179 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.180 I ggml_metal_init: simdgroup reduction   = true
0.00.552.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.181 I ggml_metal_init: has residency sets    = true
0.00.552.181 I ggml_metal_init: has bfloat            = true
0.00.552.181 I ggml_metal_init: use bfloat            = true
0.00.552.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.183 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.168 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.213 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.625.221 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.625.244 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.761 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.629.763 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.629.763 I llama_init_from_model: graph nodes  = 967
0.00.629.763 I llama_init_from_model: graph splits = 2
0.00.629.770 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.629.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.629.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.249 I main: llama threadpool init, n_threads = 4
0.00.687.300 I 
0.00.687.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.320 I 
0.00.687.503 I sampler seed: 1234
0.00.687.508 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.523 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.525 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.449.674 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49305.56 tokens per second)
0.01.449.675 I llama_perf_context_print:        load time =     677.63 ms
0.01.449.676 I llama_perf_context_print: prompt eval time =      58.09 ms /     7 tokens (    8.30 ms per token,   120.51 tokens per second)
0.01.449.676 I llama_perf_context_print:        eval time =     701.10 ms /    63 runs   (   11.13 ms per token,    89.86 tokens per second)
0.01.449.677 I llama_perf_context_print:       total time =     763.15 ms /    70 tokens
0.01.449.941 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.110s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.967 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.492 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.153 I llama_model_loader: - type  f32:  194 tensors
0.00.025.153 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.153 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.153 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.154 I print_info: file format = GGUF V3 (latest)
0.00.025.156 I print_info: file type   = Q4_K - Medium
0.00.025.158 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.655 I load: special tokens cache size = 25
0.00.039.886 I load: token to piece cache size = 0.2984 MB
0.00.039.903 I print_info: arch             = gptneox
0.00.039.904 I print_info: vocab_only       = 0
0.00.039.905 I print_info: n_ctx_train      = 2048
0.00.039.905 I print_info: n_embd           = 2048
0.00.039.905 I print_info: n_layer          = 24
0.00.039.909 I print_info: n_head           = 16
0.00.039.910 I print_info: n_head_kv        = 16
0.00.039.910 I print_info: n_rot            = 32
0.00.039.910 I print_info: n_swa            = 0
0.00.039.910 I print_info: n_embd_head_k    = 128
0.00.039.916 I print_info: n_embd_head_v    = 128
0.00.039.916 I print_info: n_gqa            = 1
0.00.039.917 I print_info: n_embd_k_gqa     = 2048
0.00.039.918 I print_info: n_embd_v_gqa     = 2048
0.00.039.918 I print_info: f_norm_eps       = 1.0e-05
0.00.039.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.920 I print_info: f_logit_scale    = 0.0e+00
0.00.039.920 I print_info: n_ff             = 8192
0.00.039.920 I print_info: n_expert         = 0
0.00.039.920 I print_info: n_expert_used    = 0
0.00.039.920 I print_info: causal attn      = 1
0.00.039.921 I print_info: pooling type     = 0
0.00.039.921 I print_info: rope type        = 2
0.00.039.921 I print_info: rope scaling     = linear
0.00.039.921 I print_info: freq_base_train  = 10000.0
0.00.039.922 I print_info: freq_scale_train = 1
0.00.039.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.922 I print_info: rope_finetuned   = unknown
0.00.039.922 I print_info: ssm_d_conv       = 0
0.00.039.922 I print_info: ssm_d_inner      = 0
0.00.039.922 I print_info: ssm_d_state      = 0
0.00.039.922 I print_info: ssm_dt_rank      = 0
0.00.039.922 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.923 I print_info: model type       = 1.4B
0.00.039.923 I print_info: model params     = 1.41 B
0.00.039.923 I print_info: general.name     = 1.4B
0.00.039.924 I print_info: vocab type       = BPE
0.00.039.924 I print_info: n_vocab          = 50304
0.00.039.924 I print_info: n_merges         = 50009
0.00.039.924 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.924 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.924 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.925 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.925 I print_info: LF token         = 187 ''
0.00.039.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.926 I print_info: max token length = 1024
0.00.039.927 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.509.819 I load_tensors: offloading 24 repeating layers to GPU
0.00.509.831 I load_tensors: offloading output layer to GPU
0.00.509.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.509.865 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.509.866 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.511.608 I llama_init_from_model: n_seq_max     = 1
0.00.511.610 I llama_init_from_model: n_ctx         = 128
0.00.511.611 I llama_init_from_model: n_ctx_per_seq = 128
0.00.511.612 I llama_init_from_model: n_batch       = 128
0.00.511.612 I llama_init_from_model: n_ubatch      = 128
0.00.511.613 I llama_init_from_model: flash_attn    = 0
0.00.511.616 I llama_init_from_model: freq_base     = 10000.0
0.00.511.616 I llama_init_from_model: freq_scale    = 1
0.00.511.617 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.511.619 I ggml_metal_init: allocating
0.00.511.684 I ggml_metal_init: found device: Apple M4
0.00.511.703 I ggml_metal_init: picking default device: Apple M4
0.00.513.477 I ggml_metal_init: using embedded metal library
0.00.520.377 I ggml_metal_init: GPU name:   Apple M4
0.00.520.383 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.520.384 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.520.385 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.520.389 I ggml_metal_init: simdgroup reduction   = true
0.00.520.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.520.389 I ggml_metal_init: has residency sets    = true
0.00.520.389 I ggml_metal_init: has bfloat            = true
0.00.520.390 I ggml_metal_init: use bfloat            = true
0.00.520.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.520.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.515 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.094 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.542.100 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.542.126 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.545.392 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.545.394 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.545.394 I llama_init_from_model: graph nodes  = 967
0.00.545.394 I llama_init_from_model: graph splits = 2
0.00.545.398 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.545.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.948 I 
0.00.571.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.063 I perplexity: tokenizing the input ..
0.00.578.251 I perplexity: tokenization took 7.184 ms
0.00.578.258 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.954 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.713.227 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.713.246 I llama_perf_context_print:        load time =     560.97 ms
0.00.713.247 I llama_perf_context_print: prompt eval time =     132.83 ms /   128 tokens (    1.04 ms per token,   963.62 tokens per second)
0.00.713.248 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.248 I llama_perf_context_print:       total time =     142.30 ms /   129 tokens
0.00.713.656 I ggml_metal_free: deallocating

real	0m0.729s
user	0m0.081s
sys	0m0.119s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.770 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.134 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.145 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.146 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.146 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.146 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.147 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.150 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.429 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.430 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.430 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.431 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.431 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.431 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.432 I llama_model_loader: - type  f32:  194 tensors
0.00.025.432 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.432 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.433 I print_info: file format = GGUF V3 (latest)
0.00.025.433 I print_info: file type   = Q5_K - Medium
0.00.025.434 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.221 I load: special tokens cache size = 25
0.00.039.489 I load: token to piece cache size = 0.2984 MB
0.00.039.503 I print_info: arch             = gptneox
0.00.039.504 I print_info: vocab_only       = 0
0.00.039.504 I print_info: n_ctx_train      = 2048
0.00.039.504 I print_info: n_embd           = 2048
0.00.039.505 I print_info: n_layer          = 24
0.00.039.507 I print_info: n_head           = 16
0.00.039.509 I print_info: n_head_kv        = 16
0.00.039.509 I print_info: n_rot            = 32
0.00.039.509 I print_info: n_swa            = 0
0.00.039.509 I print_info: n_embd_head_k    = 128
0.00.039.510 I print_info: n_embd_head_v    = 128
0.00.039.510 I print_info: n_gqa            = 1
0.00.039.511 I print_info: n_embd_k_gqa     = 2048
0.00.039.512 I print_info: n_embd_v_gqa     = 2048
0.00.039.512 I print_info: f_norm_eps       = 1.0e-05
0.00.039.513 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.514 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.514 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.515 I print_info: f_logit_scale    = 0.0e+00
0.00.039.515 I print_info: n_ff             = 8192
0.00.039.515 I print_info: n_expert         = 0
0.00.039.516 I print_info: n_expert_used    = 0
0.00.039.516 I print_info: causal attn      = 1
0.00.039.516 I print_info: pooling type     = 0
0.00.039.516 I print_info: rope type        = 2
0.00.039.516 I print_info: rope scaling     = linear
0.00.039.517 I print_info: freq_base_train  = 10000.0
0.00.039.518 I print_info: freq_scale_train = 1
0.00.039.523 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.525 I print_info: rope_finetuned   = unknown
0.00.039.525 I print_info: ssm_d_conv       = 0
0.00.039.525 I print_info: ssm_d_inner      = 0
0.00.039.525 I print_info: ssm_d_state      = 0
0.00.039.526 I print_info: ssm_dt_rank      = 0
0.00.039.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.526 I print_info: model type       = 1.4B
0.00.039.527 I print_info: model params     = 1.41 B
0.00.039.527 I print_info: general.name     = 1.4B
0.00.039.528 I print_info: vocab type       = BPE
0.00.039.528 I print_info: n_vocab          = 50304
0.00.039.528 I print_info: n_merges         = 50009
0.00.039.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: LF token         = 187 ''
0.00.039.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: max token length = 1024
0.00.039.531 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.233 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.248 I load_tensors: offloading output layer to GPU
0.00.593.249 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.282 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.283 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.594.851 I llama_init_from_model: n_seq_max     = 1
0.00.594.854 I llama_init_from_model: n_ctx         = 2048
0.00.594.855 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.855 I llama_init_from_model: n_batch       = 2048
0.00.594.856 I llama_init_from_model: n_ubatch      = 512
0.00.594.856 I llama_init_from_model: flash_attn    = 0
0.00.594.857 I llama_init_from_model: freq_base     = 10000.0
0.00.594.858 I llama_init_from_model: freq_scale    = 1
0.00.594.859 I ggml_metal_init: allocating
0.00.594.885 I ggml_metal_init: found device: Apple M4
0.00.594.892 I ggml_metal_init: picking default device: Apple M4
0.00.596.184 I ggml_metal_init: using embedded metal library
0.00.602.714 I ggml_metal_init: GPU name:   Apple M4
0.00.602.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.719 I ggml_metal_init: simdgroup reduction   = true
0.00.602.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.720 I ggml_metal_init: has residency sets    = true
0.00.602.720 I ggml_metal_init: has bfloat            = true
0.00.602.720 I ggml_metal_init: use bfloat            = true
0.00.602.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.816 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.097 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.106 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.129 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.160 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.162 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.163 I llama_init_from_model: graph nodes  = 967
0.00.679.163 I llama_init_from_model: graph splits = 2
0.00.679.170 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.879 I main: llama threadpool init, n_threads = 4
0.00.734.924 I 
0.00.734.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.946 I 
0.00.735.081 I sampler seed: 1234
0.00.735.086 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.137 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.140 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.140 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.586.937 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.586.937 I llama_perf_context_print:        load time =     723.41 ms
0.01.586.938 I llama_perf_context_print: prompt eval time =      52.99 ms /     7 tokens (    7.57 ms per token,   132.10 tokens per second)
0.01.586.939 I llama_perf_context_print:        eval time =     795.83 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.586.939 I llama_perf_context_print:       total time =     852.75 ms /    70 tokens
0.01.587.169 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.109s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.744 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.545 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.286 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.127 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.127 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.128 I llama_model_loader: - type  f32:  194 tensors
0.00.024.128 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.128 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.129 I print_info: file format = GGUF V3 (latest)
0.00.024.131 I print_info: file type   = Q5_K - Medium
0.00.024.132 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.220 I load: special tokens cache size = 25
0.00.038.716 I load: token to piece cache size = 0.2984 MB
0.00.038.733 I print_info: arch             = gptneox
0.00.038.734 I print_info: vocab_only       = 0
0.00.038.734 I print_info: n_ctx_train      = 2048
0.00.038.734 I print_info: n_embd           = 2048
0.00.038.735 I print_info: n_layer          = 24
0.00.038.743 I print_info: n_head           = 16
0.00.038.743 I print_info: n_head_kv        = 16
0.00.038.744 I print_info: n_rot            = 32
0.00.038.744 I print_info: n_swa            = 0
0.00.038.744 I print_info: n_embd_head_k    = 128
0.00.038.745 I print_info: n_embd_head_v    = 128
0.00.038.747 I print_info: n_gqa            = 1
0.00.038.747 I print_info: n_embd_k_gqa     = 2048
0.00.038.748 I print_info: n_embd_v_gqa     = 2048
0.00.038.749 I print_info: f_norm_eps       = 1.0e-05
0.00.038.749 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.749 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.750 I print_info: f_logit_scale    = 0.0e+00
0.00.038.750 I print_info: n_ff             = 8192
0.00.038.750 I print_info: n_expert         = 0
0.00.038.751 I print_info: n_expert_used    = 0
0.00.038.751 I print_info: causal attn      = 1
0.00.038.751 I print_info: pooling type     = 0
0.00.038.751 I print_info: rope type        = 2
0.00.038.751 I print_info: rope scaling     = linear
0.00.038.751 I print_info: freq_base_train  = 10000.0
0.00.038.752 I print_info: freq_scale_train = 1
0.00.038.752 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.752 I print_info: rope_finetuned   = unknown
0.00.038.752 I print_info: ssm_d_conv       = 0
0.00.038.752 I print_info: ssm_d_inner      = 0
0.00.038.752 I print_info: ssm_d_state      = 0
0.00.038.752 I print_info: ssm_dt_rank      = 0
0.00.038.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.753 I print_info: model type       = 1.4B
0.00.038.753 I print_info: model params     = 1.41 B
0.00.038.753 I print_info: general.name     = 1.4B
0.00.038.754 I print_info: vocab type       = BPE
0.00.038.754 I print_info: n_vocab          = 50304
0.00.038.754 I print_info: n_merges         = 50009
0.00.038.754 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.755 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.755 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.755 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.755 I print_info: LF token         = 187 ''
0.00.038.755 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.756 I print_info: max token length = 1024
0.00.038.756 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.270 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.285 I load_tensors: offloading output layer to GPU
0.00.586.286 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.324 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.325 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.588.015 I llama_init_from_model: n_seq_max     = 1
0.00.588.019 I llama_init_from_model: n_ctx         = 128
0.00.588.019 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.020 I llama_init_from_model: n_batch       = 128
0.00.588.020 I llama_init_from_model: n_ubatch      = 128
0.00.588.020 I llama_init_from_model: flash_attn    = 0
0.00.588.022 I llama_init_from_model: freq_base     = 10000.0
0.00.588.023 I llama_init_from_model: freq_scale    = 1
0.00.588.023 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.026 I ggml_metal_init: allocating
0.00.588.115 I ggml_metal_init: found device: Apple M4
0.00.588.129 I ggml_metal_init: picking default device: Apple M4
0.00.589.569 I ggml_metal_init: using embedded metal library
0.00.596.176 I ggml_metal_init: GPU name:   Apple M4
0.00.596.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.182 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.183 I ggml_metal_init: simdgroup reduction   = true
0.00.596.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.184 I ggml_metal_init: has residency sets    = true
0.00.596.184 I ggml_metal_init: has bfloat            = true
0.00.596.184 I ggml_metal_init: use bfloat            = true
0.00.596.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.956 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.960 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.985 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.157 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.159 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.160 I llama_init_from_model: graph nodes  = 967
0.00.620.160 I llama_init_from_model: graph splits = 2
0.00.620.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.409 I 
0.00.656.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.528 I perplexity: tokenizing the input ..
0.00.663.482 I perplexity: tokenization took 6.951 ms
0.00.663.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.861 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.810.271 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.810.287 I llama_perf_context_print:        load time =     647.66 ms
0.00.810.288 I llama_perf_context_print: prompt eval time =     145.02 ms /   128 tokens (    1.13 ms per token,   882.66 tokens per second)
0.00.810.289 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.289 I llama_perf_context_print:       total time =     153.88 ms /   129 tokens
0.00.810.686 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.078s
sys	0m0.136s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.766 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.111 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.740 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.741 I llama_model_loader: - type  f32:  194 tensors
0.00.023.741 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.741 I print_info: file format = GGUF V3 (latest)
0.00.023.742 I print_info: file type   = Q6_K
0.00.023.743 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.502 I load: special tokens cache size = 25
0.00.037.739 I load: token to piece cache size = 0.2984 MB
0.00.037.752 I print_info: arch             = gptneox
0.00.037.753 I print_info: vocab_only       = 0
0.00.037.754 I print_info: n_ctx_train      = 2048
0.00.037.754 I print_info: n_embd           = 2048
0.00.037.754 I print_info: n_layer          = 24
0.00.037.761 I print_info: n_head           = 16
0.00.037.762 I print_info: n_head_kv        = 16
0.00.037.762 I print_info: n_rot            = 32
0.00.037.762 I print_info: n_swa            = 0
0.00.037.762 I print_info: n_embd_head_k    = 128
0.00.037.762 I print_info: n_embd_head_v    = 128
0.00.037.763 I print_info: n_gqa            = 1
0.00.037.765 I print_info: n_embd_k_gqa     = 2048
0.00.037.766 I print_info: n_embd_v_gqa     = 2048
0.00.037.766 I print_info: f_norm_eps       = 1.0e-05
0.00.037.767 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.767 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.767 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.767 I print_info: f_logit_scale    = 0.0e+00
0.00.037.768 I print_info: n_ff             = 8192
0.00.037.768 I print_info: n_expert         = 0
0.00.037.768 I print_info: n_expert_used    = 0
0.00.037.768 I print_info: causal attn      = 1
0.00.037.768 I print_info: pooling type     = 0
0.00.037.768 I print_info: rope type        = 2
0.00.037.774 I print_info: rope scaling     = linear
0.00.037.774 I print_info: freq_base_train  = 10000.0
0.00.037.775 I print_info: freq_scale_train = 1
0.00.037.775 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.775 I print_info: rope_finetuned   = unknown
0.00.037.775 I print_info: ssm_d_conv       = 0
0.00.037.776 I print_info: ssm_d_inner      = 0
0.00.037.776 I print_info: ssm_d_state      = 0
0.00.037.776 I print_info: ssm_dt_rank      = 0
0.00.037.776 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.776 I print_info: model type       = 1.4B
0.00.037.777 I print_info: model params     = 1.41 B
0.00.037.777 I print_info: general.name     = 1.4B
0.00.037.777 I print_info: vocab type       = BPE
0.00.037.778 I print_info: n_vocab          = 50304
0.00.037.778 I print_info: n_merges         = 50009
0.00.037.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.778 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.780 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.780 I print_info: LF token         = 187 ''
0.00.037.780 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.781 I print_info: max token length = 1024
0.00.037.781 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.506 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.509 I load_tensors: offloading output layer to GPU
0.00.631.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.534 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.631.537 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.633.184 I llama_init_from_model: n_seq_max     = 1
0.00.633.186 I llama_init_from_model: n_ctx         = 2048
0.00.633.187 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.633.187 I llama_init_from_model: n_batch       = 2048
0.00.633.188 I llama_init_from_model: n_ubatch      = 512
0.00.633.188 I llama_init_from_model: flash_attn    = 0
0.00.633.189 I llama_init_from_model: freq_base     = 10000.0
0.00.633.190 I llama_init_from_model: freq_scale    = 1
0.00.633.191 I ggml_metal_init: allocating
0.00.633.232 I ggml_metal_init: found device: Apple M4
0.00.633.241 I ggml_metal_init: picking default device: Apple M4
0.00.634.500 I ggml_metal_init: using embedded metal library
0.00.640.259 I ggml_metal_init: GPU name:   Apple M4
0.00.640.262 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.263 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.264 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.264 I ggml_metal_init: simdgroup reduction   = true
0.00.640.264 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.265 I ggml_metal_init: has residency sets    = true
0.00.640.265 I ggml_metal_init: has bfloat            = true
0.00.640.265 I ggml_metal_init: use bfloat            = true
0.00.640.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.071 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.770 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.780 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.803 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.904 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.906 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.907 I llama_init_from_model: graph nodes  = 967
0.00.703.907 I llama_init_from_model: graph splits = 2
0.00.703.912 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.025 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.481 I main: llama threadpool init, n_threads = 4
0.00.769.532 I 
0.00.769.553 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.553 I 
0.00.769.728 I sampler seed: 1234
0.00.769.733 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.776 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.781 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.656.035 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.656.036 I llama_perf_context_print:        load time =     759.99 ms
0.01.656.037 I llama_perf_context_print: prompt eval time =      57.82 ms /     7 tokens (    8.26 ms per token,   121.07 tokens per second)
0.01.656.038 I llama_perf_context_print:        eval time =     825.53 ms /    63 runs   (   13.10 ms per token,    76.31 tokens per second)
0.01.656.038 I llama_perf_context_print:       total time =     887.28 ms /    70 tokens
0.01.656.266 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.108s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4837 (e721c05c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.943 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.553 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.241 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.980 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.982 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.982 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.983 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.983 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.984 I llama_model_loader: - type  f32:  194 tensors
0.00.023.984 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.985 I print_info: file format = GGUF V3 (latest)
0.00.023.986 I print_info: file type   = Q6_K
0.00.023.987 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.883 I load: special tokens cache size = 25
0.00.038.375 I load: token to piece cache size = 0.2984 MB
0.00.038.387 I print_info: arch             = gptneox
0.00.038.388 I print_info: vocab_only       = 0
0.00.038.388 I print_info: n_ctx_train      = 2048
0.00.038.389 I print_info: n_embd           = 2048
0.00.038.389 I print_info: n_layer          = 24
0.00.038.393 I print_info: n_head           = 16
0.00.038.394 I print_info: n_head_kv        = 16
0.00.038.394 I print_info: n_rot            = 32
0.00.038.394 I print_info: n_swa            = 0
0.00.038.394 I print_info: n_embd_head_k    = 128
0.00.038.394 I print_info: n_embd_head_v    = 128
0.00.038.395 I print_info: n_gqa            = 1
0.00.038.396 I print_info: n_embd_k_gqa     = 2048
0.00.038.396 I print_info: n_embd_v_gqa     = 2048
0.00.038.397 I print_info: f_norm_eps       = 1.0e-05
0.00.038.397 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.397 I print_info: f_logit_scale    = 0.0e+00
0.00.038.398 I print_info: n_ff             = 8192
0.00.038.398 I print_info: n_expert         = 0
0.00.038.398 I print_info: n_expert_used    = 0
0.00.038.398 I print_info: causal attn      = 1
0.00.038.398 I print_info: pooling type     = 0
0.00.038.399 I print_info: rope type        = 2
0.00.038.399 I print_info: rope scaling     = linear
0.00.038.399 I print_info: freq_base_train  = 10000.0
0.00.038.399 I print_info: freq_scale_train = 1
0.00.038.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.400 I print_info: rope_finetuned   = unknown
0.00.038.400 I print_info: ssm_d_conv       = 0
0.00.038.400 I print_info: ssm_d_inner      = 0
0.00.038.400 I print_info: ssm_d_state      = 0
0.00.038.400 I print_info: ssm_dt_rank      = 0
0.00.038.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.400 I print_info: model type       = 1.4B
0.00.038.401 I print_info: model params     = 1.41 B
0.00.038.401 I print_info: general.name     = 1.4B
0.00.038.401 I print_info: vocab type       = BPE
0.00.038.402 I print_info: n_vocab          = 50304
0.00.038.402 I print_info: n_merges         = 50009
0.00.038.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.403 I print_info: LF token         = 187 ''
0.00.038.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.403 I print_info: max token length = 1024
0.00.038.404 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.881 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.891 I load_tensors: offloading output layer to GPU
0.00.627.891 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.922 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.627.925 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.629.523 I llama_init_from_model: n_seq_max     = 1
0.00.629.526 I llama_init_from_model: n_ctx         = 128
0.00.629.526 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.527 I llama_init_from_model: n_batch       = 128
0.00.629.527 I llama_init_from_model: n_ubatch      = 128
0.00.629.528 I llama_init_from_model: flash_attn    = 0
0.00.629.529 I llama_init_from_model: freq_base     = 10000.0
0.00.629.529 I llama_init_from_model: freq_scale    = 1
0.00.629.530 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.532 I ggml_metal_init: allocating
0.00.629.584 I ggml_metal_init: found device: Apple M4
0.00.629.598 I ggml_metal_init: picking default device: Apple M4
0.00.630.968 I ggml_metal_init: using embedded metal library
0.00.637.283 I ggml_metal_init: GPU name:   Apple M4
0.00.637.287 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.288 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.289 I ggml_metal_init: simdgroup reduction   = true
0.00.637.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.290 I ggml_metal_init: has residency sets    = true
0.00.637.290 I ggml_metal_init: has bfloat            = true
0.00.637.290 I ggml_metal_init: use bfloat            = true
0.00.637.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.505 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.933 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.658.942 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.658.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.661.959 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.661.961 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.661.961 I llama_init_from_model: graph nodes  = 967
0.00.661.961 I llama_init_from_model: graph splits = 2
0.00.661.964 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.661.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.596 I 
0.00.699.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.708 I perplexity: tokenizing the input ..
0.00.706.396 I perplexity: tokenization took 6.685 ms
0.00.706.400 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.110 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.838.445 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.838.459 I llama_perf_context_print:        load time =     690.64 ms
0.00.838.460 I llama_perf_context_print: prompt eval time =     130.48 ms /   128 tokens (    1.02 ms per token,   981.02 tokens per second)
0.00.838.461 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.461 I llama_perf_context_print:       total time =     138.87 ms /   129 tokens
0.00.838.840 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.078s
sys	0m0.151s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4837 (e721c05c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152406170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1524067e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152409470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152409a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152409fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15240a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15240ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15240b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15240b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15240bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15240c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15240c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15240d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15240d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15240e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15240e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15240eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15240f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15240fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1524104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152410be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152411300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152411a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1524122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1524129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152412ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1524132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152413f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152414460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152414720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152414bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152414e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152415710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152415c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152415f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1524163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152416850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152416cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152417190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152417630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152417ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152417f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152418410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1524188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152418b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152419180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152419790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15241a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15241a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15241acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15241b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15241b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15241bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15241c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15241cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15241d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15241d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15241d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15241df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15241e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15241e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15241ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15241f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15241f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15241fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1524200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152420580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152420a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152420ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152421360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152421800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152421ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152422140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152422690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152422be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152423130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152423680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152423bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152424120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152424670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152424bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152425110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152425660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152425bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152426100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152426650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152426ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1524270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152427640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152427b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1524280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152428630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152428b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1524290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152429620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152429b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15242a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152419da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15242a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15242ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15242b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15242b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15242bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15242c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15242c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15242ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15242d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15242d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15242dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15242e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15242e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15242eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15242f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15242f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15242fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15242ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152430470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152430910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152430db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152431250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1524316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152431b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152432030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1524324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152432970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152432e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1524332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152433750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152433bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152434090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152434530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1524349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152434e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152435310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1524357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152435c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1524360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152436590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152436a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152436ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152437370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152437810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152437cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152438150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1524385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152438a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152438f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1524393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152439870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152439d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15243a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15243a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15243aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15243af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15243b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15243b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15243bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15243c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15243c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15243cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15243cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15243d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15243d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15243ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15243e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15243e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15243ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15243f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15243f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15243f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15243fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1524402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152440770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152440c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1524410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152441550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1524419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152441e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152442330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1524427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152442c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152443110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1524435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152443a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152443ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152444390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152444830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152444cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152445170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152445610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152445ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152445f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1524463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152446940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152446e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1524473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152447930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152447bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152448200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152448810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152448e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152449610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152449ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152449d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15244a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15244a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15244b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15244b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15244bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15244bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15244c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15244cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15244d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15244d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15244dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15244e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15244e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15244ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15244f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15244f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15244fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152450180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1524506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152450c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152451170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1524516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152451c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152452160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1524526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152452c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152453150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1524536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152453bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152454140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152454690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152454be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152455130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152455680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152455bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152456120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152456670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152456bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152457110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152457660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152457bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152458100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152458650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152458ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1524590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152459640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152459b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15245a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15245a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15245ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15245b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15245b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15245bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15245c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15245c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15245cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15245d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15245d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15245db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15245e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15245e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15245eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15245f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15245f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15245f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15245fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152460310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1524607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152460c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1524610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152461590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152461a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152461ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152462370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152462810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152462cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152463150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1524635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x152463a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x152463f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1524643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x152464870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152464d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1524651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x152465650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x152465af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x152465f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x152466430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152466980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1524670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1524677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152467ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152468600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1524688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1524690b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152469370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152469980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.628.967 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.628.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152504b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152504f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152505400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152505870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152505ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152506150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1525065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152506a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152506ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152507310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152507780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152507e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152508990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152509140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152509950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15250a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15250a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15250aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15250b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15250bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15250c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15250cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15250d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15250d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15250e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15250e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15250e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15250ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15250ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15250f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15250f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15250fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152510180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152510440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1525108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152510d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152511190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152511600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152511a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152511ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152512350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1525127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152512c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1525130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152513510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152513980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152513df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152514260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1525146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152514b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152514fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152515420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152515890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152515d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152516170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1525165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152516b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152517050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1525174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152517930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152517da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152518210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152518680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152518af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152518f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1525193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152519840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152519cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15251a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15251a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15251aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15251ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15251b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15251b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15251bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15251c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15251c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15251c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15251cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15251d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15251d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15251dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15251df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15251e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15251e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15251ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15251f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15251f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15251f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15251fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1525202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152520730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152520ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152521010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152521480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1525218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152521d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1525221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152522640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152522ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152522f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152523390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152523800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152523c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1525240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152524550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1525249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152524e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1525252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152525710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152525b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152525ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152526460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1525268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152526d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1525271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152527620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152527a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152527f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152528370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1525287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152528c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1525290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152529530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1525299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152529e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15252a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15252a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15252ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15252afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15252b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15252b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15252bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15252c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15252c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15252ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15252cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15252d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15252d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15252dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15252e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15252e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15252e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15252edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15252f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15252f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15252fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15252ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152530420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152530890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152530d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152531170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1525315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152531a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152531ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152532330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1525327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152532c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152533080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1525334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152533960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152533dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152534240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1525346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152534b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152534f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152535bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152535e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152536140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1525365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152536a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152536e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152537300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152537770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152537be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152538050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1525384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152538930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152538da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152539210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152539680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152539af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152539f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15253a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15253a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15253acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15253b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15253b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15253ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15253be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15253c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15253c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15253cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15253d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15253d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15253d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15253dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15253e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15253e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15253ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155e04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155e046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155e04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155e04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155e053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155e05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155e05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155e06250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155e066c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155e06b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155e07680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155e07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155e07c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155e08070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155e084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155e08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155e08dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155e09230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155e096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155e09b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155e09f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155e0a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155e0a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155e0acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155e0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155e0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155e0ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155e0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155e0c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155e0c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155e0cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155e0d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155e0d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155e0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155e0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155e0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155e0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155e0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155e0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155e0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155e0f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155e0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155e10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155e10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155e10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155e10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155e112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155e11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155e11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155e12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155e124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155e12910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155e12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155e131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155e13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155e13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155e13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155e143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155e14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155e14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155e15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155e159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155e15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155e162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155e16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155e16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155e17010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155e17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155e178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155e17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155e181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155e18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155e18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155e18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155e19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155e19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155e19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155e1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155e1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155e1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155e1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x155e1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x155e1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x155e1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x155e1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x155e1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x155e1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x155e1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x155e1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x155e1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x155e1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155e1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155e1e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155e1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155e1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155e1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155e201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155e20490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155e20900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155e20d70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152305e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152306280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1523066f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152306b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152306fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152307440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1523078b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152307d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152308190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152308600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152308a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1523091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152309cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15230a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15230ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15230b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15230bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15230c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15230c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15230d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15230d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15230de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15230e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15230ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15230f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15230f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15230f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15230fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152310230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1523106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152310ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1523110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152311520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1523117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152311c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1523120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152312b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152313020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152313520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152313a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152313f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152314420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152314920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152314e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152315290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152315700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152315b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152315fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152316450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1523168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152316d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1523171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152317610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152317a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152318250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1523186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1523189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152318fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1523197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152319c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15231a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15231a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15231aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15231aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15231b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15231b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15231bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15231c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15231c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15231ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15231cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15231d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15231d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15231de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15231e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15231e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15231ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15231f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15231f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15231fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1523203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1523208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152320e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152321390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1523218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152321e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152322380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1523228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152322e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152323370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1523238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152323e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152324360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1523248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152324e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152325350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1523258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152325df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152326340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152326890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152326de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152327330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152327880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152327dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152328320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152328870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152328dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152329310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152329860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152329db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15232a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15232a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15232acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15232b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15232b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15232bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15232bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15232c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15232c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15232cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15232d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15232d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15232db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15232dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15232e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15232e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15232edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15232f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15232f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15232fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152330030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1523304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152330970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152330e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1523312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152331750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152331bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152332090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152332530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1523329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152332e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152333310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1523337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152333c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1523340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152334590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152334a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152334ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152335370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152335810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152335cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152336150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1523365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152336a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152336f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1523373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152337870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152337d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1523381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152338650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152338af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152338f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152339430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1523398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152339d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15233a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15233a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15233ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15233aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15233b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15233b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15233bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15233c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15233c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15233cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15233d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15233d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15233d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15233de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15233e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15233e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15233ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15233f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15233f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15233f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15233fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152340330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1523407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152340c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152341110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1523415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152341a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152341fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1523424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152342a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152342f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152343250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152343860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152343e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152344480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152344c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152345110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1523453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1523459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152345ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1523467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152346c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152347120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1523475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152347d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1523482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152348810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152348d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1523492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152349800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152349d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15234a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15234a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15234ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15234b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15234b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15234bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15234c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15234c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15234cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15234d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15234d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15234dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15234e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15234e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15234ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15234f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15234f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15234fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152350240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152350790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152350ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152351230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152351780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152351cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152352220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152352770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152352cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152353210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152353760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152353cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152354200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152354750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152354ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1523551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152355740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152355c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1523561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152356730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152356c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1523571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152357720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152357c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1523581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152358710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152358c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1523591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152359700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152359c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15235a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15235a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15235ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15235b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15235b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15235b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15235be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15235c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15235c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15235cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15235d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15235d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15235d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15235de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15235e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15235e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15235ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15235f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15235f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15235fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15235fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x152360370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x152360810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x152360cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x152361150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1523615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x152361a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152361fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152362700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152362e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152363540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152363c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152363f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152364710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1523649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152364fe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.704s
user	0m0.278s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4837 (e721c05c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128f0f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128f100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128f10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128f10c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128f111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128f11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128f11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128f122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128f12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128f12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128f13280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128f13780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128f142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128f14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128f15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128f15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128f160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128f167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128f16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128f176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128f17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128f184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128f18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128f194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128f19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128f19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128f1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128f1b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128f1b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128f1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128f1bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128f1c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128f1c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128f1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128f1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128f1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128f1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128f1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128f1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128f1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128f1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128f1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128f20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128f20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128f212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128f218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128f21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128f224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128f22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128f230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128f23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128f23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128f24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128f24af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128f25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128f258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128f25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128f26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128f264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128f26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128f26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128f272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128f27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128f27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x128f28550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x128f289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x128f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x128f29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x128f29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x128f29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x128f2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x128f2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x128f2adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x128f2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x128f2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x128f2bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x128f2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x128f2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x128f2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128f2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128f2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128f2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128f2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128f2ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128f2f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128f2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128f2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128f302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128f30810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128f30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128f312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128f20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128f31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128f31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128f32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128f32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128f33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128f33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128f34400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128f34950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128f34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128f353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128f35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128f35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128f36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128f371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128f37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128f37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128f38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128f388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128f38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128f396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128f3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x128f3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x128f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x128f3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x128f3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128f3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128f3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128f3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128f3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128f3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128f3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128f3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128f3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128f3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128f3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128f3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128f3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128f40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128f405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128f40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128f40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128f413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128f41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128f41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128f42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128f42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128f42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128f42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128f43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128f438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128f43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128f441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128f44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128f44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128f45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128f45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128f46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128f466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128f46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128f47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128f474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128f47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128f48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128f48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128f49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128f49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128f499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128f49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128f4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128f4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x128f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x128f4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x128f4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x128f4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x128f4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128f4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128f4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128f4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128f4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128f4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128f4e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128f4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128f4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128f4ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128f4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128f4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128f50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128f50800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128f50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128f50f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128f51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128f51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128f52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128f52810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128f52cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128f53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128f53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128f53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128f543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128f548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128f54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128f55390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128f558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128f55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128f56380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128f568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128f56e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128f57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128f578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128f57e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128f58360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128f588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128f58e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128f59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128f598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128f59df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128f5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128f5a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x128f5ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x128f5b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x128f5b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x128f5bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x128f5c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x128f5c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x128f5cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128f5d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128f5d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x128f5ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x128f5e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128f5e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128f5eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x128f5f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128f5f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128f5fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128f602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128f60830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128f60d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128f612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128f61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128f61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128f622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128f62810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128f62d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128f632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128f63800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128f63d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128f642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128f647f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128f64d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128f65290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128f657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128f65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128f66280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128f66720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128f66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128f67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128f67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128f679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128f682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128f68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128f68c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128f690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128f69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128f69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128f69ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128f6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128f6a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x128f6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x128f6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x128f6b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x128f6ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x128f6bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x128f6c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x128f6c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x128f6cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x128f6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x128f6d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128f6db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128f6e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128f6e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128f6f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128f6f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128f6fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128f702a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128f70560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128f70b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.104.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128f4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128f51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128f70820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128f4f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128f4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128f22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128f22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128f24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128f51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128f1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128f20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128f21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128f20630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128f233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128f22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128f19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x128f319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x128f6fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x128f1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x128f1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x128f51e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x128f502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x128f1a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x128f1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x128f1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x128f70fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x128f71290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x128f71550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x128f71810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x128f71ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x128f71d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x128f72050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128f72310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128f725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128f72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128f72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128f72e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128f730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128f73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128f73650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128f73910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128f73bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128f73e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128f74150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128f74410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128f746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128f74990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128f74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128f74f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128f751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128f75490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128f75750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128f75a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128f75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128f75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128f76250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128f76510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128f767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128f76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128f76d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128f77010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128f772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128f77590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128f77850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128f77b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c7044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c7056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c7075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c707ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c708310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c708780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c708bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c709060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c7094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c709940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c709db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c70a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c70a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c70ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c70af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c70b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c70b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c70bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c70c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c70c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c70ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c70ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c70d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c70d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c70dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c70e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c70e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c70e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c70ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c70f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c70f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c70fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c70ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c7103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c710830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c710ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c711110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c711580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c7119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c711e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c7122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c712740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c712bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c713020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c713490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c713900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c713d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c7141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c714650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c714ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c714f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c7153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c715810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c715c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c7160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c7169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c716e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c7172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c717720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c717b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c718000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c718470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c7188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c718d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c7191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c719630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c719aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c719f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c71a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c71a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c71ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c71b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c71b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c71b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c71be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c71c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c71c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c71cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c71cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c71d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c71d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c71dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c71e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c71e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c71ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c71eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c71f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c71fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c720870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c720b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c720df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c721260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c7216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c721fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c722890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c722d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c723170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c7235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c723a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c723ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c724330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c7247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c724c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c725080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c7254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c725960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c725dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c726240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c7266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c726b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c726f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c727400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c727870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c727ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c728150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c7285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c728a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c729780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c729bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c72a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c72a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c72aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c72af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c72b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c72b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c72bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c72c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c72c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c72d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c72d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c72dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c72e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c72e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c72ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c72f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c72f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c72fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c7302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c7308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c730e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c731430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c7319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c731fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c732570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c732b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c7330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c7336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c733c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c734230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c7347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c734db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c735370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c735930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c735ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c7364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c736a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c737030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c7375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c737bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c738170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c738730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c738cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c7392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c739870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c739e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c73a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c73a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c73af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c73b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c73baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c73c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c73c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c73cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c73d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c73d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c73dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c73e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c73e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c73eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c73f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c73fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c73fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c7405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c740b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c741130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c7416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c741bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c7420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c7425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c742af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c7434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c7439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c743ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c7443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c7448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c744df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c7452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c7457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c745cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11c7461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11c7466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11c746bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11c7470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11c7475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11c747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11c747ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11c7484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11c7489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11c748ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c7493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c749e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c74a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c74ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c74b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c74b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c74be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c74c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c74c6e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a00a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a00a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a00af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a00b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a00be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a00c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a00cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a00dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a00dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a00e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a00e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a00edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a00f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a0133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a0149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a0177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a0189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a0196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a01a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a01a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a01b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a01bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a01c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a01d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a01ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a01e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a01e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a01eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a01efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a01f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a01f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a01fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a0217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a022470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a022990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a022f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a0234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a023aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a024050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a024600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a024bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a025160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a025710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a025cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a026270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a026820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a026dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a027380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a027930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a027e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a028330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a028d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a029230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a029730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a029c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a02a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a02a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a02ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a02b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a02b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a02ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a02bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a02c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a02c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a02ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a02d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a02d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a02dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a02e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a02e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a02ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a02f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a02f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a02fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a030030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a030530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a030a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a030f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a031430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a031930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a031e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a032330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a032830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a033230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a033730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a033c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a034130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a034630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a034b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a035030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a035530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a035a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a035f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a036430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a036930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a036e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a037330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a037830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a037d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a038230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a038730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a038c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a039130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a039b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a03a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a03a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a03aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a03af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a03b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a03b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a03be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a03c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a03c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a03cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a03d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a03d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a03dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a03e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a03e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a03eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a03f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a03f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a03fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a03ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a040930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a040ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a041490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a041a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a041ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a042600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a042c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a043220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a043a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a043eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a044170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a044780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a044d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a045580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a045a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a045ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a046360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a046b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a0475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a047b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a048050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a0485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a048af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a049040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a049590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a049ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a04a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a04a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a04aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a04b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a04b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a04bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a04c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a04c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a04cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a04d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a04d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a04daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a04dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a04e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a04ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a04efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a04f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a04fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a04ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a050520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a050a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a050fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a051510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a051a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a051fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a052500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a052a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a052fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a0534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a053a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a053f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a0544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a054a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a054f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a0554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a055a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a055f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a0564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a056a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a056f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a0574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a057a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a057f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a0584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a0589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a058f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a059490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a059930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a059dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a05a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a05a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a05abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a05b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a05b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a05b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a05be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a05c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a05c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a05cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a05d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a05d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a05d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12a05de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12a05e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12a05e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12a05ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12a05f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12a05f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12a05fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12a05fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12a060390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12a060830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a060d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a0614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a061bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a0622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a062a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a062cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a0634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a063770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a063d80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.231s
sys	0m0.189s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.42 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.18 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.60 sec*proc (2 tests)

Total Test time (real) =   1.61 sec
        1.64 real         0.51 user         0.21 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
