Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.560s
user	0m0.903s
sys	0m1.224s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking C executable ../bin/test-c
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Built target test-grammar-integration
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Built target test-chat-template
[ 61%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-autorelease
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 70%] Built target test-quantize-perf
[ 70%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-infill
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-lookup
[ 84%] Built target llama-lookup-stats
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-cli
[ 84%] Built target llama-parallel
[ 84%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-quantize
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-perplexity
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-run
[ 90%] Built target llama-speculative
[ 90%] Built target llama-tokenize
[ 90%] Built target llama-tts
[ 90%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-gen-docs
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.078s
user	0m6.204s
sys	0m9.898s

main: quantize time =  2185.14 ms
main:    total time =  2185.14 ms

main: quantize time =  1326.80 ms
main:    total time =  1326.80 ms

main: quantize time =  1340.67 ms
main:    total time =  1340.67 ms

main: quantize time =  2295.86 ms
main:    total time =  2295.86 ms

main: quantize time =  2452.46 ms
main:    total time =  2452.46 ms

main: quantize time =  4996.18 ms
main:    total time =  4996.18 ms

main: quantize time =  5669.53 ms
main:    total time =  5669.53 ms

main: quantize time =  6812.98 ms
main:    total time =  6812.98 ms

main: quantize time =  5954.59 ms
main:    total time =  5954.59 ms

main: quantize time =  4598.35 ms
main:    total time =  4598.35 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.199 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.325 I main: llama backend init
0.00.000.336 I main: load the model and apply lora adapter, if any
0.00.029.854 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.983 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.045 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.048 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.053 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.675 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.676 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.678 I llama_model_loader: - type  f32:  194 tensors
0.00.062.678 I llama_model_loader: - type  f16:   98 tensors
0.00.062.682 I print_info: file format = GGUF V3 (latest)
0.00.062.684 I print_info: file type   = all F32 (guessed)
0.00.062.686 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.962 I load: special tokens cache size = 25
0.00.101.078 I load: token to piece cache size = 0.2984 MB
0.00.101.082 I print_info: arch             = gptneox
0.00.101.082 I print_info: vocab_only       = 0
0.00.101.082 I print_info: n_ctx_train      = 2048
0.00.101.082 I print_info: n_embd           = 2048
0.00.101.082 I print_info: n_layer          = 24
0.00.101.086 I print_info: n_head           = 16
0.00.101.086 I print_info: n_head_kv        = 16
0.00.101.089 I print_info: n_rot            = 32
0.00.101.089 I print_info: n_swa            = 0
0.00.101.089 I print_info: n_embd_head_k    = 128
0.00.101.089 I print_info: n_embd_head_v    = 128
0.00.101.090 I print_info: n_gqa            = 1
0.00.101.092 I print_info: n_embd_k_gqa     = 2048
0.00.101.092 I print_info: n_embd_v_gqa     = 2048
0.00.101.093 I print_info: f_norm_eps       = 1.0e-05
0.00.101.093 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.101.094 I print_info: f_clamp_kqv      = 0.0e+00
0.00.101.094 I print_info: f_max_alibi_bias = 0.0e+00
0.00.101.094 I print_info: f_logit_scale    = 0.0e+00
0.00.101.095 I print_info: n_ff             = 8192
0.00.101.095 I print_info: n_expert         = 0
0.00.101.095 I print_info: n_expert_used    = 0
0.00.101.095 I print_info: causal attn      = 1
0.00.101.097 I print_info: pooling type     = 0
0.00.101.097 I print_info: rope type        = 2
0.00.101.097 I print_info: rope scaling     = linear
0.00.101.098 I print_info: freq_base_train  = 10000.0
0.00.101.098 I print_info: freq_scale_train = 1
0.00.101.098 I print_info: n_ctx_orig_yarn  = 2048
0.00.101.098 I print_info: rope_finetuned   = unknown
0.00.101.099 I print_info: ssm_d_conv       = 0
0.00.101.099 I print_info: ssm_d_inner      = 0
0.00.101.099 I print_info: ssm_d_state      = 0
0.00.101.099 I print_info: ssm_dt_rank      = 0
0.00.101.099 I print_info: ssm_dt_b_c_rms   = 0
0.00.101.099 I print_info: model type       = 1.4B
0.00.101.100 I print_info: model params     = 1.41 B
0.00.101.104 I print_info: general.name     = 1.4B
0.00.101.104 I print_info: vocab type       = BPE
0.00.101.105 I print_info: n_vocab          = 50304
0.00.101.105 I print_info: n_merges         = 50009
0.00.101.106 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.101.106 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.101.107 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.101.107 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.101.107 I print_info: LF token         = 128 'Ä'
0.00.101.108 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.101.108 I print_info: max token length = 1024
0.00.103.085 I load_tensors: offloading 24 repeating layers to GPU
0.00.103.085 I load_tensors: offloading output layer to GPU
0.00.103.085 I load_tensors: offloaded 25/25 layers to GPU
0.00.103.103 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.105 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.103.420 I llama_init_from_model: n_seq_max     = 1
0.00.103.421 I llama_init_from_model: n_ctx         = 2048
0.00.103.422 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.103.422 I llama_init_from_model: n_batch       = 2048
0.00.103.422 I llama_init_from_model: n_ubatch      = 512
0.00.103.422 I llama_init_from_model: flash_attn    = 0
0.00.103.423 I llama_init_from_model: freq_base     = 10000.0
0.00.103.423 I llama_init_from_model: freq_scale    = 1
0.00.103.423 I ggml_metal_init: allocating
0.00.103.426 I ggml_metal_init: found device: Apple M4
0.00.103.428 I ggml_metal_init: picking default device: Apple M4
0.00.104.126 I ggml_metal_init: using embedded metal library
0.00.113.594 I ggml_metal_init: GPU name:   Apple M4
0.00.113.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.597 I ggml_metal_init: simdgroup reduction   = true
0.00.113.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.597 I ggml_metal_init: has bfloat            = true
0.00.113.597 I ggml_metal_init: use bfloat            = true
0.00.113.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.220.881 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.239.798 I init:      Metal KV buffer size =   384.00 MiB
0.00.239.805 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.239.844 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.240.789 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.240.791 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.240.791 I llama_init_from_model: graph nodes  = 967
0.00.240.791 I llama_init_from_model: graph splits = 2
0.00.240.794 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.240.924 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.240.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.328.798 I main: llama threadpool init, n_threads = 4
0.00.328.839 I 
0.00.328.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.328.861 I 
0.00.328.931 I sampler seed: 1234
0.00.328.936 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.328.960 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.328.962 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.328.962 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.164.428 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.02.164.429 I llama_perf_context_print:        load time =     298.93 ms
0.02.164.430 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.45 tokens per second)
0.02.164.430 I llama_perf_context_print:        eval time =    1788.94 ms /    63 runs   (   28.40 ms per token,    35.22 tokens per second)
0.02.164.431 I llama_perf_context_print:       total time =    1835.63 ms /    70 tokens
0.02.164.635 I ggml_metal_free: deallocating

real	0m2.504s
user	0m0.150s
sys	0m0.104s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.783 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.314 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.324 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.392 I llama_model_loader: - type  f32:  194 tensors
0.00.028.392 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.393 I print_info: file format = GGUF V3 (latest)
0.00.028.394 I print_info: file type   = Q8_0
0.00.028.395 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.947 I load: special tokens cache size = 25
0.00.054.769 I load: token to piece cache size = 0.2984 MB
0.00.054.774 I print_info: arch             = gptneox
0.00.054.774 I print_info: vocab_only       = 0
0.00.054.775 I print_info: n_ctx_train      = 2048
0.00.054.777 I print_info: n_embd           = 2048
0.00.054.777 I print_info: n_layer          = 24
0.00.054.783 I print_info: n_head           = 16
0.00.054.784 I print_info: n_head_kv        = 16
0.00.054.787 I print_info: n_rot            = 32
0.00.054.787 I print_info: n_swa            = 0
0.00.054.787 I print_info: n_embd_head_k    = 128
0.00.054.787 I print_info: n_embd_head_v    = 128
0.00.054.788 I print_info: n_gqa            = 1
0.00.054.789 I print_info: n_embd_k_gqa     = 2048
0.00.054.789 I print_info: n_embd_v_gqa     = 2048
0.00.054.790 I print_info: f_norm_eps       = 1.0e-05
0.00.054.790 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.791 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.791 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.791 I print_info: f_logit_scale    = 0.0e+00
0.00.054.792 I print_info: n_ff             = 8192
0.00.054.792 I print_info: n_expert         = 0
0.00.054.792 I print_info: n_expert_used    = 0
0.00.054.792 I print_info: causal attn      = 1
0.00.054.792 I print_info: pooling type     = 0
0.00.054.793 I print_info: rope type        = 2
0.00.054.793 I print_info: rope scaling     = linear
0.00.054.795 I print_info: freq_base_train  = 10000.0
0.00.054.796 I print_info: freq_scale_train = 1
0.00.054.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.796 I print_info: rope_finetuned   = unknown
0.00.054.796 I print_info: ssm_d_conv       = 0
0.00.054.796 I print_info: ssm_d_inner      = 0
0.00.054.796 I print_info: ssm_d_state      = 0
0.00.054.796 I print_info: ssm_dt_rank      = 0
0.00.054.797 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.797 I print_info: model type       = 1.4B
0.00.054.797 I print_info: model params     = 1.41 B
0.00.054.798 I print_info: general.name     = 1.4B
0.00.054.798 I print_info: vocab type       = BPE
0.00.054.799 I print_info: n_vocab          = 50304
0.00.054.799 I print_info: n_merges         = 50009
0.00.054.799 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.801 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.802 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.802 I print_info: LF token         = 128 'Ä'
0.00.054.802 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.802 I print_info: max token length = 1024
0.00.057.260 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.260 I load_tensors: offloading output layer to GPU
0.00.057.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.272 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.273 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.057.639 I llama_init_from_model: n_seq_max     = 1
0.00.057.639 I llama_init_from_model: n_ctx         = 2048
0.00.057.639 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.640 I llama_init_from_model: n_batch       = 2048
0.00.057.640 I llama_init_from_model: n_ubatch      = 512
0.00.057.640 I llama_init_from_model: flash_attn    = 0
0.00.057.641 I llama_init_from_model: freq_base     = 10000.0
0.00.057.641 I llama_init_from_model: freq_scale    = 1
0.00.057.642 I ggml_metal_init: allocating
0.00.057.645 I ggml_metal_init: found device: Apple M4
0.00.057.648 I ggml_metal_init: picking default device: Apple M4
0.00.058.413 I ggml_metal_init: using embedded metal library
0.00.061.079 I ggml_metal_init: GPU name:   Apple M4
0.00.061.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.082 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.083 I ggml_metal_init: simdgroup reduction   = true
0.00.061.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.083 I ggml_metal_init: has bfloat            = true
0.00.061.083 I ggml_metal_init: use bfloat            = true
0.00.061.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.562 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.953 I init:      Metal KV buffer size =   384.00 MiB
0.00.095.960 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.999 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.302 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.304 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.305 I llama_init_from_model: graph nodes  = 967
0.00.097.305 I llama_init_from_model: graph splits = 2
0.00.097.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.434.564 I main: llama threadpool init, n_threads = 4
0.01.434.607 I 
0.01.434.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.434.632 I 
0.01.434.872 I sampler seed: 1234
0.01.434.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.434.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.434.888 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.434.888 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.549.131 I llama_perf_sampler_print:    sampling time =       1.82 ms /    71 runs   (    0.03 ms per token, 38904.11 tokens per second)
0.02.549.132 I llama_perf_context_print:        load time =    1424.78 ms
0.02.549.132 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.33 tokens per second)
0.02.549.133 I llama_perf_context_print:        eval time =    1062.17 ms /    63 runs   (   16.86 ms per token,    59.31 tokens per second)
0.02.549.134 I llama_perf_context_print:       total time =    1114.57 ms /    70 tokens
0.02.549.378 I ggml_metal_free: deallocating

real	0m2.569s
user	0m0.122s
sys	0m0.226s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.089 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.815 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.825 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.826 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.826 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.827 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.561 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.563 I llama_model_loader: - type  f32:  194 tensors
0.00.029.564 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.565 I print_info: file format = GGUF V3 (latest)
0.00.029.565 I print_info: file type   = Q4_0
0.00.029.567 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.575 I load: special tokens cache size = 25
0.00.054.681 I load: token to piece cache size = 0.2984 MB
0.00.054.684 I print_info: arch             = gptneox
0.00.054.684 I print_info: vocab_only       = 0
0.00.054.684 I print_info: n_ctx_train      = 2048
0.00.054.684 I print_info: n_embd           = 2048
0.00.054.685 I print_info: n_layer          = 24
0.00.054.688 I print_info: n_head           = 16
0.00.054.689 I print_info: n_head_kv        = 16
0.00.054.690 I print_info: n_rot            = 32
0.00.054.690 I print_info: n_swa            = 0
0.00.054.690 I print_info: n_embd_head_k    = 128
0.00.054.691 I print_info: n_embd_head_v    = 128
0.00.054.691 I print_info: n_gqa            = 1
0.00.054.694 I print_info: n_embd_k_gqa     = 2048
0.00.054.694 I print_info: n_embd_v_gqa     = 2048
0.00.054.696 I print_info: f_norm_eps       = 1.0e-05
0.00.054.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.697 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.697 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.697 I print_info: f_logit_scale    = 0.0e+00
0.00.054.698 I print_info: n_ff             = 8192
0.00.054.698 I print_info: n_expert         = 0
0.00.054.698 I print_info: n_expert_used    = 0
0.00.054.700 I print_info: causal attn      = 1
0.00.054.701 I print_info: pooling type     = 0
0.00.054.702 I print_info: rope type        = 2
0.00.054.702 I print_info: rope scaling     = linear
0.00.054.702 I print_info: freq_base_train  = 10000.0
0.00.054.702 I print_info: freq_scale_train = 1
0.00.054.703 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.703 I print_info: rope_finetuned   = unknown
0.00.054.703 I print_info: ssm_d_conv       = 0
0.00.054.703 I print_info: ssm_d_inner      = 0
0.00.054.703 I print_info: ssm_d_state      = 0
0.00.054.704 I print_info: ssm_dt_rank      = 0
0.00.054.704 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.704 I print_info: model type       = 1.4B
0.00.054.704 I print_info: model params     = 1.41 B
0.00.054.705 I print_info: general.name     = 1.4B
0.00.054.705 I print_info: vocab type       = BPE
0.00.054.705 I print_info: n_vocab          = 50304
0.00.054.705 I print_info: n_merges         = 50009
0.00.054.705 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.706 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.710 I print_info: LF token         = 128 'Ä'
0.00.054.710 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.710 I print_info: max token length = 1024
0.00.056.678 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.678 I load_tensors: offloading output layer to GPU
0.00.056.678 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.688 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.690 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.988 I llama_init_from_model: n_seq_max     = 1
0.00.056.989 I llama_init_from_model: n_ctx         = 2048
0.00.056.989 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.989 I llama_init_from_model: n_batch       = 2048
0.00.056.989 I llama_init_from_model: n_ubatch      = 512
0.00.056.989 I llama_init_from_model: flash_attn    = 0
0.00.056.990 I llama_init_from_model: freq_base     = 10000.0
0.00.056.990 I llama_init_from_model: freq_scale    = 1
0.00.056.990 I ggml_metal_init: allocating
0.00.056.993 I ggml_metal_init: found device: Apple M4
0.00.056.995 I ggml_metal_init: picking default device: Apple M4
0.00.057.629 I ggml_metal_init: using embedded metal library
0.00.059.983 I ggml_metal_init: GPU name:   Apple M4
0.00.059.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.985 I ggml_metal_init: simdgroup reduction   = true
0.00.059.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.985 I ggml_metal_init: has bfloat            = true
0.00.059.986 I ggml_metal_init: use bfloat            = true
0.00.059.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.405 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.456 I init:      Metal KV buffer size =   384.00 MiB
0.00.093.472 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.514 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.094.791 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.094.793 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.094.793 I llama_init_from_model: graph nodes  = 967
0.00.094.794 I llama_init_from_model: graph splits = 2
0.00.094.796 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.090 I main: llama threadpool init, n_threads = 4
0.00.666.131 I 
0.00.666.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.151 I 
0.00.666.368 I sampler seed: 1234
0.00.666.373 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.666.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.666.393 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.666.393 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.355.494 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.355.495 I llama_perf_context_print:        load time =     654.99 ms
0.01.355.495 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.22 tokens per second)
0.01.355.496 I llama_perf_context_print:        eval time =     642.37 ms /    63 runs   (   10.20 ms per token,    98.07 tokens per second)
0.01.355.496 I llama_perf_context_print:       total time =     689.41 ms /    70 tokens
0.01.355.695 I ggml_metal_free: deallocating

real	0m1.377s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.061 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.649 I llama_model_loader: - type  f32:  194 tensors
0.00.026.649 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.649 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.650 I print_info: file format = GGUF V3 (latest)
0.00.026.650 I print_info: file type   = Q4_1
0.00.026.651 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.261 I load: special tokens cache size = 25
0.00.052.321 I load: token to piece cache size = 0.2984 MB
0.00.052.324 I print_info: arch             = gptneox
0.00.052.324 I print_info: vocab_only       = 0
0.00.052.325 I print_info: n_ctx_train      = 2048
0.00.052.325 I print_info: n_embd           = 2048
0.00.052.325 I print_info: n_layer          = 24
0.00.052.328 I print_info: n_head           = 16
0.00.052.329 I print_info: n_head_kv        = 16
0.00.052.329 I print_info: n_rot            = 32
0.00.052.329 I print_info: n_swa            = 0
0.00.052.329 I print_info: n_embd_head_k    = 128
0.00.052.329 I print_info: n_embd_head_v    = 128
0.00.052.330 I print_info: n_gqa            = 1
0.00.052.331 I print_info: n_embd_k_gqa     = 2048
0.00.052.331 I print_info: n_embd_v_gqa     = 2048
0.00.052.334 I print_info: f_norm_eps       = 1.0e-05
0.00.052.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.335 I print_info: f_logit_scale    = 0.0e+00
0.00.052.336 I print_info: n_ff             = 8192
0.00.052.336 I print_info: n_expert         = 0
0.00.052.336 I print_info: n_expert_used    = 0
0.00.052.336 I print_info: causal attn      = 1
0.00.052.337 I print_info: pooling type     = 0
0.00.052.343 I print_info: rope type        = 2
0.00.052.345 I print_info: rope scaling     = linear
0.00.052.345 I print_info: freq_base_train  = 10000.0
0.00.052.346 I print_info: freq_scale_train = 1
0.00.052.346 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.346 I print_info: rope_finetuned   = unknown
0.00.052.346 I print_info: ssm_d_conv       = 0
0.00.052.346 I print_info: ssm_d_inner      = 0
0.00.052.346 I print_info: ssm_d_state      = 0
0.00.052.347 I print_info: ssm_dt_rank      = 0
0.00.052.348 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.348 I print_info: model type       = 1.4B
0.00.052.349 I print_info: model params     = 1.41 B
0.00.052.349 I print_info: general.name     = 1.4B
0.00.052.349 I print_info: vocab type       = BPE
0.00.052.350 I print_info: n_vocab          = 50304
0.00.052.350 I print_info: n_merges         = 50009
0.00.052.350 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.350 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.350 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.351 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.351 I print_info: LF token         = 128 'Ä'
0.00.052.351 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.351 I print_info: max token length = 1024
0.00.054.271 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.271 I load_tensors: offloading output layer to GPU
0.00.054.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.282 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.283 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.562 I llama_init_from_model: n_seq_max     = 1
0.00.054.563 I llama_init_from_model: n_ctx         = 2048
0.00.054.563 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.563 I llama_init_from_model: n_batch       = 2048
0.00.054.564 I llama_init_from_model: n_ubatch      = 512
0.00.054.564 I llama_init_from_model: flash_attn    = 0
0.00.054.564 I llama_init_from_model: freq_base     = 10000.0
0.00.054.564 I llama_init_from_model: freq_scale    = 1
0.00.054.565 I ggml_metal_init: allocating
0.00.054.568 I ggml_metal_init: found device: Apple M4
0.00.054.569 I ggml_metal_init: picking default device: Apple M4
0.00.055.139 I ggml_metal_init: using embedded metal library
0.00.057.432 I ggml_metal_init: GPU name:   Apple M4
0.00.057.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.435 I ggml_metal_init: simdgroup reduction   = true
0.00.057.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.435 I ggml_metal_init: has bfloat            = true
0.00.057.435 I ggml_metal_init: use bfloat            = true
0.00.057.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.912 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.188 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.194 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.363 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.365 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.365 I llama_init_from_model: graph nodes  = 967
0.00.087.365 I llama_init_from_model: graph splits = 2
0.00.087.368 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.514 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.366 I main: llama threadpool init, n_threads = 4
0.00.738.404 I 
0.00.738.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.442 I 
0.00.738.677 I sampler seed: 1234
0.00.738.682 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.703 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.470.882 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66355.14 tokens per second)
0.01.470.882 I llama_perf_context_print:        load time =     728.30 ms
0.01.470.883 I llama_perf_context_print: prompt eval time =      43.01 ms /     7 tokens (    6.14 ms per token,   162.74 tokens per second)
0.01.470.883 I llama_perf_context_print:        eval time =     686.35 ms /    63 runs   (   10.89 ms per token,    91.79 tokens per second)
0.01.470.884 I llama_perf_context_print:       total time =     732.52 ms /    70 tokens
0.01.471.125 I ggml_metal_free: deallocating

real	0m1.488s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.626 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.393 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.394 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.394 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.399 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.399 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.399 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.333 I llama_model_loader: - type  f32:  194 tensors
0.00.025.334 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.334 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.334 I print_info: file format = GGUF V3 (latest)
0.00.025.335 I print_info: file type   = Q5_0
0.00.025.336 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.265 I load: special tokens cache size = 25
0.00.050.322 I load: token to piece cache size = 0.2984 MB
0.00.050.324 I print_info: arch             = gptneox
0.00.050.325 I print_info: vocab_only       = 0
0.00.050.325 I print_info: n_ctx_train      = 2048
0.00.050.325 I print_info: n_embd           = 2048
0.00.050.325 I print_info: n_layer          = 24
0.00.050.328 I print_info: n_head           = 16
0.00.050.329 I print_info: n_head_kv        = 16
0.00.050.329 I print_info: n_rot            = 32
0.00.050.331 I print_info: n_swa            = 0
0.00.050.332 I print_info: n_embd_head_k    = 128
0.00.050.332 I print_info: n_embd_head_v    = 128
0.00.050.332 I print_info: n_gqa            = 1
0.00.050.333 I print_info: n_embd_k_gqa     = 2048
0.00.050.334 I print_info: n_embd_v_gqa     = 2048
0.00.050.334 I print_info: f_norm_eps       = 1.0e-05
0.00.050.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.335 I print_info: f_logit_scale    = 0.0e+00
0.00.050.336 I print_info: n_ff             = 8192
0.00.050.336 I print_info: n_expert         = 0
0.00.050.336 I print_info: n_expert_used    = 0
0.00.050.337 I print_info: causal attn      = 1
0.00.050.339 I print_info: pooling type     = 0
0.00.050.339 I print_info: rope type        = 2
0.00.050.339 I print_info: rope scaling     = linear
0.00.050.340 I print_info: freq_base_train  = 10000.0
0.00.050.340 I print_info: freq_scale_train = 1
0.00.050.340 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.340 I print_info: rope_finetuned   = unknown
0.00.050.340 I print_info: ssm_d_conv       = 0
0.00.050.340 I print_info: ssm_d_inner      = 0
0.00.050.341 I print_info: ssm_d_state      = 0
0.00.050.341 I print_info: ssm_dt_rank      = 0
0.00.050.341 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.341 I print_info: model type       = 1.4B
0.00.050.342 I print_info: model params     = 1.41 B
0.00.050.342 I print_info: general.name     = 1.4B
0.00.050.342 I print_info: vocab type       = BPE
0.00.050.342 I print_info: n_vocab          = 50304
0.00.050.343 I print_info: n_merges         = 50009
0.00.050.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.343 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.344 I print_info: LF token         = 128 'Ä'
0.00.050.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.344 I print_info: max token length = 1024
0.00.052.350 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.350 I load_tensors: offloading output layer to GPU
0.00.052.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.361 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.362 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.650 I llama_init_from_model: n_seq_max     = 1
0.00.052.651 I llama_init_from_model: n_ctx         = 2048
0.00.052.651 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.651 I llama_init_from_model: n_batch       = 2048
0.00.052.651 I llama_init_from_model: n_ubatch      = 512
0.00.052.652 I llama_init_from_model: flash_attn    = 0
0.00.052.652 I llama_init_from_model: freq_base     = 10000.0
0.00.052.652 I llama_init_from_model: freq_scale    = 1
0.00.052.653 I ggml_metal_init: allocating
0.00.052.656 I ggml_metal_init: found device: Apple M4
0.00.052.658 I ggml_metal_init: picking default device: Apple M4
0.00.053.252 I ggml_metal_init: using embedded metal library
0.00.055.630 I ggml_metal_init: GPU name:   Apple M4
0.00.055.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.633 I ggml_metal_init: simdgroup reduction   = true
0.00.055.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.633 I ggml_metal_init: has bfloat            = true
0.00.055.633 I ggml_metal_init: use bfloat            = true
0.00.055.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.462 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.314 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.319 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.405 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.406 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.407 I llama_init_from_model: graph nodes  = 967
0.00.085.407 I llama_init_from_model: graph splits = 2
0.00.085.410 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.379 I main: llama threadpool init, n_threads = 4
0.00.743.419 I 
0.00.743.462 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.463 I 
0.00.743.696 I sampler seed: 1234
0.00.743.700 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.736 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.746 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.746 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.527.340 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.527.341 I llama_perf_context_print:        load time =     734.75 ms
0.01.527.342 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.04 tokens per second)
0.01.527.346 I llama_perf_context_print:        eval time =     737.42 ms /    63 runs   (   11.71 ms per token,    85.43 tokens per second)
0.01.527.346 I llama_perf_context_print:       total time =     783.96 ms /    70 tokens
0.01.527.567 I ggml_metal_free: deallocating

real	0m1.545s
user	0m0.109s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.800 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.207 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.211 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.218 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.219 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.219 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.226 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.886 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.888 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.888 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.889 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.889 I llama_model_loader: - type  f32:  194 tensors
0.00.025.890 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.890 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.891 I print_info: file format = GGUF V3 (latest)
0.00.025.891 I print_info: file type   = Q5_1
0.00.025.892 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.764 I load: special tokens cache size = 25
0.00.050.721 I load: token to piece cache size = 0.2984 MB
0.00.050.724 I print_info: arch             = gptneox
0.00.050.724 I print_info: vocab_only       = 0
0.00.050.724 I print_info: n_ctx_train      = 2048
0.00.050.724 I print_info: n_embd           = 2048
0.00.050.724 I print_info: n_layer          = 24
0.00.050.727 I print_info: n_head           = 16
0.00.050.728 I print_info: n_head_kv        = 16
0.00.050.728 I print_info: n_rot            = 32
0.00.050.728 I print_info: n_swa            = 0
0.00.050.729 I print_info: n_embd_head_k    = 128
0.00.050.729 I print_info: n_embd_head_v    = 128
0.00.050.729 I print_info: n_gqa            = 1
0.00.050.730 I print_info: n_embd_k_gqa     = 2048
0.00.050.731 I print_info: n_embd_v_gqa     = 2048
0.00.050.731 I print_info: f_norm_eps       = 1.0e-05
0.00.050.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.732 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.732 I print_info: f_logit_scale    = 0.0e+00
0.00.050.733 I print_info: n_ff             = 8192
0.00.050.733 I print_info: n_expert         = 0
0.00.050.733 I print_info: n_expert_used    = 0
0.00.050.733 I print_info: causal attn      = 1
0.00.050.734 I print_info: pooling type     = 0
0.00.050.734 I print_info: rope type        = 2
0.00.050.734 I print_info: rope scaling     = linear
0.00.050.735 I print_info: freq_base_train  = 10000.0
0.00.050.735 I print_info: freq_scale_train = 1
0.00.050.735 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.738 I print_info: rope_finetuned   = unknown
0.00.050.738 I print_info: ssm_d_conv       = 0
0.00.050.738 I print_info: ssm_d_inner      = 0
0.00.050.738 I print_info: ssm_d_state      = 0
0.00.050.738 I print_info: ssm_dt_rank      = 0
0.00.050.738 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.739 I print_info: model type       = 1.4B
0.00.050.739 I print_info: model params     = 1.41 B
0.00.050.739 I print_info: general.name     = 1.4B
0.00.050.740 I print_info: vocab type       = BPE
0.00.050.740 I print_info: n_vocab          = 50304
0.00.050.740 I print_info: n_merges         = 50009
0.00.050.740 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.740 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.741 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.741 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.741 I print_info: LF token         = 128 'Ä'
0.00.050.741 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.742 I print_info: max token length = 1024
0.00.052.782 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.782 I load_tensors: offloading output layer to GPU
0.00.052.782 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.792 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.794 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.080 I llama_init_from_model: n_seq_max     = 1
0.00.053.081 I llama_init_from_model: n_ctx         = 2048
0.00.053.081 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.081 I llama_init_from_model: n_batch       = 2048
0.00.053.081 I llama_init_from_model: n_ubatch      = 512
0.00.053.081 I llama_init_from_model: flash_attn    = 0
0.00.053.082 I llama_init_from_model: freq_base     = 10000.0
0.00.053.082 I llama_init_from_model: freq_scale    = 1
0.00.053.083 I ggml_metal_init: allocating
0.00.053.086 I ggml_metal_init: found device: Apple M4
0.00.053.088 I ggml_metal_init: picking default device: Apple M4
0.00.053.714 I ggml_metal_init: using embedded metal library
0.00.056.099 I ggml_metal_init: GPU name:   Apple M4
0.00.056.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.102 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.102 I ggml_metal_init: simdgroup reduction   = true
0.00.056.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.103 I ggml_metal_init: has bfloat            = true
0.00.056.103 I ggml_metal_init: use bfloat            = true
0.00.056.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.869 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.921 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.929 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.972 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.977 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.979 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.979 I llama_init_from_model: graph nodes  = 967
0.00.086.979 I llama_init_from_model: graph splits = 2
0.00.086.982 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.688 I main: llama threadpool init, n_threads = 4
0.00.710.727 I 
0.00.710.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.770 I 
0.00.710.988 I sampler seed: 1234
0.00.710.994 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.005 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.006 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.007 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.541.925 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.541.926 I llama_perf_context_print:        load time =     700.88 ms
0.01.541.926 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.81 tokens per second)
0.01.541.927 I llama_perf_context_print:        eval time =     785.70 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.541.928 I llama_perf_context_print:       total time =     831.24 ms /    70 tokens
0.01.542.117 I ggml_metal_free: deallocating

real	0m1.559s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.583 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.184 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.925 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.926 I llama_model_loader: - type  f32:  194 tensors
0.00.024.926 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.926 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.927 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.927 I print_info: file format = GGUF V3 (latest)
0.00.024.928 I print_info: file type   = Q2_K - Medium
0.00.024.930 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.141 I load: special tokens cache size = 25
0.00.049.850 I load: token to piece cache size = 0.2984 MB
0.00.049.855 I print_info: arch             = gptneox
0.00.049.855 I print_info: vocab_only       = 0
0.00.049.855 I print_info: n_ctx_train      = 2048
0.00.049.855 I print_info: n_embd           = 2048
0.00.049.855 I print_info: n_layer          = 24
0.00.049.860 I print_info: n_head           = 16
0.00.049.861 I print_info: n_head_kv        = 16
0.00.049.861 I print_info: n_rot            = 32
0.00.049.861 I print_info: n_swa            = 0
0.00.049.861 I print_info: n_embd_head_k    = 128
0.00.049.861 I print_info: n_embd_head_v    = 128
0.00.049.862 I print_info: n_gqa            = 1
0.00.049.863 I print_info: n_embd_k_gqa     = 2048
0.00.049.865 I print_info: n_embd_v_gqa     = 2048
0.00.049.865 I print_info: f_norm_eps       = 1.0e-05
0.00.049.866 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.867 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.867 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.867 I print_info: f_logit_scale    = 0.0e+00
0.00.049.868 I print_info: n_ff             = 8192
0.00.049.868 I print_info: n_expert         = 0
0.00.049.868 I print_info: n_expert_used    = 0
0.00.049.868 I print_info: causal attn      = 1
0.00.049.869 I print_info: pooling type     = 0
0.00.049.869 I print_info: rope type        = 2
0.00.049.869 I print_info: rope scaling     = linear
0.00.049.869 I print_info: freq_base_train  = 10000.0
0.00.049.870 I print_info: freq_scale_train = 1
0.00.049.870 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.870 I print_info: rope_finetuned   = unknown
0.00.049.870 I print_info: ssm_d_conv       = 0
0.00.049.870 I print_info: ssm_d_inner      = 0
0.00.049.871 I print_info: ssm_d_state      = 0
0.00.049.872 I print_info: ssm_dt_rank      = 0
0.00.049.872 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.872 I print_info: model type       = 1.4B
0.00.049.873 I print_info: model params     = 1.41 B
0.00.049.873 I print_info: general.name     = 1.4B
0.00.049.873 I print_info: vocab type       = BPE
0.00.049.873 I print_info: n_vocab          = 50304
0.00.049.874 I print_info: n_merges         = 50009
0.00.049.874 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.874 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.874 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.874 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.875 I print_info: LF token         = 128 'Ä'
0.00.049.875 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.875 I print_info: max token length = 1024
0.00.051.806 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.806 I load_tensors: offloading output layer to GPU
0.00.051.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.817 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.818 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.222 I llama_init_from_model: n_seq_max     = 1
0.00.052.223 I llama_init_from_model: n_ctx         = 2048
0.00.052.223 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.223 I llama_init_from_model: n_batch       = 2048
0.00.052.223 I llama_init_from_model: n_ubatch      = 512
0.00.052.223 I llama_init_from_model: flash_attn    = 0
0.00.052.224 I llama_init_from_model: freq_base     = 10000.0
0.00.052.224 I llama_init_from_model: freq_scale    = 1
0.00.052.225 I ggml_metal_init: allocating
0.00.052.228 I ggml_metal_init: found device: Apple M4
0.00.052.230 I ggml_metal_init: picking default device: Apple M4
0.00.052.867 I ggml_metal_init: using embedded metal library
0.00.055.256 I ggml_metal_init: GPU name:   Apple M4
0.00.055.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.258 I ggml_metal_init: simdgroup reduction   = true
0.00.055.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.258 I ggml_metal_init: has bfloat            = true
0.00.055.259 I ggml_metal_init: use bfloat            = true
0.00.055.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.175 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.731 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.741 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.778 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.867 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.868 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.869 I llama_init_from_model: graph nodes  = 967
0.00.086.869 I llama_init_from_model: graph splits = 2
0.00.086.872 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.003 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.882 I main: llama threadpool init, n_threads = 4
0.00.438.917 I 
0.00.438.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.939 I 
0.00.439.155 I sampler seed: 1234
0.00.439.159 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.439.201 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.439.201 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.439.202 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.115.803 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.115.804 I llama_perf_context_print:        load time =     429.29 ms
0.01.115.805 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.88 tokens per second)
0.01.115.805 I llama_perf_context_print:        eval time =     637.88 ms /    63 runs   (   10.13 ms per token,    98.76 tokens per second)
0.01.115.806 I llama_perf_context_print:       total time =     676.92 ms /    70 tokens
0.01.116.051 I ggml_metal_free: deallocating

real	0m1.134s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.016.250 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.023.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.778 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.032.654 I llama_model_loader: - type  f32:  194 tensors
0.00.032.654 I llama_model_loader: - type q3_K:   25 tensors
0.00.032.654 I llama_model_loader: - type q4_K:   71 tensors
0.00.032.654 I llama_model_loader: - type q5_K:    1 tensors
0.00.032.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.655 I print_info: file format = GGUF V3 (latest)
0.00.032.659 I print_info: file type   = Q3_K - Medium
0.00.032.660 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.051.756 I load: special tokens cache size = 25
0.00.057.844 I load: token to piece cache size = 0.2984 MB
0.00.057.847 I print_info: arch             = gptneox
0.00.057.847 I print_info: vocab_only       = 0
0.00.057.847 I print_info: n_ctx_train      = 2048
0.00.057.848 I print_info: n_embd           = 2048
0.00.057.848 I print_info: n_layer          = 24
0.00.057.850 I print_info: n_head           = 16
0.00.057.851 I print_info: n_head_kv        = 16
0.00.057.851 I print_info: n_rot            = 32
0.00.057.852 I print_info: n_swa            = 0
0.00.057.852 I print_info: n_embd_head_k    = 128
0.00.057.852 I print_info: n_embd_head_v    = 128
0.00.057.853 I print_info: n_gqa            = 1
0.00.057.854 I print_info: n_embd_k_gqa     = 2048
0.00.057.855 I print_info: n_embd_v_gqa     = 2048
0.00.057.855 I print_info: f_norm_eps       = 1.0e-05
0.00.057.856 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.857 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.857 I print_info: f_logit_scale    = 0.0e+00
0.00.057.858 I print_info: n_ff             = 8192
0.00.057.858 I print_info: n_expert         = 0
0.00.057.858 I print_info: n_expert_used    = 0
0.00.057.858 I print_info: causal attn      = 1
0.00.057.859 I print_info: pooling type     = 0
0.00.057.860 I print_info: rope type        = 2
0.00.057.861 I print_info: rope scaling     = linear
0.00.057.861 I print_info: freq_base_train  = 10000.0
0.00.057.861 I print_info: freq_scale_train = 1
0.00.057.862 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.862 I print_info: rope_finetuned   = unknown
0.00.057.862 I print_info: ssm_d_conv       = 0
0.00.057.862 I print_info: ssm_d_inner      = 0
0.00.057.862 I print_info: ssm_d_state      = 0
0.00.057.863 I print_info: ssm_dt_rank      = 0
0.00.057.863 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.863 I print_info: model type       = 1.4B
0.00.057.863 I print_info: model params     = 1.41 B
0.00.057.864 I print_info: general.name     = 1.4B
0.00.057.864 I print_info: vocab type       = BPE
0.00.057.864 I print_info: n_vocab          = 50304
0.00.057.866 I print_info: n_merges         = 50009
0.00.057.866 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.866 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.867 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.867 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.867 I print_info: LF token         = 128 'Ä'
0.00.057.867 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.868 I print_info: max token length = 1024
0.00.059.836 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.836 I load_tensors: offloading output layer to GPU
0.00.059.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.847 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.059.848 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.060.139 I llama_init_from_model: n_seq_max     = 1
0.00.060.140 I llama_init_from_model: n_ctx         = 2048
0.00.060.140 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.140 I llama_init_from_model: n_batch       = 2048
0.00.060.140 I llama_init_from_model: n_ubatch      = 512
0.00.060.141 I llama_init_from_model: flash_attn    = 0
0.00.060.141 I llama_init_from_model: freq_base     = 10000.0
0.00.060.141 I llama_init_from_model: freq_scale    = 1
0.00.060.142 I ggml_metal_init: allocating
0.00.060.145 I ggml_metal_init: found device: Apple M4
0.00.060.147 I ggml_metal_init: picking default device: Apple M4
0.00.060.749 I ggml_metal_init: using embedded metal library
0.00.063.098 I ggml_metal_init: GPU name:   Apple M4
0.00.063.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.100 I ggml_metal_init: simdgroup reduction   = true
0.00.063.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.100 I ggml_metal_init: has bfloat            = true
0.00.063.100 I ggml_metal_init: use bfloat            = true
0.00.063.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.963 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.718 I init:      Metal KV buffer size =   384.00 MiB
0.00.092.724 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.754 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.830 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.831 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.832 I llama_init_from_model: graph nodes  = 967
0.00.093.832 I llama_init_from_model: graph splits = 2
0.00.093.835 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.528.303 I main: llama threadpool init, n_threads = 4
0.00.528.361 I 
0.00.528.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.528.390 I 
0.00.528.721 I sampler seed: 1234
0.00.528.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.528.746 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.528.748 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.528.748 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.268.953 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.01.268.953 I llama_perf_context_print:        load time =     512.04 ms
0.01.268.954 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   173.02 tokens per second)
0.01.268.955 I llama_perf_context_print:        eval time =     697.06 ms /    63 runs   (   11.06 ms per token,    90.38 tokens per second)
0.01.268.955 I llama_perf_context_print:       total time =     740.66 ms /    70 tokens
0.01.269.184 I ggml_metal_free: deallocating

real	0m1.286s
user	0m0.108s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.202 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.885 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.896 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.896 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.897 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.897 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.898 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.905 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.755 I llama_model_loader: - type  f32:  194 tensors
0.00.027.755 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.756 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.756 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.756 I print_info: file format = GGUF V3 (latest)
0.00.027.757 I print_info: file type   = Q4_K - Medium
0.00.027.757 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.772 I load: special tokens cache size = 25
0.00.053.766 I load: token to piece cache size = 0.2984 MB
0.00.053.769 I print_info: arch             = gptneox
0.00.053.770 I print_info: vocab_only       = 0
0.00.053.770 I print_info: n_ctx_train      = 2048
0.00.053.770 I print_info: n_embd           = 2048
0.00.053.770 I print_info: n_layer          = 24
0.00.053.773 I print_info: n_head           = 16
0.00.053.773 I print_info: n_head_kv        = 16
0.00.053.774 I print_info: n_rot            = 32
0.00.053.774 I print_info: n_swa            = 0
0.00.053.776 I print_info: n_embd_head_k    = 128
0.00.053.776 I print_info: n_embd_head_v    = 128
0.00.053.777 I print_info: n_gqa            = 1
0.00.053.777 I print_info: n_embd_k_gqa     = 2048
0.00.053.782 I print_info: n_embd_v_gqa     = 2048
0.00.053.782 I print_info: f_norm_eps       = 1.0e-05
0.00.053.783 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.783 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.783 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.783 I print_info: f_logit_scale    = 0.0e+00
0.00.053.784 I print_info: n_ff             = 8192
0.00.053.784 I print_info: n_expert         = 0
0.00.053.784 I print_info: n_expert_used    = 0
0.00.053.784 I print_info: causal attn      = 1
0.00.053.785 I print_info: pooling type     = 0
0.00.053.785 I print_info: rope type        = 2
0.00.053.785 I print_info: rope scaling     = linear
0.00.053.785 I print_info: freq_base_train  = 10000.0
0.00.053.786 I print_info: freq_scale_train = 1
0.00.053.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.786 I print_info: rope_finetuned   = unknown
0.00.053.786 I print_info: ssm_d_conv       = 0
0.00.053.786 I print_info: ssm_d_inner      = 0
0.00.053.786 I print_info: ssm_d_state      = 0
0.00.053.786 I print_info: ssm_dt_rank      = 0
0.00.053.786 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.787 I print_info: model type       = 1.4B
0.00.053.787 I print_info: model params     = 1.41 B
0.00.053.787 I print_info: general.name     = 1.4B
0.00.053.788 I print_info: vocab type       = BPE
0.00.053.788 I print_info: n_vocab          = 50304
0.00.053.789 I print_info: n_merges         = 50009
0.00.053.789 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.791 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.791 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.792 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.792 I print_info: LF token         = 128 'Ä'
0.00.053.792 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.792 I print_info: max token length = 1024
0.00.055.851 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.851 I load_tensors: offloading output layer to GPU
0.00.055.852 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.862 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.864 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.177 I llama_init_from_model: n_seq_max     = 1
0.00.056.178 I llama_init_from_model: n_ctx         = 2048
0.00.056.178 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.178 I llama_init_from_model: n_batch       = 2048
0.00.056.178 I llama_init_from_model: n_ubatch      = 512
0.00.056.178 I llama_init_from_model: flash_attn    = 0
0.00.056.179 I llama_init_from_model: freq_base     = 10000.0
0.00.056.179 I llama_init_from_model: freq_scale    = 1
0.00.056.179 I ggml_metal_init: allocating
0.00.056.183 I ggml_metal_init: found device: Apple M4
0.00.056.185 I ggml_metal_init: picking default device: Apple M4
0.00.056.815 I ggml_metal_init: using embedded metal library
0.00.059.197 I ggml_metal_init: GPU name:   Apple M4
0.00.059.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.199 I ggml_metal_init: simdgroup reduction   = true
0.00.059.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.200 I ggml_metal_init: has bfloat            = true
0.00.059.200 I ggml_metal_init: use bfloat            = true
0.00.059.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.174 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.907 I init:      Metal KV buffer size =   384.00 MiB
0.00.088.913 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.921 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.922 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.922 I llama_init_from_model: graph nodes  = 967
0.00.089.922 I llama_init_from_model: graph splits = 2
0.00.089.925 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.056 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.134 I main: llama threadpool init, n_threads = 4
0.00.614.170 I 
0.00.614.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.194 I 
0.00.614.435 I sampler seed: 1234
0.00.614.440 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.486 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.488 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.488 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.379.042 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.379.043 I llama_perf_context_print:        load time =     602.93 ms
0.01.379.043 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.53 tokens per second)
0.01.379.044 I llama_perf_context_print:        eval time =     714.42 ms /    63 runs   (   11.34 ms per token,    88.18 tokens per second)
0.01.379.044 I llama_perf_context_print:       total time =     764.91 ms /    70 tokens
0.01.379.290 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.112s
sys	0m0.136s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.430 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.263 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.021 I llama_model_loader: - type  f32:  194 tensors
0.00.026.022 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.022 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.023 I print_info: file format = GGUF V3 (latest)
0.00.026.023 I print_info: file type   = Q5_K - Medium
0.00.026.027 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.748 I load: special tokens cache size = 25
0.00.052.797 I load: token to piece cache size = 0.2984 MB
0.00.052.802 I print_info: arch             = gptneox
0.00.052.802 I print_info: vocab_only       = 0
0.00.052.802 I print_info: n_ctx_train      = 2048
0.00.052.803 I print_info: n_embd           = 2048
0.00.052.803 I print_info: n_layer          = 24
0.00.052.808 I print_info: n_head           = 16
0.00.052.808 I print_info: n_head_kv        = 16
0.00.052.809 I print_info: n_rot            = 32
0.00.052.809 I print_info: n_swa            = 0
0.00.052.809 I print_info: n_embd_head_k    = 128
0.00.052.809 I print_info: n_embd_head_v    = 128
0.00.052.810 I print_info: n_gqa            = 1
0.00.052.811 I print_info: n_embd_k_gqa     = 2048
0.00.052.811 I print_info: n_embd_v_gqa     = 2048
0.00.052.812 I print_info: f_norm_eps       = 1.0e-05
0.00.052.812 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.812 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.812 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.812 I print_info: f_logit_scale    = 0.0e+00
0.00.052.815 I print_info: n_ff             = 8192
0.00.052.815 I print_info: n_expert         = 0
0.00.052.815 I print_info: n_expert_used    = 0
0.00.052.816 I print_info: causal attn      = 1
0.00.052.816 I print_info: pooling type     = 0
0.00.052.816 I print_info: rope type        = 2
0.00.052.816 I print_info: rope scaling     = linear
0.00.052.816 I print_info: freq_base_train  = 10000.0
0.00.052.817 I print_info: freq_scale_train = 1
0.00.052.817 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.817 I print_info: rope_finetuned   = unknown
0.00.052.817 I print_info: ssm_d_conv       = 0
0.00.052.817 I print_info: ssm_d_inner      = 0
0.00.052.817 I print_info: ssm_d_state      = 0
0.00.052.817 I print_info: ssm_dt_rank      = 0
0.00.052.818 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.818 I print_info: model type       = 1.4B
0.00.052.818 I print_info: model params     = 1.41 B
0.00.052.818 I print_info: general.name     = 1.4B
0.00.052.819 I print_info: vocab type       = BPE
0.00.052.819 I print_info: n_vocab          = 50304
0.00.052.819 I print_info: n_merges         = 50009
0.00.052.819 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.820 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.820 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.823 I print_info: LF token         = 128 'Ä'
0.00.052.823 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.823 I print_info: max token length = 1024
0.00.054.914 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.914 I load_tensors: offloading output layer to GPU
0.00.054.914 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.925 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.927 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.259 I llama_init_from_model: n_seq_max     = 1
0.00.055.260 I llama_init_from_model: n_ctx         = 2048
0.00.055.260 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.260 I llama_init_from_model: n_batch       = 2048
0.00.055.260 I llama_init_from_model: n_ubatch      = 512
0.00.055.260 I llama_init_from_model: flash_attn    = 0
0.00.055.261 I llama_init_from_model: freq_base     = 10000.0
0.00.055.261 I llama_init_from_model: freq_scale    = 1
0.00.055.262 I ggml_metal_init: allocating
0.00.055.265 I ggml_metal_init: found device: Apple M4
0.00.055.267 I ggml_metal_init: picking default device: Apple M4
0.00.055.888 I ggml_metal_init: using embedded metal library
0.00.058.277 I ggml_metal_init: GPU name:   Apple M4
0.00.058.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.280 I ggml_metal_init: simdgroup reduction   = true
0.00.058.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.280 I ggml_metal_init: has bfloat            = true
0.00.058.280 I ggml_metal_init: use bfloat            = true
0.00.058.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.282 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.550 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.712 I init:      Metal KV buffer size =   384.00 MiB
0.00.088.728 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.722 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.724 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.724 I llama_init_from_model: graph nodes  = 967
0.00.089.724 I llama_init_from_model: graph splits = 2
0.00.089.727 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.856 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.415 I main: llama threadpool init, n_threads = 4
0.00.683.457 I 
0.00.683.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.500 I 
0.00.683.734 I sampler seed: 1234
0.00.683.740 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.799 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.803 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.803 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.532.986 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.532.987 I llama_perf_context_print:        load time =     673.98 ms
0.01.532.987 I llama_perf_context_print: prompt eval time =      51.42 ms /     7 tokens (    7.35 ms per token,   136.13 tokens per second)
0.01.532.988 I llama_perf_context_print:        eval time =     794.83 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.532.989 I llama_perf_context_print:       total time =     849.57 ms /    70 tokens
0.01.533.210 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.873 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.664 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.669 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.678 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.682 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.682 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.707 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.673 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.674 I llama_model_loader: - type  f32:  194 tensors
0.00.026.674 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.675 I print_info: file format = GGUF V3 (latest)
0.00.026.675 I print_info: file type   = Q6_K
0.00.026.676 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.810 I load: special tokens cache size = 25
0.00.051.704 I load: token to piece cache size = 0.2984 MB
0.00.051.707 I print_info: arch             = gptneox
0.00.051.707 I print_info: vocab_only       = 0
0.00.051.707 I print_info: n_ctx_train      = 2048
0.00.051.707 I print_info: n_embd           = 2048
0.00.051.708 I print_info: n_layer          = 24
0.00.051.710 I print_info: n_head           = 16
0.00.051.711 I print_info: n_head_kv        = 16
0.00.051.711 I print_info: n_rot            = 32
0.00.051.712 I print_info: n_swa            = 0
0.00.051.713 I print_info: n_embd_head_k    = 128
0.00.051.713 I print_info: n_embd_head_v    = 128
0.00.051.713 I print_info: n_gqa            = 1
0.00.051.714 I print_info: n_embd_k_gqa     = 2048
0.00.051.715 I print_info: n_embd_v_gqa     = 2048
0.00.051.715 I print_info: f_norm_eps       = 1.0e-05
0.00.051.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.716 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.716 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.716 I print_info: f_logit_scale    = 0.0e+00
0.00.051.717 I print_info: n_ff             = 8192
0.00.051.717 I print_info: n_expert         = 0
0.00.051.718 I print_info: n_expert_used    = 0
0.00.051.718 I print_info: causal attn      = 1
0.00.051.718 I print_info: pooling type     = 0
0.00.051.720 I print_info: rope type        = 2
0.00.051.720 I print_info: rope scaling     = linear
0.00.051.720 I print_info: freq_base_train  = 10000.0
0.00.051.721 I print_info: freq_scale_train = 1
0.00.051.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.721 I print_info: rope_finetuned   = unknown
0.00.051.721 I print_info: ssm_d_conv       = 0
0.00.051.721 I print_info: ssm_d_inner      = 0
0.00.051.721 I print_info: ssm_d_state      = 0
0.00.051.721 I print_info: ssm_dt_rank      = 0
0.00.051.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.722 I print_info: model type       = 1.4B
0.00.051.722 I print_info: model params     = 1.41 B
0.00.051.722 I print_info: general.name     = 1.4B
0.00.051.723 I print_info: vocab type       = BPE
0.00.051.723 I print_info: n_vocab          = 50304
0.00.051.723 I print_info: n_merges         = 50009
0.00.051.723 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.724 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.724 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.724 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.724 I print_info: LF token         = 128 'Ä'
0.00.051.724 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.725 I print_info: max token length = 1024
0.00.053.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.776 I load_tensors: offloading output layer to GPU
0.00.053.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.786 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.788 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.086 I llama_init_from_model: n_seq_max     = 1
0.00.054.087 I llama_init_from_model: n_ctx         = 2048
0.00.054.087 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.087 I llama_init_from_model: n_batch       = 2048
0.00.054.087 I llama_init_from_model: n_ubatch      = 512
0.00.054.088 I llama_init_from_model: flash_attn    = 0
0.00.054.088 I llama_init_from_model: freq_base     = 10000.0
0.00.054.088 I llama_init_from_model: freq_scale    = 1
0.00.054.089 I ggml_metal_init: allocating
0.00.054.091 I ggml_metal_init: found device: Apple M4
0.00.054.094 I ggml_metal_init: picking default device: Apple M4
0.00.054.678 I ggml_metal_init: using embedded metal library
0.00.057.045 I ggml_metal_init: GPU name:   Apple M4
0.00.057.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.048 I ggml_metal_init: simdgroup reduction   = true
0.00.057.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.048 I ggml_metal_init: has bfloat            = true
0.00.057.048 I ggml_metal_init: use bfloat            = true
0.00.057.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.896 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.906 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.915 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.947 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.011 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.012 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.012 I llama_init_from_model: graph nodes  = 967
0.00.088.012 I llama_init_from_model: graph splits = 2
0.00.088.015 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.181 I main: llama threadpool init, n_threads = 4
0.00.743.222 I 
0.00.743.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.241 I 
0.00.743.453 I sampler seed: 1234
0.00.743.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.469 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.469 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.470 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.625.285 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.625.285 I llama_perf_context_print:        load time =     733.30 ms
0.01.625.286 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.51 tokens per second)
0.01.625.287 I llama_perf_context_print:        eval time =     824.39 ms /    63 runs   (   13.09 ms per token,    76.42 tokens per second)
0.01.625.287 I llama_perf_context_print:       total time =     882.11 ms /    70 tokens
0.01.625.511 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.543 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.508 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.624 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.643 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.644 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.645 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.649 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.650 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.652 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.653 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.662 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.203 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.385 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.391 I llama_model_loader: - type  f32:  194 tensors
0.00.056.391 I llama_model_loader: - type  f16:   98 tensors
0.00.056.392 I print_info: file format = GGUF V3 (latest)
0.00.056.393 I print_info: file type   = all F32 (guessed)
0.00.056.397 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.624 I load: special tokens cache size = 25
0.00.092.463 I load: token to piece cache size = 0.2984 MB
0.00.092.467 I print_info: arch             = gptneox
0.00.092.467 I print_info: vocab_only       = 0
0.00.092.467 I print_info: n_ctx_train      = 2048
0.00.092.468 I print_info: n_embd           = 2048
0.00.092.468 I print_info: n_layer          = 24
0.00.092.471 I print_info: n_head           = 16
0.00.092.471 I print_info: n_head_kv        = 16
0.00.092.474 I print_info: n_rot            = 32
0.00.092.474 I print_info: n_swa            = 0
0.00.092.474 I print_info: n_embd_head_k    = 128
0.00.092.475 I print_info: n_embd_head_v    = 128
0.00.092.475 I print_info: n_gqa            = 1
0.00.092.476 I print_info: n_embd_k_gqa     = 2048
0.00.092.476 I print_info: n_embd_v_gqa     = 2048
0.00.092.477 I print_info: f_norm_eps       = 1.0e-05
0.00.092.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.478 I print_info: f_logit_scale    = 0.0e+00
0.00.092.479 I print_info: n_ff             = 8192
0.00.092.479 I print_info: n_expert         = 0
0.00.092.479 I print_info: n_expert_used    = 0
0.00.092.479 I print_info: causal attn      = 1
0.00.092.479 I print_info: pooling type     = 0
0.00.092.479 I print_info: rope type        = 2
0.00.092.480 I print_info: rope scaling     = linear
0.00.092.480 I print_info: freq_base_train  = 10000.0
0.00.092.480 I print_info: freq_scale_train = 1
0.00.092.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.481 I print_info: rope_finetuned   = unknown
0.00.092.481 I print_info: ssm_d_conv       = 0
0.00.092.481 I print_info: ssm_d_inner      = 0
0.00.092.481 I print_info: ssm_d_state      = 0
0.00.092.481 I print_info: ssm_dt_rank      = 0
0.00.092.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.482 I print_info: model type       = 1.4B
0.00.092.483 I print_info: model params     = 1.41 B
0.00.092.483 I print_info: general.name     = 1.4B
0.00.092.483 I print_info: vocab type       = BPE
0.00.092.484 I print_info: n_vocab          = 50304
0.00.092.484 I print_info: n_merges         = 50009
0.00.092.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.487 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.487 I print_info: LF token         = 128 'Ä'
0.00.092.487 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.488 I print_info: max token length = 1024
0.00.094.272 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.272 I load_tensors: offloading output layer to GPU
0.00.094.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.282 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.284 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.580 I llama_init_from_model: n_seq_max     = 1
0.00.094.581 I llama_init_from_model: n_ctx         = 128
0.00.094.581 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.581 I llama_init_from_model: n_batch       = 128
0.00.094.581 I llama_init_from_model: n_ubatch      = 128
0.00.094.582 I llama_init_from_model: flash_attn    = 0
0.00.094.582 I llama_init_from_model: freq_base     = 10000.0
0.00.094.582 I llama_init_from_model: freq_scale    = 1
0.00.094.583 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.583 I ggml_metal_init: allocating
0.00.094.586 I ggml_metal_init: found device: Apple M4
0.00.094.588 I ggml_metal_init: picking default device: Apple M4
0.00.095.216 I ggml_metal_init: using embedded metal library
0.00.097.906 I ggml_metal_init: GPU name:   Apple M4
0.00.097.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.908 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.909 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.909 I ggml_metal_init: simdgroup reduction   = true
0.00.097.909 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.909 I ggml_metal_init: has bfloat            = true
0.00.097.909 I ggml_metal_init: use bfloat            = true
0.00.097.910 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.601 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.086 I init:      Metal KV buffer size =    24.00 MiB
0.00.109.089 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.115 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.025 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.110.026 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.110.027 I llama_init_from_model: graph nodes  = 967
0.00.110.027 I llama_init_from_model: graph splits = 2
0.00.110.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.131.011 I 
0.01.131.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.131.109 I perplexity: tokenizing the input ..
0.01.144.452 I perplexity: tokenization took 13.337 ms
0.01.144.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.267.185 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.269.069 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.269.137 I llama_perf_context_print:        load time =    1107.49 ms
0.01.269.139 I llama_perf_context_print: prompt eval time =     121.82 ms /   128 tokens (    0.95 ms per token,  1050.71 tokens per second)
0.01.269.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.269.141 I llama_perf_context_print:       total time =     138.13 ms /   129 tokens
0.01.269.876 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.128s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.344 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.544 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.669 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.074 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.077 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.078 I llama_model_loader: - type  f32:  194 tensors
0.00.040.079 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.080 I print_info: file format = GGUF V3 (latest)
0.00.040.080 I print_info: file type   = Q8_0
0.00.040.081 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.066.345 I load: special tokens cache size = 25
0.00.073.032 I load: token to piece cache size = 0.2984 MB
0.00.073.035 I print_info: arch             = gptneox
0.00.073.035 I print_info: vocab_only       = 0
0.00.073.035 I print_info: n_ctx_train      = 2048
0.00.073.035 I print_info: n_embd           = 2048
0.00.073.036 I print_info: n_layer          = 24
0.00.073.040 I print_info: n_head           = 16
0.00.073.041 I print_info: n_head_kv        = 16
0.00.073.041 I print_info: n_rot            = 32
0.00.073.041 I print_info: n_swa            = 0
0.00.073.041 I print_info: n_embd_head_k    = 128
0.00.073.043 I print_info: n_embd_head_v    = 128
0.00.073.044 I print_info: n_gqa            = 1
0.00.073.052 I print_info: n_embd_k_gqa     = 2048
0.00.073.060 I print_info: n_embd_v_gqa     = 2048
0.00.073.061 I print_info: f_norm_eps       = 1.0e-05
0.00.073.062 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.062 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.062 I print_info: f_logit_scale    = 0.0e+00
0.00.073.064 I print_info: n_ff             = 8192
0.00.073.064 I print_info: n_expert         = 0
0.00.073.065 I print_info: n_expert_used    = 0
0.00.073.065 I print_info: causal attn      = 1
0.00.073.065 I print_info: pooling type     = 0
0.00.073.065 I print_info: rope type        = 2
0.00.073.065 I print_info: rope scaling     = linear
0.00.073.066 I print_info: freq_base_train  = 10000.0
0.00.073.066 I print_info: freq_scale_train = 1
0.00.073.066 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.066 I print_info: rope_finetuned   = unknown
0.00.073.067 I print_info: ssm_d_conv       = 0
0.00.073.067 I print_info: ssm_d_inner      = 0
0.00.073.068 I print_info: ssm_d_state      = 0
0.00.073.068 I print_info: ssm_dt_rank      = 0
0.00.073.068 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.069 I print_info: model type       = 1.4B
0.00.073.069 I print_info: model params     = 1.41 B
0.00.073.069 I print_info: general.name     = 1.4B
0.00.073.070 I print_info: vocab type       = BPE
0.00.073.070 I print_info: n_vocab          = 50304
0.00.073.071 I print_info: n_merges         = 50009
0.00.073.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.072 I print_info: LF token         = 128 'Ä'
0.00.073.073 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.073 I print_info: max token length = 1024
0.00.075.618 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.618 I load_tensors: offloading output layer to GPU
0.00.075.619 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.630 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.631 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.075.956 I llama_init_from_model: n_seq_max     = 1
0.00.075.957 I llama_init_from_model: n_ctx         = 128
0.00.075.957 I llama_init_from_model: n_ctx_per_seq = 128
0.00.075.957 I llama_init_from_model: n_batch       = 128
0.00.075.958 I llama_init_from_model: n_ubatch      = 128
0.00.075.958 I llama_init_from_model: flash_attn    = 0
0.00.075.958 I llama_init_from_model: freq_base     = 10000.0
0.00.075.958 I llama_init_from_model: freq_scale    = 1
0.00.075.959 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.959 I ggml_metal_init: allocating
0.00.075.962 I ggml_metal_init: found device: Apple M4
0.00.075.964 I ggml_metal_init: picking default device: Apple M4
0.00.076.685 I ggml_metal_init: using embedded metal library
0.00.079.459 I ggml_metal_init: GPU name:   Apple M4
0.00.079.461 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.461 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.462 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.462 I ggml_metal_init: simdgroup reduction   = true
0.00.079.462 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.462 I ggml_metal_init: has bfloat            = true
0.00.079.462 I ggml_metal_init: use bfloat            = true
0.00.079.462 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.258 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.690 I init:      Metal KV buffer size =    24.00 MiB
0.00.090.694 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.757 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.091.758 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.091.759 I llama_init_from_model: graph nodes  = 967
0.00.091.759 I llama_init_from_model: graph splits = 2
0.00.091.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.972.312 I 
0.00.972.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.972.353 I perplexity: tokenizing the input ..
0.00.980.460 I perplexity: tokenization took 8.105 ms
0.00.980.464 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.105.052 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.106.342 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.106.364 I llama_perf_context_print:        load time =     958.76 ms
0.01.106.365 I llama_perf_context_print: prompt eval time =     124.35 ms /   128 tokens (    0.97 ms per token,  1029.39 tokens per second)
0.01.106.366 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.106.367 I llama_perf_context_print:       total time =     134.05 ms /   129 tokens
0.01.106.831 I ggml_metal_free: deallocating

real	0m1.128s
user	0m0.100s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.228 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.401 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.399 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.400 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.638 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.640 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.640 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.641 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.641 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.642 I llama_model_loader: - type  f32:  194 tensors
0.00.026.642 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.643 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.647 I print_info: file format = GGUF V3 (latest)
0.00.026.648 I print_info: file type   = Q4_0
0.00.026.649 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.048 I load: special tokens cache size = 25
0.00.051.965 I load: token to piece cache size = 0.2984 MB
0.00.051.970 I print_info: arch             = gptneox
0.00.051.970 I print_info: vocab_only       = 0
0.00.051.971 I print_info: n_ctx_train      = 2048
0.00.051.971 I print_info: n_embd           = 2048
0.00.051.973 I print_info: n_layer          = 24
0.00.051.977 I print_info: n_head           = 16
0.00.051.978 I print_info: n_head_kv        = 16
0.00.051.978 I print_info: n_rot            = 32
0.00.051.979 I print_info: n_swa            = 0
0.00.051.979 I print_info: n_embd_head_k    = 128
0.00.051.979 I print_info: n_embd_head_v    = 128
0.00.051.980 I print_info: n_gqa            = 1
0.00.051.981 I print_info: n_embd_k_gqa     = 2048
0.00.051.981 I print_info: n_embd_v_gqa     = 2048
0.00.051.982 I print_info: f_norm_eps       = 1.0e-05
0.00.051.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.983 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.983 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.983 I print_info: f_logit_scale    = 0.0e+00
0.00.051.984 I print_info: n_ff             = 8192
0.00.051.984 I print_info: n_expert         = 0
0.00.051.984 I print_info: n_expert_used    = 0
0.00.051.984 I print_info: causal attn      = 1
0.00.051.984 I print_info: pooling type     = 0
0.00.051.984 I print_info: rope type        = 2
0.00.051.985 I print_info: rope scaling     = linear
0.00.051.985 I print_info: freq_base_train  = 10000.0
0.00.051.986 I print_info: freq_scale_train = 1
0.00.051.986 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.988 I print_info: rope_finetuned   = unknown
0.00.051.988 I print_info: ssm_d_conv       = 0
0.00.051.988 I print_info: ssm_d_inner      = 0
0.00.051.988 I print_info: ssm_d_state      = 0
0.00.051.989 I print_info: ssm_dt_rank      = 0
0.00.051.989 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.989 I print_info: model type       = 1.4B
0.00.051.990 I print_info: model params     = 1.41 B
0.00.051.990 I print_info: general.name     = 1.4B
0.00.051.990 I print_info: vocab type       = BPE
0.00.051.991 I print_info: n_vocab          = 50304
0.00.051.991 I print_info: n_merges         = 50009
0.00.051.992 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.992 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.992 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.993 I print_info: LF token         = 128 'Ä'
0.00.051.993 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.993 I print_info: max token length = 1024
0.00.053.927 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.927 I load_tensors: offloading output layer to GPU
0.00.053.928 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.938 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.939 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.227 I llama_init_from_model: n_seq_max     = 1
0.00.054.228 I llama_init_from_model: n_ctx         = 128
0.00.054.228 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.228 I llama_init_from_model: n_batch       = 128
0.00.054.228 I llama_init_from_model: n_ubatch      = 128
0.00.054.229 I llama_init_from_model: flash_attn    = 0
0.00.054.229 I llama_init_from_model: freq_base     = 10000.0
0.00.054.229 I llama_init_from_model: freq_scale    = 1
0.00.054.230 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.230 I ggml_metal_init: allocating
0.00.054.233 I ggml_metal_init: found device: Apple M4
0.00.054.235 I ggml_metal_init: picking default device: Apple M4
0.00.054.830 I ggml_metal_init: using embedded metal library
0.00.057.197 I ggml_metal_init: GPU name:   Apple M4
0.00.057.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.199 I ggml_metal_init: simdgroup reduction   = true
0.00.057.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.199 I ggml_metal_init: has bfloat            = true
0.00.057.200 I ggml_metal_init: use bfloat            = true
0.00.057.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.852 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.094 I init:      Metal KV buffer size =    24.00 MiB
0.00.069.098 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.141 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.142 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.143 I llama_init_from_model: graph nodes  = 967
0.00.070.143 I llama_init_from_model: graph splits = 2
0.00.070.144 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.544 I 
0.00.606.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.591 I perplexity: tokenizing the input ..
0.00.614.208 I perplexity: tokenization took 7.615 ms
0.00.614.212 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.085 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.737.580 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.737.609 I llama_perf_context_print:        load time =     596.14 ms
0.00.737.610 I llama_perf_context_print: prompt eval time =     121.64 ms /   128 tokens (    0.95 ms per token,  1052.30 tokens per second)
0.00.737.611 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.611 I llama_perf_context_print:       total time =     131.07 ms /   129 tokens
0.00.737.933 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.079s
sys	0m0.078s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.152 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.971 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.974 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.975 I llama_model_loader: - type  f32:  194 tensors
0.00.024.975 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.975 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.976 I print_info: file format = GGUF V3 (latest)
0.00.024.977 I print_info: file type   = Q4_1
0.00.024.978 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.325 I load: special tokens cache size = 25
0.00.051.493 I load: token to piece cache size = 0.2984 MB
0.00.051.497 I print_info: arch             = gptneox
0.00.051.498 I print_info: vocab_only       = 0
0.00.051.498 I print_info: n_ctx_train      = 2048
0.00.051.498 I print_info: n_embd           = 2048
0.00.051.498 I print_info: n_layer          = 24
0.00.051.502 I print_info: n_head           = 16
0.00.051.503 I print_info: n_head_kv        = 16
0.00.051.503 I print_info: n_rot            = 32
0.00.051.503 I print_info: n_swa            = 0
0.00.051.504 I print_info: n_embd_head_k    = 128
0.00.051.504 I print_info: n_embd_head_v    = 128
0.00.051.504 I print_info: n_gqa            = 1
0.00.051.505 I print_info: n_embd_k_gqa     = 2048
0.00.051.508 I print_info: n_embd_v_gqa     = 2048
0.00.051.509 I print_info: f_norm_eps       = 1.0e-05
0.00.051.509 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.510 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.510 I print_info: f_logit_scale    = 0.0e+00
0.00.051.511 I print_info: n_ff             = 8192
0.00.051.511 I print_info: n_expert         = 0
0.00.051.511 I print_info: n_expert_used    = 0
0.00.051.512 I print_info: causal attn      = 1
0.00.051.512 I print_info: pooling type     = 0
0.00.051.512 I print_info: rope type        = 2
0.00.051.513 I print_info: rope scaling     = linear
0.00.051.513 I print_info: freq_base_train  = 10000.0
0.00.051.514 I print_info: freq_scale_train = 1
0.00.051.515 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.515 I print_info: rope_finetuned   = unknown
0.00.051.515 I print_info: ssm_d_conv       = 0
0.00.051.515 I print_info: ssm_d_inner      = 0
0.00.051.515 I print_info: ssm_d_state      = 0
0.00.051.515 I print_info: ssm_dt_rank      = 0
0.00.051.516 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.516 I print_info: model type       = 1.4B
0.00.051.516 I print_info: model params     = 1.41 B
0.00.051.516 I print_info: general.name     = 1.4B
0.00.051.518 I print_info: vocab type       = BPE
0.00.051.518 I print_info: n_vocab          = 50304
0.00.051.518 I print_info: n_merges         = 50009
0.00.051.518 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.518 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.519 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.519 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.519 I print_info: LF token         = 128 'Ä'
0.00.051.519 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.519 I print_info: max token length = 1024
0.00.053.258 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.258 I load_tensors: offloading output layer to GPU
0.00.053.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.264 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.264 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.671 I llama_init_from_model: n_seq_max     = 1
0.00.053.672 I llama_init_from_model: n_ctx         = 128
0.00.053.672 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.673 I llama_init_from_model: n_batch       = 128
0.00.053.673 I llama_init_from_model: n_ubatch      = 128
0.00.053.673 I llama_init_from_model: flash_attn    = 0
0.00.053.673 I llama_init_from_model: freq_base     = 10000.0
0.00.053.674 I llama_init_from_model: freq_scale    = 1
0.00.053.674 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.675 I ggml_metal_init: allocating
0.00.053.679 I ggml_metal_init: found device: Apple M4
0.00.053.681 I ggml_metal_init: picking default device: Apple M4
0.00.054.298 I ggml_metal_init: using embedded metal library
0.00.056.748 I ggml_metal_init: GPU name:   Apple M4
0.00.056.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.751 I ggml_metal_init: simdgroup reduction   = true
0.00.056.751 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.751 I ggml_metal_init: has bfloat            = true
0.00.056.752 I ggml_metal_init: use bfloat            = true
0.00.056.752 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.811 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.103 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.112 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.049 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.050 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.050 I llama_init_from_model: graph nodes  = 967
0.00.068.051 I llama_init_from_model: graph splits = 2
0.00.068.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.490 I 
0.00.701.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.558 I perplexity: tokenizing the input ..
0.00.710.108 I perplexity: tokenization took 8.548 ms
0.00.710.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.980 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.833.403 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.833.427 I llama_perf_context_print:        load time =     692.60 ms
0.00.833.428 I llama_perf_context_print: prompt eval time =     121.62 ms /   128 tokens (    0.95 ms per token,  1052.44 tokens per second)
0.00.833.429 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.429 I llama_perf_context_print:       total time =     131.94 ms /   129 tokens
0.00.833.754 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.080s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.266 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.255 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.445 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.447 I llama_model_loader: - type  f32:  194 tensors
0.00.026.448 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.448 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.449 I print_info: file format = GGUF V3 (latest)
0.00.026.451 I print_info: file type   = Q5_0
0.00.026.453 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.444 I load: special tokens cache size = 25
0.00.052.728 I load: token to piece cache size = 0.2984 MB
0.00.052.732 I print_info: arch             = gptneox
0.00.052.732 I print_info: vocab_only       = 0
0.00.052.732 I print_info: n_ctx_train      = 2048
0.00.052.733 I print_info: n_embd           = 2048
0.00.052.733 I print_info: n_layer          = 24
0.00.052.737 I print_info: n_head           = 16
0.00.052.738 I print_info: n_head_kv        = 16
0.00.052.738 I print_info: n_rot            = 32
0.00.052.738 I print_info: n_swa            = 0
0.00.052.738 I print_info: n_embd_head_k    = 128
0.00.052.738 I print_info: n_embd_head_v    = 128
0.00.052.739 I print_info: n_gqa            = 1
0.00.052.740 I print_info: n_embd_k_gqa     = 2048
0.00.052.741 I print_info: n_embd_v_gqa     = 2048
0.00.052.741 I print_info: f_norm_eps       = 1.0e-05
0.00.052.743 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.743 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.743 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.743 I print_info: f_logit_scale    = 0.0e+00
0.00.052.744 I print_info: n_ff             = 8192
0.00.052.744 I print_info: n_expert         = 0
0.00.052.744 I print_info: n_expert_used    = 0
0.00.052.745 I print_info: causal attn      = 1
0.00.052.745 I print_info: pooling type     = 0
0.00.052.745 I print_info: rope type        = 2
0.00.052.747 I print_info: rope scaling     = linear
0.00.052.747 I print_info: freq_base_train  = 10000.0
0.00.052.747 I print_info: freq_scale_train = 1
0.00.052.747 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.748 I print_info: rope_finetuned   = unknown
0.00.052.748 I print_info: ssm_d_conv       = 0
0.00.052.748 I print_info: ssm_d_inner      = 0
0.00.052.748 I print_info: ssm_d_state      = 0
0.00.052.748 I print_info: ssm_dt_rank      = 0
0.00.052.748 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.748 I print_info: model type       = 1.4B
0.00.052.749 I print_info: model params     = 1.41 B
0.00.052.749 I print_info: general.name     = 1.4B
0.00.052.750 I print_info: vocab type       = BPE
0.00.052.750 I print_info: n_vocab          = 50304
0.00.052.750 I print_info: n_merges         = 50009
0.00.052.750 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.754 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.754 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.754 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.754 I print_info: LF token         = 128 'Ä'
0.00.052.754 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.755 I print_info: max token length = 1024
0.00.054.654 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.654 I load_tensors: offloading output layer to GPU
0.00.054.654 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.665 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.666 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.959 I llama_init_from_model: n_seq_max     = 1
0.00.054.960 I llama_init_from_model: n_ctx         = 128
0.00.054.960 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.961 I llama_init_from_model: n_batch       = 128
0.00.054.961 I llama_init_from_model: n_ubatch      = 128
0.00.054.961 I llama_init_from_model: flash_attn    = 0
0.00.054.961 I llama_init_from_model: freq_base     = 10000.0
0.00.054.962 I llama_init_from_model: freq_scale    = 1
0.00.054.962 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.963 I ggml_metal_init: allocating
0.00.054.966 I ggml_metal_init: found device: Apple M4
0.00.054.968 I ggml_metal_init: picking default device: Apple M4
0.00.055.602 I ggml_metal_init: using embedded metal library
0.00.058.138 I ggml_metal_init: GPU name:   Apple M4
0.00.058.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.140 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.141 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.141 I ggml_metal_init: simdgroup reduction   = true
0.00.058.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.141 I ggml_metal_init: has bfloat            = true
0.00.058.141 I ggml_metal_init: use bfloat            = true
0.00.058.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.300 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.586 I init:      Metal KV buffer size =    24.00 MiB
0.00.069.592 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.070.493 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.070.494 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.070.494 I llama_init_from_model: graph nodes  = 967
0.00.070.494 I llama_init_from_model: graph splits = 2
0.00.070.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.505 I 
0.00.718.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.555 I perplexity: tokenizing the input ..
0.00.726.606 I perplexity: tokenization took 8.049 ms
0.00.726.611 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.771 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.862.952 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.862.976 I llama_perf_context_print:        load time =     708.23 ms
0.00.862.977 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.65 tokens per second)
0.00.862.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.978 I llama_perf_context_print:       total time =     144.47 ms /   129 tokens
0.00.863.545 I ggml_metal_free: deallocating

real	0m0.880s
user	0m0.081s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.629 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.020.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.397 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.404 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.182 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.029.182 I llama_model_loader: - type  f32:  194 tensors
0.00.029.183 I llama_model_loader: - type q5_1:   97 tensors
0.00.029.183 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.184 I print_info: file format = GGUF V3 (latest)
0.00.029.184 I print_info: file type   = Q5_1
0.00.029.185 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.048.905 I load: special tokens cache size = 25
0.00.054.635 I load: token to piece cache size = 0.2984 MB
0.00.054.638 I print_info: arch             = gptneox
0.00.054.638 I print_info: vocab_only       = 0
0.00.054.639 I print_info: n_ctx_train      = 2048
0.00.054.639 I print_info: n_embd           = 2048
0.00.054.639 I print_info: n_layer          = 24
0.00.054.642 I print_info: n_head           = 16
0.00.054.643 I print_info: n_head_kv        = 16
0.00.054.643 I print_info: n_rot            = 32
0.00.054.643 I print_info: n_swa            = 0
0.00.054.643 I print_info: n_embd_head_k    = 128
0.00.054.644 I print_info: n_embd_head_v    = 128
0.00.054.644 I print_info: n_gqa            = 1
0.00.054.645 I print_info: n_embd_k_gqa     = 2048
0.00.054.646 I print_info: n_embd_v_gqa     = 2048
0.00.054.646 I print_info: f_norm_eps       = 1.0e-05
0.00.054.647 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.648 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.648 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.649 I print_info: f_logit_scale    = 0.0e+00
0.00.054.649 I print_info: n_ff             = 8192
0.00.054.652 I print_info: n_expert         = 0
0.00.054.652 I print_info: n_expert_used    = 0
0.00.054.652 I print_info: causal attn      = 1
0.00.054.652 I print_info: pooling type     = 0
0.00.054.652 I print_info: rope type        = 2
0.00.054.652 I print_info: rope scaling     = linear
0.00.054.653 I print_info: freq_base_train  = 10000.0
0.00.054.653 I print_info: freq_scale_train = 1
0.00.054.653 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.653 I print_info: rope_finetuned   = unknown
0.00.054.654 I print_info: ssm_d_conv       = 0
0.00.054.654 I print_info: ssm_d_inner      = 0
0.00.054.654 I print_info: ssm_d_state      = 0
0.00.054.654 I print_info: ssm_dt_rank      = 0
0.00.054.654 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.654 I print_info: model type       = 1.4B
0.00.054.656 I print_info: model params     = 1.41 B
0.00.054.656 I print_info: general.name     = 1.4B
0.00.054.657 I print_info: vocab type       = BPE
0.00.054.657 I print_info: n_vocab          = 50304
0.00.054.657 I print_info: n_merges         = 50009
0.00.054.657 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.658 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.658 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.660 I print_info: LF token         = 128 'Ä'
0.00.054.660 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.660 I print_info: max token length = 1024
0.00.056.630 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.630 I load_tensors: offloading output layer to GPU
0.00.056.630 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.640 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.642 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.056.932 I llama_init_from_model: n_seq_max     = 1
0.00.056.933 I llama_init_from_model: n_ctx         = 128
0.00.056.933 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.934 I llama_init_from_model: n_batch       = 128
0.00.056.934 I llama_init_from_model: n_ubatch      = 128
0.00.056.934 I llama_init_from_model: flash_attn    = 0
0.00.056.934 I llama_init_from_model: freq_base     = 10000.0
0.00.056.935 I llama_init_from_model: freq_scale    = 1
0.00.056.935 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.935 I ggml_metal_init: allocating
0.00.056.939 I ggml_metal_init: found device: Apple M4
0.00.056.941 I ggml_metal_init: picking default device: Apple M4
0.00.057.539 I ggml_metal_init: using embedded metal library
0.00.059.937 I ggml_metal_init: GPU name:   Apple M4
0.00.059.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.940 I ggml_metal_init: simdgroup reduction   = true
0.00.059.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.940 I ggml_metal_init: has bfloat            = true
0.00.059.940 I ggml_metal_init: use bfloat            = true
0.00.059.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.866 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.260 I init:      Metal KV buffer size =    24.00 MiB
0.00.071.265 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.293 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.158 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.159 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.159 I llama_init_from_model: graph nodes  = 967
0.00.072.159 I llama_init_from_model: graph splits = 2
0.00.072.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.962 I 
0.00.680.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.007 I perplexity: tokenizing the input ..
0.00.688.801 I perplexity: tokenization took 7.792 ms
0.00.688.804 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.588 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.824.762 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.824.790 I llama_perf_context_print:        load time =     672.33 ms
0.00.824.791 I llama_perf_context_print: prompt eval time =     134.56 ms /   128 tokens (    1.05 ms per token,   951.27 tokens per second)
0.00.824.792 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.792 I llama_perf_context_print:       total time =     143.83 ms /   129 tokens
0.00.825.265 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.078s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.658 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.025.583 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.594 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.596 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.850 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.296 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.035.297 I llama_model_loader: - type  f32:  194 tensors
0.00.035.297 I llama_model_loader: - type q2_K:   49 tensors
0.00.035.297 I llama_model_loader: - type q3_K:   48 tensors
0.00.035.298 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.298 I print_info: file format = GGUF V3 (latest)
0.00.035.299 I print_info: file type   = Q2_K - Medium
0.00.035.299 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.060.699 I load: special tokens cache size = 25
0.00.069.820 I load: token to piece cache size = 0.2984 MB
0.00.069.824 I print_info: arch             = gptneox
0.00.069.825 I print_info: vocab_only       = 0
0.00.069.825 I print_info: n_ctx_train      = 2048
0.00.069.825 I print_info: n_embd           = 2048
0.00.069.826 I print_info: n_layer          = 24
0.00.069.829 I print_info: n_head           = 16
0.00.069.830 I print_info: n_head_kv        = 16
0.00.069.831 I print_info: n_rot            = 32
0.00.069.831 I print_info: n_swa            = 0
0.00.069.831 I print_info: n_embd_head_k    = 128
0.00.069.831 I print_info: n_embd_head_v    = 128
0.00.069.832 I print_info: n_gqa            = 1
0.00.069.833 I print_info: n_embd_k_gqa     = 2048
0.00.069.834 I print_info: n_embd_v_gqa     = 2048
0.00.069.835 I print_info: f_norm_eps       = 1.0e-05
0.00.069.836 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.836 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.836 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.836 I print_info: f_logit_scale    = 0.0e+00
0.00.069.837 I print_info: n_ff             = 8192
0.00.069.838 I print_info: n_expert         = 0
0.00.069.838 I print_info: n_expert_used    = 0
0.00.069.838 I print_info: causal attn      = 1
0.00.069.838 I print_info: pooling type     = 0
0.00.069.841 I print_info: rope type        = 2
0.00.069.841 I print_info: rope scaling     = linear
0.00.069.842 I print_info: freq_base_train  = 10000.0
0.00.069.842 I print_info: freq_scale_train = 1
0.00.069.842 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.843 I print_info: rope_finetuned   = unknown
0.00.069.843 I print_info: ssm_d_conv       = 0
0.00.069.843 I print_info: ssm_d_inner      = 0
0.00.069.843 I print_info: ssm_d_state      = 0
0.00.069.843 I print_info: ssm_dt_rank      = 0
0.00.069.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.844 I print_info: model type       = 1.4B
0.00.069.844 I print_info: model params     = 1.41 B
0.00.069.845 I print_info: general.name     = 1.4B
0.00.069.845 I print_info: vocab type       = BPE
0.00.069.845 I print_info: n_vocab          = 50304
0.00.069.846 I print_info: n_merges         = 50009
0.00.069.846 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.846 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.851 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.852 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.852 I print_info: LF token         = 128 'Ä'
0.00.069.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.853 I print_info: max token length = 1024
0.00.072.370 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.371 I load_tensors: offloading output layer to GPU
0.00.072.371 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.382 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.072.384 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.072.822 I llama_init_from_model: n_seq_max     = 1
0.00.072.824 I llama_init_from_model: n_ctx         = 128
0.00.072.824 I llama_init_from_model: n_ctx_per_seq = 128
0.00.072.824 I llama_init_from_model: n_batch       = 128
0.00.072.825 I llama_init_from_model: n_ubatch      = 128
0.00.072.825 I llama_init_from_model: flash_attn    = 0
0.00.072.825 I llama_init_from_model: freq_base     = 10000.0
0.00.072.826 I llama_init_from_model: freq_scale    = 1
0.00.072.826 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.827 I ggml_metal_init: allocating
0.00.072.831 I ggml_metal_init: found device: Apple M4
0.00.072.834 I ggml_metal_init: picking default device: Apple M4
0.00.073.716 I ggml_metal_init: using embedded metal library
0.00.077.627 I ggml_metal_init: GPU name:   Apple M4
0.00.077.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.631 I ggml_metal_init: simdgroup reduction   = true
0.00.077.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.632 I ggml_metal_init: has bfloat            = true
0.00.077.632 I ggml_metal_init: use bfloat            = true
0.00.077.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.857 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.408 I init:      Metal KV buffer size =    24.00 MiB
0.00.091.413 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.091.439 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.471 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.092.472 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.092.473 I llama_init_from_model: graph nodes  = 967
0.00.092.473 I llama_init_from_model: graph splits = 2
0.00.092.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.092.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.465.242 I 
0.00.465.276 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.465.292 I perplexity: tokenizing the input ..
0.00.475.902 I perplexity: tokenization took 10.608 ms
0.00.475.909 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.608.848 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.610.022 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.610.052 I llama_perf_context_print:        load time =     449.58 ms
0.00.610.053 I llama_perf_context_print: prompt eval time =     132.71 ms /   128 tokens (    1.04 ms per token,   964.52 tokens per second)
0.00.610.054 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.610.054 I llama_perf_context_print:       total time =     144.81 ms /   129 tokens
0.00.610.567 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.097s
sys	0m0.080s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.831 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.019.362 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.364 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.365 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.366 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.366 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.367 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.370 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.374 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.335 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.410 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.365 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.366 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.028.366 I llama_model_loader: - type  f32:  194 tensors
0.00.028.367 I llama_model_loader: - type q3_K:   25 tensors
0.00.028.367 I llama_model_loader: - type q4_K:   71 tensors
0.00.028.367 I llama_model_loader: - type q5_K:    1 tensors
0.00.028.367 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.368 I print_info: file format = GGUF V3 (latest)
0.00.028.368 I print_info: file type   = Q3_K - Medium
0.00.028.369 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.047.897 I load: special tokens cache size = 25
0.00.053.936 I load: token to piece cache size = 0.2984 MB
0.00.053.938 I print_info: arch             = gptneox
0.00.053.939 I print_info: vocab_only       = 0
0.00.053.939 I print_info: n_ctx_train      = 2048
0.00.053.939 I print_info: n_embd           = 2048
0.00.053.939 I print_info: n_layer          = 24
0.00.053.942 I print_info: n_head           = 16
0.00.053.943 I print_info: n_head_kv        = 16
0.00.053.943 I print_info: n_rot            = 32
0.00.053.943 I print_info: n_swa            = 0
0.00.053.943 I print_info: n_embd_head_k    = 128
0.00.053.944 I print_info: n_embd_head_v    = 128
0.00.053.944 I print_info: n_gqa            = 1
0.00.053.945 I print_info: n_embd_k_gqa     = 2048
0.00.053.946 I print_info: n_embd_v_gqa     = 2048
0.00.053.946 I print_info: f_norm_eps       = 1.0e-05
0.00.053.947 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.947 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.949 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.949 I print_info: f_logit_scale    = 0.0e+00
0.00.053.950 I print_info: n_ff             = 8192
0.00.053.950 I print_info: n_expert         = 0
0.00.053.951 I print_info: n_expert_used    = 0
0.00.053.952 I print_info: causal attn      = 1
0.00.053.952 I print_info: pooling type     = 0
0.00.053.952 I print_info: rope type        = 2
0.00.053.952 I print_info: rope scaling     = linear
0.00.053.953 I print_info: freq_base_train  = 10000.0
0.00.053.953 I print_info: freq_scale_train = 1
0.00.053.953 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.953 I print_info: rope_finetuned   = unknown
0.00.053.953 I print_info: ssm_d_conv       = 0
0.00.053.954 I print_info: ssm_d_inner      = 0
0.00.053.954 I print_info: ssm_d_state      = 0
0.00.053.954 I print_info: ssm_dt_rank      = 0
0.00.053.954 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.955 I print_info: model type       = 1.4B
0.00.053.956 I print_info: model params     = 1.41 B
0.00.053.956 I print_info: general.name     = 1.4B
0.00.053.957 I print_info: vocab type       = BPE
0.00.053.957 I print_info: n_vocab          = 50304
0.00.053.958 I print_info: n_merges         = 50009
0.00.053.958 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.958 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.959 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.959 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.959 I print_info: LF token         = 128 'Ä'
0.00.053.959 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.960 I print_info: max token length = 1024
0.00.055.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.906 I load_tensors: offloading output layer to GPU
0.00.055.907 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.917 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.919 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.056.205 I llama_init_from_model: n_seq_max     = 1
0.00.056.206 I llama_init_from_model: n_ctx         = 128
0.00.056.206 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.206 I llama_init_from_model: n_batch       = 128
0.00.056.206 I llama_init_from_model: n_ubatch      = 128
0.00.056.207 I llama_init_from_model: flash_attn    = 0
0.00.056.207 I llama_init_from_model: freq_base     = 10000.0
0.00.056.207 I llama_init_from_model: freq_scale    = 1
0.00.056.208 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.208 I ggml_metal_init: allocating
0.00.056.211 I ggml_metal_init: found device: Apple M4
0.00.056.213 I ggml_metal_init: picking default device: Apple M4
0.00.056.805 I ggml_metal_init: using embedded metal library
0.00.059.190 I ggml_metal_init: GPU name:   Apple M4
0.00.059.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.193 I ggml_metal_init: simdgroup reduction   = true
0.00.059.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.193 I ggml_metal_init: has bfloat            = true
0.00.059.193 I ggml_metal_init: use bfloat            = true
0.00.059.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.129 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.371 I init:      Metal KV buffer size =    24.00 MiB
0.00.070.376 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.404 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.329 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.330 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.330 I llama_init_from_model: graph nodes  = 967
0.00.071.330 I llama_init_from_model: graph splits = 2
0.00.071.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.985 I 
0.00.481.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.481.032 I perplexity: tokenizing the input ..
0.00.489.140 I perplexity: tokenization took 8.107 ms
0.00.489.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.621.451 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.622.625 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.622.649 I llama_perf_context_print:        load time =     472.15 ms
0.00.622.650 I llama_perf_context_print: prompt eval time =     132.08 ms /   128 tokens (    1.03 ms per token,   969.13 tokens per second)
0.00.622.651 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.622.651 I llama_perf_context_print:       total time =     141.67 ms /   129 tokens
0.00.623.126 I ggml_metal_free: deallocating

real	0m0.637s
user	0m0.079s
sys	0m0.075s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.862 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.641 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.023.646 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.649 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.651 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.652 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.652 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.086 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.086 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.086 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.087 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.032.088 I llama_model_loader: - type  f32:  194 tensors
0.00.032.088 I llama_model_loader: - type q4_K:   61 tensors
0.00.032.088 I llama_model_loader: - type q5_K:   24 tensors
0.00.032.088 I llama_model_loader: - type q6_K:   13 tensors
0.00.032.089 I print_info: file format = GGUF V3 (latest)
0.00.032.089 I print_info: file type   = Q4_K - Medium
0.00.032.090 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.052.384 I load: special tokens cache size = 25
0.00.058.432 I load: token to piece cache size = 0.2984 MB
0.00.058.435 I print_info: arch             = gptneox
0.00.058.435 I print_info: vocab_only       = 0
0.00.058.435 I print_info: n_ctx_train      = 2048
0.00.058.435 I print_info: n_embd           = 2048
0.00.058.435 I print_info: n_layer          = 24
0.00.058.438 I print_info: n_head           = 16
0.00.058.439 I print_info: n_head_kv        = 16
0.00.058.439 I print_info: n_rot            = 32
0.00.058.439 I print_info: n_swa            = 0
0.00.058.440 I print_info: n_embd_head_k    = 128
0.00.058.440 I print_info: n_embd_head_v    = 128
0.00.058.440 I print_info: n_gqa            = 1
0.00.058.441 I print_info: n_embd_k_gqa     = 2048
0.00.058.442 I print_info: n_embd_v_gqa     = 2048
0.00.058.442 I print_info: f_norm_eps       = 1.0e-05
0.00.058.443 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.443 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.443 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.443 I print_info: f_logit_scale    = 0.0e+00
0.00.058.444 I print_info: n_ff             = 8192
0.00.058.444 I print_info: n_expert         = 0
0.00.058.444 I print_info: n_expert_used    = 0
0.00.058.444 I print_info: causal attn      = 1
0.00.058.444 I print_info: pooling type     = 0
0.00.058.444 I print_info: rope type        = 2
0.00.058.445 I print_info: rope scaling     = linear
0.00.058.445 I print_info: freq_base_train  = 10000.0
0.00.058.446 I print_info: freq_scale_train = 1
0.00.058.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.448 I print_info: rope_finetuned   = unknown
0.00.058.448 I print_info: ssm_d_conv       = 0
0.00.058.448 I print_info: ssm_d_inner      = 0
0.00.058.448 I print_info: ssm_d_state      = 0
0.00.058.448 I print_info: ssm_dt_rank      = 0
0.00.058.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.449 I print_info: model type       = 1.4B
0.00.058.449 I print_info: model params     = 1.41 B
0.00.058.451 I print_info: general.name     = 1.4B
0.00.058.451 I print_info: vocab type       = BPE
0.00.058.452 I print_info: n_vocab          = 50304
0.00.058.452 I print_info: n_merges         = 50009
0.00.058.452 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.452 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.452 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.453 I print_info: LF token         = 128 'Ä'
0.00.058.453 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.454 I print_info: max token length = 1024
0.00.060.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.390 I load_tensors: offloading output layer to GPU
0.00.060.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.401 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.060.402 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.060.697 I llama_init_from_model: n_seq_max     = 1
0.00.060.698 I llama_init_from_model: n_ctx         = 128
0.00.060.698 I llama_init_from_model: n_ctx_per_seq = 128
0.00.060.698 I llama_init_from_model: n_batch       = 128
0.00.060.698 I llama_init_from_model: n_ubatch      = 128
0.00.060.699 I llama_init_from_model: flash_attn    = 0
0.00.060.699 I llama_init_from_model: freq_base     = 10000.0
0.00.060.699 I llama_init_from_model: freq_scale    = 1
0.00.060.700 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.700 I ggml_metal_init: allocating
0.00.060.703 I ggml_metal_init: found device: Apple M4
0.00.060.705 I ggml_metal_init: picking default device: Apple M4
0.00.061.281 I ggml_metal_init: using embedded metal library
0.00.063.650 I ggml_metal_init: GPU name:   Apple M4
0.00.063.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.651 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.652 I ggml_metal_init: simdgroup reduction   = true
0.00.063.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.652 I ggml_metal_init: has bfloat            = true
0.00.063.653 I ggml_metal_init: use bfloat            = true
0.00.063.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.483 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.810 I init:      Metal KV buffer size =    24.00 MiB
0.00.074.815 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.843 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.075.752 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.075.753 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.075.753 I llama_init_from_model: graph nodes  = 967
0.00.075.753 I llama_init_from_model: graph splits = 2
0.00.075.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.058 I 
0.00.648.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.097 I perplexity: tokenizing the input ..
0.00.656.170 I perplexity: tokenization took 8.072 ms
0.00.656.174 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.729 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.791.901 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.791.931 I llama_perf_context_print:        load time =     638.19 ms
0.00.791.932 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.88 tokens per second)
0.00.791.933 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.933 I llama_perf_context_print:       total time =     143.88 ms /   129 tokens
0.00.792.407 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.079s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.794 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.175 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.022.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.070 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.970 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.971 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.971 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.971 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.972 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.972 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.030.972 I llama_model_loader: - type  f32:  194 tensors
0.00.030.973 I llama_model_loader: - type q5_K:   61 tensors
0.00.030.973 I llama_model_loader: - type q6_K:   37 tensors
0.00.030.973 I print_info: file format = GGUF V3 (latest)
0.00.030.974 I print_info: file type   = Q5_K - Medium
0.00.030.974 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.050.551 I load: special tokens cache size = 25
0.00.056.489 I load: token to piece cache size = 0.2984 MB
0.00.056.492 I print_info: arch             = gptneox
0.00.056.492 I print_info: vocab_only       = 0
0.00.056.493 I print_info: n_ctx_train      = 2048
0.00.056.493 I print_info: n_embd           = 2048
0.00.056.493 I print_info: n_layer          = 24
0.00.056.496 I print_info: n_head           = 16
0.00.056.497 I print_info: n_head_kv        = 16
0.00.056.497 I print_info: n_rot            = 32
0.00.056.498 I print_info: n_swa            = 0
0.00.056.498 I print_info: n_embd_head_k    = 128
0.00.056.498 I print_info: n_embd_head_v    = 128
0.00.056.499 I print_info: n_gqa            = 1
0.00.056.500 I print_info: n_embd_k_gqa     = 2048
0.00.056.500 I print_info: n_embd_v_gqa     = 2048
0.00.056.501 I print_info: f_norm_eps       = 1.0e-05
0.00.056.501 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.502 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.502 I print_info: f_logit_scale    = 0.0e+00
0.00.056.503 I print_info: n_ff             = 8192
0.00.056.503 I print_info: n_expert         = 0
0.00.056.503 I print_info: n_expert_used    = 0
0.00.056.503 I print_info: causal attn      = 1
0.00.056.503 I print_info: pooling type     = 0
0.00.056.503 I print_info: rope type        = 2
0.00.056.504 I print_info: rope scaling     = linear
0.00.056.506 I print_info: freq_base_train  = 10000.0
0.00.056.507 I print_info: freq_scale_train = 1
0.00.056.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.507 I print_info: rope_finetuned   = unknown
0.00.056.508 I print_info: ssm_d_conv       = 0
0.00.056.508 I print_info: ssm_d_inner      = 0
0.00.056.508 I print_info: ssm_d_state      = 0
0.00.056.508 I print_info: ssm_dt_rank      = 0
0.00.056.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.508 I print_info: model type       = 1.4B
0.00.056.509 I print_info: model params     = 1.41 B
0.00.056.509 I print_info: general.name     = 1.4B
0.00.056.514 I print_info: vocab type       = BPE
0.00.056.514 I print_info: n_vocab          = 50304
0.00.056.514 I print_info: n_merges         = 50009
0.00.056.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.514 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.515 I print_info: LF token         = 128 'Ä'
0.00.056.515 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.515 I print_info: max token length = 1024
0.00.058.510 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.510 I load_tensors: offloading output layer to GPU
0.00.058.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.520 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.058.522 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.058.803 I llama_init_from_model: n_seq_max     = 1
0.00.058.804 I llama_init_from_model: n_ctx         = 128
0.00.058.804 I llama_init_from_model: n_ctx_per_seq = 128
0.00.058.805 I llama_init_from_model: n_batch       = 128
0.00.058.805 I llama_init_from_model: n_ubatch      = 128
0.00.058.805 I llama_init_from_model: flash_attn    = 0
0.00.058.805 I llama_init_from_model: freq_base     = 10000.0
0.00.058.806 I llama_init_from_model: freq_scale    = 1
0.00.058.806 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.806 I ggml_metal_init: allocating
0.00.058.809 I ggml_metal_init: found device: Apple M4
0.00.058.811 I ggml_metal_init: picking default device: Apple M4
0.00.059.385 I ggml_metal_init: using embedded metal library
0.00.061.772 I ggml_metal_init: GPU name:   Apple M4
0.00.061.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.774 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.774 I ggml_metal_init: simdgroup reduction   = true
0.00.061.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.774 I ggml_metal_init: has bfloat            = true
0.00.061.774 I ggml_metal_init: use bfloat            = true
0.00.061.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.445 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.854 I init:      Metal KV buffer size =    24.00 MiB
0.00.072.856 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.073.725 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.073.726 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.073.726 I llama_init_from_model: graph nodes  = 967
0.00.073.727 I llama_init_from_model: graph splits = 2
0.00.073.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.436 I 
0.00.713.469 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.482 I perplexity: tokenizing the input ..
0.00.721.499 I perplexity: tokenization took 8.014 ms
0.00.721.502 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.565 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.862.752 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.862.773 I llama_perf_context_print:        load time =     704.64 ms
0.00.862.773 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.35 tokens per second)
0.00.862.774 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.775 I llama_perf_context_print:       total time =     149.34 ms /   129 tokens
0.00.863.149 I ggml_metal_free: deallocating

real	0m0.876s
user	0m0.078s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.880 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.881 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.881 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.882 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.883 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.883 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.884 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.501 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.503 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.503 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.504 I llama_model_loader: - type  f32:  194 tensors
0.00.025.504 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.505 I print_info: file format = GGUF V3 (latest)
0.00.025.505 I print_info: file type   = Q6_K
0.00.025.506 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.222 I load: special tokens cache size = 25
0.00.051.050 I load: token to piece cache size = 0.2984 MB
0.00.051.054 I print_info: arch             = gptneox
0.00.051.054 I print_info: vocab_only       = 0
0.00.051.054 I print_info: n_ctx_train      = 2048
0.00.051.054 I print_info: n_embd           = 2048
0.00.051.055 I print_info: n_layer          = 24
0.00.051.057 I print_info: n_head           = 16
0.00.051.058 I print_info: n_head_kv        = 16
0.00.051.060 I print_info: n_rot            = 32
0.00.051.060 I print_info: n_swa            = 0
0.00.051.060 I print_info: n_embd_head_k    = 128
0.00.051.060 I print_info: n_embd_head_v    = 128
0.00.051.061 I print_info: n_gqa            = 1
0.00.051.062 I print_info: n_embd_k_gqa     = 2048
0.00.051.067 I print_info: n_embd_v_gqa     = 2048
0.00.051.068 I print_info: f_norm_eps       = 1.0e-05
0.00.051.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.070 I print_info: f_logit_scale    = 0.0e+00
0.00.051.071 I print_info: n_ff             = 8192
0.00.051.071 I print_info: n_expert         = 0
0.00.051.071 I print_info: n_expert_used    = 0
0.00.051.071 I print_info: causal attn      = 1
0.00.051.072 I print_info: pooling type     = 0
0.00.051.072 I print_info: rope type        = 2
0.00.051.072 I print_info: rope scaling     = linear
0.00.051.072 I print_info: freq_base_train  = 10000.0
0.00.051.073 I print_info: freq_scale_train = 1
0.00.051.074 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.075 I print_info: rope_finetuned   = unknown
0.00.051.075 I print_info: ssm_d_conv       = 0
0.00.051.075 I print_info: ssm_d_inner      = 0
0.00.051.075 I print_info: ssm_d_state      = 0
0.00.051.075 I print_info: ssm_dt_rank      = 0
0.00.051.075 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.076 I print_info: model type       = 1.4B
0.00.051.076 I print_info: model params     = 1.41 B
0.00.051.076 I print_info: general.name     = 1.4B
0.00.051.076 I print_info: vocab type       = BPE
0.00.051.078 I print_info: n_vocab          = 50304
0.00.051.078 I print_info: n_merges         = 50009
0.00.051.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.079 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.079 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.082 I print_info: LF token         = 128 'Ä'
0.00.051.082 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.083 I print_info: max token length = 1024
0.00.053.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.102 I load_tensors: offloading output layer to GPU
0.00.053.102 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.112 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.114 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.402 I llama_init_from_model: n_seq_max     = 1
0.00.053.402 I llama_init_from_model: n_ctx         = 128
0.00.053.403 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.403 I llama_init_from_model: n_batch       = 128
0.00.053.403 I llama_init_from_model: n_ubatch      = 128
0.00.053.403 I llama_init_from_model: flash_attn    = 0
0.00.053.403 I llama_init_from_model: freq_base     = 10000.0
0.00.053.404 I llama_init_from_model: freq_scale    = 1
0.00.053.404 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.404 I ggml_metal_init: allocating
0.00.053.407 I ggml_metal_init: found device: Apple M4
0.00.053.409 I ggml_metal_init: picking default device: Apple M4
0.00.053.985 I ggml_metal_init: using embedded metal library
0.00.056.322 I ggml_metal_init: GPU name:   Apple M4
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.325 I ggml_metal_init: simdgroup reduction   = true
0.00.056.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.325 I ggml_metal_init: has bfloat            = true
0.00.056.325 I ggml_metal_init: use bfloat            = true
0.00.056.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.062 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.304 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.309 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.331 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.171 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.172 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.173 I llama_init_from_model: graph nodes  = 967
0.00.068.173 I llama_init_from_model: graph splits = 2
0.00.068.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.517 I 
0.00.388.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.560 I perplexity: tokenizing the input ..
0.00.396.220 I perplexity: tokenization took 7.659 ms
0.00.396.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.536.525 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.537.734 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.537.758 I llama_perf_context_print:        load time =     378.57 ms
0.00.537.759 I llama_perf_context_print: prompt eval time =     140.05 ms /   128 tokens (    1.09 ms per token,   913.99 tokens per second)
0.00.537.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.537.760 I llama_perf_context_print:       total time =     149.25 ms /   129 tokens
0.00.538.245 I ggml_metal_free: deallocating

real	0m0.553s
user	0m0.078s
sys	0m0.081s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4474 (e7f94f84) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.615 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.095 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.113 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.113 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.114 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.114 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.119 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.071 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.072 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.073 I llama_model_loader: - type  f32:  194 tensors
0.00.054.073 I llama_model_loader: - type  f16:   98 tensors
0.00.054.074 I print_info: file format = GGUF V3 (latest)
0.00.054.075 I print_info: file type   = all F32 (guessed)
0.00.054.081 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.783 I load: special tokens cache size = 25
0.00.086.340 I load: token to piece cache size = 0.2984 MB
0.00.086.343 I print_info: arch             = gptneox
0.00.086.344 I print_info: vocab_only       = 0
0.00.086.344 I print_info: n_ctx_train      = 2048
0.00.086.344 I print_info: n_embd           = 2048
0.00.086.344 I print_info: n_layer          = 24
0.00.086.347 I print_info: n_head           = 16
0.00.086.348 I print_info: n_head_kv        = 16
0.00.086.349 I print_info: n_rot            = 32
0.00.086.349 I print_info: n_swa            = 0
0.00.086.351 I print_info: n_embd_head_k    = 128
0.00.086.351 I print_info: n_embd_head_v    = 128
0.00.086.351 I print_info: n_gqa            = 1
0.00.086.352 I print_info: n_embd_k_gqa     = 2048
0.00.086.353 I print_info: n_embd_v_gqa     = 2048
0.00.086.353 I print_info: f_norm_eps       = 1.0e-05
0.00.086.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.354 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.354 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.354 I print_info: f_logit_scale    = 0.0e+00
0.00.086.355 I print_info: n_ff             = 8192
0.00.086.355 I print_info: n_expert         = 0
0.00.086.355 I print_info: n_expert_used    = 0
0.00.086.355 I print_info: causal attn      = 1
0.00.086.356 I print_info: pooling type     = 0
0.00.086.356 I print_info: rope type        = 2
0.00.086.356 I print_info: rope scaling     = linear
0.00.086.356 I print_info: freq_base_train  = 10000.0
0.00.086.357 I print_info: freq_scale_train = 1
0.00.086.357 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.357 I print_info: rope_finetuned   = unknown
0.00.086.357 I print_info: ssm_d_conv       = 0
0.00.086.357 I print_info: ssm_d_inner      = 0
0.00.086.357 I print_info: ssm_d_state      = 0
0.00.086.358 I print_info: ssm_dt_rank      = 0
0.00.086.358 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.358 I print_info: model type       = 1.4B
0.00.086.358 I print_info: model params     = 1.41 B
0.00.086.358 I print_info: general.name     = 1.4B
0.00.086.359 I print_info: vocab type       = BPE
0.00.086.359 I print_info: n_vocab          = 50304
0.00.086.359 I print_info: n_merges         = 50009
0.00.086.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.361 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.361 I print_info: LF token         = 128 'Ä'
0.00.086.361 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.361 I print_info: max token length = 1024
0.00.088.995 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.995 I load_tensors: offloading output layer to GPU
0.00.088.996 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.006 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.007 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.089.341 I llama_init_from_model: n_seq_max     = 1
0.00.089.342 I llama_init_from_model: n_ctx         = 128
0.00.089.342 I llama_init_from_model: n_ctx_per_seq = 128
0.00.089.342 I llama_init_from_model: n_batch       = 128
0.00.089.342 I llama_init_from_model: n_ubatch      = 128
0.00.089.342 I llama_init_from_model: flash_attn    = 0
0.00.089.343 I llama_init_from_model: freq_base     = 10000.0
0.00.089.343 I llama_init_from_model: freq_scale    = 1
0.00.089.343 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.344 I ggml_metal_init: allocating
0.00.089.347 I ggml_metal_init: found device: Apple M4
0.00.089.349 I ggml_metal_init: picking default device: Apple M4
0.00.089.940 I ggml_metal_init: using embedded metal library
0.00.092.411 I ggml_metal_init: GPU name:   Apple M4
0.00.092.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.414 I ggml_metal_init: simdgroup reduction   = true
0.00.092.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.414 I ggml_metal_init: has bfloat            = true
0.00.092.414 I ggml_metal_init: use bfloat            = true
0.00.092.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.541 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.818 I init:      Metal KV buffer size =    24.00 MiB
0.00.102.822 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.693 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.694 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.694 I llama_init_from_model: graph nodes  = 967
0.00.103.694 I llama_init_from_model: graph splits = 2
0.00.103.695 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.696 I 
0.00.103.722 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.724 I compute_imatrix: tokenizing the input ..
0.00.110.555 I compute_imatrix: tokenization took 6.83 ms
0.00.110.557 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.662.392 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.664.907 I llama_perf_context_print:        load time =    1640.78 ms
0.01.664.908 I llama_perf_context_print: prompt eval time =    1551.20 ms /   128 tokens (   12.12 ms per token,    82.52 tokens per second)
0.01.664.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.664.909 I llama_perf_context_print:       total time =    1643.29 ms /   129 tokens
0.01.665.612 I ggml_metal_free: deallocating

real	0m1.851s
user	0m0.165s
sys	0m0.263s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4474 (e7f94f84)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11860a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11860a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11860afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11860b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11860bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11860c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11860c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11860cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11860d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11860d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11860dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11860e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11860ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11860f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11860fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1186102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1186109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118611100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118611820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118611ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118612710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118612e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118613550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118613df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118614510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1186147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118615a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118615f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118616250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1186166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1186169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118617a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118617ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118618380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118618820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118619160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118619600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118619f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11861a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11861a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11861acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11861b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11861bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11861c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11861c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11861ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11861d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11861da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11861e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11861e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11861ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11861f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11861f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11861fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118620230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1186204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118620990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118620e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1186212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118621770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118621c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1186220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118622550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1186229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118622e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118623330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1186237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118623c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1186241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118624710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1186251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118625700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1186261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1186266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118627190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1186276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118627c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118628180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1186286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118629170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1186296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118629c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11862a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11862a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11862ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11862b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11862b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11862bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11861b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11862c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11862c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11862cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11862d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11862d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11862dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11862e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11862e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11862ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11862f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11862f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11862fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118630280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1186307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118630d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1186311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118631660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118631b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118631fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118632440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1186328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118632d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118633220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1186336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118633b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118634000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1186344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118634940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118636060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118636500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1186369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118636e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1186372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118637780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118637c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1186380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118638560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118638ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1186397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11863a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11863a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11863aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11863af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11863b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11863b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11863bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11863c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11863c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11863cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11863cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11863d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11863d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11863dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11863e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11863e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11863eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11863efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11863f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11863f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11863fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118640240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1186406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118640b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118641020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1186414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118641960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1186422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118642740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118643080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118643520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1186439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118643e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118644300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1186447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118644c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1186450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118645a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118645ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118646360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118646800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118646ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118647140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1186475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118647a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118647f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118648470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1186489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118648f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118649460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118649720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118649d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11864a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11864a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11864b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11864b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11864b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11864beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11864c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11864ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11864d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11864d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11864da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11864e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11864e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11864ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11864f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11864f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11864fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118650220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118650770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118650cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118651210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118651760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118651cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118652200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118652750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118652ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1186531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118653740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118653c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1186541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118654730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118654c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1186551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118655720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118655c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1186561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118656710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118656c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1186571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118657700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1186581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1186586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118658c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118659190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1186596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118659c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11865a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11865a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11865ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11865b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11865b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11865bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11865c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11865c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11865cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11865d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11865d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11865dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11865e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11865e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11865ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11865f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11865f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11865fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118660120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118660670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118660bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118661060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118661500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1186619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118661e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1186622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118662780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118662c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1186630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118663560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118663a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118663ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118664340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1186647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118664c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118665120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118665670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118665d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1186664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118666bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1186672f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1186675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118668060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118668670 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.143.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118668320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1186499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11864a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11861d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11861d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11861f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11864c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118614a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11861b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11861bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11861c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11861a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11861cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118613a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11861fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11862c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118667870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118616c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118616f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11864c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11864ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1186150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118615360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118615620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118668ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118668d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118669050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118669310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1186695d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118669890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118669b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118669e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11866a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11866a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11866a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11866a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11866abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11866ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11866b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11866b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11866b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11866b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11866bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11866bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11866c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11866c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11866c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11866ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11866ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11866cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11866d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11866d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11866d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11866da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11866dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11866e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11866e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11866e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11866e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11866eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11866edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11866f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11866f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11866f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11866f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11866fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11866fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118670110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1186703d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118670690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118670950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118670c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118670ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118671190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118671450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118671710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1186719d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118671c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118671f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118672210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1186724d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118672790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118672a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118672d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118672fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118673290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118673550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118673810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118673ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118673d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118674050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118674310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1186745d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118674890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118674b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118674e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1186750d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118675390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118675650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118675910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118675bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118675e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118676150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118676410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1186766d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118676990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118676c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118676f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1186771d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118677490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118677750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118677a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118677cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118677f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118678250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118678510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1186787d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118678a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118678d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118679010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1186792d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118679590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118679850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118679b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118679dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11867a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11867a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11867a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11867a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11867ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11867ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11867b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11867b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11867b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11867b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11867bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11867bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11867c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11867c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11867c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11867c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11867cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11867cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11867d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11867d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11867d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11867da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11867dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11867dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11867e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11867e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11867e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11867ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11867ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11867f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11867f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11867f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11867f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11867fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11867fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1186800d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118680390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118680650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118680910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118680bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118680e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118681150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118681410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1186816d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118681990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118681c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118681f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1186821d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118682490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118682750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118682a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118682cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118682f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118683250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118683510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1186837d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118683a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118683d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118684010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1186842d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118684590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118684850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118684b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118684dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118685090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118685350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118685610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1186858d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118685b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118685e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118686110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1186863d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118686690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118686950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118686c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118686ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118687190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118687450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118687710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1186879d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118687c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118687f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118688210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1186886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118688e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118689120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1186893e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118689850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118689cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11868a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11868a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11868aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11868ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11868b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11868b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11868bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11868c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11868c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11868c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11868cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11868d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11868d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11868dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11868df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11868e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11868e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11868eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11868f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11868f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11868f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11868fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1186902d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118690740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118690bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118691020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118691490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118691900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118691d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1186921e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118692650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118692ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118692f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1186933a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118693810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118693c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1186940f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118694560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1186949d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118694e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1186952b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118695720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118695b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118696000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118696470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1186968e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118696d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1186971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118697630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118697aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118697f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118698380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1186987f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118698c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1186990d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118699540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1186999b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118699e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11869a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11869a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11869ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11869afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11869b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11869b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11869bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11869c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11869c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11869ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11869d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11869dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11869e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11869ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11869ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11869f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11869f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11869fdd0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11869cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11869fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11869efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1186a0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1186a04f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1186a07b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1186a0a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1186a0d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1186a0ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1186a12b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1186a1570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1186a1830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1186a1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1186a23d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1186a2a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1186a2cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1186a2f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1186a3240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1186a3500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1186a37c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1186a3a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1186a3d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1186a4000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1186a42c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1186a4580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1186a4840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1186a4b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1186a4dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1186a5080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1186a5340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1186a5600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1186a58c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1186a5b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1186a5e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1186a6100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1186a63c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1186a6680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1186a6940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1186a6c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1186a6ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1186a7180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1186a7440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1186a7700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1186a79c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1186a7c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1186a7f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1186a8200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1186a84c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1186a8780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1186a8a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1186a8d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1186a8fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1186a9280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1186a9540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1186a9800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1186a9ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1186a9d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1186aa040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1186aa300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1186aa5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1186aa880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1186aab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1186aae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1186ab0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1186ab380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1186ab640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1186ab900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1186abbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1186abe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1186ac140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1186ac400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1186ac6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1186ac980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1186acc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1186acf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1186ad1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1186ad480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1186ad740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1186ada00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1186adcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1186adf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1186ae240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1186ae500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1186ae7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1186aea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1186aed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1186af000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1186af2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1186af580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1186af840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1186afb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1186afdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1186b0080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1186b0340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1186b0600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1186b08c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1186b0b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1186b0e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1186b1100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1186b13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1186b1680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1186b1940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1186b1c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1186b1ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1186b2180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1186b2440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1186b2700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1186b29c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1186b2c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1186b2f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1186b3200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1186b34c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1186b3780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1186b3a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1186b3d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1186b3fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1186b4280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1186b4540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1186b4800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1186b4ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1186b4d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1186b5040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1186b5300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1186b55c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1186b5880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1186b5b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1186b5e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1186b60c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1186b6380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1186b6640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1186b6900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1186b6bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1186b6e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1186b7140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1186b7400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1186b76c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1186b7980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1186b7c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1186b7f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1186b81c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1186b8480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1186b8740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1186b8a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1186b8cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1186b8f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1186b9240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1186b9500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1186b97c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1186b9a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1186b9d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1186ba000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1186ba2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1186ba580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1186ba840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1186bab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1186badc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1186bb080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1186bb340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1186bb600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1186bb8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1186bbb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1186bbe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1186bc100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1186bc3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1186bc680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1186bc940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1186bcc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1186bcec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1186bd180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1186bd440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1186bd700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1186bd9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1186bdc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1186bdf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1186be200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1186be4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1186be780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1186bea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1186bed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1186befc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1186bf280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1186bf540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1186bf800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1186bfac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1186bfd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1186c0040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1186c0300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1186c05c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1186c0880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1186c0b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1186c0e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1186c10c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1186c1380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1186c1640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1186c1900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1186c1bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1186c1e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1186c2140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1186c2400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1186c26c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1186c2980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1186c2c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1186c2f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1186c31c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1186c3480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1186c3740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1186c3a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1186c3cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1186c3f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1186c4240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1186c4810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1186c4ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1186c4d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1186c5050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1186c5310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1186c55d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1186c5890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1186c5b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1186c5e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1186c60d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1186c6390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1186c6650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1186c6910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1186c6bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1186c6e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1186c7150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1186c7410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1186c76d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1186c7990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1186c7c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1186c7f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1186c81d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1186c8490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1186c8750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1186c8a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1186c8cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1186c8f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1186c9250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1186c9510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1186c97d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1186c9a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1186c9d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1186ca010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1186ca2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1186ca590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1186ca850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1186cab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1186cadd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1186cb090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1186cb350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1186cb610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1186cb8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1186cbb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1186cbe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1186cc110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1186cc3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1186cc690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1186cc950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1186ccc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1186cced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1186cd190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1186cd450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1186cd710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1186cd9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1186cdc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1186cdf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1186ce210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1186ce4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1186ce790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1186cea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1186ced10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1186cefd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1186cf290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1186cf550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1186cf810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1186cfc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1186cfed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1186d03d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1186d08d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1186d0dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1186d12d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1186d17d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1186d1cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1186d26e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1186d2e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1186d3520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1186d3c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1186d3f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1186d46f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1186d49b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1186d4fc0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.806s
user	0m0.301s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4474 (e7f94f84)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14200cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14200d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14200dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14200e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14200e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14200ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14200f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14200f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14200fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142010360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142010860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142010d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142011880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142012030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142012840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142012f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142013680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142013da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1420144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142014c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1420153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142015ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1420161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1420171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142017470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142017a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1420186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142018c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142018ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142019390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142019650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142019ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14201a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14201a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14201ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14201b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14201b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14201b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14201be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14201c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14201c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14201cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14201d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14201d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14201d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14201df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14201e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14201ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14201f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14201fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1420200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1420206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142020ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1420214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142021970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1420220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1420226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142022ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142023190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142023630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142023f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142024410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1420248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142024d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1420251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142025690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142025b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142025fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142026470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142026910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1420273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142027900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142027e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1420283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1420288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142028e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142029390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1420298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142029e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14202a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14202a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14202ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14202b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14202b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14202be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14202c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14202c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14202ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14202d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14202d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14202ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14202e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14202e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14201e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14202ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14202f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14202fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14202ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1420304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1420309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142030f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142031490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1420319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142031f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142032480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1420329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142032f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142033470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1420339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142033e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142034300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1420347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1420350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142035580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142035a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142035ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142036360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142036800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142036ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142037140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1420375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142037a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142037f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1420383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142038860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142038d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1420391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142039640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142039ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142039f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14203a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14203a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14203ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14203b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14203b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14203bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14203bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14203c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14203c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14203cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14203d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14203d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14203dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14203e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14203e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14203e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14203ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14203f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14203f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14203fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1420400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142040540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1420409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142040e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142041320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1420417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142041c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142042100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1420425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142042a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142042ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142043380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142043820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142043cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142044160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142044600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142044aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142044f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1420453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142045880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142045d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1420461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142046660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142046b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142046fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142047440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1420478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142047d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1420486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142048b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142049000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1420494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142049940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142049de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14204a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14204a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14204abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14204b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14204b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14204bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14204c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14204c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14204c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14204cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14204d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14204dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14204e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14204e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14204eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14204f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14204f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14204fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142050290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142050730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142050ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142051430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142051980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142051ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142052420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142052970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142052ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142053410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142053960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142053eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142054400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142054950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142054ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1420553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142055940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142055e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1420563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142056930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142056e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1420573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142057920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142057e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1420583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142058910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142058e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1420593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142059900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142059e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14205a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14205a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14205ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14205b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14205b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14205be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14205c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14205c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14205ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14205d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14205d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14205de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14205e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14205e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14205ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14205f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14205f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14205fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142060340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142060890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142060de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142061330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142061880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142061dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142062320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142062870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142062dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142063310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142063860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142063d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1420641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142064640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142064ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142064f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142065420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1420658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142065d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142066200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1420666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142066b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142066fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142067480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142067920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142067dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142068310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142068a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142069150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142069870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142069f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14206a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14206aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14206ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14206b310 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.255 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.259 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14170b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14170bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14170c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14170c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14170ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14170ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14170d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14170d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14170dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14170e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14170e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14170ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14170f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14170fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1417106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141710dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1417114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141711c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141712330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141712b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141713220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141713940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141714060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141714780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141715160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141715420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141715d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141716170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141716670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141716b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141716ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1417172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141717720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141717b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1417180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1417185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141718af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141718ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1417194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1417199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141719ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14171a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14171a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14171ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14171b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14171b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14171bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14171bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14171c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14171c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14171cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14171d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14171d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14171dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14171e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14171e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14171ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14171f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14171f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14171fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141720500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1417209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141720e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1417212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141721780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141721c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1417220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141722560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141722a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141722ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1417233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141723940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141723e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1417243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141724930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141724e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1417253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141725920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141725e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1417263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141726910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141726e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1417273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141727900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141727e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1417283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1417288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141728e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141729390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1417298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141729e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14172a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14172a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14172ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14172b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14172b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14172be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14172c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14172c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14172ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14172d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14172d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14172ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14172e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14172e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14172ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14172f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14172f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14172fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141730320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1417307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141730c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141731100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1417315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141731a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141731ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141732380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141732820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141733160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141733600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141733aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141733f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1417343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141734880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1417351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141735660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141735b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141735fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141736440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1417368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141736d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141737220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1417376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141737b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141738000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1417384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141738940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141738de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141739280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141739720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141739bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14173a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14173a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14173a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14173ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14173b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14173b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14173bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14173c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14173c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14173ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14173cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14173d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14173d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14173dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14173e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14173e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14173ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14173ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14173f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14173f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14173fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141740180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141740620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141740ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141740f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141741400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1417418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141741d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1417421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141742680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141742b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141742fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141743460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141743900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141743da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141744240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1417446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141744b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141745020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1417454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141745960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141745e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1417462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141746740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141746be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141747080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141747520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141747a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141747fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141748510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141748a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141748d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141749330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141749940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141749f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14174a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14174abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14174aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14174b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14174bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14174c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14174c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14174cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14174d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14174d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14174dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14174e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14174e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14174ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14174f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14174f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14174fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1417502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141750810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141750d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1417512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141751800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141751d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1417522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1417527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141752d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141753290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1417537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141754280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1417547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141754d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141755270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1417557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141755d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141756260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1417567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141756d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141757250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1417577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141757cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141758240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141758790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141758ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141759230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141759780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141759cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14175a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14175a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14175acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14175b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14175b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14175bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14175c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14175c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14175cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14175d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14175d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14175dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14175e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14175e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14175ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14175f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14175f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14175fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1417601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141760660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141760b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141760fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141761440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1417618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141761d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141762220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1417626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141762b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141763000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1417634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141763940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141763de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141764280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141764720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141764c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141765390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141765ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1417661d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1417668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141766bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1417673a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141767660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141767c70 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141604c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1416050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141605540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1416059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141605e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141606290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141606700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141606b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141606fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141607450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1416078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141607fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141608ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141609270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141609a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14160a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14160a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14160afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14160b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14160bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14160c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14160cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14160d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14160db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14160e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14160e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14160e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14160ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14160f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14160f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14160f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14160fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141610350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141610610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141610a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141610ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141611360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1416117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141611c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1416120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141612520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141612990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141612e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141613270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1416136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141613b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1416148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1416155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141616340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1416167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141616d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1416183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1416195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14161a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14161a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14161abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14161b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14161b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14161b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14161bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14161c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14161c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14161cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14161cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14161d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14161d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14161dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14161e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14161e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14161e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14161ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14161f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14161f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14161fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141620020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141620900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1416211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141621650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141621ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141621f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1416223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1416230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141623560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1416239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141624260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141624520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141624e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141625270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1416256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141625b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141625fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141626430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1416268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1416275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141627a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141627ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141628340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1416287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141629090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141629500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141629970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141629de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14162a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14162a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14162ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14162afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14162b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14162b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14162bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14162c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14162c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14162ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14162ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14162d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14162d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14162dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14162e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14162e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14162e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14162edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14162f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14162f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14162fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14162ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1416303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141630860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141630cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141631140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1416315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141631a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141632300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141632770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141632be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141633050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1416334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141633930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141633da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141634680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141634af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141634f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1416353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141635840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141636120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141636a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1416372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141637bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141638030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1416384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141638910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141638d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1416391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141639660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141639ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141639f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14163a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14163a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14163ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14163b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14163b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14163b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14163be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14163c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14163c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14163cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14163d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14163d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14163d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14163dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14163e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14163e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14163eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14163ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14163f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14163f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14163fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1416400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141640550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1416409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141640e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1416412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141641710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141642290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141642550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141642810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141642c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1416430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141643560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1416439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141643e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1416442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141644720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141645000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141645470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1416458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1416461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141646630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141646aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141646f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141647380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1416477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141647c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1416480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1416489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141648e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141649290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141649700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141649b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141649fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14164a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14164a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14164ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14164b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14164b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14164ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14164bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14164c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14164c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14164cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14164d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14164d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14164d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14164de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14164e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14164e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14164eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14164efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14164f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14164f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14164fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141650180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1416505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141650a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141650ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1416517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141651c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141652090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141652500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141652970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141652de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141653250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1416536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141653b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141653fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141654410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141654880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141654cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141655160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1416555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141656920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141657040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141657760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141657e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141658140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1416585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141658bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1416591c0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.937s
user	0m0.245s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
