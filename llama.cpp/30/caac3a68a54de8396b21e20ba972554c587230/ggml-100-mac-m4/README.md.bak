### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.55 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.28 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.26 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.49 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  176.91 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.87 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.08 sec*proc (28 tests)

Total Test time (real) = 220.09 sec

real	3m40.117s
user	7m33.415s
sys	0m6.196s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.12 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.33 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.18 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.18 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.27 sec*proc (28 tests)

Total Test time (real) =  51.28 sec

real	0m51.290s
user	1m11.223s
sys	0m5.573s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.073 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.518 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.694 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.701 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.704 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.706 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.707 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.708 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.709 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.710 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.714 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.714 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.715 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.718 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.719 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.722 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.723 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.723 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.724 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.725 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.633 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.635 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.636 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.636 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.637 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.638 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.638 I llama_model_loader: - type  f32:  124 tensors
0.00.025.639 I llama_model_loader: - type  f16:   73 tensors
0.00.030.265 I llm_load_vocab: special tokens cache size = 5
0.00.032.551 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.032.580 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.032.581 I llm_load_print_meta: arch             = bert
0.00.032.582 I llm_load_print_meta: vocab type       = WPM
0.00.032.582 I llm_load_print_meta: n_vocab          = 30522
0.00.032.582 I llm_load_print_meta: n_merges         = 0
0.00.032.582 I llm_load_print_meta: vocab_only       = 0
0.00.032.583 I llm_load_print_meta: n_ctx_train      = 512
0.00.032.583 I llm_load_print_meta: n_embd           = 384
0.00.032.583 I llm_load_print_meta: n_layer          = 12
0.00.032.587 I llm_load_print_meta: n_head           = 12
0.00.032.588 I llm_load_print_meta: n_head_kv        = 12
0.00.032.588 I llm_load_print_meta: n_rot            = 32
0.00.032.588 I llm_load_print_meta: n_swa            = 0
0.00.032.588 I llm_load_print_meta: n_embd_head_k    = 32
0.00.032.589 I llm_load_print_meta: n_embd_head_v    = 32
0.00.032.590 I llm_load_print_meta: n_gqa            = 1
0.00.032.590 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.032.591 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.032.592 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.032.592 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.032.593 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.032.593 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.032.593 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.032.594 I llm_load_print_meta: n_ff             = 1536
0.00.032.594 I llm_load_print_meta: n_expert         = 0
0.00.032.597 I llm_load_print_meta: n_expert_used    = 0
0.00.032.597 I llm_load_print_meta: causal attn      = 0
0.00.032.597 I llm_load_print_meta: pooling type     = 2
0.00.032.597 I llm_load_print_meta: rope type        = 2
0.00.032.598 I llm_load_print_meta: rope scaling     = linear
0.00.032.598 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.032.599 I llm_load_print_meta: freq_scale_train = 1
0.00.032.599 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.032.599 I llm_load_print_meta: rope_finetuned   = unknown
0.00.032.599 I llm_load_print_meta: ssm_d_conv       = 0
0.00.032.600 I llm_load_print_meta: ssm_d_inner      = 0
0.00.032.600 I llm_load_print_meta: ssm_d_state      = 0
0.00.032.600 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.032.600 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.032.600 I llm_load_print_meta: model type       = 33M
0.00.032.601 I llm_load_print_meta: model ftype      = F16
0.00.032.601 I llm_load_print_meta: model params     = 33.21 M
0.00.032.602 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.032.602 I llm_load_print_meta: general.name     = Bge Small
0.00.032.603 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.032.603 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.032.604 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.032.604 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.032.606 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.032.606 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.032.606 I llm_load_print_meta: max token length = 21
0.00.034.618 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.624 I llm_load_tensors: offloading output layer to GPU
0.00.034.625 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.654 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.655 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.238 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.240 I llama_new_context_with_model: n_ctx         = 512
0.00.035.240 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.240 I llama_new_context_with_model: n_batch       = 2048
0.00.035.241 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.241 I llama_new_context_with_model: flash_attn    = 0
0.00.035.241 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.242 I llama_new_context_with_model: freq_scale    = 1
0.00.035.243 I ggml_metal_init: allocating
0.00.035.247 I ggml_metal_init: found device: Apple M4
0.00.035.250 I ggml_metal_init: picking default device: Apple M4
0.00.036.085 I ggml_metal_init: using embedded metal library
0.00.040.081 I ggml_metal_init: GPU name:   Apple M4
0.00.040.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.084 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.084 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.085 I ggml_metal_init: simdgroup reduction   = true
0.00.040.085 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.085 I ggml_metal_init: has bfloat            = true
0.00.040.085 I ggml_metal_init: use bfloat            = true
0.00.040.086 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.087 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.272 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.052.857 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.859 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.860 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.053.616 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.053.617 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.053.618 I llama_new_context_with_model: graph nodes  = 429
0.00.053.618 I llama_new_context_with_model: graph splits = 2
0.00.053.639 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.110 I 
0.00.060.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.800 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.581 I llama_perf_context_print:        load time =      44.59 ms
0.00.065.586 I llama_perf_context_print: prompt eval time =       4.63 ms /     9 tokens (    0.51 ms per token,  1944.26 tokens per second)
0.00.065.587 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.587 I llama_perf_context_print:       total time =       5.47 ms /    10 tokens
0.00.065.712 I ggml_metal_free: deallocating

real	0m0.243s
user	0m0.048s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.025 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.071 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.076 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.078 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.081 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.082 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.083 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.084 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.084 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.084 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.085 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.087 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.087 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.087 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.088 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.088 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.088 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.090 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.235 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.236 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.237 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.237 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.237 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.238 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.238 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.238 I llama_model_loader: - type  f32:  124 tensors
0.00.014.238 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.666 I llm_load_vocab: special tokens cache size = 5
0.00.017.946 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.954 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.955 I llm_load_print_meta: arch             = bert
0.00.017.955 I llm_load_print_meta: vocab type       = WPM
0.00.017.956 I llm_load_print_meta: n_vocab          = 30522
0.00.017.956 I llm_load_print_meta: n_merges         = 0
0.00.017.956 I llm_load_print_meta: vocab_only       = 0
0.00.017.956 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.956 I llm_load_print_meta: n_embd           = 384
0.00.017.956 I llm_load_print_meta: n_layer          = 12
0.00.017.959 I llm_load_print_meta: n_head           = 12
0.00.017.959 I llm_load_print_meta: n_head_kv        = 12
0.00.017.959 I llm_load_print_meta: n_rot            = 32
0.00.017.960 I llm_load_print_meta: n_swa            = 0
0.00.017.960 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.961 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.962 I llm_load_print_meta: n_gqa            = 1
0.00.017.963 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.963 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.964 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.964 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.964 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.964 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.966 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.967 I llm_load_print_meta: n_ff             = 1536
0.00.017.967 I llm_load_print_meta: n_expert         = 0
0.00.017.967 I llm_load_print_meta: n_expert_used    = 0
0.00.017.967 I llm_load_print_meta: causal attn      = 0
0.00.017.967 I llm_load_print_meta: pooling type     = 2
0.00.017.967 I llm_load_print_meta: rope type        = 2
0.00.017.968 I llm_load_print_meta: rope scaling     = linear
0.00.017.968 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.968 I llm_load_print_meta: freq_scale_train = 1
0.00.017.968 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.969 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.969 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.969 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.969 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.969 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.969 I llm_load_print_meta: model type       = 33M
0.00.017.970 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.970 I llm_load_print_meta: model params     = 33.21 M
0.00.017.970 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.971 I llm_load_print_meta: general.name     = Bge Small
0.00.017.971 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.971 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.971 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.972 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.972 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.972 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.972 I llm_load_print_meta: max token length = 21
0.00.019.307 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.309 I llm_load_tensors: offloading output layer to GPU
0.00.019.309 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.317 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.318 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.693 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.694 I llama_new_context_with_model: n_ctx         = 512
0.00.019.694 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.694 I llama_new_context_with_model: n_batch       = 2048
0.00.019.694 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.695 I llama_new_context_with_model: flash_attn    = 0
0.00.019.695 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.695 I llama_new_context_with_model: freq_scale    = 1
0.00.019.696 I ggml_metal_init: allocating
0.00.019.701 I ggml_metal_init: found device: Apple M4
0.00.019.703 I ggml_metal_init: picking default device: Apple M4
0.00.020.334 I ggml_metal_init: using embedded metal library
0.00.022.875 I ggml_metal_init: GPU name:   Apple M4
0.00.022.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.879 I ggml_metal_init: simdgroup reduction   = true
0.00.022.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.879 I ggml_metal_init: has bfloat            = true
0.00.022.880 I ggml_metal_init: use bfloat            = true
0.00.022.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.382 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.032.877 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.880 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.881 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.460 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.461 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.461 I llama_new_context_with_model: graph nodes  = 429
0.00.033.462 I llama_new_context_with_model: graph splits = 2
0.00.033.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.475 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.532 I 
0.00.038.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.091 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.574 I llama_perf_context_print:        load time =      29.50 ms
0.00.043.575 I llama_perf_context_print: prompt eval time =       4.36 ms /     9 tokens (    0.48 ms per token,  2066.59 tokens per second)
0.00.043.576 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.577 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.043.759 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.029s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.153 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.501 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.981 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.989 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.991 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.992 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.993 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.994 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.995 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.996 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.996 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.997 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.003 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.003 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.004 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.008 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.902 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.902 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.902 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.903 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.903 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.904 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.904 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.904 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.905 I llama_model_loader: - type  f32:   40 tensors
0.00.050.906 I llama_model_loader: - type  f16:   30 tensors
0.00.069.739 W llm_load_vocab: empty token at index 5
0.00.074.635 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.076.075 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.076.108 I llm_load_vocab: special tokens cache size = 5
0.00.337.437 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.337.442 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.337.443 I llm_load_print_meta: arch             = jina-bert-v2
0.00.337.444 I llm_load_print_meta: vocab type       = BPE
0.00.337.444 I llm_load_print_meta: n_vocab          = 61056
0.00.337.444 I llm_load_print_meta: n_merges         = 39382
0.00.337.447 I llm_load_print_meta: vocab_only       = 0
0.00.337.448 I llm_load_print_meta: n_ctx_train      = 8192
0.00.337.448 I llm_load_print_meta: n_embd           = 384
0.00.337.448 I llm_load_print_meta: n_layer          = 4
0.00.337.455 I llm_load_print_meta: n_head           = 12
0.00.337.456 I llm_load_print_meta: n_head_kv        = 12
0.00.337.457 I llm_load_print_meta: n_rot            = 32
0.00.337.457 I llm_load_print_meta: n_swa            = 0
0.00.337.457 I llm_load_print_meta: n_embd_head_k    = 32
0.00.337.457 I llm_load_print_meta: n_embd_head_v    = 32
0.00.337.458 I llm_load_print_meta: n_gqa            = 1
0.00.337.460 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.337.460 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.337.461 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.337.462 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.337.462 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.337.463 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.337.463 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.337.463 I llm_load_print_meta: n_ff             = 1536
0.00.337.464 I llm_load_print_meta: n_expert         = 0
0.00.337.464 I llm_load_print_meta: n_expert_used    = 0
0.00.337.464 I llm_load_print_meta: causal attn      = 0
0.00.337.464 I llm_load_print_meta: pooling type     = -1
0.00.337.465 I llm_load_print_meta: rope type        = -1
0.00.337.465 I llm_load_print_meta: rope scaling     = linear
0.00.337.466 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.337.466 I llm_load_print_meta: freq_scale_train = 1
0.00.337.466 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.337.466 I llm_load_print_meta: rope_finetuned   = unknown
0.00.337.467 I llm_load_print_meta: ssm_d_conv       = 0
0.00.337.467 I llm_load_print_meta: ssm_d_inner      = 0
0.00.337.467 I llm_load_print_meta: ssm_d_state      = 0
0.00.337.467 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.337.467 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.337.468 I llm_load_print_meta: model type       = 33M
0.00.337.468 I llm_load_print_meta: model ftype      = F16
0.00.337.468 I llm_load_print_meta: model params     = 32.90 M
0.00.337.469 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.337.469 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.337.469 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.337.469 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.337.470 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.337.470 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.337.474 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.337.474 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.337.474 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.337.475 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.337.475 I llm_load_print_meta: max token length = 45
0.00.338.607 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.338.607 I llm_load_tensors: offloading output layer to GPU
0.00.338.608 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.338.634 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.636 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.339.498 I llama_new_context_with_model: n_seq_max     = 1
0.00.339.499 I llama_new_context_with_model: n_ctx         = 8192
0.00.339.499 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.339.500 I llama_new_context_with_model: n_batch       = 2048
0.00.339.500 I llama_new_context_with_model: n_ubatch      = 2048
0.00.339.500 I llama_new_context_with_model: flash_attn    = 0
0.00.339.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.339.500 I llama_new_context_with_model: freq_scale    = 1
0.00.339.501 I ggml_metal_init: allocating
0.00.339.504 I ggml_metal_init: found device: Apple M4
0.00.339.506 I ggml_metal_init: picking default device: Apple M4
0.00.340.524 I ggml_metal_init: using embedded metal library
0.00.343.497 I ggml_metal_init: GPU name:   Apple M4
0.00.343.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.499 I ggml_metal_init: simdgroup reduction   = true
0.00.343.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.500 I ggml_metal_init: has bfloat            = true
0.00.343.500 I ggml_metal_init: use bfloat            = true
0.00.343.500 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.175 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.355.704 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.355.706 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.355.708 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.356.351 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.356.352 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.356.352 I llama_new_context_with_model: graph nodes  = 154
0.00.356.352 I llama_new_context_with_model: graph splits = 2
0.00.356.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.356.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.929 I 
0.00.368.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.369.219 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.369.221 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.369.225 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.369.226 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.369.229 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.369.231 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.369.754 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.373.457 I llama_perf_context_print:        load time =     345.42 ms
0.00.373.458 I llama_perf_context_print: prompt eval time =       3.69 ms /    62 tokens (    0.06 ms per token, 16783.97 tokens per second)
0.00.373.459 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.373.460 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.373.658 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.344s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.113 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.224 I main: llama backend init
0.00.000.230 I main: load the model and apply lora adapter, if any
0.00.077.095 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.096.514 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.096.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.096.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.096.530 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.096.531 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.096.532 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.096.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.096.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.096.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.096.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.096.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.096.539 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.096.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.096.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.096.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.096.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.096.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.105.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.107.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.116.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.116.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.116.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.116.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.116.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.116.819 I llama_model_loader: - type  f32:  194 tensors
0.00.116.820 I llama_model_loader: - type  f16:   98 tensors
0.00.151.729 I llm_load_vocab: special tokens cache size = 25
0.00.158.880 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.158.883 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.158.883 I llm_load_print_meta: arch             = gptneox
0.00.158.884 I llm_load_print_meta: vocab type       = BPE
0.00.158.884 I llm_load_print_meta: n_vocab          = 50304
0.00.158.884 I llm_load_print_meta: n_merges         = 50009
0.00.158.884 I llm_load_print_meta: vocab_only       = 0
0.00.158.884 I llm_load_print_meta: n_ctx_train      = 2048
0.00.158.884 I llm_load_print_meta: n_embd           = 2048
0.00.158.885 I llm_load_print_meta: n_layer          = 24
0.00.158.888 I llm_load_print_meta: n_head           = 16
0.00.158.889 I llm_load_print_meta: n_head_kv        = 16
0.00.158.889 I llm_load_print_meta: n_rot            = 32
0.00.158.889 I llm_load_print_meta: n_swa            = 0
0.00.158.889 I llm_load_print_meta: n_embd_head_k    = 128
0.00.158.889 I llm_load_print_meta: n_embd_head_v    = 128
0.00.158.890 I llm_load_print_meta: n_gqa            = 1
0.00.158.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.158.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.158.892 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.158.893 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.158.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.158.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.158.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.158.894 I llm_load_print_meta: n_ff             = 8192
0.00.158.894 I llm_load_print_meta: n_expert         = 0
0.00.158.894 I llm_load_print_meta: n_expert_used    = 0
0.00.158.894 I llm_load_print_meta: causal attn      = 1
0.00.158.894 I llm_load_print_meta: pooling type     = 0
0.00.158.894 I llm_load_print_meta: rope type        = 2
0.00.158.894 I llm_load_print_meta: rope scaling     = linear
0.00.158.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.158.895 I llm_load_print_meta: freq_scale_train = 1
0.00.158.897 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.158.897 I llm_load_print_meta: rope_finetuned   = unknown
0.00.158.897 I llm_load_print_meta: ssm_d_conv       = 0
0.00.158.897 I llm_load_print_meta: ssm_d_inner      = 0
0.00.158.897 I llm_load_print_meta: ssm_d_state      = 0
0.00.158.897 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.158.898 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.158.898 I llm_load_print_meta: model type       = 1.4B
0.00.158.898 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.158.899 I llm_load_print_meta: model params     = 1.41 B
0.00.158.899 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.158.899 I llm_load_print_meta: general.name     = 1.4B
0.00.158.900 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.158.900 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.158.900 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.158.900 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.158.900 I llm_load_print_meta: LF token         = 128 ''
0.00.158.901 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.158.901 I llm_load_print_meta: max token length = 1024
0.00.160.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.160.997 I llm_load_tensors: offloading output layer to GPU
0.00.160.997 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.161.015 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.161.016 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.161.987 I llama_new_context_with_model: n_seq_max     = 1
0.00.161.988 I llama_new_context_with_model: n_ctx         = 2048
0.00.161.988 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.161.988 I llama_new_context_with_model: n_batch       = 2048
0.00.161.988 I llama_new_context_with_model: n_ubatch      = 512
0.00.161.989 I llama_new_context_with_model: flash_attn    = 0
0.00.161.989 I llama_new_context_with_model: freq_base     = 10000.0
0.00.161.989 I llama_new_context_with_model: freq_scale    = 1
0.00.161.990 I ggml_metal_init: allocating
0.00.162.000 I ggml_metal_init: found device: Apple M4
0.00.162.002 I ggml_metal_init: picking default device: Apple M4
0.00.162.734 I ggml_metal_init: using embedded metal library
0.00.282.902 I ggml_metal_init: GPU name:   Apple M4
0.00.282.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.282.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.282.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.282.906 I ggml_metal_init: simdgroup reduction   = true
0.00.282.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.282.906 I ggml_metal_init: has bfloat            = true
0.00.282.906 I ggml_metal_init: use bfloat            = true
0.00.282.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.282.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.351.299 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.389.925 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.389.932 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.389.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.391.499 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.391.501 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.391.502 I llama_new_context_with_model: graph nodes  = 967
0.00.391.502 I llama_new_context_with_model: graph splits = 2
0.00.391.529 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.391.767 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.391.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.710 I main: llama threadpool init, n_threads = 4
0.00.471.744 I 
0.00.471.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.781 I 
0.00.471.855 I sampler seed: 1234
0.00.471.860 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.471.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.471.887 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.471.887 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.274.624 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.02.274.625 I llama_perf_context_print:        load time =     394.60 ms
0.02.274.626 I llama_perf_context_print: prompt eval time =      44.05 ms /     7 tokens (    6.29 ms per token,   158.90 tokens per second)
0.02.274.627 I llama_perf_context_print:        eval time =    1755.71 ms /    63 runs   (   27.87 ms per token,    35.88 tokens per second)
0.02.274.627 I llama_perf_context_print:       total time =    1802.92 ms /    70 tokens
0.02.274.866 I ggml_metal_free: deallocating

real	0m2.594s
user	0m0.183s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.753 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.239 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.112 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.525 I llama_model_loader: - type  f32:  194 tensors
0.00.051.526 I llama_model_loader: - type  f16:   98 tensors
0.00.078.879 I llm_load_vocab: special tokens cache size = 25
0.00.085.263 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.266 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.266 I llm_load_print_meta: arch             = gptneox
0.00.085.266 I llm_load_print_meta: vocab type       = BPE
0.00.085.266 I llm_load_print_meta: n_vocab          = 50304
0.00.085.267 I llm_load_print_meta: n_merges         = 50009
0.00.085.267 I llm_load_print_meta: vocab_only       = 0
0.00.085.267 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.267 I llm_load_print_meta: n_embd           = 2048
0.00.085.267 I llm_load_print_meta: n_layer          = 24
0.00.085.269 I llm_load_print_meta: n_head           = 16
0.00.085.270 I llm_load_print_meta: n_head_kv        = 16
0.00.085.270 I llm_load_print_meta: n_rot            = 32
0.00.085.270 I llm_load_print_meta: n_swa            = 0
0.00.085.271 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.271 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.271 I llm_load_print_meta: n_gqa            = 1
0.00.085.272 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.273 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.273 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.274 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.274 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.274 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.274 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.275 I llm_load_print_meta: n_ff             = 8192
0.00.085.275 I llm_load_print_meta: n_expert         = 0
0.00.085.275 I llm_load_print_meta: n_expert_used    = 0
0.00.085.275 I llm_load_print_meta: causal attn      = 1
0.00.085.276 I llm_load_print_meta: pooling type     = 0
0.00.085.276 I llm_load_print_meta: rope type        = 2
0.00.085.276 I llm_load_print_meta: rope scaling     = linear
0.00.085.276 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.277 I llm_load_print_meta: freq_scale_train = 1
0.00.085.277 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.277 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.277 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.278 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.278 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.278 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.278 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.278 I llm_load_print_meta: model type       = 1.4B
0.00.085.279 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.279 I llm_load_print_meta: model params     = 1.41 B
0.00.085.280 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.280 I llm_load_print_meta: general.name     = 1.4B
0.00.085.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.282 I llm_load_print_meta: LF token         = 128 ''
0.00.085.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.282 I llm_load_print_meta: max token length = 1024
0.00.087.190 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.190 I llm_load_tensors: offloading output layer to GPU
0.00.087.190 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.200 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.202 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.084 I llama_new_context_with_model: n_ctx         = 128
0.00.088.084 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.084 I llama_new_context_with_model: n_batch       = 128
0.00.088.084 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.084 I llama_new_context_with_model: flash_attn    = 0
0.00.088.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.085 I llama_new_context_with_model: freq_scale    = 1
0.00.088.085 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.086 I ggml_metal_init: allocating
0.00.088.089 I ggml_metal_init: found device: Apple M4
0.00.088.091 I ggml_metal_init: picking default device: Apple M4
0.00.088.698 I ggml_metal_init: using embedded metal library
0.00.091.201 I ggml_metal_init: GPU name:   Apple M4
0.00.091.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.204 I ggml_metal_init: simdgroup reduction   = true
0.00.091.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.204 I ggml_metal_init: has bfloat            = true
0.00.091.204 I ggml_metal_init: use bfloat            = true
0.00.091.205 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.317 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.101.631 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.634 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.647 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.506 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.507 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.508 I llama_new_context_with_model: graph nodes  = 967
0.00.102.508 I llama_new_context_with_model: graph splits = 2
0.00.102.520 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.157.456 I 
0.01.157.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.157.519 I perplexity: tokenizing the input ..
0.01.167.025 I perplexity: tokenization took 9.504 ms
0.01.167.029 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.287.930 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.289.578 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.289.599 I llama_perf_context_print:        load time =    1135.21 ms
0.01.289.601 I llama_perf_context_print: prompt eval time =     120.61 ms /   128 tokens (    0.94 ms per token,  1061.24 tokens per second)
0.01.289.603 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.289.603 I llama_perf_context_print:       total time =     132.15 ms /   129 tokens
0.01.290.125 I ggml_metal_free: deallocating

real	0m1.486s
user	0m0.121s
sys	0m0.266s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.848 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.972 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.974 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.787 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.606 I llama_model_loader: - type  f32:  194 tensors
0.00.027.606 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.766 I llm_load_vocab: special tokens cache size = 25
0.00.055.868 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.873 I llm_load_print_meta: arch             = gptneox
0.00.055.873 I llm_load_print_meta: vocab type       = BPE
0.00.055.873 I llm_load_print_meta: n_vocab          = 50304
0.00.055.873 I llm_load_print_meta: n_merges         = 50009
0.00.055.874 I llm_load_print_meta: vocab_only       = 0
0.00.055.874 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.874 I llm_load_print_meta: n_embd           = 2048
0.00.055.874 I llm_load_print_meta: n_layer          = 24
0.00.055.880 I llm_load_print_meta: n_head           = 16
0.00.055.880 I llm_load_print_meta: n_head_kv        = 16
0.00.055.880 I llm_load_print_meta: n_rot            = 32
0.00.055.884 I llm_load_print_meta: n_swa            = 0
0.00.055.884 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.884 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.885 I llm_load_print_meta: n_gqa            = 1
0.00.055.886 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.887 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.887 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.888 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.888 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.888 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.890 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.890 I llm_load_print_meta: n_ff             = 8192
0.00.055.890 I llm_load_print_meta: n_expert         = 0
0.00.055.891 I llm_load_print_meta: n_expert_used    = 0
0.00.055.891 I llm_load_print_meta: causal attn      = 1
0.00.055.892 I llm_load_print_meta: pooling type     = 0
0.00.055.893 I llm_load_print_meta: rope type        = 2
0.00.055.893 I llm_load_print_meta: rope scaling     = linear
0.00.055.894 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.894 I llm_load_print_meta: freq_scale_train = 1
0.00.055.894 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.894 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.895 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.895 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.895 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.895 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.896 I llm_load_print_meta: model type       = 1.4B
0.00.055.899 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.900 I llm_load_print_meta: model params     = 1.41 B
0.00.055.900 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.900 I llm_load_print_meta: general.name     = 1.4B
0.00.055.901 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.901 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.901 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.902 I llm_load_print_meta: LF token         = 128 ''
0.00.055.902 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.902 I llm_load_print_meta: max token length = 1024
0.00.058.030 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.031 I llm_load_tensors: offloading output layer to GPU
0.00.058.031 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.042 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.043 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.924 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.925 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.925 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.925 I llama_new_context_with_model: n_batch       = 2048
0.00.058.926 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.926 I llama_new_context_with_model: flash_attn    = 0
0.00.058.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.926 I llama_new_context_with_model: freq_scale    = 1
0.00.058.927 I ggml_metal_init: allocating
0.00.058.934 I ggml_metal_init: found device: Apple M4
0.00.058.937 I ggml_metal_init: picking default device: Apple M4
0.00.059.684 I ggml_metal_init: using embedded metal library
0.00.062.329 I ggml_metal_init: GPU name:   Apple M4
0.00.062.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.331 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.332 I ggml_metal_init: simdgroup reduction   = true
0.00.062.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.332 I ggml_metal_init: has bfloat            = true
0.00.062.332 I ggml_metal_init: use bfloat            = true
0.00.062.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.875 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.097.678 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.719 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.729 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.731 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.731 I llama_new_context_with_model: graph nodes  = 967
0.00.098.731 I llama_new_context_with_model: graph splits = 2
0.00.098.749 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.409.227 I main: llama threadpool init, n_threads = 4
0.01.409.266 I 
0.01.409.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.409.296 I 
0.01.409.472 I sampler seed: 1234
0.01.409.476 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.409.512 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.409.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.409.514 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.494.918 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.02.494.919 I llama_perf_context_print:        load time =    1399.37 ms
0.02.494.920 I llama_perf_context_print: prompt eval time =      40.22 ms /     7 tokens (    5.75 ms per token,   174.03 tokens per second)
0.02.494.920 I llama_perf_context_print:        eval time =    1042.38 ms /    63 runs   (   16.55 ms per token,    60.44 tokens per second)
0.02.494.921 I llama_perf_context_print:       total time =    1085.70 ms /    70 tokens
0.02.495.107 I ggml_metal_free: deallocating

real	0m2.512s
user	0m0.113s
sys	0m0.271s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.475 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.381 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.393 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.393 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.394 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.394 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.236 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.241 I llama_model_loader: - type  f32:  194 tensors
0.00.034.241 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.661 I llm_load_vocab: special tokens cache size = 25
0.00.064.557 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.559 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.560 I llm_load_print_meta: arch             = gptneox
0.00.064.560 I llm_load_print_meta: vocab type       = BPE
0.00.064.560 I llm_load_print_meta: n_vocab          = 50304
0.00.064.560 I llm_load_print_meta: n_merges         = 50009
0.00.064.560 I llm_load_print_meta: vocab_only       = 0
0.00.064.561 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.561 I llm_load_print_meta: n_embd           = 2048
0.00.064.561 I llm_load_print_meta: n_layer          = 24
0.00.064.564 I llm_load_print_meta: n_head           = 16
0.00.064.565 I llm_load_print_meta: n_head_kv        = 16
0.00.064.565 I llm_load_print_meta: n_rot            = 32
0.00.064.565 I llm_load_print_meta: n_swa            = 0
0.00.064.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.565 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.566 I llm_load_print_meta: n_gqa            = 1
0.00.064.567 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.568 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.568 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.569 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.569 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.569 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.569 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.570 I llm_load_print_meta: n_ff             = 8192
0.00.064.570 I llm_load_print_meta: n_expert         = 0
0.00.064.570 I llm_load_print_meta: n_expert_used    = 0
0.00.064.570 I llm_load_print_meta: causal attn      = 1
0.00.064.571 I llm_load_print_meta: pooling type     = 0
0.00.064.571 I llm_load_print_meta: rope type        = 2
0.00.064.571 I llm_load_print_meta: rope scaling     = linear
0.00.064.571 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.572 I llm_load_print_meta: freq_scale_train = 1
0.00.064.572 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.572 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.572 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.572 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.572 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.573 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.573 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.573 I llm_load_print_meta: model type       = 1.4B
0.00.064.576 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.577 I llm_load_print_meta: model params     = 1.41 B
0.00.064.577 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.578 I llm_load_print_meta: general.name     = 1.4B
0.00.064.578 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.578 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.578 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.578 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.579 I llm_load_print_meta: LF token         = 128 ''
0.00.064.579 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.579 I llm_load_print_meta: max token length = 1024
0.00.066.361 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.361 I llm_load_tensors: offloading output layer to GPU
0.00.066.362 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.372 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.373 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.225 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.226 I llama_new_context_with_model: n_ctx         = 128
0.00.067.226 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.226 I llama_new_context_with_model: n_batch       = 128
0.00.067.226 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.226 I llama_new_context_with_model: flash_attn    = 0
0.00.067.227 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.227 I llama_new_context_with_model: freq_scale    = 1
0.00.067.227 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.228 I ggml_metal_init: allocating
0.00.067.234 I ggml_metal_init: found device: Apple M4
0.00.067.237 I ggml_metal_init: picking default device: Apple M4
0.00.067.887 I ggml_metal_init: using embedded metal library
0.00.070.347 I ggml_metal_init: GPU name:   Apple M4
0.00.070.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.348 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.349 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.349 I ggml_metal_init: simdgroup reduction   = true
0.00.070.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.349 I ggml_metal_init: has bfloat            = true
0.00.070.349 I ggml_metal_init: use bfloat            = true
0.00.070.350 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.630 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.080.934 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.941 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.957 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.884 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.886 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.886 I llama_new_context_with_model: graph nodes  = 967
0.00.081.886 I llama_new_context_with_model: graph splits = 2
0.00.081.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.022.430 I 
0.01.022.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.022.498 I perplexity: tokenizing the input ..
0.01.030.052 I perplexity: tokenization took 7.553 ms
0.01.030.055 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.154.472 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.155.550 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.155.562 I llama_perf_context_print:        load time =    1009.95 ms
0.01.155.563 I llama_perf_context_print: prompt eval time =     124.17 ms /   128 tokens (    0.97 ms per token,  1030.86 tokens per second)
0.01.155.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.155.564 I llama_perf_context_print:       total time =     133.13 ms /   129 tokens
0.01.155.982 I ggml_metal_free: deallocating

real	0m1.174s
user	0m0.092s
sys	0m0.204s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.016.967 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.638 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.646 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.653 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.653 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.654 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.742 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.470 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.470 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.471 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.471 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.472 I llama_model_loader: - type  f32:  194 tensors
0.00.043.472 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.565 I llm_load_vocab: special tokens cache size = 25
0.00.080.997 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.081.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.081.003 I llm_load_print_meta: arch             = gptneox
0.00.081.003 I llm_load_print_meta: vocab type       = BPE
0.00.081.004 I llm_load_print_meta: n_vocab          = 50304
0.00.081.004 I llm_load_print_meta: n_merges         = 50009
0.00.081.004 I llm_load_print_meta: vocab_only       = 0
0.00.081.004 I llm_load_print_meta: n_ctx_train      = 2048
0.00.081.005 I llm_load_print_meta: n_embd           = 2048
0.00.081.008 I llm_load_print_meta: n_layer          = 24
0.00.081.012 I llm_load_print_meta: n_head           = 16
0.00.081.013 I llm_load_print_meta: n_head_kv        = 16
0.00.081.015 I llm_load_print_meta: n_rot            = 32
0.00.081.015 I llm_load_print_meta: n_swa            = 0
0.00.081.015 I llm_load_print_meta: n_embd_head_k    = 128
0.00.081.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.081.016 I llm_load_print_meta: n_gqa            = 1
0.00.081.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.081.026 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.081.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.081.027 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.081.027 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.081.028 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.081.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.081.029 I llm_load_print_meta: n_ff             = 8192
0.00.081.029 I llm_load_print_meta: n_expert         = 0
0.00.081.029 I llm_load_print_meta: n_expert_used    = 0
0.00.081.030 I llm_load_print_meta: causal attn      = 1
0.00.081.030 I llm_load_print_meta: pooling type     = 0
0.00.081.030 I llm_load_print_meta: rope type        = 2
0.00.081.030 I llm_load_print_meta: rope scaling     = linear
0.00.081.031 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.081.031 I llm_load_print_meta: freq_scale_train = 1
0.00.081.032 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.081.032 I llm_load_print_meta: rope_finetuned   = unknown
0.00.081.032 I llm_load_print_meta: ssm_d_conv       = 0
0.00.081.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.081.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.081.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.081.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.081.034 I llm_load_print_meta: model type       = 1.4B
0.00.081.034 I llm_load_print_meta: model ftype      = Q4_0
0.00.081.035 I llm_load_print_meta: model params     = 1.41 B
0.00.081.035 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.081.036 I llm_load_print_meta: general.name     = 1.4B
0.00.081.038 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.081.038 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.081.038 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.081.039 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.081.039 I llm_load_print_meta: LF token         = 128 ''
0.00.081.039 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.081.040 I llm_load_print_meta: max token length = 1024
0.00.083.978 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.979 I llm_load_tensors: offloading output layer to GPU
0.00.083.979 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.991 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.083.992 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.085.446 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.448 I llama_new_context_with_model: n_ctx         = 2048
0.00.085.448 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.085.448 I llama_new_context_with_model: n_batch       = 2048
0.00.085.449 I llama_new_context_with_model: n_ubatch      = 512
0.00.085.449 I llama_new_context_with_model: flash_attn    = 0
0.00.085.450 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.450 I llama_new_context_with_model: freq_scale    = 1
0.00.085.451 I ggml_metal_init: allocating
0.00.085.459 I ggml_metal_init: found device: Apple M4
0.00.085.462 I ggml_metal_init: picking default device: Apple M4
0.00.086.416 I ggml_metal_init: using embedded metal library
0.00.090.282 I ggml_metal_init: GPU name:   Apple M4
0.00.090.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.285 I ggml_metal_init: simdgroup reduction   = true
0.00.090.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.286 I ggml_metal_init: has bfloat            = true
0.00.090.286 I ggml_metal_init: use bfloat            = true
0.00.090.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.719 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.127.502 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.511 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.127.537 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.128.663 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.128.665 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.128.665 I llama_new_context_with_model: graph nodes  = 967
0.00.128.665 I llama_new_context_with_model: graph splits = 2
0.00.128.682 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.128.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.128.827 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.826.417 I main: llama threadpool init, n_threads = 4
0.00.826.503 I 
0.00.826.562 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.826.564 I 
0.00.826.845 I sampler seed: 1234
0.00.826.853 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.826.869 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.826.872 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.826.872 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.516.671 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.516.671 I llama_perf_context_print:        load time =     809.44 ms
0.01.516.672 I llama_perf_context_print: prompt eval time =      48.88 ms /     7 tokens (    6.98 ms per token,   143.20 tokens per second)
0.01.516.673 I llama_perf_context_print:        eval time =     637.91 ms /    63 runs   (   10.13 ms per token,    98.76 tokens per second)
0.01.516.673 I llama_perf_context_print:       total time =     690.26 ms /    70 tokens
0.01.516.851 I ggml_metal_free: deallocating

real	0m1.538s
user	0m0.131s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.390 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.967 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.972 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.978 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.978 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.979 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.979 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.979 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.981 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.566 I llama_model_loader: - type  f32:  194 tensors
0.00.023.566 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.566 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.322 I llm_load_vocab: special tokens cache size = 25
0.00.050.239 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.242 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.242 I llm_load_print_meta: arch             = gptneox
0.00.050.242 I llm_load_print_meta: vocab type       = BPE
0.00.050.243 I llm_load_print_meta: n_vocab          = 50304
0.00.050.243 I llm_load_print_meta: n_merges         = 50009
0.00.050.243 I llm_load_print_meta: vocab_only       = 0
0.00.050.243 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.243 I llm_load_print_meta: n_embd           = 2048
0.00.050.244 I llm_load_print_meta: n_layer          = 24
0.00.050.246 I llm_load_print_meta: n_head           = 16
0.00.050.246 I llm_load_print_meta: n_head_kv        = 16
0.00.050.247 I llm_load_print_meta: n_rot            = 32
0.00.050.247 I llm_load_print_meta: n_swa            = 0
0.00.050.247 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.247 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.248 I llm_load_print_meta: n_gqa            = 1
0.00.050.249 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.249 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.250 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.250 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.251 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.252 I llm_load_print_meta: n_ff             = 8192
0.00.050.252 I llm_load_print_meta: n_expert         = 0
0.00.050.254 I llm_load_print_meta: n_expert_used    = 0
0.00.050.255 I llm_load_print_meta: causal attn      = 1
0.00.050.255 I llm_load_print_meta: pooling type     = 0
0.00.050.255 I llm_load_print_meta: rope type        = 2
0.00.050.255 I llm_load_print_meta: rope scaling     = linear
0.00.050.255 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.256 I llm_load_print_meta: freq_scale_train = 1
0.00.050.256 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.256 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.256 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.257 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.257 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.257 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.257 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.257 I llm_load_print_meta: model type       = 1.4B
0.00.050.258 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.259 I llm_load_print_meta: model params     = 1.41 B
0.00.050.260 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.260 I llm_load_print_meta: general.name     = 1.4B
0.00.050.260 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.260 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.261 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.261 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.261 I llm_load_print_meta: LF token         = 128 ''
0.00.050.261 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.262 I llm_load_print_meta: max token length = 1024
0.00.051.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.963 I llm_load_tensors: offloading output layer to GPU
0.00.051.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.973 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.973 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.798 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.799 I llama_new_context_with_model: n_ctx         = 128
0.00.052.799 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.799 I llama_new_context_with_model: n_batch       = 128
0.00.052.800 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.800 I llama_new_context_with_model: flash_attn    = 0
0.00.052.800 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.800 I llama_new_context_with_model: freq_scale    = 1
0.00.052.801 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.801 I ggml_metal_init: allocating
0.00.052.804 I ggml_metal_init: found device: Apple M4
0.00.052.806 I ggml_metal_init: picking default device: Apple M4
0.00.053.362 I ggml_metal_init: using embedded metal library
0.00.055.677 I ggml_metal_init: GPU name:   Apple M4
0.00.055.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.679 I ggml_metal_init: simdgroup reduction   = true
0.00.055.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.680 I ggml_metal_init: has bfloat            = true
0.00.055.680 I ggml_metal_init: use bfloat            = true
0.00.055.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.129 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.352 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.356 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.371 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.272 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.273 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.274 I llama_new_context_with_model: graph nodes  = 967
0.00.067.274 I llama_new_context_with_model: graph splits = 2
0.00.067.286 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.287 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.551 I 
0.00.673.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.599 I perplexity: tokenizing the input ..
0.00.680.853 I perplexity: tokenization took 7.253 ms
0.00.680.859 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.843 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.804.936 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.804.953 I llama_perf_context_print:        load time =     664.16 ms
0.00.804.954 I llama_perf_context_print: prompt eval time =     122.76 ms /   128 tokens (    0.96 ms per token,  1042.65 tokens per second)
0.00.804.955 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.956 I llama_perf_context_print:       total time =     131.40 ms /   129 tokens
0.00.805.408 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.078s
sys	0m0.127s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.241 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.032.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.190 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.191 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.192 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.240 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.306 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.679 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.681 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.682 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.041.683 I llama_model_loader: - type  f32:  194 tensors
0.00.041.683 I llama_model_loader: - type q4_1:   97 tensors
0.00.041.683 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.074 I llm_load_vocab: special tokens cache size = 25
0.00.076.272 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.275 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.276 I llm_load_print_meta: arch             = gptneox
0.00.076.276 I llm_load_print_meta: vocab type       = BPE
0.00.076.276 I llm_load_print_meta: n_vocab          = 50304
0.00.076.276 I llm_load_print_meta: n_merges         = 50009
0.00.076.277 I llm_load_print_meta: vocab_only       = 0
0.00.076.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.277 I llm_load_print_meta: n_embd           = 2048
0.00.076.277 I llm_load_print_meta: n_layer          = 24
0.00.076.280 I llm_load_print_meta: n_head           = 16
0.00.076.281 I llm_load_print_meta: n_head_kv        = 16
0.00.076.281 I llm_load_print_meta: n_rot            = 32
0.00.076.281 I llm_load_print_meta: n_swa            = 0
0.00.076.281 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.282 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.283 I llm_load_print_meta: n_gqa            = 1
0.00.076.284 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.284 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.285 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.285 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.285 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.286 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.286 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.287 I llm_load_print_meta: n_ff             = 8192
0.00.076.287 I llm_load_print_meta: n_expert         = 0
0.00.076.287 I llm_load_print_meta: n_expert_used    = 0
0.00.076.287 I llm_load_print_meta: causal attn      = 1
0.00.076.287 I llm_load_print_meta: pooling type     = 0
0.00.076.287 I llm_load_print_meta: rope type        = 2
0.00.076.288 I llm_load_print_meta: rope scaling     = linear
0.00.076.288 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.289 I llm_load_print_meta: freq_scale_train = 1
0.00.076.289 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.289 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.290 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.290 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.290 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.290 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.290 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.290 I llm_load_print_meta: model type       = 1.4B
0.00.076.291 I llm_load_print_meta: model ftype      = Q4_1
0.00.076.291 I llm_load_print_meta: model params     = 1.41 B
0.00.076.292 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.076.292 I llm_load_print_meta: general.name     = 1.4B
0.00.076.292 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.293 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.293 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.293 I llm_load_print_meta: LF token         = 128 ''
0.00.076.294 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.294 I llm_load_print_meta: max token length = 1024
0.00.078.571 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.571 I llm_load_tensors: offloading output layer to GPU
0.00.078.572 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.582 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.078.583 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.079.661 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.661 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.662 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.662 I llama_new_context_with_model: n_batch       = 2048
0.00.079.662 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.662 I llama_new_context_with_model: flash_attn    = 0
0.00.079.663 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.663 I llama_new_context_with_model: freq_scale    = 1
0.00.079.664 I ggml_metal_init: allocating
0.00.079.670 I ggml_metal_init: found device: Apple M4
0.00.079.674 I ggml_metal_init: picking default device: Apple M4
0.00.080.389 I ggml_metal_init: using embedded metal library
0.00.083.555 I ggml_metal_init: GPU name:   Apple M4
0.00.083.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.559 I ggml_metal_init: simdgroup reduction   = true
0.00.083.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.559 I ggml_metal_init: has bfloat            = true
0.00.083.559 I ggml_metal_init: use bfloat            = true
0.00.083.560 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.695 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.118.571 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.577 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.595 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.599 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.600 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.600 I llama_new_context_with_model: graph nodes  = 967
0.00.119.600 I llama_new_context_with_model: graph splits = 2
0.00.119.616 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.749 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.910.797 I main: llama threadpool init, n_threads = 4
0.00.910.844 I 
0.00.910.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.910.874 I 
0.00.911.035 I sampler seed: 1234
0.00.911.039 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.911.049 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.911.049 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.911.049 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.639.293 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66854.99 tokens per second)
0.01.639.293 I llama_perf_context_print:        load time =     901.55 ms
0.01.639.294 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.62 tokens per second)
0.01.639.295 I llama_perf_context_print:        eval time =     681.50 ms /    63 runs   (   10.82 ms per token,    92.44 tokens per second)
0.01.639.295 I llama_perf_context_print:       total time =     728.50 ms /    70 tokens
0.01.639.433 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.124s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.887 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.692 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.693 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.694 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.694 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.695 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.697 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.699 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.700 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.064 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.065 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.065 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.066 I llama_model_loader: - type  f32:  194 tensors
0.00.023.066 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.066 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.049 I llm_load_vocab: special tokens cache size = 25
0.00.048.954 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.958 I llm_load_print_meta: arch             = gptneox
0.00.048.958 I llm_load_print_meta: vocab type       = BPE
0.00.048.958 I llm_load_print_meta: n_vocab          = 50304
0.00.048.958 I llm_load_print_meta: n_merges         = 50009
0.00.048.959 I llm_load_print_meta: vocab_only       = 0
0.00.048.959 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.959 I llm_load_print_meta: n_embd           = 2048
0.00.048.959 I llm_load_print_meta: n_layer          = 24
0.00.048.962 I llm_load_print_meta: n_head           = 16
0.00.048.962 I llm_load_print_meta: n_head_kv        = 16
0.00.048.963 I llm_load_print_meta: n_rot            = 32
0.00.048.965 I llm_load_print_meta: n_swa            = 0
0.00.048.965 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.965 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.966 I llm_load_print_meta: n_gqa            = 1
0.00.048.968 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.969 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.970 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.970 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.970 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.970 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.971 I llm_load_print_meta: n_ff             = 8192
0.00.048.971 I llm_load_print_meta: n_expert         = 0
0.00.048.971 I llm_load_print_meta: n_expert_used    = 0
0.00.048.972 I llm_load_print_meta: causal attn      = 1
0.00.048.972 I llm_load_print_meta: pooling type     = 0
0.00.048.972 I llm_load_print_meta: rope type        = 2
0.00.048.973 I llm_load_print_meta: rope scaling     = linear
0.00.048.974 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.974 I llm_load_print_meta: freq_scale_train = 1
0.00.048.974 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.975 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.975 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.975 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.976 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.976 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.976 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.976 I llm_load_print_meta: model type       = 1.4B
0.00.048.977 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.977 I llm_load_print_meta: model params     = 1.41 B
0.00.048.978 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.978 I llm_load_print_meta: general.name     = 1.4B
0.00.048.978 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.978 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.979 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.979 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.980 I llm_load_print_meta: LF token         = 128 ''
0.00.048.980 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.980 I llm_load_print_meta: max token length = 1024
0.00.050.577 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.577 I llm_load_tensors: offloading output layer to GPU
0.00.050.577 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.587 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.588 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.445 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.446 I llama_new_context_with_model: n_ctx         = 128
0.00.051.446 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.446 I llama_new_context_with_model: n_batch       = 128
0.00.051.447 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.447 I llama_new_context_with_model: flash_attn    = 0
0.00.051.447 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.447 I llama_new_context_with_model: freq_scale    = 1
0.00.051.448 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.448 I ggml_metal_init: allocating
0.00.051.453 I ggml_metal_init: found device: Apple M4
0.00.051.456 I ggml_metal_init: picking default device: Apple M4
0.00.051.993 I ggml_metal_init: using embedded metal library
0.00.054.307 I ggml_metal_init: GPU name:   Apple M4
0.00.054.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.309 I ggml_metal_init: simdgroup reduction   = true
0.00.054.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.309 I ggml_metal_init: has bfloat            = true
0.00.054.309 I ggml_metal_init: use bfloat            = true
0.00.054.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.515 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.064.743 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.745 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.758 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.612 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.613 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.613 I llama_new_context_with_model: graph nodes  = 967
0.00.065.614 I llama_new_context_with_model: graph splits = 2
0.00.065.625 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.891 I 
0.00.739.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.936 I perplexity: tokenizing the input ..
0.00.747.517 I perplexity: tokenization took 7.577 ms
0.00.747.520 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.649 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.871.742 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.871.758 I llama_perf_context_print:        load time =     731.00 ms
0.00.871.759 I llama_perf_context_print: prompt eval time =     122.91 ms /   128 tokens (    0.96 ms per token,  1041.43 tokens per second)
0.00.871.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.763 I llama_perf_context_print:       total time =     131.87 ms /   129 tokens
0.00.872.205 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.077s
sys	0m0.147s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.649 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.848 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.857 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.860 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.862 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.719 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.720 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.721 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.721 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.722 I llama_model_loader: - type  f32:  194 tensors
0.00.024.722 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.723 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.754 I llm_load_vocab: special tokens cache size = 25
0.00.051.784 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.787 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.787 I llm_load_print_meta: arch             = gptneox
0.00.051.787 I llm_load_print_meta: vocab type       = BPE
0.00.051.788 I llm_load_print_meta: n_vocab          = 50304
0.00.051.788 I llm_load_print_meta: n_merges         = 50009
0.00.051.788 I llm_load_print_meta: vocab_only       = 0
0.00.051.788 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.788 I llm_load_print_meta: n_embd           = 2048
0.00.051.789 I llm_load_print_meta: n_layer          = 24
0.00.051.791 I llm_load_print_meta: n_head           = 16
0.00.051.792 I llm_load_print_meta: n_head_kv        = 16
0.00.051.792 I llm_load_print_meta: n_rot            = 32
0.00.051.792 I llm_load_print_meta: n_swa            = 0
0.00.051.792 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.793 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.793 I llm_load_print_meta: n_gqa            = 1
0.00.051.794 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.795 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.796 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.796 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.798 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.798 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.799 I llm_load_print_meta: n_ff             = 8192
0.00.051.799 I llm_load_print_meta: n_expert         = 0
0.00.051.799 I llm_load_print_meta: n_expert_used    = 0
0.00.051.799 I llm_load_print_meta: causal attn      = 1
0.00.051.800 I llm_load_print_meta: pooling type     = 0
0.00.051.801 I llm_load_print_meta: rope type        = 2
0.00.051.801 I llm_load_print_meta: rope scaling     = linear
0.00.051.802 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.802 I llm_load_print_meta: freq_scale_train = 1
0.00.051.802 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.804 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.804 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.804 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.805 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.805 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.805 I llm_load_print_meta: model type       = 1.4B
0.00.051.805 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.806 I llm_load_print_meta: model params     = 1.41 B
0.00.051.807 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.807 I llm_load_print_meta: general.name     = 1.4B
0.00.051.807 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.807 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.807 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.808 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.808 I llm_load_print_meta: LF token         = 128 ''
0.00.051.808 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.808 I llm_load_print_meta: max token length = 1024
0.00.053.656 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.657 I llm_load_tensors: offloading output layer to GPU
0.00.053.657 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.667 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.668 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.535 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.535 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.536 I llama_new_context_with_model: n_batch       = 2048
0.00.054.536 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.536 I llama_new_context_with_model: flash_attn    = 0
0.00.054.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.537 I llama_new_context_with_model: freq_scale    = 1
0.00.054.537 I ggml_metal_init: allocating
0.00.054.540 I ggml_metal_init: found device: Apple M4
0.00.054.542 I ggml_metal_init: picking default device: Apple M4
0.00.055.151 I ggml_metal_init: using embedded metal library
0.00.057.501 I ggml_metal_init: GPU name:   Apple M4
0.00.057.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.503 I ggml_metal_init: simdgroup reduction   = true
0.00.057.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.503 I ggml_metal_init: has bfloat            = true
0.00.057.504 I ggml_metal_init: use bfloat            = true
0.00.057.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.352 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.782 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.787 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.804 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.840 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.841 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.842 I llama_new_context_with_model: graph nodes  = 967
0.00.087.842 I llama_new_context_with_model: graph splits = 2
0.00.087.857 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.395 I main: llama threadpool init, n_threads = 4
0.00.842.428 I 
0.00.842.455 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.455 I 
0.00.842.603 I sampler seed: 1234
0.00.842.607 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.643 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.648 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.648 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.626.729 I llama_perf_sampler_print:    sampling time =       1.59 ms /    71 runs   (    0.02 ms per token, 44738.50 tokens per second)
0.01.626.730 I llama_perf_context_print:        load time =     832.74 ms
0.01.626.730 I llama_perf_context_print: prompt eval time =      43.06 ms /     7 tokens (    6.15 ms per token,   162.55 tokens per second)
0.01.626.731 I llama_perf_context_print:        eval time =     738.08 ms /    63 runs   (   11.72 ms per token,    85.36 tokens per second)
0.01.626.731 I llama_perf_context_print:       total time =     784.34 ms /    70 tokens
0.01.626.954 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.110s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.704 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.252 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.261 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.262 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.262 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.263 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.264 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.264 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.956 I llama_model_loader: - type  f32:  194 tensors
0.00.024.956 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.956 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.743 I llm_load_vocab: special tokens cache size = 25
0.00.051.553 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.556 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.556 I llm_load_print_meta: arch             = gptneox
0.00.051.557 I llm_load_print_meta: vocab type       = BPE
0.00.051.557 I llm_load_print_meta: n_vocab          = 50304
0.00.051.557 I llm_load_print_meta: n_merges         = 50009
0.00.051.557 I llm_load_print_meta: vocab_only       = 0
0.00.051.557 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.558 I llm_load_print_meta: n_embd           = 2048
0.00.051.558 I llm_load_print_meta: n_layer          = 24
0.00.051.561 I llm_load_print_meta: n_head           = 16
0.00.051.562 I llm_load_print_meta: n_head_kv        = 16
0.00.051.562 I llm_load_print_meta: n_rot            = 32
0.00.051.562 I llm_load_print_meta: n_swa            = 0
0.00.051.562 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.562 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.565 I llm_load_print_meta: n_gqa            = 1
0.00.051.566 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.567 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.567 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.568 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.570 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.570 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.570 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.571 I llm_load_print_meta: n_ff             = 8192
0.00.051.571 I llm_load_print_meta: n_expert         = 0
0.00.051.571 I llm_load_print_meta: n_expert_used    = 0
0.00.051.571 I llm_load_print_meta: causal attn      = 1
0.00.051.571 I llm_load_print_meta: pooling type     = 0
0.00.051.571 I llm_load_print_meta: rope type        = 2
0.00.051.572 I llm_load_print_meta: rope scaling     = linear
0.00.051.572 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.572 I llm_load_print_meta: freq_scale_train = 1
0.00.051.573 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.573 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.573 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.573 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.573 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.573 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.573 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.575 I llm_load_print_meta: model type       = 1.4B
0.00.051.575 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.576 I llm_load_print_meta: model params     = 1.41 B
0.00.051.576 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.578 I llm_load_print_meta: general.name     = 1.4B
0.00.051.578 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: LF token         = 128 ''
0.00.051.579 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.580 I llm_load_print_meta: max token length = 1024
0.00.053.156 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.156 I llm_load_tensors: offloading output layer to GPU
0.00.053.156 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.166 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.167 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.989 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.990 I llama_new_context_with_model: n_ctx         = 128
0.00.053.990 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.990 I llama_new_context_with_model: n_batch       = 128
0.00.053.990 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.990 I llama_new_context_with_model: flash_attn    = 0
0.00.053.991 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.991 I llama_new_context_with_model: freq_scale    = 1
0.00.053.991 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.992 I ggml_metal_init: allocating
0.00.053.995 I ggml_metal_init: found device: Apple M4
0.00.053.997 I ggml_metal_init: picking default device: Apple M4
0.00.054.549 I ggml_metal_init: using embedded metal library
0.00.056.878 I ggml_metal_init: GPU name:   Apple M4
0.00.056.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.880 I ggml_metal_init: simdgroup reduction   = true
0.00.056.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.880 I ggml_metal_init: has bfloat            = true
0.00.056.881 I ggml_metal_init: use bfloat            = true
0.00.056.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.494 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.500 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.515 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.348 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.349 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.349 I llama_new_context_with_model: graph nodes  = 967
0.00.068.350 I llama_new_context_with_model: graph splits = 2
0.00.068.361 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.922 I 
0.00.765.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.966 I perplexity: tokenizing the input ..
0.00.773.863 I perplexity: tokenization took 7.896 ms
0.00.773.867 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.909.487 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.910.593 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.910.608 I llama_perf_context_print:        load time =     755.21 ms
0.00.910.609 I llama_perf_context_print: prompt eval time =     135.40 ms /   128 tokens (    1.06 ms per token,   945.35 tokens per second)
0.00.910.610 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.910.610 I llama_perf_context_print:       total time =     144.69 ms /   129 tokens
0.00.911.072 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.079s
sys	0m0.161s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.682 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.613 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.666 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.882 I llama_model_loader: - type  f32:  194 tensors
0.00.023.882 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.883 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.812 I llm_load_vocab: special tokens cache size = 25
0.00.050.721 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.724 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.724 I llm_load_print_meta: arch             = gptneox
0.00.050.724 I llm_load_print_meta: vocab type       = BPE
0.00.050.725 I llm_load_print_meta: n_vocab          = 50304
0.00.050.725 I llm_load_print_meta: n_merges         = 50009
0.00.050.725 I llm_load_print_meta: vocab_only       = 0
0.00.050.725 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.725 I llm_load_print_meta: n_embd           = 2048
0.00.050.726 I llm_load_print_meta: n_layer          = 24
0.00.050.728 I llm_load_print_meta: n_head           = 16
0.00.050.729 I llm_load_print_meta: n_head_kv        = 16
0.00.050.729 I llm_load_print_meta: n_rot            = 32
0.00.050.729 I llm_load_print_meta: n_swa            = 0
0.00.050.731 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.731 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.732 I llm_load_print_meta: n_gqa            = 1
0.00.050.733 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.734 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.734 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.735 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.735 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.735 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.736 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.736 I llm_load_print_meta: n_ff             = 8192
0.00.050.736 I llm_load_print_meta: n_expert         = 0
0.00.050.737 I llm_load_print_meta: n_expert_used    = 0
0.00.050.739 I llm_load_print_meta: causal attn      = 1
0.00.050.739 I llm_load_print_meta: pooling type     = 0
0.00.050.739 I llm_load_print_meta: rope type        = 2
0.00.050.740 I llm_load_print_meta: rope scaling     = linear
0.00.050.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.740 I llm_load_print_meta: freq_scale_train = 1
0.00.050.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.741 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.741 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.741 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.741 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.741 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.742 I llm_load_print_meta: model type       = 1.4B
0.00.050.742 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.742 I llm_load_print_meta: model params     = 1.41 B
0.00.050.743 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.745 I llm_load_print_meta: general.name     = 1.4B
0.00.050.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.746 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.746 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.746 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.746 I llm_load_print_meta: LF token         = 128 ''
0.00.050.746 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.747 I llm_load_print_meta: max token length = 1024
0.00.052.451 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.451 I llm_load_tensors: offloading output layer to GPU
0.00.052.451 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.461 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.462 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.271 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.271 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.271 I llama_new_context_with_model: n_batch       = 2048
0.00.053.271 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.272 I llama_new_context_with_model: flash_attn    = 0
0.00.053.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.272 I llama_new_context_with_model: freq_scale    = 1
0.00.053.273 I ggml_metal_init: allocating
0.00.053.279 I ggml_metal_init: found device: Apple M4
0.00.053.282 I ggml_metal_init: picking default device: Apple M4
0.00.053.877 I ggml_metal_init: using embedded metal library
0.00.056.236 I ggml_metal_init: GPU name:   Apple M4
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.239 I ggml_metal_init: simdgroup reduction   = true
0.00.056.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.239 I ggml_metal_init: has bfloat            = true
0.00.056.239 I ggml_metal_init: use bfloat            = true
0.00.056.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.240 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.583 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.917 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.922 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.940 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.839 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.841 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.841 I llama_new_context_with_model: graph nodes  = 967
0.00.084.841 I llama_new_context_with_model: graph splits = 2
0.00.084.856 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.038 I main: llama threadpool init, n_threads = 4
0.00.764.082 I 
0.00.764.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.115 I 
0.00.764.267 I sampler seed: 1234
0.00.764.272 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.306 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.306 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.600.657 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.600.657 I llama_perf_context_print:        load time =     755.35 ms
0.01.600.658 I llama_perf_context_print: prompt eval time =      42.60 ms /     7 tokens (    6.09 ms per token,   164.33 tokens per second)
0.01.600.659 I llama_perf_context_print:        eval time =     790.73 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.600.659 I llama_perf_context_print:       total time =     836.62 ms /    70 tokens
0.01.600.856 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.108s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.133 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.859 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.860 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.864 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.866 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.866 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.571 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.572 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.573 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.573 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.573 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.574 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.574 I llama_model_loader: - type  f32:  194 tensors
0.00.023.575 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.575 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.370 I llm_load_vocab: special tokens cache size = 25
0.00.050.434 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.437 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.437 I llm_load_print_meta: arch             = gptneox
0.00.050.437 I llm_load_print_meta: vocab type       = BPE
0.00.050.438 I llm_load_print_meta: n_vocab          = 50304
0.00.050.438 I llm_load_print_meta: n_merges         = 50009
0.00.050.438 I llm_load_print_meta: vocab_only       = 0
0.00.050.438 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.438 I llm_load_print_meta: n_embd           = 2048
0.00.050.438 I llm_load_print_meta: n_layer          = 24
0.00.050.441 I llm_load_print_meta: n_head           = 16
0.00.050.442 I llm_load_print_meta: n_head_kv        = 16
0.00.050.442 I llm_load_print_meta: n_rot            = 32
0.00.050.444 I llm_load_print_meta: n_swa            = 0
0.00.050.444 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.444 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.445 I llm_load_print_meta: n_gqa            = 1
0.00.050.446 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.447 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.447 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.447 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.449 I llm_load_print_meta: n_ff             = 8192
0.00.050.449 I llm_load_print_meta: n_expert         = 0
0.00.050.449 I llm_load_print_meta: n_expert_used    = 0
0.00.050.449 I llm_load_print_meta: causal attn      = 1
0.00.050.449 I llm_load_print_meta: pooling type     = 0
0.00.050.449 I llm_load_print_meta: rope type        = 2
0.00.050.449 I llm_load_print_meta: rope scaling     = linear
0.00.050.450 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.452 I llm_load_print_meta: freq_scale_train = 1
0.00.050.452 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.453 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.453 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.453 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.453 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.453 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.453 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.454 I llm_load_print_meta: model type       = 1.4B
0.00.050.454 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.454 I llm_load_print_meta: model params     = 1.41 B
0.00.050.455 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.455 I llm_load_print_meta: general.name     = 1.4B
0.00.050.455 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: LF token         = 128 ''
0.00.050.457 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.457 I llm_load_print_meta: max token length = 1024
0.00.052.047 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.047 I llm_load_tensors: offloading output layer to GPU
0.00.052.047 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.057 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.058 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.865 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.865 I llama_new_context_with_model: n_ctx         = 128
0.00.052.866 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.866 I llama_new_context_with_model: n_batch       = 128
0.00.052.866 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.866 I llama_new_context_with_model: flash_attn    = 0
0.00.052.866 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.867 I llama_new_context_with_model: freq_scale    = 1
0.00.052.867 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.867 I ggml_metal_init: allocating
0.00.052.870 I ggml_metal_init: found device: Apple M4
0.00.052.872 I ggml_metal_init: picking default device: Apple M4
0.00.053.407 I ggml_metal_init: using embedded metal library
0.00.055.713 I ggml_metal_init: GPU name:   Apple M4
0.00.055.714 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.715 I ggml_metal_init: simdgroup reduction   = true
0.00.055.715 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.716 I ggml_metal_init: has bfloat            = true
0.00.055.716 I ggml_metal_init: use bfloat            = true
0.00.055.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.717 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.981 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.266 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.269 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.284 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.180 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.181 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.182 I llama_new_context_with_model: graph nodes  = 967
0.00.067.182 I llama_new_context_with_model: graph splits = 2
0.00.067.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.196 I 
0.00.715.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.272 I perplexity: tokenizing the input ..
0.00.722.767 I perplexity: tokenization took 7.491 ms
0.00.722.771 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.970 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.859.074 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.859.090 I llama_perf_context_print:        load time =     706.06 ms
0.00.859.091 I llama_perf_context_print: prompt eval time =     134.98 ms /   128 tokens (    1.05 ms per token,   948.30 tokens per second)
0.00.859.091 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.092 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.859.522 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.079s
sys	0m0.164s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.229 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.231 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.232 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.110 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.112 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.112 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.112 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.113 I llama_model_loader: - type  f32:  194 tensors
0.00.025.113 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.114 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.114 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.006 I llm_load_vocab: special tokens cache size = 25
0.00.052.858 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.863 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.864 I llm_load_print_meta: arch             = gptneox
0.00.052.864 I llm_load_print_meta: vocab type       = BPE
0.00.052.864 I llm_load_print_meta: n_vocab          = 50304
0.00.052.864 I llm_load_print_meta: n_merges         = 50009
0.00.052.865 I llm_load_print_meta: vocab_only       = 0
0.00.052.865 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.865 I llm_load_print_meta: n_embd           = 2048
0.00.052.865 I llm_load_print_meta: n_layer          = 24
0.00.052.869 I llm_load_print_meta: n_head           = 16
0.00.052.870 I llm_load_print_meta: n_head_kv        = 16
0.00.052.870 I llm_load_print_meta: n_rot            = 32
0.00.052.870 I llm_load_print_meta: n_swa            = 0
0.00.052.870 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.871 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.871 I llm_load_print_meta: n_gqa            = 1
0.00.052.872 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.873 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.873 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.874 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.874 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.874 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.874 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.875 I llm_load_print_meta: n_ff             = 8192
0.00.052.875 I llm_load_print_meta: n_expert         = 0
0.00.052.875 I llm_load_print_meta: n_expert_used    = 0
0.00.052.875 I llm_load_print_meta: causal attn      = 1
0.00.052.875 I llm_load_print_meta: pooling type     = 0
0.00.052.876 I llm_load_print_meta: rope type        = 2
0.00.052.876 I llm_load_print_meta: rope scaling     = linear
0.00.052.876 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.876 I llm_load_print_meta: freq_scale_train = 1
0.00.052.877 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.877 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.877 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.877 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.877 I llm_load_print_meta: model type       = 1.4B
0.00.052.878 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.878 I llm_load_print_meta: model params     = 1.41 B
0.00.052.879 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.879 I llm_load_print_meta: general.name     = 1.4B
0.00.052.879 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.879 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.879 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.880 I llm_load_print_meta: LF token         = 128 ''
0.00.052.880 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.880 I llm_load_print_meta: max token length = 1024
0.00.054.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.712 I llm_load_tensors: offloading output layer to GPU
0.00.054.713 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.723 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.724 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.586 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.591 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.592 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.592 I llama_new_context_with_model: n_batch       = 2048
0.00.055.592 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.592 I llama_new_context_with_model: flash_attn    = 0
0.00.055.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.593 I llama_new_context_with_model: freq_scale    = 1
0.00.055.594 I ggml_metal_init: allocating
0.00.055.601 I ggml_metal_init: found device: Apple M4
0.00.055.603 I ggml_metal_init: picking default device: Apple M4
0.00.056.236 I ggml_metal_init: using embedded metal library
0.00.058.582 I ggml_metal_init: GPU name:   Apple M4
0.00.058.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.585 I ggml_metal_init: simdgroup reduction   = true
0.00.058.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.585 I ggml_metal_init: has bfloat            = true
0.00.058.585 I ggml_metal_init: use bfloat            = true
0.00.058.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.874 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.514 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.522 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.540 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.642 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.646 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.646 I llama_new_context_with_model: graph nodes  = 967
0.00.089.646 I llama_new_context_with_model: graph splits = 2
0.00.089.670 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.810 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.448.454 I main: llama threadpool init, n_threads = 4
0.00.448.498 I 
0.00.448.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.448.531 I 
0.00.448.694 I sampler seed: 1234
0.00.448.700 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.448.710 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.448.711 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.448.711 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.139.786 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.139.787 I llama_perf_context_print:        load time =     437.61 ms
0.01.139.788 I llama_perf_context_print: prompt eval time =      36.16 ms /     7 tokens (    5.17 ms per token,   193.59 tokens per second)
0.01.139.788 I llama_perf_context_print:        eval time =     651.84 ms /    63 runs   (   10.35 ms per token,    96.65 tokens per second)
0.01.139.788 I llama_perf_context_print:       total time =     691.34 ms /    70 tokens
0.01.139.974 I ggml_metal_free: deallocating

real	0m1.156s
user	0m0.111s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.902 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.414 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.415 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.416 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.416 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.115 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.117 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.118 I llama_model_loader: - type  f32:  194 tensors
0.00.024.119 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.119 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.119 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.897 I llm_load_vocab: special tokens cache size = 25
0.00.050.711 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.715 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.715 I llm_load_print_meta: arch             = gptneox
0.00.050.715 I llm_load_print_meta: vocab type       = BPE
0.00.050.716 I llm_load_print_meta: n_vocab          = 50304
0.00.050.716 I llm_load_print_meta: n_merges         = 50009
0.00.050.716 I llm_load_print_meta: vocab_only       = 0
0.00.050.716 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.716 I llm_load_print_meta: n_embd           = 2048
0.00.050.719 I llm_load_print_meta: n_layer          = 24
0.00.050.723 I llm_load_print_meta: n_head           = 16
0.00.050.724 I llm_load_print_meta: n_head_kv        = 16
0.00.050.724 I llm_load_print_meta: n_rot            = 32
0.00.050.725 I llm_load_print_meta: n_swa            = 0
0.00.050.725 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.725 I llm_load_print_meta: n_gqa            = 1
0.00.050.726 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.727 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.727 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.728 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.728 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.728 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.728 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.729 I llm_load_print_meta: n_ff             = 8192
0.00.050.729 I llm_load_print_meta: n_expert         = 0
0.00.050.729 I llm_load_print_meta: n_expert_used    = 0
0.00.050.729 I llm_load_print_meta: causal attn      = 1
0.00.050.729 I llm_load_print_meta: pooling type     = 0
0.00.050.729 I llm_load_print_meta: rope type        = 2
0.00.050.730 I llm_load_print_meta: rope scaling     = linear
0.00.050.730 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.730 I llm_load_print_meta: freq_scale_train = 1
0.00.050.730 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.731 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.732 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.733 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.733 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.733 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.733 I llm_load_print_meta: model type       = 1.4B
0.00.050.733 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.734 I llm_load_print_meta: model params     = 1.41 B
0.00.050.734 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.734 I llm_load_print_meta: general.name     = 1.4B
0.00.050.734 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.735 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.735 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.735 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.735 I llm_load_print_meta: LF token         = 128 ''
0.00.050.736 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.736 I llm_load_print_meta: max token length = 1024
0.00.052.615 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.615 I llm_load_tensors: offloading output layer to GPU
0.00.052.615 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.626 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.628 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.500 I llama_new_context_with_model: n_ctx         = 128
0.00.053.500 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.501 I llama_new_context_with_model: n_batch       = 128
0.00.053.501 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.501 I llama_new_context_with_model: flash_attn    = 0
0.00.053.502 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.502 I llama_new_context_with_model: freq_scale    = 1
0.00.053.502 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.503 I ggml_metal_init: allocating
0.00.053.506 I ggml_metal_init: found device: Apple M4
0.00.053.508 I ggml_metal_init: picking default device: Apple M4
0.00.054.136 I ggml_metal_init: using embedded metal library
0.00.056.516 I ggml_metal_init: GPU name:   Apple M4
0.00.056.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.519 I ggml_metal_init: simdgroup reduction   = true
0.00.056.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.519 I ggml_metal_init: has bfloat            = true
0.00.056.519 I ggml_metal_init: use bfloat            = true
0.00.056.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.645 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.918 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.920 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.946 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.845 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.846 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.846 I llama_new_context_with_model: graph nodes  = 967
0.00.068.847 I llama_new_context_with_model: graph splits = 2
0.00.068.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.335 I 
0.00.381.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.395 I perplexity: tokenizing the input ..
0.00.389.567 I perplexity: tokenization took 8.17 ms
0.00.389.575 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.521.798 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.522.961 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.522.984 I llama_perf_context_print:        load time =     371.42 ms
0.00.522.987 I llama_perf_context_print: prompt eval time =     132.00 ms /   128 tokens (    1.03 ms per token,   969.72 tokens per second)
0.00.522.988 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.522.988 I llama_perf_context_print:       total time =     141.66 ms /   129 tokens
0.00.523.402 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.079s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.811 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.253 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.258 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.261 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.262 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.266 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.080 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.923 I llama_model_loader: - type  f32:  194 tensors
0.00.022.924 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.924 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.924 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.924 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.864 I llm_load_vocab: special tokens cache size = 25
0.00.048.566 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.568 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.569 I llm_load_print_meta: arch             = gptneox
0.00.048.569 I llm_load_print_meta: vocab type       = BPE
0.00.048.569 I llm_load_print_meta: n_vocab          = 50304
0.00.048.569 I llm_load_print_meta: n_merges         = 50009
0.00.048.570 I llm_load_print_meta: vocab_only       = 0
0.00.048.570 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.570 I llm_load_print_meta: n_embd           = 2048
0.00.048.570 I llm_load_print_meta: n_layer          = 24
0.00.048.572 I llm_load_print_meta: n_head           = 16
0.00.048.573 I llm_load_print_meta: n_head_kv        = 16
0.00.048.573 I llm_load_print_meta: n_rot            = 32
0.00.048.574 I llm_load_print_meta: n_swa            = 0
0.00.048.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.575 I llm_load_print_meta: n_gqa            = 1
0.00.048.576 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.577 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.580 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.581 I llm_load_print_meta: n_ff             = 8192
0.00.048.581 I llm_load_print_meta: n_expert         = 0
0.00.048.582 I llm_load_print_meta: n_expert_used    = 0
0.00.048.582 I llm_load_print_meta: causal attn      = 1
0.00.048.582 I llm_load_print_meta: pooling type     = 0
0.00.048.582 I llm_load_print_meta: rope type        = 2
0.00.048.583 I llm_load_print_meta: rope scaling     = linear
0.00.048.583 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.583 I llm_load_print_meta: freq_scale_train = 1
0.00.048.583 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.584 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.584 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.584 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.584 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.584 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.585 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.585 I llm_load_print_meta: model type       = 1.4B
0.00.048.589 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.589 I llm_load_print_meta: model params     = 1.41 B
0.00.048.592 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.593 I llm_load_print_meta: general.name     = 1.4B
0.00.048.593 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.593 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.593 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.600 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.602 I llm_load_print_meta: LF token         = 128 ''
0.00.048.602 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.603 I llm_load_print_meta: max token length = 1024
0.00.050.361 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.361 I llm_load_tensors: offloading output layer to GPU
0.00.050.361 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.371 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.372 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.204 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.204 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.204 I llama_new_context_with_model: n_batch       = 2048
0.00.051.204 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.204 I llama_new_context_with_model: flash_attn    = 0
0.00.051.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.205 I llama_new_context_with_model: freq_scale    = 1
0.00.051.205 I ggml_metal_init: allocating
0.00.051.208 I ggml_metal_init: found device: Apple M4
0.00.051.210 I ggml_metal_init: picking default device: Apple M4
0.00.051.776 I ggml_metal_init: using embedded metal library
0.00.054.143 I ggml_metal_init: GPU name:   Apple M4
0.00.054.144 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.145 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.145 I ggml_metal_init: simdgroup reduction   = true
0.00.054.145 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.146 I ggml_metal_init: has bfloat            = true
0.00.054.146 I ggml_metal_init: use bfloat            = true
0.00.054.146 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.742 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.082.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.744 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.764 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.706 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.707 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.708 I llama_new_context_with_model: graph nodes  = 967
0.00.083.708 I llama_new_context_with_model: graph splits = 2
0.00.083.723 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.841 I main: llama threadpool init, n_threads = 4
0.00.575.891 I 
0.00.575.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.942 I 
0.00.576.102 I sampler seed: 1234
0.00.576.108 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.576.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.576.150 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.576.150 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.325.024 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.325.025 I llama_perf_context_print:        load time =     567.02 ms
0.01.325.025 I llama_perf_context_print: prompt eval time =      40.94 ms /     7 tokens (    5.85 ms per token,   170.99 tokens per second)
0.01.325.026 I llama_perf_context_print:        eval time =     704.82 ms /    63 runs   (   11.19 ms per token,    89.38 tokens per second)
0.01.325.026 I llama_perf_context_print:       total time =     749.19 ms /    70 tokens
0.01.325.220 I ggml_metal_free: deallocating

real	0m1.341s
user	0m0.109s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.530 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.982 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.983 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.983 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.983 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.984 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.819 I llama_model_loader: - type  f32:  194 tensors
0.00.023.819 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.819 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.820 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.565 I llm_load_vocab: special tokens cache size = 25
0.00.050.485 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.487 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.488 I llm_load_print_meta: arch             = gptneox
0.00.050.488 I llm_load_print_meta: vocab type       = BPE
0.00.050.488 I llm_load_print_meta: n_vocab          = 50304
0.00.050.488 I llm_load_print_meta: n_merges         = 50009
0.00.050.489 I llm_load_print_meta: vocab_only       = 0
0.00.050.489 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.489 I llm_load_print_meta: n_embd           = 2048
0.00.050.489 I llm_load_print_meta: n_layer          = 24
0.00.050.492 I llm_load_print_meta: n_head           = 16
0.00.050.493 I llm_load_print_meta: n_head_kv        = 16
0.00.050.493 I llm_load_print_meta: n_rot            = 32
0.00.050.493 I llm_load_print_meta: n_swa            = 0
0.00.050.493 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.494 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.495 I llm_load_print_meta: n_gqa            = 1
0.00.050.495 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.496 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.498 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.498 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.499 I llm_load_print_meta: n_ff             = 8192
0.00.050.500 I llm_load_print_meta: n_expert         = 0
0.00.050.500 I llm_load_print_meta: n_expert_used    = 0
0.00.050.500 I llm_load_print_meta: causal attn      = 1
0.00.050.501 I llm_load_print_meta: pooling type     = 0
0.00.050.501 I llm_load_print_meta: rope type        = 2
0.00.050.501 I llm_load_print_meta: rope scaling     = linear
0.00.050.502 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.502 I llm_load_print_meta: freq_scale_train = 1
0.00.050.502 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.502 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.504 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.504 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.504 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.505 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.505 I llm_load_print_meta: model type       = 1.4B
0.00.050.505 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.506 I llm_load_print_meta: model params     = 1.41 B
0.00.050.506 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.506 I llm_load_print_meta: general.name     = 1.4B
0.00.050.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.507 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.511 I llm_load_print_meta: LF token         = 128 ''
0.00.050.512 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.512 I llm_load_print_meta: max token length = 1024
0.00.052.491 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.491 I llm_load_tensors: offloading output layer to GPU
0.00.052.491 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.502 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.503 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.400 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.401 I llama_new_context_with_model: n_ctx         = 128
0.00.053.401 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.402 I llama_new_context_with_model: n_batch       = 128
0.00.053.402 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.402 I llama_new_context_with_model: flash_attn    = 0
0.00.053.402 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.403 I llama_new_context_with_model: freq_scale    = 1
0.00.053.403 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.403 I ggml_metal_init: allocating
0.00.053.407 I ggml_metal_init: found device: Apple M4
0.00.053.409 I ggml_metal_init: picking default device: Apple M4
0.00.053.954 I ggml_metal_init: using embedded metal library
0.00.056.274 I ggml_metal_init: GPU name:   Apple M4
0.00.056.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.276 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.276 I ggml_metal_init: simdgroup reduction   = true
0.00.056.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.277 I ggml_metal_init: has bfloat            = true
0.00.056.277 I ggml_metal_init: use bfloat            = true
0.00.056.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.844 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.081 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.083 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.098 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.019 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.021 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.021 I llama_new_context_with_model: graph nodes  = 967
0.00.068.021 I llama_new_context_with_model: graph splits = 2
0.00.068.034 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.035 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.548 I 
0.00.484.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.602 I perplexity: tokenizing the input ..
0.00.492.990 I perplexity: tokenization took 8.385 ms
0.00.492.994 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.927 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.024 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.038 I llama_perf_context_print:        load time =     475.01 ms
0.00.628.039 I llama_perf_context_print: prompt eval time =     133.71 ms /   128 tokens (    1.04 ms per token,   957.32 tokens per second)
0.00.628.040 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.040 I llama_perf_context_print:       total time =     143.49 ms /   129 tokens
0.00.628.436 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.079s
sys	0m0.086s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.298 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.195 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.197 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.197 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.206 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.207 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.207 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.207 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.208 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.088 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.926 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.928 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.929 I llama_model_loader: - type  f32:  194 tensors
0.00.024.929 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.929 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.929 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.692 I llm_load_vocab: special tokens cache size = 25
0.00.051.438 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.441 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.442 I llm_load_print_meta: arch             = gptneox
0.00.051.442 I llm_load_print_meta: vocab type       = BPE
0.00.051.442 I llm_load_print_meta: n_vocab          = 50304
0.00.051.442 I llm_load_print_meta: n_merges         = 50009
0.00.051.442 I llm_load_print_meta: vocab_only       = 0
0.00.051.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.443 I llm_load_print_meta: n_embd           = 2048
0.00.051.443 I llm_load_print_meta: n_layer          = 24
0.00.051.445 I llm_load_print_meta: n_head           = 16
0.00.051.446 I llm_load_print_meta: n_head_kv        = 16
0.00.051.446 I llm_load_print_meta: n_rot            = 32
0.00.051.446 I llm_load_print_meta: n_swa            = 0
0.00.051.447 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.447 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.447 I llm_load_print_meta: n_gqa            = 1
0.00.051.448 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.449 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.450 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.450 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.450 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.450 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.451 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.451 I llm_load_print_meta: n_ff             = 8192
0.00.051.451 I llm_load_print_meta: n_expert         = 0
0.00.051.453 I llm_load_print_meta: n_expert_used    = 0
0.00.051.455 I llm_load_print_meta: causal attn      = 1
0.00.051.455 I llm_load_print_meta: pooling type     = 0
0.00.051.455 I llm_load_print_meta: rope type        = 2
0.00.051.455 I llm_load_print_meta: rope scaling     = linear
0.00.051.456 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.456 I llm_load_print_meta: freq_scale_train = 1
0.00.051.456 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.456 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.456 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.458 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.458 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.458 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.458 I llm_load_print_meta: model type       = 1.4B
0.00.051.459 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.459 I llm_load_print_meta: model params     = 1.41 B
0.00.051.460 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.460 I llm_load_print_meta: general.name     = 1.4B
0.00.051.460 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.462 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.463 I llm_load_print_meta: LF token         = 128 ''
0.00.051.463 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.463 I llm_load_print_meta: max token length = 1024
0.00.053.275 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.275 I llm_load_tensors: offloading output layer to GPU
0.00.053.276 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.286 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.287 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.144 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.144 I llama_new_context_with_model: n_batch       = 2048
0.00.054.144 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.144 I llama_new_context_with_model: flash_attn    = 0
0.00.054.145 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.145 I llama_new_context_with_model: freq_scale    = 1
0.00.054.145 I ggml_metal_init: allocating
0.00.054.152 I ggml_metal_init: found device: Apple M4
0.00.054.154 I ggml_metal_init: picking default device: Apple M4
0.00.054.753 I ggml_metal_init: using embedded metal library
0.00.057.087 I ggml_metal_init: GPU name:   Apple M4
0.00.057.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.091 I ggml_metal_init: simdgroup reduction   = true
0.00.057.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.091 I ggml_metal_init: has bfloat            = true
0.00.057.091 I ggml_metal_init: use bfloat            = true
0.00.057.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.534 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.697 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.705 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.727 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.783 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.785 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.785 I llama_new_context_with_model: graph nodes  = 967
0.00.086.785 I llama_new_context_with_model: graph splits = 2
0.00.086.801 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.934 I main: llama threadpool init, n_threads = 4
0.00.683.972 I 
0.00.684.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.005 I 
0.00.684.156 I sampler seed: 1234
0.00.684.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.171 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.171 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.437.370 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.01.437.370 I llama_perf_context_print:        load time =     674.63 ms
0.01.437.371 I llama_perf_context_print: prompt eval time =      47.50 ms /     7 tokens (    6.79 ms per token,   147.36 tokens per second)
0.01.437.373 I llama_perf_context_print:        eval time =     702.65 ms /    63 runs   (   11.15 ms per token,    89.66 tokens per second)
0.01.437.374 I llama_perf_context_print:       total time =     753.44 ms /    70 tokens
0.01.437.560 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.110s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.079 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.645 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.652 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.652 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.654 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.655 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.659 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.239 I llama_model_loader: - type  f32:  194 tensors
0.00.025.239 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.239 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.239 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.999 I llm_load_vocab: special tokens cache size = 25
0.00.050.773 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.776 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.776 I llm_load_print_meta: arch             = gptneox
0.00.050.777 I llm_load_print_meta: vocab type       = BPE
0.00.050.777 I llm_load_print_meta: n_vocab          = 50304
0.00.050.777 I llm_load_print_meta: n_merges         = 50009
0.00.050.777 I llm_load_print_meta: vocab_only       = 0
0.00.050.778 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.778 I llm_load_print_meta: n_embd           = 2048
0.00.050.778 I llm_load_print_meta: n_layer          = 24
0.00.050.781 I llm_load_print_meta: n_head           = 16
0.00.050.782 I llm_load_print_meta: n_head_kv        = 16
0.00.050.783 I llm_load_print_meta: n_rot            = 32
0.00.050.783 I llm_load_print_meta: n_swa            = 0
0.00.050.783 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.783 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.784 I llm_load_print_meta: n_gqa            = 1
0.00.050.785 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.786 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.787 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.789 I llm_load_print_meta: n_ff             = 8192
0.00.050.791 I llm_load_print_meta: n_expert         = 0
0.00.050.791 I llm_load_print_meta: n_expert_used    = 0
0.00.050.791 I llm_load_print_meta: causal attn      = 1
0.00.050.791 I llm_load_print_meta: pooling type     = 0
0.00.050.791 I llm_load_print_meta: rope type        = 2
0.00.050.791 I llm_load_print_meta: rope scaling     = linear
0.00.050.792 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.792 I llm_load_print_meta: freq_scale_train = 1
0.00.050.792 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.793 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.793 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.793 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.793 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.793 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.793 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.793 I llm_load_print_meta: model type       = 1.4B
0.00.050.798 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.798 I llm_load_print_meta: model params     = 1.41 B
0.00.050.798 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.799 I llm_load_print_meta: general.name     = 1.4B
0.00.050.799 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.799 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.799 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.799 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.800 I llm_load_print_meta: LF token         = 128 ''
0.00.050.800 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.800 I llm_load_print_meta: max token length = 1024
0.00.052.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.544 I llm_load_tensors: offloading output layer to GPU
0.00.052.545 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.554 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.555 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.379 I llama_new_context_with_model: n_ctx         = 128
0.00.053.380 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.380 I llama_new_context_with_model: n_batch       = 128
0.00.053.380 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.380 I llama_new_context_with_model: flash_attn    = 0
0.00.053.380 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.381 I llama_new_context_with_model: freq_scale    = 1
0.00.053.381 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.381 I ggml_metal_init: allocating
0.00.053.387 I ggml_metal_init: found device: Apple M4
0.00.053.390 I ggml_metal_init: picking default device: Apple M4
0.00.053.938 I ggml_metal_init: using embedded metal library
0.00.056.258 I ggml_metal_init: GPU name:   Apple M4
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.260 I ggml_metal_init: simdgroup reduction   = true
0.00.056.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.260 I ggml_metal_init: has bfloat            = true
0.00.056.260 I ggml_metal_init: use bfloat            = true
0.00.056.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.659 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.022 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.024 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.038 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.945 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.946 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.946 I llama_new_context_with_model: graph nodes  = 967
0.00.067.946 I llama_new_context_with_model: graph splits = 2
0.00.067.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.722 I 
0.00.608.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.774 I perplexity: tokenizing the input ..
0.00.616.317 I perplexity: tokenization took 7.542 ms
0.00.616.321 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.750.658 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.751.787 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.751.801 I llama_perf_context_print:        load time =     597.64 ms
0.00.751.801 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.39 tokens per second)
0.00.751.802 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.751.803 I llama_perf_context_print:       total time =     143.08 ms /   129 tokens
0.00.752.371 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.078s
sys	0m0.132s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.292 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.530 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.536 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.296 I llama_model_loader: - type  f32:  194 tensors
0.00.023.297 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.297 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.446 I llm_load_vocab: special tokens cache size = 25
0.00.049.326 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.329 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.329 I llm_load_print_meta: arch             = gptneox
0.00.049.329 I llm_load_print_meta: vocab type       = BPE
0.00.049.330 I llm_load_print_meta: n_vocab          = 50304
0.00.049.330 I llm_load_print_meta: n_merges         = 50009
0.00.049.330 I llm_load_print_meta: vocab_only       = 0
0.00.049.330 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.330 I llm_load_print_meta: n_embd           = 2048
0.00.049.331 I llm_load_print_meta: n_layer          = 24
0.00.049.333 I llm_load_print_meta: n_head           = 16
0.00.049.334 I llm_load_print_meta: n_head_kv        = 16
0.00.049.334 I llm_load_print_meta: n_rot            = 32
0.00.049.334 I llm_load_print_meta: n_swa            = 0
0.00.049.335 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.337 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.338 I llm_load_print_meta: n_gqa            = 1
0.00.049.339 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.339 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.340 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.340 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.340 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.342 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.342 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.343 I llm_load_print_meta: n_ff             = 8192
0.00.049.343 I llm_load_print_meta: n_expert         = 0
0.00.049.343 I llm_load_print_meta: n_expert_used    = 0
0.00.049.343 I llm_load_print_meta: causal attn      = 1
0.00.049.343 I llm_load_print_meta: pooling type     = 0
0.00.049.343 I llm_load_print_meta: rope type        = 2
0.00.049.344 I llm_load_print_meta: rope scaling     = linear
0.00.049.344 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.344 I llm_load_print_meta: freq_scale_train = 1
0.00.049.344 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.345 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.345 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.346 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.347 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.347 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.348 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.348 I llm_load_print_meta: model type       = 1.4B
0.00.049.348 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.349 I llm_load_print_meta: model params     = 1.41 B
0.00.049.349 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.349 I llm_load_print_meta: general.name     = 1.4B
0.00.049.351 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.351 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.351 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.351 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.352 I llm_load_print_meta: LF token         = 128 ''
0.00.049.352 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.353 I llm_load_print_meta: max token length = 1024
0.00.051.065 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.065 I llm_load_tensors: offloading output layer to GPU
0.00.051.065 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.075 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.076 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.924 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.924 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.924 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.925 I llama_new_context_with_model: n_batch       = 2048
0.00.051.925 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.925 I llama_new_context_with_model: flash_attn    = 0
0.00.051.925 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.926 I llama_new_context_with_model: freq_scale    = 1
0.00.051.926 I ggml_metal_init: allocating
0.00.051.929 I ggml_metal_init: found device: Apple M4
0.00.051.931 I ggml_metal_init: picking default device: Apple M4
0.00.052.510 I ggml_metal_init: using embedded metal library
0.00.054.858 I ggml_metal_init: GPU name:   Apple M4
0.00.054.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.861 I ggml_metal_init: simdgroup reduction   = true
0.00.054.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.861 I ggml_metal_init: has bfloat            = true
0.00.054.861 I ggml_metal_init: use bfloat            = true
0.00.054.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.447 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.550 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.555 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.558 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.560 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.560 I llama_new_context_with_model: graph nodes  = 967
0.00.084.560 I llama_new_context_with_model: graph splits = 2
0.00.084.575 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.760 I main: llama threadpool init, n_threads = 4
0.00.750.801 I 
0.00.750.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.859 I 
0.00.751.016 I sampler seed: 1234
0.00.751.020 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.070 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.072 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.072 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.594.246 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.594.246 I llama_perf_context_print:        load time =     742.46 ms
0.01.594.247 I llama_perf_context_print: prompt eval time =      51.99 ms /     7 tokens (    7.43 ms per token,   134.65 tokens per second)
0.01.594.247 I llama_perf_context_print:        eval time =     788.08 ms /    63 runs   (   12.51 ms per token,    79.94 tokens per second)
0.01.594.248 I llama_perf_context_print:       total time =     843.49 ms /    70 tokens
0.01.594.445 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.108s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.615 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.621 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.622 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.087 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.090 I llama_model_loader: - type  f32:  194 tensors
0.00.024.090 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.090 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.693 I llm_load_vocab: special tokens cache size = 25
0.00.050.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.628 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.628 I llm_load_print_meta: arch             = gptneox
0.00.050.628 I llm_load_print_meta: vocab type       = BPE
0.00.050.629 I llm_load_print_meta: n_vocab          = 50304
0.00.050.629 I llm_load_print_meta: n_merges         = 50009
0.00.050.629 I llm_load_print_meta: vocab_only       = 0
0.00.050.629 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.629 I llm_load_print_meta: n_embd           = 2048
0.00.050.630 I llm_load_print_meta: n_layer          = 24
0.00.050.633 I llm_load_print_meta: n_head           = 16
0.00.050.633 I llm_load_print_meta: n_head_kv        = 16
0.00.050.633 I llm_load_print_meta: n_rot            = 32
0.00.050.634 I llm_load_print_meta: n_swa            = 0
0.00.050.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.635 I llm_load_print_meta: n_gqa            = 1
0.00.050.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.638 I llm_load_print_meta: n_ff             = 8192
0.00.050.638 I llm_load_print_meta: n_expert         = 0
0.00.050.639 I llm_load_print_meta: n_expert_used    = 0
0.00.050.639 I llm_load_print_meta: causal attn      = 1
0.00.050.641 I llm_load_print_meta: pooling type     = 0
0.00.050.641 I llm_load_print_meta: rope type        = 2
0.00.050.641 I llm_load_print_meta: rope scaling     = linear
0.00.050.642 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.642 I llm_load_print_meta: freq_scale_train = 1
0.00.050.642 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.642 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.642 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.643 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.643 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.643 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.643 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.643 I llm_load_print_meta: model type       = 1.4B
0.00.050.644 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.644 I llm_load_print_meta: model params     = 1.41 B
0.00.050.645 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.645 I llm_load_print_meta: general.name     = 1.4B
0.00.050.645 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.646 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.647 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: LF token         = 128 ''
0.00.050.648 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: max token length = 1024
0.00.052.211 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.212 I llm_load_tensors: offloading output layer to GPU
0.00.052.212 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.222 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.223 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.059 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.060 I llama_new_context_with_model: n_ctx         = 128
0.00.053.060 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.060 I llama_new_context_with_model: n_batch       = 128
0.00.053.060 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.061 I llama_new_context_with_model: flash_attn    = 0
0.00.053.061 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.061 I llama_new_context_with_model: freq_scale    = 1
0.00.053.062 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.062 I ggml_metal_init: allocating
0.00.053.067 I ggml_metal_init: found device: Apple M4
0.00.053.069 I ggml_metal_init: picking default device: Apple M4
0.00.053.627 I ggml_metal_init: using embedded metal library
0.00.055.905 I ggml_metal_init: GPU name:   Apple M4
0.00.055.907 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.908 I ggml_metal_init: simdgroup reduction   = true
0.00.055.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.908 I ggml_metal_init: has bfloat            = true
0.00.055.908 I ggml_metal_init: use bfloat            = true
0.00.055.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.457 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.735 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.738 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.753 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.635 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.636 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.636 I llama_new_context_with_model: graph nodes  = 967
0.00.067.636 I llama_new_context_with_model: graph splits = 2
0.00.067.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.272 I 
0.00.739.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.310 I perplexity: tokenizing the input ..
0.00.746.465 I perplexity: tokenization took 7.153 ms
0.00.746.468 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.598 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.888.718 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.888.736 I llama_perf_context_print:        load time =     729.29 ms
0.00.888.737 I llama_perf_context_print: prompt eval time =     140.91 ms /   128 tokens (    1.10 ms per token,   908.40 tokens per second)
0.00.888.738 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.888.738 I llama_perf_context_print:       total time =     149.46 ms /   129 tokens
0.00.889.168 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.078s
sys	0m0.160s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.524 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.761 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.761 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.763 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.765 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.765 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.277 I llama_model_loader: - type  f32:  194 tensors
0.00.024.278 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.389 I llm_load_vocab: special tokens cache size = 25
0.00.050.258 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.261 I llm_load_print_meta: arch             = gptneox
0.00.050.262 I llm_load_print_meta: vocab type       = BPE
0.00.050.262 I llm_load_print_meta: n_vocab          = 50304
0.00.050.262 I llm_load_print_meta: n_merges         = 50009
0.00.050.262 I llm_load_print_meta: vocab_only       = 0
0.00.050.263 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.263 I llm_load_print_meta: n_embd           = 2048
0.00.050.263 I llm_load_print_meta: n_layer          = 24
0.00.050.265 I llm_load_print_meta: n_head           = 16
0.00.050.266 I llm_load_print_meta: n_head_kv        = 16
0.00.050.266 I llm_load_print_meta: n_rot            = 32
0.00.050.266 I llm_load_print_meta: n_swa            = 0
0.00.050.267 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.267 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.268 I llm_load_print_meta: n_gqa            = 1
0.00.050.268 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.269 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.270 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.270 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.273 I llm_load_print_meta: n_ff             = 8192
0.00.050.273 I llm_load_print_meta: n_expert         = 0
0.00.050.274 I llm_load_print_meta: n_expert_used    = 0
0.00.050.274 I llm_load_print_meta: causal attn      = 1
0.00.050.274 I llm_load_print_meta: pooling type     = 0
0.00.050.274 I llm_load_print_meta: rope type        = 2
0.00.050.274 I llm_load_print_meta: rope scaling     = linear
0.00.050.275 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.275 I llm_load_print_meta: freq_scale_train = 1
0.00.050.278 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.278 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.278 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.278 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.278 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.278 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.279 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.279 I llm_load_print_meta: model type       = 1.4B
0.00.050.279 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.280 I llm_load_print_meta: model params     = 1.41 B
0.00.050.280 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.280 I llm_load_print_meta: general.name     = 1.4B
0.00.050.280 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.281 I llm_load_print_meta: LF token         = 128 ''
0.00.050.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.282 I llm_load_print_meta: max token length = 1024
0.00.052.062 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.062 I llm_load_tensors: offloading output layer to GPU
0.00.052.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.072 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.073 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.942 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.943 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.943 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.943 I llama_new_context_with_model: n_batch       = 2048
0.00.052.943 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.943 I llama_new_context_with_model: flash_attn    = 0
0.00.052.944 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.944 I llama_new_context_with_model: freq_scale    = 1
0.00.052.944 I ggml_metal_init: allocating
0.00.052.947 I ggml_metal_init: found device: Apple M4
0.00.052.949 I ggml_metal_init: picking default device: Apple M4
0.00.053.524 I ggml_metal_init: using embedded metal library
0.00.055.812 I ggml_metal_init: GPU name:   Apple M4
0.00.055.814 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.815 I ggml_metal_init: simdgroup reduction   = true
0.00.055.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.815 I ggml_metal_init: has bfloat            = true
0.00.055.815 I ggml_metal_init: use bfloat            = true
0.00.055.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.385 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.889 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.897 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.923 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.958 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.960 I llama_new_context_with_model: graph nodes  = 967
0.00.085.960 I llama_new_context_with_model: graph splits = 2
0.00.085.976 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.120 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.920 I main: llama threadpool init, n_threads = 4
0.00.808.961 I 
0.00.809.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.000 I 
0.00.809.156 I sampler seed: 1234
0.00.809.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.171 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.171 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.171 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.676.094 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.676.094 I llama_perf_context_print:        load time =     799.39 ms
0.01.676.099 I llama_perf_context_print: prompt eval time =      54.91 ms /     7 tokens (    7.84 ms per token,   127.47 tokens per second)
0.01.676.100 I llama_perf_context_print:        eval time =     809.11 ms /    63 runs   (   12.84 ms per token,    77.86 tokens per second)
0.01.676.100 I llama_perf_context_print:       total time =     867.18 ms /    70 tokens
0.01.676.267 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.109s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4388 (30caac3a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.071 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.690 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.693 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.693 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.693 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.694 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.341 I llama_model_loader: - type  f32:  194 tensors
0.00.024.342 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.954 I llm_load_vocab: special tokens cache size = 25
0.00.049.756 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.759 I llm_load_print_meta: arch             = gptneox
0.00.049.759 I llm_load_print_meta: vocab type       = BPE
0.00.049.760 I llm_load_print_meta: n_vocab          = 50304
0.00.049.760 I llm_load_print_meta: n_merges         = 50009
0.00.049.760 I llm_load_print_meta: vocab_only       = 0
0.00.049.760 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.760 I llm_load_print_meta: n_embd           = 2048
0.00.049.761 I llm_load_print_meta: n_layer          = 24
0.00.049.763 I llm_load_print_meta: n_head           = 16
0.00.049.764 I llm_load_print_meta: n_head_kv        = 16
0.00.049.764 I llm_load_print_meta: n_rot            = 32
0.00.049.764 I llm_load_print_meta: n_swa            = 0
0.00.049.764 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.765 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.765 I llm_load_print_meta: n_gqa            = 1
0.00.049.766 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.767 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.767 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.768 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.768 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.768 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.768 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.769 I llm_load_print_meta: n_ff             = 8192
0.00.049.769 I llm_load_print_meta: n_expert         = 0
0.00.049.769 I llm_load_print_meta: n_expert_used    = 0
0.00.049.769 I llm_load_print_meta: causal attn      = 1
0.00.049.770 I llm_load_print_meta: pooling type     = 0
0.00.049.770 I llm_load_print_meta: rope type        = 2
0.00.049.770 I llm_load_print_meta: rope scaling     = linear
0.00.049.770 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.772 I llm_load_print_meta: freq_scale_train = 1
0.00.049.772 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.772 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.772 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.772 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.772 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.773 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.773 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.773 I llm_load_print_meta: model type       = 1.4B
0.00.049.775 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.775 I llm_load_print_meta: model params     = 1.41 B
0.00.049.776 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.776 I llm_load_print_meta: general.name     = 1.4B
0.00.049.776 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.776 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.776 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: LF token         = 128 ''
0.00.049.777 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: max token length = 1024
0.00.051.353 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.353 I llm_load_tensors: offloading output layer to GPU
0.00.051.353 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.363 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.364 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.255 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.255 I llama_new_context_with_model: n_ctx         = 128
0.00.052.255 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.256 I llama_new_context_with_model: n_batch       = 128
0.00.052.256 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.256 I llama_new_context_with_model: flash_attn    = 0
0.00.052.256 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.257 I llama_new_context_with_model: freq_scale    = 1
0.00.052.257 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.257 I ggml_metal_init: allocating
0.00.052.261 I ggml_metal_init: found device: Apple M4
0.00.052.262 I ggml_metal_init: picking default device: Apple M4
0.00.052.846 I ggml_metal_init: using embedded metal library
0.00.055.152 I ggml_metal_init: GPU name:   Apple M4
0.00.055.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.154 I ggml_metal_init: simdgroup reduction   = true
0.00.055.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.154 I ggml_metal_init: has bfloat            = true
0.00.055.154 I ggml_metal_init: use bfloat            = true
0.00.055.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.155 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.502 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.868 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.870 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.893 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.809 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.810 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.811 I llama_new_context_with_model: graph nodes  = 967
0.00.066.811 I llama_new_context_with_model: graph splits = 2
0.00.066.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.340.017 I 
0.00.340.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.340.065 I perplexity: tokenizing the input ..
0.00.347.295 I perplexity: tokenization took 7.229 ms
0.00.347.304 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.487.550 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.488.670 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.488.688 I llama_perf_context_print:        load time =     329.94 ms
0.00.488.689 I llama_perf_context_print: prompt eval time =     140.03 ms /   128 tokens (    1.09 ms per token,   914.11 tokens per second)
0.00.488.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.488.690 I llama_perf_context_print:       total time =     148.67 ms /   129 tokens
0.00.489.002 I ggml_metal_free: deallocating

real	0m0.503s
user	0m0.077s
sys	0m0.085s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4388 (30caac3a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e90aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e90b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e90b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e90bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e90c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e90c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e90ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e90d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e90d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e90deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e90e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e90e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e90f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e90fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e910390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e910ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e9111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e9118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e912010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e9127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e912f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e913620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e913d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e9145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e914d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e914fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e9155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e916240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e916780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e916a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e916ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e9171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e917a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e917f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e918230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e9186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e918b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e919010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e9194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e919950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e919df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e91a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e91a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e91abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e91ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e91b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e91bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e91c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e91c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e91cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e91d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e91dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e91e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e91e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e91f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e91f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e91f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e91fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e920230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e920a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e920ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e921180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e921620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e921ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e921f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e922400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e9228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e922d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e9231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e923680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e923b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e923fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e924460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e9249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e924f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e925450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e9259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e925ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e926440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e926990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e926ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e927430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e927980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e927ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e928420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e928970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e928ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e929410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e929960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e929eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e92a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e92a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e92aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e92b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e92b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e92be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e92c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e91c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e92c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e92d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e92d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e92daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e92dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e92e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e92ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e92efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e92f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e92fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e92ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e930520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e930a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e930fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e931510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e9319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e931e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e9322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e932790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e932c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e9330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e933570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e933a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e933eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e934350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e9347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e934c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e935130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e9355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e935a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e935f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e9363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e936850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e936cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e937190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e937630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e937ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e937f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e938410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e9388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e938d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e9391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e939690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e939b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e939fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e93a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e93a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e93adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e93b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e93b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e93bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e93c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e93c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e93c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e93ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e93d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e93d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e93dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e93e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e93e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e93e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e93ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e93f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e93f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e93fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e9400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e940590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e940a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e940ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e941370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e941810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e941cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e942150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e9425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e942a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e942f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e9433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e943870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e943d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e9441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e944650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e944af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e944f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e945430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e9458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e945d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e946210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e9466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e946b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e946ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e947490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e947930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e947dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e948270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e948710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e948c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e9491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e949700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e949c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e949f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e94a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e94ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e94b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e94b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e94bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e94c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e94c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e94ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e94d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e94d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e94dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e94e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e94ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e94ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e94f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e94fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e94ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e9504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e950a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e950f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e9514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e951a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e951f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e9524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e9529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e952f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e953490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e9539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e953f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e954480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e9549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e954f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e955470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e9559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e955f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e956460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e9569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e956f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e957450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e9579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e957ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e958440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e958990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e958ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e959430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e959980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e959ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e95a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e95a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e95aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e95b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e95b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e95beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e95c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e95c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e95cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e95d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e95d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e95de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e95e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e95e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e95ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e95f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e95f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e95fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e9603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e960910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e960e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e9613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e961850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e961cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e962190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e962630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e962ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e962f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e963410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e9638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e963d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e9641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e964690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e964b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e964fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e965470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e965910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e965e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e966580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e966ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e9673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e967ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e967da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e968590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e968850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e968e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.155.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.155.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14eb04f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14eb053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14eb05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14eb05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14eb06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14eb06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14eb069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14eb06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14eb072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14eb07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14eb07ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14eb08290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14eb08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14eb09560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14eb09d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14eb0a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14eb0abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14eb0b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14eb0b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14eb0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14eb0c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14eb0cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14eb0d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14eb0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14eb0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14eb0e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14eb0ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14eb0eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14eb0f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14eb0f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14eb0fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14eb10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14eb105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14eb10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14eb10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14eb11140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14eb115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14eb11a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14eb11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14eb12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14eb12770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14eb12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14eb13050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14eb134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14eb13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14eb13da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14eb14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14eb14680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14eb14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14eb14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14eb153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14eb15840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14eb15cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14eb16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14eb16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14eb16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14eb16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14eb17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14eb178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14eb17d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14eb181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14eb18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14eb18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14eb18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14eb19380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14eb197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14eb19c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14eb1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14eb1a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14eb1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14eb1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14eb1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14eb1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14eb1bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14eb1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14eb1c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14eb1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14eb1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14eb1d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14eb1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14eb1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14eb1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14eb1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14eb1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14eb1ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14eb1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14eb1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14eb1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14eb1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14eb20270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14eb206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14eb20b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14eb20fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14eb21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14eb218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14eb21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14eb22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14eb225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14eb22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14eb22ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14eb23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14eb237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14eb23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14eb24090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14eb24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14eb24970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14eb24de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14eb25250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14eb256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14eb25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14eb25fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14eb26410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14eb26880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14eb26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14eb27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14eb275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14eb27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14eb27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14eb28320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14eb28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14eb28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14eb29070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14eb294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14eb29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14eb29dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14eb2a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14eb2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14eb2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14eb2af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14eb2b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14eb2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14eb2bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14eb2c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14eb2c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14eb2ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14eb2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14eb2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14eb2d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14eb2dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14eb2e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14eb2e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14eb2e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14eb2eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14eb2f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14eb2f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14eb2faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14eb2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14eb303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14eb30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14eb30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14eb31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14eb31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14eb31a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14eb31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14eb322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14eb32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14eb32bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14eb33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14eb334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14eb33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14eb33d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14eb341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14eb34660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14eb34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14eb34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14eb353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14eb35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14eb35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14eb36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14eb36570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14eb369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14eb36e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14eb372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14eb37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14eb37ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14eb38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14eb38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14eb388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14eb38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14eb391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14eb39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14eb39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14eb39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14eb3a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14eb3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14eb3ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14eb3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14eb3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14eb3b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14eb3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14eb3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14eb3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14eb3cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14eb3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14eb3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14eb3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14eb3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14eb3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14eb3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14eb3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14eb3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14eb3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14eb3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14eb3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14eb400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14eb40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14eb409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14eb40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14eb413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14eb41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14eb42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14eb42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14eb428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14eb42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14eb431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14eb43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14eb43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14eb43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14eb44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14eb447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14eb44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14eb450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14eb45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14eb459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14eb45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14eb46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14eb46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14eb46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14eb46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14eb47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14eb478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14eb47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14eb481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14eb48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14eb48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14eb48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14eb49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14eb497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14eb49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14eb4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14eb4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14eb4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14eb4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14eb4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14eb4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14eb4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14eb4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14eb4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14eb4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14eb4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14eb4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14eb4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14eb4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14eb4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14eb4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14eb4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14eb4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14eb4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14eb4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14eb4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14eb4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14eb50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14eb506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14eb50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14eb50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14eb51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14eb51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14eb51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14eb52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14eb525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14eb52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14eb52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14eb53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14eb53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14eb53c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14eb54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14eb544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14eb54950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14eb54dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14eb55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14eb556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14eb55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14eb55f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14eb569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14eb57110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14eb57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14eb57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14eb58210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14eb58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14eb58c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14eb59290 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e90ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e90ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e90f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e90f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e90fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e910050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e9104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e910930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e910da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e911210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e911680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e911af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e9123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e912b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e913340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e913a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e914120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e914810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e914f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e915880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e915f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e916660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e916d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e917440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e917b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e917fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e918410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e918880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e918cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e919160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e9195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e919a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e919eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e91a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e91a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e91aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e91aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e91b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e91b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e91bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e91c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e91c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e91c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e91cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e91d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e91d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e91db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e91df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e91e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e91e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e91ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e91f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e91f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e91fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e91fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e920310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e920780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e920bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e921060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e9214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e921940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e921db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e922220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e922690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e922b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e922f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e9233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e923850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e923cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e924130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e9245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e924a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e924e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e9252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e925760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e925bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e926040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e9264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e926920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e926d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e927200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e927670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e927ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e927f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e9283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e928830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e928ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e929110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e929580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e9299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e929e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e92a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e92a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e92abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e92b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e92b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e92b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e92bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e92c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e92c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e92cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e92cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e92d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e92d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e92dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e92e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e92e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e92e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e92ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e92f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e92f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e92fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e930000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e930470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e9308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e930d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e9311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e931630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e931aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e931f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e932380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e9327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e932c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e9330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e933540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e9339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e933e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e934290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e934700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e934b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e934fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e935450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e9358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e935d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e9361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e936610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e936a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e936ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e937360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e9377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e937c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e9380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e938520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e938990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e938e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e939270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e9396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e939b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e939fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e93a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e93a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e93ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e93b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e93b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e93ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e93bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e93c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e93c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e93cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e93d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e93d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e93d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e93dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e93e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e93e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e93eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e93efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e93f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e93f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e93fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e940160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e9405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e940a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e940eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e941320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e941790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e941c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e942070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e9424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e942950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e942dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e943230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e9436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e943b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e943f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e9443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e944860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e944cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e945140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e9455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e945a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e945e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e946300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e946770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e946be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e947050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e9474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e947930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e947da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e948210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e948680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e948af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e948f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e9493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e949840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e949cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e94a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e94a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e94aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e94ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e94b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e94ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e94bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e94c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e94c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e94cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e94d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e94d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e94d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e94dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e94e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e94e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e94eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e94efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e94f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e94f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e94fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e950160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e9505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e950a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e950eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e951320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e951790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e951c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e952070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e9524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e952950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e952dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e953230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e9536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e953b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e953f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e9543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e954860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e954cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e955140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e9555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e955a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e955e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e956300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e956770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e956be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e957050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e9574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e957930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e957da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e958210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e958680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e958af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e958f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e9593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e959840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e959cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e95a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e95a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e95aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e95ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e95b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e95b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e95bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e95c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e95c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e95c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e95cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e95d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e95d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e95dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e95df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e95e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e95e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e95ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e95f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e95f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e95fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e9604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e960bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e9612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e961710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e961b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e961ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e962460 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.925s
user	0m0.319s
sys	0m0.294s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4388 (30caac3a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136f0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136f0e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136f0e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136f0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136f0f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136f0f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136f0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136f10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136f109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136f10ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136f113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136f118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136f123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136f12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136f133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136f13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136f141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136f14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136f15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136f15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136f15f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136f16640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136f16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136f17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136f17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136f185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136f19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136f197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136f19a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136f19f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136f1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136f1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136f1af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136f1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136f1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136f1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136f1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136f1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136f1ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136f1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136f1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136f1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136f1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136f1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136f1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136f1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136f20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136f20620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136f20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136f21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136f21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136f22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136f224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136f22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136f22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136f23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136f23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136f241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136f24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136f24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136f24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136f25420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136f258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136f25d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136f26200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136f26b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136f26fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136f27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136f279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136f27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136f289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136f28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136f29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136f299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136f29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136f2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136f2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136f2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136f2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136f2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136f2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136f2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136f2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136f2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136f2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136f2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136f2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136f2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136f2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136f1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136f2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136f30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136f30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136f30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136f31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136f31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136f31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136f32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136f32550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136f32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136f32ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136f33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136f33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136f33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136f34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136f349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136f34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136f35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136f357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136f35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136f360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136f36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136f36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136f36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136f37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136f37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136f37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136f38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136f385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136f38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136f38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136f393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136f39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136f39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136f3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136f3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136f3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136f3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136f3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136f3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136f3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136f3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136f3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136f3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136f3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136f3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136f3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136f3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136f3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136f3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136f3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136f3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136f3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136f3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136f3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136f402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136f40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136f40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136f410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136f41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136f419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136f41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136f42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136f427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136f42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136f43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136f435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136f43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136f43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136f44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136f44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136f44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136f45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136f45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136f45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136f45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136f463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136f46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136f46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136f471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136f47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136f47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136f47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136f48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136f488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136f48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136f49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136f496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136f49b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136f4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136f4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136f4a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136f4adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136f4b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136f4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136f4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136f4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136f4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136f4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136f4cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136f4d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136f4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136f4e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136f4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136f4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136f4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136f4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136f4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136f504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136f50960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136f50e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136f512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136f51a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136f51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136f524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136f52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136f52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136f534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136f53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136f53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136f544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136f54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136f54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136f554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136f55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136f55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136f564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136f56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136f56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136f574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136f579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136f57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136f58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136f589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136f58f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136f59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136f599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136f59f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136f5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136f5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136f5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136f5b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136f5b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136f5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136f5c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136f5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136f5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136f5d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136f5d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136f5dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136f5e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136f5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136f5eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136f5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136f5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136f5fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136f60410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136f60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136f60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136f61400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136f61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136f61ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136f623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136f62940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136f62e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136f633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136f63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136f63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136f643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136f64870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136f64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136f651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136f65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136f65af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136f65f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136f66430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136f668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136f66d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136f67210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136f676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136f67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136f67ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136f68490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136f68930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136f68e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136f695a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136f69cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136f6a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136f6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136f6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136f6b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136f6b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136f6be80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e08330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e08c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e09080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e09dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e0a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e0ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e0d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e0e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e0ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e0f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e10350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e13520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e13990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e42760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e43920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e44320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e46a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e46e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e4acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e4b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e4b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e4c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e4cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e4d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e4d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e4dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e4ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e4ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e4fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e50100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e50e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e51730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e51ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e52010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e52d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e53640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e53ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e53f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e54390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e54800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e55550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e56b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e57d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e58a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e58f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e59370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e59de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e5a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e5c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e5c680 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e08740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e08bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e09020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e09490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e09900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e09d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e0a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e0aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e0af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e0b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e0c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e0cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e0db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e0f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e10080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e16380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e42df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e43b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e45480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e45d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e46f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e47390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e47800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e47c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e4a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e52980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e52df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e53260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e5a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e5b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e5ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e5be80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.911s
user	0m0.241s
sys	0m0.130s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.15 sec*proc (2 tests)

Total Test time (real) =   1.16 sec
        1.19 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
