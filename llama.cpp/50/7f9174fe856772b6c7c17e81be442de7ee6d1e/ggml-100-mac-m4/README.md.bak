### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.08 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.07 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.82 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.86 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.90 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.83 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 253.08 sec*proc (29 tests)

Total Test time (real) = 253.09 sec

real	4m13.120s
user	8m40.002s
sys	0m7.256s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.05 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.81 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.69 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.11 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.63 sec*proc (29 tests)

Total Test time (real) =  54.64 sec

real	0m54.651s
user	1m17.042s
sys	0m6.281s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.120 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.668 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.441 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.450 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.451 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.452 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.452 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.453 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.456 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.457 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.457 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.458 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.458 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.461 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.462 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.462 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.463 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.464 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.464 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.465 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.003 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.005 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.006 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.006 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.007 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.008 I llama_model_loader: - type  f32:  124 tensors
0.00.025.008 I llama_model_loader: - type  f16:   73 tensors
0.00.025.009 I print_info: file format = GGUF V3 (latest)
0.00.025.010 I print_info: file type   = F16
0.00.025.011 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.248 I load: special tokens cache size = 5
0.00.031.336 I load: token to piece cache size = 0.2032 MB
0.00.031.340 I print_info: arch             = bert
0.00.031.340 I print_info: vocab_only       = 0
0.00.031.340 I print_info: n_ctx_train      = 512
0.00.031.341 I print_info: n_embd           = 384
0.00.031.341 I print_info: n_layer          = 12
0.00.031.344 I print_info: n_head           = 12
0.00.031.345 I print_info: n_head_kv        = 12
0.00.031.345 I print_info: n_rot            = 32
0.00.031.345 I print_info: n_swa            = 0
0.00.031.345 I print_info: n_embd_head_k    = 32
0.00.031.345 I print_info: n_embd_head_v    = 32
0.00.031.346 I print_info: n_gqa            = 1
0.00.031.347 I print_info: n_embd_k_gqa     = 384
0.00.031.348 I print_info: n_embd_v_gqa     = 384
0.00.031.349 I print_info: f_norm_eps       = 1.0e-12
0.00.031.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.352 I print_info: f_logit_scale    = 0.0e+00
0.00.031.353 I print_info: n_ff             = 1536
0.00.031.354 I print_info: n_expert         = 0
0.00.031.354 I print_info: n_expert_used    = 0
0.00.031.355 I print_info: causal attn      = 0
0.00.031.355 I print_info: pooling type     = 2
0.00.031.355 I print_info: rope type        = 2
0.00.031.355 I print_info: rope scaling     = linear
0.00.031.357 I print_info: freq_base_train  = 10000.0
0.00.031.358 I print_info: freq_scale_train = 1
0.00.031.358 I print_info: n_ctx_orig_yarn  = 512
0.00.031.358 I print_info: rope_finetuned   = unknown
0.00.031.359 I print_info: ssm_d_conv       = 0
0.00.031.359 I print_info: ssm_d_inner      = 0
0.00.031.359 I print_info: ssm_d_state      = 0
0.00.031.359 I print_info: ssm_dt_rank      = 0
0.00.031.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.360 I print_info: model type       = 33M
0.00.031.360 I print_info: model params     = 33.21 M
0.00.031.361 I print_info: general.name     = Bge Small
0.00.031.362 I print_info: vocab type       = WPM
0.00.031.363 I print_info: n_vocab          = 30522
0.00.031.363 I print_info: n_merges         = 0
0.00.031.363 I print_info: BOS token        = 101 '[CLS]'
0.00.031.363 I print_info: UNK token        = 100 '[UNK]'
0.00.031.364 I print_info: SEP token        = 102 '[SEP]'
0.00.031.364 I print_info: PAD token        = 0 '[PAD]'
0.00.031.364 I print_info: MASK token       = 103 '[MASK]'
0.00.031.364 I print_info: LF token         = 0 '[PAD]'
0.00.031.365 I print_info: max token length = 21
0.00.031.365 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.034.428 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.429 I load_tensors: offloading output layer to GPU
0.00.034.429 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.454 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.455 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.741 I llama_init_from_model: n_seq_max     = 1
0.00.034.742 I llama_init_from_model: n_ctx         = 512
0.00.034.743 I llama_init_from_model: n_ctx_per_seq = 512
0.00.034.743 I llama_init_from_model: n_batch       = 2048
0.00.034.743 I llama_init_from_model: n_ubatch      = 2048
0.00.034.743 I llama_init_from_model: flash_attn    = 0
0.00.034.744 I llama_init_from_model: freq_base     = 10000.0
0.00.034.744 I llama_init_from_model: freq_scale    = 1
0.00.034.745 I ggml_metal_init: allocating
0.00.034.749 I ggml_metal_init: found device: Apple M4
0.00.034.754 I ggml_metal_init: picking default device: Apple M4
0.00.035.473 I ggml_metal_init: using embedded metal library
0.00.039.566 I ggml_metal_init: GPU name:   Apple M4
0.00.039.569 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.569 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.570 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.570 I ggml_metal_init: simdgroup reduction   = true
0.00.039.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.571 I ggml_metal_init: has residency sets    = true
0.00.039.571 I ggml_metal_init: has bfloat            = true
0.00.039.571 I ggml_metal_init: use bfloat            = true
0.00.039.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.199 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.861 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.863 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.865 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.053 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.054 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.054 I llama_init_from_model: graph nodes  = 429
0.00.053.055 I llama_init_from_model: graph splits = 2
0.00.053.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.056 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.645 I 
0.00.058.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.276 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.379 I llama_perf_context_print:        load time =      43.97 ms
0.00.064.381 I llama_perf_context_print: prompt eval time =       4.96 ms /     9 tokens (    0.55 ms per token,  1815.98 tokens per second)
0.00.064.381 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.382 I llama_perf_context_print:       total time =       5.73 ms /    10 tokens
0.00.064.521 I ggml_metal_free: deallocating

real	0m0.240s
user	0m0.046s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.213 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.887 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.892 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.892 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.893 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.893 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.894 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.894 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.895 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.895 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.895 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.898 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.898 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.899 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.899 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.899 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.899 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.286 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.931 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.932 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.933 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.933 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.934 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.934 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.934 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.934 I llama_model_loader: - type  f32:  124 tensors
0.00.014.935 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.935 I print_info: file format = GGUF V3 (latest)
0.00.014.936 I print_info: file type   = Q8_0
0.00.014.937 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.358 I load: special tokens cache size = 5
0.00.018.621 I load: token to piece cache size = 0.2032 MB
0.00.018.624 I print_info: arch             = bert
0.00.018.624 I print_info: vocab_only       = 0
0.00.018.625 I print_info: n_ctx_train      = 512
0.00.018.625 I print_info: n_embd           = 384
0.00.018.625 I print_info: n_layer          = 12
0.00.018.629 I print_info: n_head           = 12
0.00.018.630 I print_info: n_head_kv        = 12
0.00.018.630 I print_info: n_rot            = 32
0.00.018.630 I print_info: n_swa            = 0
0.00.018.630 I print_info: n_embd_head_k    = 32
0.00.018.630 I print_info: n_embd_head_v    = 32
0.00.018.633 I print_info: n_gqa            = 1
0.00.018.634 I print_info: n_embd_k_gqa     = 384
0.00.018.635 I print_info: n_embd_v_gqa     = 384
0.00.018.635 I print_info: f_norm_eps       = 1.0e-12
0.00.018.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.636 I print_info: f_logit_scale    = 0.0e+00
0.00.018.637 I print_info: n_ff             = 1536
0.00.018.638 I print_info: n_expert         = 0
0.00.018.638 I print_info: n_expert_used    = 0
0.00.018.638 I print_info: causal attn      = 0
0.00.018.638 I print_info: pooling type     = 2
0.00.018.638 I print_info: rope type        = 2
0.00.018.639 I print_info: rope scaling     = linear
0.00.018.639 I print_info: freq_base_train  = 10000.0
0.00.018.639 I print_info: freq_scale_train = 1
0.00.018.639 I print_info: n_ctx_orig_yarn  = 512
0.00.018.639 I print_info: rope_finetuned   = unknown
0.00.018.640 I print_info: ssm_d_conv       = 0
0.00.018.640 I print_info: ssm_d_inner      = 0
0.00.018.640 I print_info: ssm_d_state      = 0
0.00.018.640 I print_info: ssm_dt_rank      = 0
0.00.018.640 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.640 I print_info: model type       = 33M
0.00.018.640 I print_info: model params     = 33.21 M
0.00.018.641 I print_info: general.name     = Bge Small
0.00.018.641 I print_info: vocab type       = WPM
0.00.018.641 I print_info: n_vocab          = 30522
0.00.018.641 I print_info: n_merges         = 0
0.00.018.642 I print_info: BOS token        = 101 '[CLS]'
0.00.018.642 I print_info: UNK token        = 100 '[UNK]'
0.00.018.646 I print_info: SEP token        = 102 '[SEP]'
0.00.018.647 I print_info: PAD token        = 0 '[PAD]'
0.00.018.648 I print_info: MASK token       = 103 '[MASK]'
0.00.018.648 I print_info: LF token         = 0 '[PAD]'
0.00.018.648 I print_info: max token length = 21
0.00.018.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.266 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.267 I load_tensors: offloading output layer to GPU
0.00.020.268 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.274 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.275 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.463 I llama_init_from_model: n_seq_max     = 1
0.00.020.464 I llama_init_from_model: n_ctx         = 512
0.00.020.464 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.464 I llama_init_from_model: n_batch       = 2048
0.00.020.464 I llama_init_from_model: n_ubatch      = 2048
0.00.020.464 I llama_init_from_model: flash_attn    = 0
0.00.020.465 I llama_init_from_model: freq_base     = 10000.0
0.00.020.465 I llama_init_from_model: freq_scale    = 1
0.00.020.466 I ggml_metal_init: allocating
0.00.020.470 I ggml_metal_init: found device: Apple M4
0.00.020.473 I ggml_metal_init: picking default device: Apple M4
0.00.020.998 I ggml_metal_init: using embedded metal library
0.00.023.508 I ggml_metal_init: GPU name:   Apple M4
0.00.023.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.510 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.511 I ggml_metal_init: simdgroup reduction   = true
0.00.023.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.511 I ggml_metal_init: has residency sets    = true
0.00.023.511 I ggml_metal_init: has bfloat            = true
0.00.023.511 I ggml_metal_init: use bfloat            = true
0.00.023.512 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.810 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.415 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.417 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.420 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.450 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.451 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.451 I llama_init_from_model: graph nodes  = 429
0.00.034.451 I llama_init_from_model: graph splits = 2
0.00.034.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.523 I 
0.00.038.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.062 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.428 I llama_perf_context_print:        load time =      29.30 ms
0.00.043.428 I llama_perf_context_print: prompt eval time =       4.24 ms /     9 tokens (    0.47 ms per token,  2124.14 tokens per second)
0.00.043.429 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.429 I llama_perf_context_print:       total time =       4.90 ms /    10 tokens
0.00.043.605 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.029s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.280 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.313 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.776 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.783 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.785 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.785 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.786 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.787 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.788 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.789 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.789 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.794 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.797 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.797 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.798 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.407 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.407 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.407 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.408 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.408 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.408 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.409 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.409 I llama_model_loader: - type  f32:   40 tensors
0.00.047.410 I llama_model_loader: - type  f16:   30 tensors
0.00.047.410 I print_info: file format = GGUF V3 (latest)
0.00.047.411 I print_info: file type   = F16
0.00.047.412 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.051.675 W load: empty token at index 5
0.00.056.688 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.199 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.058.233 I load: special tokens cache size = 5
0.00.320.172 I load: token to piece cache size = 1.5060 MB
0.00.320.181 I print_info: arch             = jina-bert-v2
0.00.320.181 I print_info: vocab_only       = 0
0.00.320.181 I print_info: n_ctx_train      = 8192
0.00.320.181 I print_info: n_embd           = 384
0.00.320.181 I print_info: n_layer          = 4
0.00.320.186 I print_info: n_head           = 12
0.00.320.187 I print_info: n_head_kv        = 12
0.00.320.187 I print_info: n_rot            = 32
0.00.320.187 I print_info: n_swa            = 0
0.00.320.187 I print_info: n_embd_head_k    = 32
0.00.320.187 I print_info: n_embd_head_v    = 32
0.00.320.188 I print_info: n_gqa            = 1
0.00.320.189 I print_info: n_embd_k_gqa     = 384
0.00.320.189 I print_info: n_embd_v_gqa     = 384
0.00.320.190 I print_info: f_norm_eps       = 1.0e-12
0.00.320.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.320.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.320.191 I print_info: f_max_alibi_bias = 8.0e+00
0.00.320.191 I print_info: f_logit_scale    = 0.0e+00
0.00.320.193 I print_info: n_ff             = 1536
0.00.320.193 I print_info: n_expert         = 0
0.00.320.193 I print_info: n_expert_used    = 0
0.00.320.194 I print_info: causal attn      = 0
0.00.320.194 I print_info: pooling type     = -1
0.00.320.194 I print_info: rope type        = -1
0.00.320.194 I print_info: rope scaling     = linear
0.00.320.197 I print_info: freq_base_train  = 10000.0
0.00.320.197 I print_info: freq_scale_train = 1
0.00.320.197 I print_info: n_ctx_orig_yarn  = 8192
0.00.320.197 I print_info: rope_finetuned   = unknown
0.00.320.197 I print_info: ssm_d_conv       = 0
0.00.320.198 I print_info: ssm_d_inner      = 0
0.00.320.198 I print_info: ssm_d_state      = 0
0.00.320.198 I print_info: ssm_dt_rank      = 0
0.00.320.198 I print_info: ssm_dt_b_c_rms   = 0
0.00.320.200 I print_info: model type       = 33M
0.00.320.200 I print_info: model params     = 32.90 M
0.00.320.200 I print_info: general.name     = Jina Bert Implementation
0.00.320.201 I print_info: vocab type       = BPE
0.00.320.201 I print_info: n_vocab          = 61056
0.00.320.201 I print_info: n_merges         = 39382
0.00.320.202 I print_info: BOS token        = 0 '<s>'
0.00.320.202 I print_info: EOS token        = 2 '</s>'
0.00.320.202 I print_info: UNK token        = 3 '<unk>'
0.00.320.204 I print_info: SEP token        = 2 '</s>'
0.00.320.204 I print_info: PAD token        = 1 '<pad>'
0.00.320.204 I print_info: MASK token       = 4 '<mask>'
0.00.320.204 I print_info: EOG token        = 2 '</s>'
0.00.320.204 I print_info: max token length = 45
0.00.320.205 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.465 I load_tensors: offloading 4 repeating layers to GPU
0.00.321.466 I load_tensors: offloading output layer to GPU
0.00.321.466 I load_tensors: offloaded 5/5 layers to GPU
0.00.321.486 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.321.487 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.321.668 I llama_init_from_model: n_seq_max     = 1
0.00.321.669 I llama_init_from_model: n_ctx         = 8192
0.00.321.669 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.321.669 I llama_init_from_model: n_batch       = 2048
0.00.321.669 I llama_init_from_model: n_ubatch      = 2048
0.00.321.670 I llama_init_from_model: flash_attn    = 0
0.00.321.670 I llama_init_from_model: freq_base     = 10000.0
0.00.321.670 I llama_init_from_model: freq_scale    = 1
0.00.321.671 I ggml_metal_init: allocating
0.00.321.675 I ggml_metal_init: found device: Apple M4
0.00.321.678 I ggml_metal_init: picking default device: Apple M4
0.00.322.254 I ggml_metal_init: using embedded metal library
0.00.324.838 I ggml_metal_init: GPU name:   Apple M4
0.00.324.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.324.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.324.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.324.841 I ggml_metal_init: simdgroup reduction   = true
0.00.324.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.324.841 I ggml_metal_init: has residency sets    = true
0.00.324.842 I ggml_metal_init: has bfloat            = true
0.00.324.842 I ggml_metal_init: use bfloat            = true
0.00.324.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.324.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.335.163 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.338.351 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.338.353 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.338.355 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.344.630 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.344.634 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.344.634 I llama_init_from_model: graph nodes  = 154
0.00.344.634 I llama_init_from_model: graph splits = 2
0.00.344.636 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.344.636 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.763 I 
0.00.351.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.351.894 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.351.895 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.351.898 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.351.898 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.351.903 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.351.903 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.352.390 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.355.894 I llama_perf_context_print:        load time =     330.44 ms
0.00.355.896 I llama_perf_context_print: prompt eval time =       3.50 ms /    62 tokens (    0.06 ms per token, 17739.63 tokens per second)
0.00.355.897 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.355.897 I llama_perf_context_print:       total time =       4.13 ms /    63 tokens
0.00.356.160 I ggml_metal_free: deallocating

real	0m1.068s
user	0m0.329s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.230 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.390 I main: llama backend init
0.00.000.397 I main: load the model and apply lora adapter, if any
0.00.049.068 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.062.098 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.081.093 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.094 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.094 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.096 I llama_model_loader: - type  f32:  194 tensors
0.00.081.096 I llama_model_loader: - type  f16:   98 tensors
0.00.081.098 I print_info: file format = GGUF V3 (latest)
0.00.081.100 I print_info: file type   = all F32 (guessed)
0.00.081.102 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.112 I load: special tokens cache size = 25
0.00.105.357 I load: token to piece cache size = 0.2984 MB
0.00.105.360 I print_info: arch             = gptneox
0.00.105.361 I print_info: vocab_only       = 0
0.00.105.361 I print_info: n_ctx_train      = 2048
0.00.105.361 I print_info: n_embd           = 2048
0.00.105.361 I print_info: n_layer          = 24
0.00.105.366 I print_info: n_head           = 16
0.00.105.367 I print_info: n_head_kv        = 16
0.00.105.367 I print_info: n_rot            = 32
0.00.105.367 I print_info: n_swa            = 0
0.00.105.368 I print_info: n_embd_head_k    = 128
0.00.105.368 I print_info: n_embd_head_v    = 128
0.00.105.368 I print_info: n_gqa            = 1
0.00.105.369 I print_info: n_embd_k_gqa     = 2048
0.00.105.370 I print_info: n_embd_v_gqa     = 2048
0.00.105.371 I print_info: f_norm_eps       = 1.0e-05
0.00.105.371 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.372 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.372 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.372 I print_info: f_logit_scale    = 0.0e+00
0.00.105.373 I print_info: n_ff             = 8192
0.00.105.373 I print_info: n_expert         = 0
0.00.105.373 I print_info: n_expert_used    = 0
0.00.105.373 I print_info: causal attn      = 1
0.00.105.374 I print_info: pooling type     = 0
0.00.105.374 I print_info: rope type        = 2
0.00.105.374 I print_info: rope scaling     = linear
0.00.105.375 I print_info: freq_base_train  = 10000.0
0.00.105.375 I print_info: freq_scale_train = 1
0.00.105.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.375 I print_info: rope_finetuned   = unknown
0.00.105.375 I print_info: ssm_d_conv       = 0
0.00.105.377 I print_info: ssm_d_inner      = 0
0.00.105.377 I print_info: ssm_d_state      = 0
0.00.105.378 I print_info: ssm_dt_rank      = 0
0.00.105.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.378 I print_info: model type       = 1.4B
0.00.105.378 I print_info: model params     = 1.41 B
0.00.105.379 I print_info: general.name     = 1.4B
0.00.105.379 I print_info: vocab type       = BPE
0.00.105.380 I print_info: n_vocab          = 50304
0.00.105.380 I print_info: n_merges         = 50009
0.00.105.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.381 I print_info: LF token         = 187 ''
0.00.105.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.382 I print_info: max token length = 1024
0.00.105.382 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.142.230 I load_tensors: offloading 24 repeating layers to GPU
0.00.142.233 I load_tensors: offloading output layer to GPU
0.00.142.234 I load_tensors: offloaded 25/25 layers to GPU
0.00.142.255 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.142.256 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.142.600 I llama_init_from_model: n_seq_max     = 1
0.00.142.601 I llama_init_from_model: n_ctx         = 2048
0.00.142.601 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.142.601 I llama_init_from_model: n_batch       = 2048
0.00.142.601 I llama_init_from_model: n_ubatch      = 512
0.00.142.601 I llama_init_from_model: flash_attn    = 0
0.00.142.602 I llama_init_from_model: freq_base     = 10000.0
0.00.142.602 I llama_init_from_model: freq_scale    = 1
0.00.142.603 I ggml_metal_init: allocating
0.00.142.621 I ggml_metal_init: found device: Apple M4
0.00.142.625 I ggml_metal_init: picking default device: Apple M4
0.00.143.231 I ggml_metal_init: using embedded metal library
0.00.440.535 I ggml_metal_init: GPU name:   Apple M4
0.00.440.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.440.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.440.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.440.550 I ggml_metal_init: simdgroup reduction   = true
0.00.440.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.440.551 I ggml_metal_init: has residency sets    = true
0.00.440.551 I ggml_metal_init: has bfloat            = true
0.00.440.551 I ggml_metal_init: use bfloat            = true
0.00.440.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.440.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.476.604 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.516.901 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.516.909 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.516.934 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.520.674 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.520.677 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.520.678 I llama_init_from_model: graph nodes  = 967
0.00.520.678 I llama_init_from_model: graph splits = 2
0.00.520.681 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.520.810 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.520.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.838 I main: llama threadpool init, n_threads = 4
0.00.589.874 I 
0.00.589.910 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.911 I 
0.00.589.962 I sampler seed: 1234
0.00.589.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.998 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.429.574 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.02.429.575 I llama_perf_context_print:        load time =     539.62 ms
0.02.429.575 I llama_perf_context_print: prompt eval time =      54.67 ms /     7 tokens (    7.81 ms per token,   128.03 tokens per second)
0.02.429.576 I llama_perf_context_print:        eval time =    1782.08 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.429.576 I llama_perf_context_print:       total time =    1840.88 ms /    70 tokens
0.02.429.763 I ggml_metal_free: deallocating

real	0m2.776s
user	0m0.149s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.759 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.776 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.784 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.805 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.546 I llama_model_loader: - type  f32:  194 tensors
0.00.056.546 I llama_model_loader: - type  f16:   98 tensors
0.00.056.547 I print_info: file format = GGUF V3 (latest)
0.00.056.548 I print_info: file type   = all F32 (guessed)
0.00.056.550 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.318 I load: special tokens cache size = 25
0.00.077.878 I load: token to piece cache size = 0.2984 MB
0.00.077.881 I print_info: arch             = gptneox
0.00.077.882 I print_info: vocab_only       = 0
0.00.077.882 I print_info: n_ctx_train      = 2048
0.00.077.882 I print_info: n_embd           = 2048
0.00.077.882 I print_info: n_layer          = 24
0.00.077.886 I print_info: n_head           = 16
0.00.077.887 I print_info: n_head_kv        = 16
0.00.077.887 I print_info: n_rot            = 32
0.00.077.887 I print_info: n_swa            = 0
0.00.077.887 I print_info: n_embd_head_k    = 128
0.00.077.889 I print_info: n_embd_head_v    = 128
0.00.077.890 I print_info: n_gqa            = 1
0.00.077.891 I print_info: n_embd_k_gqa     = 2048
0.00.077.892 I print_info: n_embd_v_gqa     = 2048
0.00.077.892 I print_info: f_norm_eps       = 1.0e-05
0.00.077.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.894 I print_info: f_logit_scale    = 0.0e+00
0.00.077.895 I print_info: n_ff             = 8192
0.00.077.895 I print_info: n_expert         = 0
0.00.077.895 I print_info: n_expert_used    = 0
0.00.077.896 I print_info: causal attn      = 1
0.00.077.896 I print_info: pooling type     = 0
0.00.077.896 I print_info: rope type        = 2
0.00.077.896 I print_info: rope scaling     = linear
0.00.077.897 I print_info: freq_base_train  = 10000.0
0.00.077.897 I print_info: freq_scale_train = 1
0.00.077.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.897 I print_info: rope_finetuned   = unknown
0.00.077.898 I print_info: ssm_d_conv       = 0
0.00.077.898 I print_info: ssm_d_inner      = 0
0.00.077.898 I print_info: ssm_d_state      = 0
0.00.077.899 I print_info: ssm_dt_rank      = 0
0.00.077.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.900 I print_info: model type       = 1.4B
0.00.077.900 I print_info: model params     = 1.41 B
0.00.077.900 I print_info: general.name     = 1.4B
0.00.077.901 I print_info: vocab type       = BPE
0.00.077.901 I print_info: n_vocab          = 50304
0.00.077.901 I print_info: n_merges         = 50009
0.00.077.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.902 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.902 I print_info: LF token         = 187 ''
0.00.077.904 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.904 I print_info: max token length = 1024
0.00.077.904 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.204.887 I load_tensors: offloading 24 repeating layers to GPU
0.01.204.892 I load_tensors: offloading output layer to GPU
0.01.204.893 I load_tensors: offloaded 25/25 layers to GPU
0.01.204.919 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.204.922 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.205.901 I llama_init_from_model: n_seq_max     = 1
0.01.205.902 I llama_init_from_model: n_ctx         = 128
0.01.205.903 I llama_init_from_model: n_ctx_per_seq = 128
0.01.205.903 I llama_init_from_model: n_batch       = 128
0.01.205.903 I llama_init_from_model: n_ubatch      = 128
0.01.205.903 I llama_init_from_model: flash_attn    = 0
0.01.205.904 I llama_init_from_model: freq_base     = 10000.0
0.01.205.904 I llama_init_from_model: freq_scale    = 1
0.01.205.905 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.205.909 I ggml_metal_init: allocating
0.01.206.006 I ggml_metal_init: found device: Apple M4
0.01.206.015 I ggml_metal_init: picking default device: Apple M4
0.01.207.208 I ggml_metal_init: using embedded metal library
0.01.210.975 I ggml_metal_init: GPU name:   Apple M4
0.01.210.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.210.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.210.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.210.979 I ggml_metal_init: simdgroup reduction   = true
0.01.210.980 I ggml_metal_init: simdgroup matrix mul. = true
0.01.210.980 I ggml_metal_init: has residency sets    = true
0.01.210.980 I ggml_metal_init: has bfloat            = true
0.01.210.980 I ggml_metal_init: use bfloat            = true
0.01.210.981 I ggml_metal_init: hasUnifiedMemory      = true
0.01.210.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.221.549 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.223.260 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.223.262 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.223.292 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.224.912 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.224.913 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.224.914 I llama_init_from_model: graph nodes  = 967
0.01.224.914 I llama_init_from_model: graph splits = 2
0.01.224.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.224.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.260.503 I 
0.01.260.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.260.561 I perplexity: tokenizing the input ..
0.01.265.594 I perplexity: tokenization took 5.031 ms
0.01.265.616 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.383.685 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.385.096 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.385.109 I llama_perf_context_print:        load time =    1235.71 ms
0.01.385.110 I llama_perf_context_print: prompt eval time =     117.80 ms /   128 tokens (    0.92 ms per token,  1086.59 tokens per second)
0.01.385.111 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.385.112 I llama_perf_context_print:       total time =     124.61 ms /   129 tokens
0.01.385.435 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.098s
sys	0m0.239s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.369 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.370 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.373 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.374 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.376 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.376 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.377 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.353 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.306 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.307 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.307 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.308 I llama_model_loader: - type  f32:  194 tensors
0.00.036.308 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.308 I print_info: file format = GGUF V3 (latest)
0.00.036.309 I print_info: file type   = Q8_0
0.00.036.312 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.286 I load: special tokens cache size = 25
0.00.052.184 I load: token to piece cache size = 0.2984 MB
0.00.052.188 I print_info: arch             = gptneox
0.00.052.189 I print_info: vocab_only       = 0
0.00.052.189 I print_info: n_ctx_train      = 2048
0.00.052.190 I print_info: n_embd           = 2048
0.00.052.191 I print_info: n_layer          = 24
0.00.052.197 I print_info: n_head           = 16
0.00.052.198 I print_info: n_head_kv        = 16
0.00.052.198 I print_info: n_rot            = 32
0.00.052.198 I print_info: n_swa            = 0
0.00.052.198 I print_info: n_embd_head_k    = 128
0.00.052.199 I print_info: n_embd_head_v    = 128
0.00.052.199 I print_info: n_gqa            = 1
0.00.052.200 I print_info: n_embd_k_gqa     = 2048
0.00.052.200 I print_info: n_embd_v_gqa     = 2048
0.00.052.201 I print_info: f_norm_eps       = 1.0e-05
0.00.052.202 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.202 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.202 I print_info: f_logit_scale    = 0.0e+00
0.00.052.203 I print_info: n_ff             = 8192
0.00.052.203 I print_info: n_expert         = 0
0.00.052.203 I print_info: n_expert_used    = 0
0.00.052.203 I print_info: causal attn      = 1
0.00.052.204 I print_info: pooling type     = 0
0.00.052.204 I print_info: rope type        = 2
0.00.052.204 I print_info: rope scaling     = linear
0.00.052.204 I print_info: freq_base_train  = 10000.0
0.00.052.205 I print_info: freq_scale_train = 1
0.00.052.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.206 I print_info: rope_finetuned   = unknown
0.00.052.206 I print_info: ssm_d_conv       = 0
0.00.052.206 I print_info: ssm_d_inner      = 0
0.00.052.206 I print_info: ssm_d_state      = 0
0.00.052.206 I print_info: ssm_dt_rank      = 0
0.00.052.206 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.207 I print_info: model type       = 1.4B
0.00.052.207 I print_info: model params     = 1.41 B
0.00.052.208 I print_info: general.name     = 1.4B
0.00.052.209 I print_info: vocab type       = BPE
0.00.052.209 I print_info: n_vocab          = 50304
0.00.052.210 I print_info: n_merges         = 50009
0.00.052.210 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.210 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.210 I print_info: LF token         = 187 ''
0.00.052.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.211 I print_info: max token length = 1024
0.00.052.211 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.221.825 I load_tensors: offloading 24 repeating layers to GPU
0.01.221.830 I load_tensors: offloading output layer to GPU
0.01.221.831 I load_tensors: offloaded 25/25 layers to GPU
0.01.221.856 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.221.857 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.222.975 I llama_init_from_model: n_seq_max     = 1
0.01.222.977 I llama_init_from_model: n_ctx         = 2048
0.01.222.977 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.222.978 I llama_init_from_model: n_batch       = 2048
0.01.222.978 I llama_init_from_model: n_ubatch      = 512
0.01.222.978 I llama_init_from_model: flash_attn    = 0
0.01.222.979 I llama_init_from_model: freq_base     = 10000.0
0.01.222.979 I llama_init_from_model: freq_scale    = 1
0.01.222.980 I ggml_metal_init: allocating
0.01.222.994 I ggml_metal_init: found device: Apple M4
0.01.223.002 I ggml_metal_init: picking default device: Apple M4
0.01.224.215 I ggml_metal_init: using embedded metal library
0.01.229.762 I ggml_metal_init: GPU name:   Apple M4
0.01.229.766 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.229.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.229.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.229.768 I ggml_metal_init: simdgroup reduction   = true
0.01.229.768 I ggml_metal_init: simdgroup matrix mul. = true
0.01.229.769 I ggml_metal_init: has residency sets    = true
0.01.229.769 I ggml_metal_init: has bfloat            = true
0.01.229.769 I ggml_metal_init: use bfloat            = true
0.01.229.770 I ggml_metal_init: hasUnifiedMemory      = true
0.01.229.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.245.066 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.300.374 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.300.381 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.300.406 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.305.001 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.305.003 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.305.004 I llama_init_from_model: graph nodes  = 967
0.01.305.004 I llama_init_from_model: graph splits = 2
0.01.305.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.305.144 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.305.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.358.356 I main: llama threadpool init, n_threads = 4
0.01.358.399 I 
0.01.358.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.358.426 I 
0.01.358.576 I sampler seed: 1234
0.01.358.580 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.358.602 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.358.602 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.358.602 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.442.235 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.02.442.235 I llama_perf_context_print:        load time =    1347.69 ms
0.02.442.237 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.75 tokens per second)
0.02.442.237 I llama_perf_context_print:        eval time =    1041.03 ms /    63 runs   (   16.52 ms per token,    60.52 tokens per second)
0.02.442.238 I llama_perf_context_print:       total time =    1084.57 ms /    70 tokens
0.02.442.520 I ggml_metal_free: deallocating

real	0m2.460s
user	0m0.109s
sys	0m0.277s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.066 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.067 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.067 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.068 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.068 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.071 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.074 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.969 I llama_model_loader: - type  f32:  194 tensors
0.00.026.969 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.970 I print_info: file format = GGUF V3 (latest)
0.00.026.970 I print_info: file type   = Q8_0
0.00.026.971 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.300 I load: special tokens cache size = 25
0.00.041.306 I load: token to piece cache size = 0.2984 MB
0.00.041.313 I print_info: arch             = gptneox
0.00.041.313 I print_info: vocab_only       = 0
0.00.041.313 I print_info: n_ctx_train      = 2048
0.00.041.313 I print_info: n_embd           = 2048
0.00.041.314 I print_info: n_layer          = 24
0.00.041.317 I print_info: n_head           = 16
0.00.041.318 I print_info: n_head_kv        = 16
0.00.041.318 I print_info: n_rot            = 32
0.00.041.318 I print_info: n_swa            = 0
0.00.041.319 I print_info: n_embd_head_k    = 128
0.00.041.319 I print_info: n_embd_head_v    = 128
0.00.041.320 I print_info: n_gqa            = 1
0.00.041.320 I print_info: n_embd_k_gqa     = 2048
0.00.041.321 I print_info: n_embd_v_gqa     = 2048
0.00.041.322 I print_info: f_norm_eps       = 1.0e-05
0.00.041.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.323 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.323 I print_info: f_logit_scale    = 0.0e+00
0.00.041.324 I print_info: n_ff             = 8192
0.00.041.324 I print_info: n_expert         = 0
0.00.041.324 I print_info: n_expert_used    = 0
0.00.041.324 I print_info: causal attn      = 1
0.00.041.324 I print_info: pooling type     = 0
0.00.041.324 I print_info: rope type        = 2
0.00.041.325 I print_info: rope scaling     = linear
0.00.041.325 I print_info: freq_base_train  = 10000.0
0.00.041.325 I print_info: freq_scale_train = 1
0.00.041.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.326 I print_info: rope_finetuned   = unknown
0.00.041.326 I print_info: ssm_d_conv       = 0
0.00.041.326 I print_info: ssm_d_inner      = 0
0.00.041.326 I print_info: ssm_d_state      = 0
0.00.041.327 I print_info: ssm_dt_rank      = 0
0.00.041.327 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.327 I print_info: model type       = 1.4B
0.00.041.327 I print_info: model params     = 1.41 B
0.00.041.327 I print_info: general.name     = 1.4B
0.00.041.328 I print_info: vocab type       = BPE
0.00.041.328 I print_info: n_vocab          = 50304
0.00.041.329 I print_info: n_merges         = 50009
0.00.041.330 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.330 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.330 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.331 I print_info: LF token         = 187 ''
0.00.041.331 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.331 I print_info: max token length = 1024
0.00.041.332 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.891.399 I load_tensors: offloading 24 repeating layers to GPU
0.00.891.407 I load_tensors: offloading output layer to GPU
0.00.891.408 I load_tensors: offloaded 25/25 layers to GPU
0.00.891.441 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.891.442 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.892.831 I llama_init_from_model: n_seq_max     = 1
0.00.892.833 I llama_init_from_model: n_ctx         = 128
0.00.892.833 I llama_init_from_model: n_ctx_per_seq = 128
0.00.892.833 I llama_init_from_model: n_batch       = 128
0.00.892.834 I llama_init_from_model: n_ubatch      = 128
0.00.892.834 I llama_init_from_model: flash_attn    = 0
0.00.892.835 I llama_init_from_model: freq_base     = 10000.0
0.00.892.836 I llama_init_from_model: freq_scale    = 1
0.00.892.836 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.892.837 I ggml_metal_init: allocating
0.00.892.871 I ggml_metal_init: found device: Apple M4
0.00.892.881 I ggml_metal_init: picking default device: Apple M4
0.00.894.178 I ggml_metal_init: using embedded metal library
0.00.899.608 I ggml_metal_init: GPU name:   Apple M4
0.00.899.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.899.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.899.612 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.899.613 I ggml_metal_init: simdgroup reduction   = true
0.00.899.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.899.613 I ggml_metal_init: has residency sets    = true
0.00.899.614 I ggml_metal_init: has bfloat            = true
0.00.899.614 I ggml_metal_init: use bfloat            = true
0.00.899.615 I ggml_metal_init: hasUnifiedMemory      = true
0.00.899.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.916.198 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.919.702 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.919.709 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.919.745 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.922.905 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.922.907 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.922.907 I llama_init_from_model: graph nodes  = 967
0.00.922.907 I llama_init_from_model: graph splits = 2
0.00.922.910 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.910 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.951.247 I 
0.00.951.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.951.330 I perplexity: tokenizing the input ..
0.00.959.028 I perplexity: tokenization took 7.695 ms
0.00.959.047 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.098.074 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.099.580 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.099.593 I llama_perf_context_print:        load time =     940.39 ms
0.01.099.594 I llama_perf_context_print: prompt eval time =     138.04 ms /   128 tokens (    1.08 ms per token,   927.26 tokens per second)
0.01.099.595 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.099.595 I llama_perf_context_print:       total time =     148.35 ms /   129 tokens
0.01.099.956 I ggml_metal_free: deallocating

real	0m1.114s
user	0m0.078s
sys	0m0.162s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.010.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.247 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.250 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.250 I llama_model_loader: - type  f32:  194 tensors
0.00.027.250 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.250 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.251 I print_info: file format = GGUF V3 (latest)
0.00.027.252 I print_info: file type   = Q4_0
0.00.027.252 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.325 I load: special tokens cache size = 25
0.00.041.362 I load: token to piece cache size = 0.2984 MB
0.00.041.365 I print_info: arch             = gptneox
0.00.041.366 I print_info: vocab_only       = 0
0.00.041.366 I print_info: n_ctx_train      = 2048
0.00.041.366 I print_info: n_embd           = 2048
0.00.041.366 I print_info: n_layer          = 24
0.00.041.370 I print_info: n_head           = 16
0.00.041.371 I print_info: n_head_kv        = 16
0.00.041.371 I print_info: n_rot            = 32
0.00.041.373 I print_info: n_swa            = 0
0.00.041.373 I print_info: n_embd_head_k    = 128
0.00.041.373 I print_info: n_embd_head_v    = 128
0.00.041.374 I print_info: n_gqa            = 1
0.00.041.375 I print_info: n_embd_k_gqa     = 2048
0.00.041.378 I print_info: n_embd_v_gqa     = 2048
0.00.041.379 I print_info: f_norm_eps       = 1.0e-05
0.00.041.379 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.379 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.379 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.380 I print_info: f_logit_scale    = 0.0e+00
0.00.041.380 I print_info: n_ff             = 8192
0.00.041.380 I print_info: n_expert         = 0
0.00.041.381 I print_info: n_expert_used    = 0
0.00.041.381 I print_info: causal attn      = 1
0.00.041.381 I print_info: pooling type     = 0
0.00.041.381 I print_info: rope type        = 2
0.00.041.383 I print_info: rope scaling     = linear
0.00.041.384 I print_info: freq_base_train  = 10000.0
0.00.041.384 I print_info: freq_scale_train = 1
0.00.041.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.384 I print_info: rope_finetuned   = unknown
0.00.041.384 I print_info: ssm_d_conv       = 0
0.00.041.384 I print_info: ssm_d_inner      = 0
0.00.041.385 I print_info: ssm_d_state      = 0
0.00.041.385 I print_info: ssm_dt_rank      = 0
0.00.041.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.385 I print_info: model type       = 1.4B
0.00.041.386 I print_info: model params     = 1.41 B
0.00.041.386 I print_info: general.name     = 1.4B
0.00.041.386 I print_info: vocab type       = BPE
0.00.041.387 I print_info: n_vocab          = 50304
0.00.041.387 I print_info: n_merges         = 50009
0.00.041.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.387 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.387 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.388 I print_info: LF token         = 187 ''
0.00.041.388 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.388 I print_info: max token length = 1024
0.00.041.389 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.704 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.716 I load_tensors: offloading output layer to GPU
0.00.605.717 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.750 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.605.752 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.607.212 I llama_init_from_model: n_seq_max     = 1
0.00.607.215 I llama_init_from_model: n_ctx         = 2048
0.00.607.216 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.607.217 I llama_init_from_model: n_batch       = 2048
0.00.607.217 I llama_init_from_model: n_ubatch      = 512
0.00.607.218 I llama_init_from_model: flash_attn    = 0
0.00.607.221 I llama_init_from_model: freq_base     = 10000.0
0.00.607.221 I llama_init_from_model: freq_scale    = 1
0.00.607.224 I ggml_metal_init: allocating
0.00.607.296 I ggml_metal_init: found device: Apple M4
0.00.607.310 I ggml_metal_init: picking default device: Apple M4
0.00.609.186 I ggml_metal_init: using embedded metal library
0.00.615.379 I ggml_metal_init: GPU name:   Apple M4
0.00.615.383 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.384 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.385 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.386 I ggml_metal_init: simdgroup reduction   = true
0.00.615.386 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.387 I ggml_metal_init: has residency sets    = true
0.00.615.387 I ggml_metal_init: has bfloat            = true
0.00.615.387 I ggml_metal_init: use bfloat            = true
0.00.615.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.913 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.157 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.163 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.187 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.176 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.178 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.178 I llama_init_from_model: graph nodes  = 967
0.00.696.178 I llama_init_from_model: graph splits = 2
0.00.696.183 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.394 I main: llama threadpool init, n_threads = 4
0.00.753.443 I 
0.00.753.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.468 I 
0.00.753.646 I sampler seed: 1234
0.00.753.651 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.695 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.698 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.699 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.435.016 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.435.016 I llama_perf_context_print:        load time =     741.77 ms
0.01.435.017 I llama_perf_context_print: prompt eval time =      48.96 ms /     7 tokens (    6.99 ms per token,   142.98 tokens per second)
0.01.435.019 I llama_perf_context_print:        eval time =     629.44 ms /    63 runs   (    9.99 ms per token,   100.09 tokens per second)
0.01.435.019 I llama_perf_context_print:       total time =     682.31 ms /    70 tokens
0.01.435.338 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.199 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.393 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.396 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.408 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.197 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.029 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.030 I llama_model_loader: - type  f32:  194 tensors
0.00.026.030 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.031 I print_info: file format = GGUF V3 (latest)
0.00.026.032 I print_info: file type   = Q4_0
0.00.026.033 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.946 I load: special tokens cache size = 25
0.00.040.025 I load: token to piece cache size = 0.2984 MB
0.00.040.029 I print_info: arch             = gptneox
0.00.040.029 I print_info: vocab_only       = 0
0.00.040.030 I print_info: n_ctx_train      = 2048
0.00.040.030 I print_info: n_embd           = 2048
0.00.040.030 I print_info: n_layer          = 24
0.00.040.035 I print_info: n_head           = 16
0.00.040.036 I print_info: n_head_kv        = 16
0.00.040.036 I print_info: n_rot            = 32
0.00.040.036 I print_info: n_swa            = 0
0.00.040.036 I print_info: n_embd_head_k    = 128
0.00.040.036 I print_info: n_embd_head_v    = 128
0.00.040.037 I print_info: n_gqa            = 1
0.00.040.038 I print_info: n_embd_k_gqa     = 2048
0.00.040.038 I print_info: n_embd_v_gqa     = 2048
0.00.040.039 I print_info: f_norm_eps       = 1.0e-05
0.00.040.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.040 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.040 I print_info: f_logit_scale    = 0.0e+00
0.00.040.041 I print_info: n_ff             = 8192
0.00.040.041 I print_info: n_expert         = 0
0.00.040.041 I print_info: n_expert_used    = 0
0.00.040.041 I print_info: causal attn      = 1
0.00.040.041 I print_info: pooling type     = 0
0.00.040.044 I print_info: rope type        = 2
0.00.040.044 I print_info: rope scaling     = linear
0.00.040.045 I print_info: freq_base_train  = 10000.0
0.00.040.045 I print_info: freq_scale_train = 1
0.00.040.045 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.045 I print_info: rope_finetuned   = unknown
0.00.040.046 I print_info: ssm_d_conv       = 0
0.00.040.046 I print_info: ssm_d_inner      = 0
0.00.040.046 I print_info: ssm_d_state      = 0
0.00.040.046 I print_info: ssm_dt_rank      = 0
0.00.040.046 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.047 I print_info: model type       = 1.4B
0.00.040.047 I print_info: model params     = 1.41 B
0.00.040.047 I print_info: general.name     = 1.4B
0.00.040.048 I print_info: vocab type       = BPE
0.00.040.048 I print_info: n_vocab          = 50304
0.00.040.048 I print_info: n_merges         = 50009
0.00.040.048 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.048 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: LF token         = 187 ''
0.00.040.049 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.049 I print_info: max token length = 1024
0.00.040.050 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.225 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.240 I load_tensors: offloading output layer to GPU
0.00.600.241 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.285 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.600.287 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.601.892 I llama_init_from_model: n_seq_max     = 1
0.00.601.895 I llama_init_from_model: n_ctx         = 128
0.00.601.896 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.896 I llama_init_from_model: n_batch       = 128
0.00.601.897 I llama_init_from_model: n_ubatch      = 128
0.00.601.897 I llama_init_from_model: flash_attn    = 0
0.00.601.899 I llama_init_from_model: freq_base     = 10000.0
0.00.601.899 I llama_init_from_model: freq_scale    = 1
0.00.601.900 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.902 I ggml_metal_init: allocating
0.00.602.010 I ggml_metal_init: found device: Apple M4
0.00.602.024 I ggml_metal_init: picking default device: Apple M4
0.00.603.888 I ggml_metal_init: using embedded metal library
0.00.609.379 I ggml_metal_init: GPU name:   Apple M4
0.00.609.384 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.385 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.387 I ggml_metal_init: simdgroup reduction   = true
0.00.609.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.387 I ggml_metal_init: has residency sets    = true
0.00.609.388 I ggml_metal_init: has bfloat            = true
0.00.609.388 I ggml_metal_init: use bfloat            = true
0.00.609.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.849 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.476 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.480 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.507 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.687 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.689 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.690 I llama_init_from_model: graph nodes  = 967
0.00.635.690 I llama_init_from_model: graph splits = 2
0.00.635.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.374 I 
0.00.664.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.488 I perplexity: tokenizing the input ..
0.00.671.636 I perplexity: tokenization took 7.143 ms
0.00.671.657 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.010 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.810.341 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.810.354 I llama_perf_context_print:        load time =     654.16 ms
0.00.810.355 I llama_perf_context_print: prompt eval time =     136.43 ms /   128 tokens (    1.07 ms per token,   938.20 tokens per second)
0.00.810.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.356 I llama_perf_context_print:       total time =     145.98 ms /   129 tokens
0.00.810.739 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.080s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.390 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.390 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.390 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.392 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.393 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.393 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.393 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.397 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.762 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.762 I llama_model_loader: - type  f32:  194 tensors
0.00.024.762 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.763 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.763 I print_info: file format = GGUF V3 (latest)
0.00.024.764 I print_info: file type   = Q4_1
0.00.024.765 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.597 I load: special tokens cache size = 25
0.00.038.601 I load: token to piece cache size = 0.2984 MB
0.00.038.603 I print_info: arch             = gptneox
0.00.038.604 I print_info: vocab_only       = 0
0.00.038.604 I print_info: n_ctx_train      = 2048
0.00.038.604 I print_info: n_embd           = 2048
0.00.038.604 I print_info: n_layer          = 24
0.00.038.607 I print_info: n_head           = 16
0.00.038.608 I print_info: n_head_kv        = 16
0.00.038.608 I print_info: n_rot            = 32
0.00.038.608 I print_info: n_swa            = 0
0.00.038.608 I print_info: n_embd_head_k    = 128
0.00.038.608 I print_info: n_embd_head_v    = 128
0.00.038.609 I print_info: n_gqa            = 1
0.00.038.610 I print_info: n_embd_k_gqa     = 2048
0.00.038.612 I print_info: n_embd_v_gqa     = 2048
0.00.038.613 I print_info: f_norm_eps       = 1.0e-05
0.00.038.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.616 I print_info: f_logit_scale    = 0.0e+00
0.00.038.616 I print_info: n_ff             = 8192
0.00.038.616 I print_info: n_expert         = 0
0.00.038.617 I print_info: n_expert_used    = 0
0.00.038.617 I print_info: causal attn      = 1
0.00.038.617 I print_info: pooling type     = 0
0.00.038.618 I print_info: rope type        = 2
0.00.038.620 I print_info: rope scaling     = linear
0.00.038.620 I print_info: freq_base_train  = 10000.0
0.00.038.620 I print_info: freq_scale_train = 1
0.00.038.620 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.621 I print_info: rope_finetuned   = unknown
0.00.038.621 I print_info: ssm_d_conv       = 0
0.00.038.621 I print_info: ssm_d_inner      = 0
0.00.038.621 I print_info: ssm_d_state      = 0
0.00.038.621 I print_info: ssm_dt_rank      = 0
0.00.038.621 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.621 I print_info: model type       = 1.4B
0.00.038.622 I print_info: model params     = 1.41 B
0.00.038.622 I print_info: general.name     = 1.4B
0.00.038.622 I print_info: vocab type       = BPE
0.00.038.623 I print_info: n_vocab          = 50304
0.00.038.623 I print_info: n_merges         = 50009
0.00.038.627 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.629 I print_info: LF token         = 187 ''
0.00.038.629 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.629 I print_info: max token length = 1024
0.00.038.629 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.689.285 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.301 I load_tensors: offloading output layer to GPU
0.00.689.302 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.338 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.689.339 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.690.876 I llama_init_from_model: n_seq_max     = 1
0.00.690.878 I llama_init_from_model: n_ctx         = 2048
0.00.690.879 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.690.880 I llama_init_from_model: n_batch       = 2048
0.00.690.880 I llama_init_from_model: n_ubatch      = 512
0.00.690.881 I llama_init_from_model: flash_attn    = 0
0.00.690.886 I llama_init_from_model: freq_base     = 10000.0
0.00.690.886 I llama_init_from_model: freq_scale    = 1
0.00.690.895 I ggml_metal_init: allocating
0.00.690.976 I ggml_metal_init: found device: Apple M4
0.00.690.989 I ggml_metal_init: picking default device: Apple M4
0.00.692.904 I ggml_metal_init: using embedded metal library
0.00.699.673 I ggml_metal_init: GPU name:   Apple M4
0.00.699.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.682 I ggml_metal_init: simdgroup reduction   = true
0.00.699.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.683 I ggml_metal_init: has residency sets    = true
0.00.699.683 I ggml_metal_init: has bfloat            = true
0.00.699.683 I ggml_metal_init: use bfloat            = true
0.00.699.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.084 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.775.698 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.775.708 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.775.731 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.780.180 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.780.183 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.780.183 I llama_init_from_model: graph nodes  = 967
0.00.780.183 I llama_init_from_model: graph splits = 2
0.00.780.188 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.780.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.780.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.837.610 I main: llama threadpool init, n_threads = 4
0.00.837.654 I 
0.00.837.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.837.679 I 
0.00.837.833 I sampler seed: 1234
0.00.837.838 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.837.862 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.837.863 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.837.863 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.553.576 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.553.577 I llama_perf_context_print:        load time =     828.09 ms
0.01.553.578 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.20 tokens per second)
0.01.553.579 I llama_perf_context_print:        eval time =     664.29 ms /    63 runs   (   10.54 ms per token,    94.84 tokens per second)
0.01.553.579 I llama_perf_context_print:       total time =     716.70 ms /    70 tokens
0.01.553.838 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.109s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.871 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.873 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.518 I llama_model_loader: - type  f32:  194 tensors
0.00.024.519 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.519 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.519 I print_info: file format = GGUF V3 (latest)
0.00.024.520 I print_info: file type   = Q4_1
0.00.024.521 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.450 I load: special tokens cache size = 25
0.00.038.468 I load: token to piece cache size = 0.2984 MB
0.00.038.472 I print_info: arch             = gptneox
0.00.038.472 I print_info: vocab_only       = 0
0.00.038.472 I print_info: n_ctx_train      = 2048
0.00.038.472 I print_info: n_embd           = 2048
0.00.038.473 I print_info: n_layer          = 24
0.00.038.477 I print_info: n_head           = 16
0.00.038.478 I print_info: n_head_kv        = 16
0.00.038.478 I print_info: n_rot            = 32
0.00.038.478 I print_info: n_swa            = 0
0.00.038.478 I print_info: n_embd_head_k    = 128
0.00.038.478 I print_info: n_embd_head_v    = 128
0.00.038.480 I print_info: n_gqa            = 1
0.00.038.481 I print_info: n_embd_k_gqa     = 2048
0.00.038.482 I print_info: n_embd_v_gqa     = 2048
0.00.038.482 I print_info: f_norm_eps       = 1.0e-05
0.00.038.483 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.483 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.483 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.483 I print_info: f_logit_scale    = 0.0e+00
0.00.038.484 I print_info: n_ff             = 8192
0.00.038.484 I print_info: n_expert         = 0
0.00.038.484 I print_info: n_expert_used    = 0
0.00.038.484 I print_info: causal attn      = 1
0.00.038.485 I print_info: pooling type     = 0
0.00.038.485 I print_info: rope type        = 2
0.00.038.485 I print_info: rope scaling     = linear
0.00.038.485 I print_info: freq_base_train  = 10000.0
0.00.038.486 I print_info: freq_scale_train = 1
0.00.038.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.486 I print_info: rope_finetuned   = unknown
0.00.038.486 I print_info: ssm_d_conv       = 0
0.00.038.486 I print_info: ssm_d_inner      = 0
0.00.038.487 I print_info: ssm_d_state      = 0
0.00.038.487 I print_info: ssm_dt_rank      = 0
0.00.038.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.489 I print_info: model type       = 1.4B
0.00.038.489 I print_info: model params     = 1.41 B
0.00.038.490 I print_info: general.name     = 1.4B
0.00.038.490 I print_info: vocab type       = BPE
0.00.038.490 I print_info: n_vocab          = 50304
0.00.038.491 I print_info: n_merges         = 50009
0.00.038.491 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.491 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.491 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.491 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.492 I print_info: LF token         = 187 ''
0.00.038.492 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.492 I print_info: max token length = 1024
0.00.038.493 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.677.307 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.316 I load_tensors: offloading output layer to GPU
0.00.677.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.343 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.677.348 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.679.082 I llama_init_from_model: n_seq_max     = 1
0.00.679.085 I llama_init_from_model: n_ctx         = 128
0.00.679.086 I llama_init_from_model: n_ctx_per_seq = 128
0.00.679.086 I llama_init_from_model: n_batch       = 128
0.00.679.087 I llama_init_from_model: n_ubatch      = 128
0.00.679.087 I llama_init_from_model: flash_attn    = 0
0.00.679.088 I llama_init_from_model: freq_base     = 10000.0
0.00.679.089 I llama_init_from_model: freq_scale    = 1
0.00.679.090 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.679.103 I ggml_metal_init: allocating
0.00.679.160 I ggml_metal_init: found device: Apple M4
0.00.679.174 I ggml_metal_init: picking default device: Apple M4
0.00.680.831 I ggml_metal_init: using embedded metal library
0.00.687.591 I ggml_metal_init: GPU name:   Apple M4
0.00.687.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.687.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.687.598 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.687.598 I ggml_metal_init: simdgroup reduction   = true
0.00.687.599 I ggml_metal_init: simdgroup matrix mul. = true
0.00.687.599 I ggml_metal_init: has residency sets    = true
0.00.687.599 I ggml_metal_init: has bfloat            = true
0.00.687.599 I ggml_metal_init: use bfloat            = true
0.00.687.600 I ggml_metal_init: hasUnifiedMemory      = true
0.00.687.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.820 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.709.409 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.709.454 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.626 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.712.628 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.712.628 I llama_init_from_model: graph nodes  = 967
0.00.712.629 I llama_init_from_model: graph splits = 2
0.00.712.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.712.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.513 I 
0.00.740.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.610 I perplexity: tokenizing the input ..
0.00.747.391 I perplexity: tokenization took 6.78 ms
0.00.747.404 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.098 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.880.436 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.880.454 I llama_perf_context_print:        load time =     731.69 ms
0.00.880.455 I llama_perf_context_print: prompt eval time =     131.46 ms /   128 tokens (    1.03 ms per token,   973.65 tokens per second)
0.00.880.455 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.880.456 I llama_perf_context_print:       total time =     139.95 ms /   129 tokens
0.00.880.889 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.078s
sys	0m0.130s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.618 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.624 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.624 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.579 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.486 I llama_model_loader: - type  f32:  194 tensors
0.00.026.486 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.487 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.487 I print_info: file format = GGUF V3 (latest)
0.00.026.491 I print_info: file type   = Q5_0
0.00.026.493 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.895 I load: special tokens cache size = 25
0.00.041.179 I load: token to piece cache size = 0.2984 MB
0.00.041.184 I print_info: arch             = gptneox
0.00.041.184 I print_info: vocab_only       = 0
0.00.041.184 I print_info: n_ctx_train      = 2048
0.00.041.185 I print_info: n_embd           = 2048
0.00.041.185 I print_info: n_layer          = 24
0.00.041.190 I print_info: n_head           = 16
0.00.041.191 I print_info: n_head_kv        = 16
0.00.041.191 I print_info: n_rot            = 32
0.00.041.191 I print_info: n_swa            = 0
0.00.041.191 I print_info: n_embd_head_k    = 128
0.00.041.191 I print_info: n_embd_head_v    = 128
0.00.041.192 I print_info: n_gqa            = 1
0.00.041.193 I print_info: n_embd_k_gqa     = 2048
0.00.041.194 I print_info: n_embd_v_gqa     = 2048
0.00.041.194 I print_info: f_norm_eps       = 1.0e-05
0.00.041.195 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.196 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.196 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.196 I print_info: f_logit_scale    = 0.0e+00
0.00.041.197 I print_info: n_ff             = 8192
0.00.041.197 I print_info: n_expert         = 0
0.00.041.197 I print_info: n_expert_used    = 0
0.00.041.197 I print_info: causal attn      = 1
0.00.041.197 I print_info: pooling type     = 0
0.00.041.199 I print_info: rope type        = 2
0.00.041.200 I print_info: rope scaling     = linear
0.00.041.200 I print_info: freq_base_train  = 10000.0
0.00.041.200 I print_info: freq_scale_train = 1
0.00.041.200 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.202 I print_info: rope_finetuned   = unknown
0.00.041.202 I print_info: ssm_d_conv       = 0
0.00.041.202 I print_info: ssm_d_inner      = 0
0.00.041.202 I print_info: ssm_d_state      = 0
0.00.041.202 I print_info: ssm_dt_rank      = 0
0.00.041.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.203 I print_info: model type       = 1.4B
0.00.041.203 I print_info: model params     = 1.41 B
0.00.041.203 I print_info: general.name     = 1.4B
0.00.041.204 I print_info: vocab type       = BPE
0.00.041.204 I print_info: n_vocab          = 50304
0.00.041.204 I print_info: n_merges         = 50009
0.00.041.204 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.204 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.205 I print_info: LF token         = 187 ''
0.00.041.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.205 I print_info: max token length = 1024
0.00.041.206 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.075 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.080 I load_tensors: offloading output layer to GPU
0.00.649.080 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.098 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.649.102 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.649.986 I llama_init_from_model: n_seq_max     = 1
0.00.649.990 I llama_init_from_model: n_ctx         = 2048
0.00.649.990 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.991 I llama_init_from_model: n_batch       = 2048
0.00.649.991 I llama_init_from_model: n_ubatch      = 512
0.00.649.991 I llama_init_from_model: flash_attn    = 0
0.00.649.993 I llama_init_from_model: freq_base     = 10000.0
0.00.649.994 I llama_init_from_model: freq_scale    = 1
0.00.649.995 I ggml_metal_init: allocating
0.00.650.030 I ggml_metal_init: found device: Apple M4
0.00.650.040 I ggml_metal_init: picking default device: Apple M4
0.00.651.083 I ggml_metal_init: using embedded metal library
0.00.655.207 I ggml_metal_init: GPU name:   Apple M4
0.00.655.216 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.217 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.217 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.218 I ggml_metal_init: simdgroup reduction   = true
0.00.655.218 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.218 I ggml_metal_init: has residency sets    = true
0.00.655.219 I ggml_metal_init: has bfloat            = true
0.00.655.219 I ggml_metal_init: use bfloat            = true
0.00.655.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.018 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.701.638 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.701.662 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.130 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.707.132 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.707.132 I llama_init_from_model: graph nodes  = 967
0.00.707.133 I llama_init_from_model: graph splits = 2
0.00.707.139 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.305 I main: llama threadpool init, n_threads = 4
0.00.763.344 I 
0.00.763.363 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.363 I 
0.00.763.509 I sampler seed: 1234
0.00.763.513 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.524 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.524 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.524 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.550.848 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47523.43 tokens per second)
0.01.550.849 I llama_perf_context_print:        load time =     752.72 ms
0.01.550.851 I llama_perf_context_print: prompt eval time =      42.70 ms /     7 tokens (    6.10 ms per token,   163.93 tokens per second)
0.01.550.852 I llama_perf_context_print:        eval time =     742.08 ms /    63 runs   (   11.78 ms per token,    84.90 tokens per second)
0.01.550.852 I llama_perf_context_print:       total time =     788.25 ms /    70 tokens
0.01.551.108 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.105s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.253 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.258 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.259 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.268 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.969 I llama_model_loader: - type  f32:  194 tensors
0.00.024.969 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.970 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.970 I print_info: file format = GGUF V3 (latest)
0.00.024.971 I print_info: file type   = Q5_0
0.00.024.972 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.799 I load: special tokens cache size = 25
0.00.039.134 I load: token to piece cache size = 0.2984 MB
0.00.039.138 I print_info: arch             = gptneox
0.00.039.138 I print_info: vocab_only       = 0
0.00.039.138 I print_info: n_ctx_train      = 2048
0.00.039.138 I print_info: n_embd           = 2048
0.00.039.138 I print_info: n_layer          = 24
0.00.039.143 I print_info: n_head           = 16
0.00.039.144 I print_info: n_head_kv        = 16
0.00.039.145 I print_info: n_rot            = 32
0.00.039.145 I print_info: n_swa            = 0
0.00.039.145 I print_info: n_embd_head_k    = 128
0.00.039.145 I print_info: n_embd_head_v    = 128
0.00.039.146 I print_info: n_gqa            = 1
0.00.039.147 I print_info: n_embd_k_gqa     = 2048
0.00.039.147 I print_info: n_embd_v_gqa     = 2048
0.00.039.150 I print_info: f_norm_eps       = 1.0e-05
0.00.039.150 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.150 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.150 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.150 I print_info: f_logit_scale    = 0.0e+00
0.00.039.151 I print_info: n_ff             = 8192
0.00.039.151 I print_info: n_expert         = 0
0.00.039.151 I print_info: n_expert_used    = 0
0.00.039.152 I print_info: causal attn      = 1
0.00.039.152 I print_info: pooling type     = 0
0.00.039.152 I print_info: rope type        = 2
0.00.039.152 I print_info: rope scaling     = linear
0.00.039.154 I print_info: freq_base_train  = 10000.0
0.00.039.154 I print_info: freq_scale_train = 1
0.00.039.154 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.154 I print_info: rope_finetuned   = unknown
0.00.039.155 I print_info: ssm_d_conv       = 0
0.00.039.156 I print_info: ssm_d_inner      = 0
0.00.039.156 I print_info: ssm_d_state      = 0
0.00.039.156 I print_info: ssm_dt_rank      = 0
0.00.039.156 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.157 I print_info: model type       = 1.4B
0.00.039.157 I print_info: model params     = 1.41 B
0.00.039.157 I print_info: general.name     = 1.4B
0.00.039.158 I print_info: vocab type       = BPE
0.00.039.158 I print_info: n_vocab          = 50304
0.00.039.158 I print_info: n_merges         = 50009
0.00.039.158 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.159 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.159 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.159 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.159 I print_info: LF token         = 187 ''
0.00.039.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.160 I print_info: max token length = 1024
0.00.039.160 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.703.907 I load_tensors: offloading 24 repeating layers to GPU
0.00.703.923 I load_tensors: offloading output layer to GPU
0.00.703.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.703.956 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.703.958 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.705.664 I llama_init_from_model: n_seq_max     = 1
0.00.705.667 I llama_init_from_model: n_ctx         = 128
0.00.705.668 I llama_init_from_model: n_ctx_per_seq = 128
0.00.705.668 I llama_init_from_model: n_batch       = 128
0.00.705.668 I llama_init_from_model: n_ubatch      = 128
0.00.705.669 I llama_init_from_model: flash_attn    = 0
0.00.705.671 I llama_init_from_model: freq_base     = 10000.0
0.00.705.672 I llama_init_from_model: freq_scale    = 1
0.00.705.672 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.705.674 I ggml_metal_init: allocating
0.00.705.757 I ggml_metal_init: found device: Apple M4
0.00.705.770 I ggml_metal_init: picking default device: Apple M4
0.00.707.550 I ggml_metal_init: using embedded metal library
0.00.714.394 I ggml_metal_init: GPU name:   Apple M4
0.00.714.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.400 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.401 I ggml_metal_init: simdgroup reduction   = true
0.00.714.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.401 I ggml_metal_init: has residency sets    = true
0.00.714.402 I ggml_metal_init: has bfloat            = true
0.00.714.402 I ggml_metal_init: use bfloat            = true
0.00.714.403 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.686 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.121 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.735.124 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.735.159 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.582 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.738.584 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.738.584 I llama_init_from_model: graph nodes  = 967
0.00.738.585 I llama_init_from_model: graph splits = 2
0.00.738.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.738.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.094 I 
0.00.768.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.194 I perplexity: tokenizing the input ..
0.00.775.522 I perplexity: tokenization took 7.326 ms
0.00.775.543 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.653 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.912.985 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.912.999 I llama_perf_context_print:        load time =     758.15 ms
0.00.913.000 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.91 tokens per second)
0.00.913.001 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.913.002 I llama_perf_context_print:       total time =     144.91 ms /   129 tokens
0.00.913.405 I ggml_metal_free: deallocating

real	0m0.929s
user	0m0.080s
sys	0m0.146s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.364 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.028.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.372 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.373 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.374 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.374 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.386 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.307 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.037.121 I llama_model_loader: - type  f32:  194 tensors
0.00.037.122 I llama_model_loader: - type q5_1:   97 tensors
0.00.037.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.122 I print_info: file format = GGUF V3 (latest)
0.00.037.123 I print_info: file type   = Q5_1
0.00.037.124 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.910 I load: special tokens cache size = 25
0.00.051.192 I load: token to piece cache size = 0.2984 MB
0.00.051.195 I print_info: arch             = gptneox
0.00.051.195 I print_info: vocab_only       = 0
0.00.051.195 I print_info: n_ctx_train      = 2048
0.00.051.195 I print_info: n_embd           = 2048
0.00.051.196 I print_info: n_layer          = 24
0.00.051.198 I print_info: n_head           = 16
0.00.051.199 I print_info: n_head_kv        = 16
0.00.051.199 I print_info: n_rot            = 32
0.00.051.199 I print_info: n_swa            = 0
0.00.051.200 I print_info: n_embd_head_k    = 128
0.00.051.200 I print_info: n_embd_head_v    = 128
0.00.051.200 I print_info: n_gqa            = 1
0.00.051.203 I print_info: n_embd_k_gqa     = 2048
0.00.051.204 I print_info: n_embd_v_gqa     = 2048
0.00.051.205 I print_info: f_norm_eps       = 1.0e-05
0.00.051.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.206 I print_info: f_logit_scale    = 0.0e+00
0.00.051.207 I print_info: n_ff             = 8192
0.00.051.207 I print_info: n_expert         = 0
0.00.051.208 I print_info: n_expert_used    = 0
0.00.051.208 I print_info: causal attn      = 1
0.00.051.208 I print_info: pooling type     = 0
0.00.051.208 I print_info: rope type        = 2
0.00.051.208 I print_info: rope scaling     = linear
0.00.051.208 I print_info: freq_base_train  = 10000.0
0.00.051.209 I print_info: freq_scale_train = 1
0.00.051.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.209 I print_info: rope_finetuned   = unknown
0.00.051.209 I print_info: ssm_d_conv       = 0
0.00.051.209 I print_info: ssm_d_inner      = 0
0.00.051.209 I print_info: ssm_d_state      = 0
0.00.051.209 I print_info: ssm_dt_rank      = 0
0.00.051.210 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.210 I print_info: model type       = 1.4B
0.00.051.210 I print_info: model params     = 1.41 B
0.00.051.210 I print_info: general.name     = 1.4B
0.00.051.211 I print_info: vocab type       = BPE
0.00.051.211 I print_info: n_vocab          = 50304
0.00.051.211 I print_info: n_merges         = 50009
0.00.051.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.212 I print_info: LF token         = 187 ''
0.00.051.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.213 I print_info: max token length = 1024
0.00.051.213 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.163.071 I load_tensors: offloading 24 repeating layers to GPU
0.01.163.075 I load_tensors: offloading output layer to GPU
0.01.163.075 I load_tensors: offloaded 25/25 layers to GPU
0.01.163.093 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.01.163.094 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.01.163.957 I llama_init_from_model: n_seq_max     = 1
0.01.163.959 I llama_init_from_model: n_ctx         = 2048
0.01.163.959 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.163.960 I llama_init_from_model: n_batch       = 2048
0.01.163.960 I llama_init_from_model: n_ubatch      = 512
0.01.163.960 I llama_init_from_model: flash_attn    = 0
0.01.163.961 I llama_init_from_model: freq_base     = 10000.0
0.01.163.962 I llama_init_from_model: freq_scale    = 1
0.01.163.963 I ggml_metal_init: allocating
0.01.163.995 I ggml_metal_init: found device: Apple M4
0.01.164.006 I ggml_metal_init: picking default device: Apple M4
0.01.165.157 I ggml_metal_init: using embedded metal library
0.01.169.487 I ggml_metal_init: GPU name:   Apple M4
0.01.169.492 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.169.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.169.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.169.494 I ggml_metal_init: simdgroup reduction   = true
0.01.169.494 I ggml_metal_init: simdgroup matrix mul. = true
0.01.169.495 I ggml_metal_init: has residency sets    = true
0.01.169.495 I ggml_metal_init: has bfloat            = true
0.01.169.495 I ggml_metal_init: use bfloat            = true
0.01.169.498 I ggml_metal_init: hasUnifiedMemory      = true
0.01.169.500 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.184.393 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.215.428 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.215.434 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.215.457 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.219.664 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.219.666 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.219.666 I llama_init_from_model: graph nodes  = 967
0.01.219.667 I llama_init_from_model: graph splits = 2
0.01.219.673 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.219.797 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.219.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.276.674 I main: llama threadpool init, n_threads = 4
0.01.276.721 I 
0.01.276.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.276.744 I 
0.01.276.923 I sampler seed: 1234
0.01.276.927 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.276.938 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.276.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.276.940 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.120.618 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.02.120.619 I llama_perf_context_print:        load time =    1267.24 ms
0.02.120.620 I llama_perf_context_print: prompt eval time =      42.21 ms /     7 tokens (    6.03 ms per token,   165.83 tokens per second)
0.02.120.621 I llama_perf_context_print:        eval time =     798.73 ms /    63 runs   (   12.68 ms per token,    78.88 tokens per second)
0.02.120.622 I llama_perf_context_print:       total time =     844.63 ms /    70 tokens
0.02.120.886 I ggml_metal_free: deallocating

real	0m2.142s
user	0m0.105s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.879 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.521 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.522 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.523 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.417 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.239 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.240 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.240 I llama_model_loader: - type  f32:  194 tensors
0.00.024.241 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.241 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.242 I print_info: file format = GGUF V3 (latest)
0.00.024.242 I print_info: file type   = Q5_1
0.00.024.243 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.249 I load: special tokens cache size = 25
0.00.038.237 I load: token to piece cache size = 0.2984 MB
0.00.038.241 I print_info: arch             = gptneox
0.00.038.241 I print_info: vocab_only       = 0
0.00.038.242 I print_info: n_ctx_train      = 2048
0.00.038.242 I print_info: n_embd           = 2048
0.00.038.242 I print_info: n_layer          = 24
0.00.038.246 I print_info: n_head           = 16
0.00.038.246 I print_info: n_head_kv        = 16
0.00.038.247 I print_info: n_rot            = 32
0.00.038.247 I print_info: n_swa            = 0
0.00.038.247 I print_info: n_embd_head_k    = 128
0.00.038.247 I print_info: n_embd_head_v    = 128
0.00.038.248 I print_info: n_gqa            = 1
0.00.038.249 I print_info: n_embd_k_gqa     = 2048
0.00.038.250 I print_info: n_embd_v_gqa     = 2048
0.00.038.250 I print_info: f_norm_eps       = 1.0e-05
0.00.038.251 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.251 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.251 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.251 I print_info: f_logit_scale    = 0.0e+00
0.00.038.252 I print_info: n_ff             = 8192
0.00.038.252 I print_info: n_expert         = 0
0.00.038.256 I print_info: n_expert_used    = 0
0.00.038.256 I print_info: causal attn      = 1
0.00.038.256 I print_info: pooling type     = 0
0.00.038.256 I print_info: rope type        = 2
0.00.038.256 I print_info: rope scaling     = linear
0.00.038.257 I print_info: freq_base_train  = 10000.0
0.00.038.257 I print_info: freq_scale_train = 1
0.00.038.257 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.258 I print_info: rope_finetuned   = unknown
0.00.038.258 I print_info: ssm_d_conv       = 0
0.00.038.258 I print_info: ssm_d_inner      = 0
0.00.038.258 I print_info: ssm_d_state      = 0
0.00.038.258 I print_info: ssm_dt_rank      = 0
0.00.038.258 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.259 I print_info: model type       = 1.4B
0.00.038.259 I print_info: model params     = 1.41 B
0.00.038.259 I print_info: general.name     = 1.4B
0.00.038.260 I print_info: vocab type       = BPE
0.00.038.260 I print_info: n_vocab          = 50304
0.00.038.261 I print_info: n_merges         = 50009
0.00.038.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.262 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.262 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.262 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.263 I print_info: LF token         = 187 ''
0.00.038.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.263 I print_info: max token length = 1024
0.00.038.266 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.477 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.493 I load_tensors: offloading output layer to GPU
0.00.614.494 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.528 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.614.536 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.616.248 I llama_init_from_model: n_seq_max     = 1
0.00.616.252 I llama_init_from_model: n_ctx         = 128
0.00.616.252 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.253 I llama_init_from_model: n_batch       = 128
0.00.616.253 I llama_init_from_model: n_ubatch      = 128
0.00.616.254 I llama_init_from_model: flash_attn    = 0
0.00.616.256 I llama_init_from_model: freq_base     = 10000.0
0.00.616.256 I llama_init_from_model: freq_scale    = 1
0.00.616.257 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.259 I ggml_metal_init: allocating
0.00.616.331 I ggml_metal_init: found device: Apple M4
0.00.616.346 I ggml_metal_init: picking default device: Apple M4
0.00.617.859 I ggml_metal_init: using embedded metal library
0.00.624.428 I ggml_metal_init: GPU name:   Apple M4
0.00.624.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.435 I ggml_metal_init: simdgroup reduction   = true
0.00.624.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.436 I ggml_metal_init: has residency sets    = true
0.00.624.436 I ggml_metal_init: has bfloat            = true
0.00.624.436 I ggml_metal_init: use bfloat            = true
0.00.624.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.641.430 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.923 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.644.927 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.644.962 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.211 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.648.212 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.648.213 I llama_init_from_model: graph nodes  = 967
0.00.648.213 I llama_init_from_model: graph splits = 2
0.00.648.216 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.648.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.612 I 
0.00.674.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.723 I perplexity: tokenizing the input ..
0.00.682.104 I perplexity: tokenization took 7.377 ms
0.00.682.129 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.225 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.819.554 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.819.572 I llama_perf_context_print:        load time =     665.72 ms
0.00.819.573 I llama_perf_context_print: prompt eval time =     135.18 ms /   128 tokens (    1.06 ms per token,   946.86 tokens per second)
0.00.819.573 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.574 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.819.942 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.079s
sys	0m0.141s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.953 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.578 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.269 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.271 I llama_model_loader: - type  f32:  194 tensors
0.00.025.272 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.272 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.272 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.273 I print_info: file format = GGUF V3 (latest)
0.00.025.273 I print_info: file type   = Q2_K - Medium
0.00.025.274 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.383 I load: special tokens cache size = 25
0.00.039.565 I load: token to piece cache size = 0.2984 MB
0.00.039.568 I print_info: arch             = gptneox
0.00.039.568 I print_info: vocab_only       = 0
0.00.039.569 I print_info: n_ctx_train      = 2048
0.00.039.569 I print_info: n_embd           = 2048
0.00.039.569 I print_info: n_layer          = 24
0.00.039.572 I print_info: n_head           = 16
0.00.039.572 I print_info: n_head_kv        = 16
0.00.039.573 I print_info: n_rot            = 32
0.00.039.573 I print_info: n_swa            = 0
0.00.039.573 I print_info: n_embd_head_k    = 128
0.00.039.573 I print_info: n_embd_head_v    = 128
0.00.039.574 I print_info: n_gqa            = 1
0.00.039.575 I print_info: n_embd_k_gqa     = 2048
0.00.039.575 I print_info: n_embd_v_gqa     = 2048
0.00.039.576 I print_info: f_norm_eps       = 1.0e-05
0.00.039.576 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.576 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.577 I print_info: f_logit_scale    = 0.0e+00
0.00.039.577 I print_info: n_ff             = 8192
0.00.039.578 I print_info: n_expert         = 0
0.00.039.578 I print_info: n_expert_used    = 0
0.00.039.578 I print_info: causal attn      = 1
0.00.039.578 I print_info: pooling type     = 0
0.00.039.578 I print_info: rope type        = 2
0.00.039.580 I print_info: rope scaling     = linear
0.00.039.581 I print_info: freq_base_train  = 10000.0
0.00.039.581 I print_info: freq_scale_train = 1
0.00.039.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.581 I print_info: rope_finetuned   = unknown
0.00.039.582 I print_info: ssm_d_conv       = 0
0.00.039.582 I print_info: ssm_d_inner      = 0
0.00.039.583 I print_info: ssm_d_state      = 0
0.00.039.583 I print_info: ssm_dt_rank      = 0
0.00.039.584 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.584 I print_info: model type       = 1.4B
0.00.039.584 I print_info: model params     = 1.41 B
0.00.039.584 I print_info: general.name     = 1.4B
0.00.039.585 I print_info: vocab type       = BPE
0.00.039.585 I print_info: n_vocab          = 50304
0.00.039.585 I print_info: n_merges         = 50009
0.00.039.585 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: LF token         = 187 ''
0.00.039.586 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.590 I print_info: max token length = 1024
0.00.039.591 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.346.257 I load_tensors: offloading 24 repeating layers to GPU
0.00.346.272 I load_tensors: offloading output layer to GPU
0.00.346.273 I load_tensors: offloaded 25/25 layers to GPU
0.00.346.308 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.346.310 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.348.022 I llama_init_from_model: n_seq_max     = 1
0.00.348.026 I llama_init_from_model: n_ctx         = 2048
0.00.348.027 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.348.028 I llama_init_from_model: n_batch       = 2048
0.00.348.028 I llama_init_from_model: n_ubatch      = 512
0.00.348.028 I llama_init_from_model: flash_attn    = 0
0.00.348.030 I llama_init_from_model: freq_base     = 10000.0
0.00.348.030 I llama_init_from_model: freq_scale    = 1
0.00.348.032 I ggml_metal_init: allocating
0.00.348.122 I ggml_metal_init: found device: Apple M4
0.00.348.135 I ggml_metal_init: picking default device: Apple M4
0.00.349.994 I ggml_metal_init: using embedded metal library
0.00.355.454 I ggml_metal_init: GPU name:   Apple M4
0.00.355.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.476 I ggml_metal_init: simdgroup reduction   = true
0.00.355.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.477 I ggml_metal_init: has residency sets    = true
0.00.355.477 I ggml_metal_init: has bfloat            = true
0.00.355.477 I ggml_metal_init: use bfloat            = true
0.00.355.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.376.485 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.063 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.438.071 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.438.095 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.442.670 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.442.672 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.442.672 I llama_init_from_model: graph nodes  = 967
0.00.442.672 I llama_init_from_model: graph splits = 2
0.00.442.683 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.442.807 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.442.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.529 I main: llama threadpool init, n_threads = 4
0.00.501.574 I 
0.00.501.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.597 I 
0.00.501.769 I sampler seed: 1234
0.00.501.773 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.784 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.784 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.786 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.187.856 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.187.857 I llama_perf_context_print:        load time =     490.88 ms
0.01.187.858 I llama_perf_context_print: prompt eval time =      44.73 ms /     7 tokens (    6.39 ms per token,   156.51 tokens per second)
0.01.187.859 I llama_perf_context_print:        eval time =     638.51 ms /    63 runs   (   10.14 ms per token,    98.67 tokens per second)
0.01.187.859 I llama_perf_context_print:       total time =     687.02 ms /    70 tokens
0.01.188.112 I ggml_metal_free: deallocating

real	0m1.207s
user	0m0.112s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.958 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.376 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.168 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.170 I llama_model_loader: - type  f32:  194 tensors
0.00.025.171 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.171 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.171 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.172 I print_info: file format = GGUF V3 (latest)
0.00.025.172 I print_info: file type   = Q2_K - Medium
0.00.025.173 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.162 I load: special tokens cache size = 25
0.00.039.318 I load: token to piece cache size = 0.2984 MB
0.00.039.322 I print_info: arch             = gptneox
0.00.039.323 I print_info: vocab_only       = 0
0.00.039.323 I print_info: n_ctx_train      = 2048
0.00.039.323 I print_info: n_embd           = 2048
0.00.039.323 I print_info: n_layer          = 24
0.00.039.328 I print_info: n_head           = 16
0.00.039.328 I print_info: n_head_kv        = 16
0.00.039.329 I print_info: n_rot            = 32
0.00.039.329 I print_info: n_swa            = 0
0.00.039.329 I print_info: n_embd_head_k    = 128
0.00.039.329 I print_info: n_embd_head_v    = 128
0.00.039.330 I print_info: n_gqa            = 1
0.00.039.331 I print_info: n_embd_k_gqa     = 2048
0.00.039.333 I print_info: n_embd_v_gqa     = 2048
0.00.039.334 I print_info: f_norm_eps       = 1.0e-05
0.00.039.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.335 I print_info: f_logit_scale    = 0.0e+00
0.00.039.337 I print_info: n_ff             = 8192
0.00.039.337 I print_info: n_expert         = 0
0.00.039.337 I print_info: n_expert_used    = 0
0.00.039.337 I print_info: causal attn      = 1
0.00.039.337 I print_info: pooling type     = 0
0.00.039.338 I print_info: rope type        = 2
0.00.039.338 I print_info: rope scaling     = linear
0.00.039.338 I print_info: freq_base_train  = 10000.0
0.00.039.339 I print_info: freq_scale_train = 1
0.00.039.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.339 I print_info: rope_finetuned   = unknown
0.00.039.339 I print_info: ssm_d_conv       = 0
0.00.039.340 I print_info: ssm_d_inner      = 0
0.00.039.340 I print_info: ssm_d_state      = 0
0.00.039.340 I print_info: ssm_dt_rank      = 0
0.00.039.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.340 I print_info: model type       = 1.4B
0.00.039.341 I print_info: model params     = 1.41 B
0.00.039.341 I print_info: general.name     = 1.4B
0.00.039.343 I print_info: vocab type       = BPE
0.00.039.343 I print_info: n_vocab          = 50304
0.00.039.343 I print_info: n_merges         = 50009
0.00.039.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: LF token         = 187 ''
0.00.039.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: max token length = 1024
0.00.039.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.344.656 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.669 I load_tensors: offloading output layer to GPU
0.00.344.670 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.700 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.705 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.420 I llama_init_from_model: n_seq_max     = 1
0.00.346.424 I llama_init_from_model: n_ctx         = 128
0.00.346.425 I llama_init_from_model: n_ctx_per_seq = 128
0.00.346.425 I llama_init_from_model: n_batch       = 128
0.00.346.426 I llama_init_from_model: n_ubatch      = 128
0.00.346.426 I llama_init_from_model: flash_attn    = 0
0.00.346.427 I llama_init_from_model: freq_base     = 10000.0
0.00.346.428 I llama_init_from_model: freq_scale    = 1
0.00.346.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.346.431 I ggml_metal_init: allocating
0.00.346.500 I ggml_metal_init: found device: Apple M4
0.00.346.513 I ggml_metal_init: picking default device: Apple M4
0.00.348.236 I ggml_metal_init: using embedded metal library
0.00.353.770 I ggml_metal_init: GPU name:   Apple M4
0.00.353.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.786 I ggml_metal_init: simdgroup reduction   = true
0.00.353.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.787 I ggml_metal_init: has residency sets    = true
0.00.353.787 I ggml_metal_init: has bfloat            = true
0.00.353.787 I ggml_metal_init: use bfloat            = true
0.00.353.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.998 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.378.752 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.378.759 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.378.830 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.382.447 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.382.449 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.382.450 I llama_init_from_model: graph nodes  = 967
0.00.382.450 I llama_init_from_model: graph splits = 2
0.00.382.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.382.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.481 I 
0.00.414.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.583 I perplexity: tokenizing the input ..
0.00.421.184 I perplexity: tokenization took 6.599 ms
0.00.421.194 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.366 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.560.884 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.560.901 I llama_perf_context_print:        load time =     404.51 ms
0.00.560.902 I llama_perf_context_print: prompt eval time =     137.94 ms /   128 tokens (    1.08 ms per token,   927.92 tokens per second)
0.00.560.903 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.903 I llama_perf_context_print:       total time =     146.42 ms /   129 tokens
0.00.561.236 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.079s
sys	0m0.093s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.232 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.852 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.853 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.854 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.855 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.856 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.857 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.857 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.497 I llama_model_loader: - type  f32:  194 tensors
0.00.026.497 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.498 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.498 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.498 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.499 I print_info: file format = GGUF V3 (latest)
0.00.026.499 I print_info: file type   = Q3_K - Medium
0.00.026.500 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.267 I load: special tokens cache size = 25
0.00.040.423 I load: token to piece cache size = 0.2984 MB
0.00.040.426 I print_info: arch             = gptneox
0.00.040.426 I print_info: vocab_only       = 0
0.00.040.426 I print_info: n_ctx_train      = 2048
0.00.040.426 I print_info: n_embd           = 2048
0.00.040.427 I print_info: n_layer          = 24
0.00.040.429 I print_info: n_head           = 16
0.00.040.430 I print_info: n_head_kv        = 16
0.00.040.430 I print_info: n_rot            = 32
0.00.040.430 I print_info: n_swa            = 0
0.00.040.430 I print_info: n_embd_head_k    = 128
0.00.040.432 I print_info: n_embd_head_v    = 128
0.00.040.433 I print_info: n_gqa            = 1
0.00.040.434 I print_info: n_embd_k_gqa     = 2048
0.00.040.434 I print_info: n_embd_v_gqa     = 2048
0.00.040.435 I print_info: f_norm_eps       = 1.0e-05
0.00.040.437 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.437 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.437 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.437 I print_info: f_logit_scale    = 0.0e+00
0.00.040.438 I print_info: n_ff             = 8192
0.00.040.438 I print_info: n_expert         = 0
0.00.040.438 I print_info: n_expert_used    = 0
0.00.040.438 I print_info: causal attn      = 1
0.00.040.439 I print_info: pooling type     = 0
0.00.040.439 I print_info: rope type        = 2
0.00.040.439 I print_info: rope scaling     = linear
0.00.040.440 I print_info: freq_base_train  = 10000.0
0.00.040.441 I print_info: freq_scale_train = 1
0.00.040.441 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.441 I print_info: rope_finetuned   = unknown
0.00.040.441 I print_info: ssm_d_conv       = 0
0.00.040.441 I print_info: ssm_d_inner      = 0
0.00.040.443 I print_info: ssm_d_state      = 0
0.00.040.443 I print_info: ssm_dt_rank      = 0
0.00.040.443 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.443 I print_info: model type       = 1.4B
0.00.040.443 I print_info: model params     = 1.41 B
0.00.040.444 I print_info: general.name     = 1.4B
0.00.040.444 I print_info: vocab type       = BPE
0.00.040.444 I print_info: n_vocab          = 50304
0.00.040.444 I print_info: n_merges         = 50009
0.00.040.445 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.445 I print_info: LF token         = 187 ''
0.00.040.446 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.446 I print_info: max token length = 1024
0.00.040.446 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.449.315 I load_tensors: offloading 24 repeating layers to GPU
0.00.449.331 I load_tensors: offloading output layer to GPU
0.00.449.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.449.368 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.449.378 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.451.094 I llama_init_from_model: n_seq_max     = 1
0.00.451.098 I llama_init_from_model: n_ctx         = 2048
0.00.451.099 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.451.100 I llama_init_from_model: n_batch       = 2048
0.00.451.100 I llama_init_from_model: n_ubatch      = 512
0.00.451.100 I llama_init_from_model: flash_attn    = 0
0.00.451.102 I llama_init_from_model: freq_base     = 10000.0
0.00.451.102 I llama_init_from_model: freq_scale    = 1
0.00.451.104 I ggml_metal_init: allocating
0.00.451.182 I ggml_metal_init: found device: Apple M4
0.00.451.195 I ggml_metal_init: picking default device: Apple M4
0.00.453.110 I ggml_metal_init: using embedded metal library
0.00.458.383 I ggml_metal_init: GPU name:   Apple M4
0.00.458.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.458.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.458.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.458.391 I ggml_metal_init: simdgroup reduction   = true
0.00.458.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.458.392 I ggml_metal_init: has residency sets    = true
0.00.458.392 I ggml_metal_init: has bfloat            = true
0.00.458.392 I ggml_metal_init: use bfloat            = true
0.00.458.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.458.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.478.224 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.540.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.540.144 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.540.166 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.544.903 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.544.906 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.544.906 I llama_init_from_model: graph nodes  = 967
0.00.544.906 I llama_init_from_model: graph splits = 2
0.00.544.912 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.545.044 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.545.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.465 I main: llama threadpool init, n_threads = 4
0.00.604.512 I 
0.00.604.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.535 I 
0.00.604.691 I sampler seed: 1234
0.00.604.695 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.604.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.604.706 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.604.706 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.351.708 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.351.709 I llama_perf_context_print:        load time =     593.53 ms
0.01.351.710 I llama_perf_context_print: prompt eval time =      50.30 ms /     7 tokens (    7.19 ms per token,   139.16 tokens per second)
0.01.351.711 I llama_perf_context_print:        eval time =     693.73 ms /    63 runs   (   11.01 ms per token,    90.81 tokens per second)
0.01.351.712 I llama_perf_context_print:       total time =     747.94 ms /    70 tokens
0.01.351.938 I ggml_metal_free: deallocating

real	0m1.368s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.062 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.064 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.065 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.789 I llama_model_loader: - type  f32:  194 tensors
0.00.024.789 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.790 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.790 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.790 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.791 I print_info: file format = GGUF V3 (latest)
0.00.024.793 I print_info: file type   = Q3_K - Medium
0.00.024.794 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.845 I load: special tokens cache size = 25
0.00.038.891 I load: token to piece cache size = 0.2984 MB
0.00.038.894 I print_info: arch             = gptneox
0.00.038.894 I print_info: vocab_only       = 0
0.00.038.894 I print_info: n_ctx_train      = 2048
0.00.038.894 I print_info: n_embd           = 2048
0.00.038.895 I print_info: n_layer          = 24
0.00.038.898 I print_info: n_head           = 16
0.00.038.899 I print_info: n_head_kv        = 16
0.00.038.899 I print_info: n_rot            = 32
0.00.038.899 I print_info: n_swa            = 0
0.00.038.900 I print_info: n_embd_head_k    = 128
0.00.038.900 I print_info: n_embd_head_v    = 128
0.00.038.901 I print_info: n_gqa            = 1
0.00.038.901 I print_info: n_embd_k_gqa     = 2048
0.00.038.902 I print_info: n_embd_v_gqa     = 2048
0.00.038.903 I print_info: f_norm_eps       = 1.0e-05
0.00.038.903 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.904 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.904 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.904 I print_info: f_logit_scale    = 0.0e+00
0.00.038.905 I print_info: n_ff             = 8192
0.00.038.905 I print_info: n_expert         = 0
0.00.038.905 I print_info: n_expert_used    = 0
0.00.038.905 I print_info: causal attn      = 1
0.00.038.906 I print_info: pooling type     = 0
0.00.038.907 I print_info: rope type        = 2
0.00.038.908 I print_info: rope scaling     = linear
0.00.038.908 I print_info: freq_base_train  = 10000.0
0.00.038.908 I print_info: freq_scale_train = 1
0.00.038.909 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.909 I print_info: rope_finetuned   = unknown
0.00.038.909 I print_info: ssm_d_conv       = 0
0.00.038.909 I print_info: ssm_d_inner      = 0
0.00.038.909 I print_info: ssm_d_state      = 0
0.00.038.909 I print_info: ssm_dt_rank      = 0
0.00.038.909 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.910 I print_info: model type       = 1.4B
0.00.038.910 I print_info: model params     = 1.41 B
0.00.038.914 I print_info: general.name     = 1.4B
0.00.038.915 I print_info: vocab type       = BPE
0.00.038.915 I print_info: n_vocab          = 50304
0.00.038.915 I print_info: n_merges         = 50009
0.00.038.915 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: LF token         = 187 ''
0.00.038.916 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.917 I print_info: max token length = 1024
0.00.038.917 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.465.114 I load_tensors: offloading 24 repeating layers to GPU
0.00.465.122 I load_tensors: offloading output layer to GPU
0.00.465.123 I load_tensors: offloaded 25/25 layers to GPU
0.00.465.148 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.465.150 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.466.656 I llama_init_from_model: n_seq_max     = 1
0.00.466.660 I llama_init_from_model: n_ctx         = 128
0.00.466.660 I llama_init_from_model: n_ctx_per_seq = 128
0.00.466.661 I llama_init_from_model: n_batch       = 128
0.00.466.661 I llama_init_from_model: n_ubatch      = 128
0.00.466.662 I llama_init_from_model: flash_attn    = 0
0.00.466.665 I llama_init_from_model: freq_base     = 10000.0
0.00.466.665 I llama_init_from_model: freq_scale    = 1
0.00.466.666 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.466.672 I ggml_metal_init: allocating
0.00.466.723 I ggml_metal_init: found device: Apple M4
0.00.466.734 I ggml_metal_init: picking default device: Apple M4
0.00.468.340 I ggml_metal_init: using embedded metal library
0.00.473.677 I ggml_metal_init: GPU name:   Apple M4
0.00.473.688 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.473.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.473.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.473.690 I ggml_metal_init: simdgroup reduction   = true
0.00.473.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.473.691 I ggml_metal_init: has residency sets    = true
0.00.473.691 I ggml_metal_init: has bfloat            = true
0.00.473.691 I ggml_metal_init: use bfloat            = true
0.00.473.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.473.698 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.494.502 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.498.086 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.498.093 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.498.127 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.501.428 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.501.430 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.501.430 I llama_init_from_model: graph nodes  = 967
0.00.501.430 I llama_init_from_model: graph splits = 2
0.00.501.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.501.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.583 I 
0.00.527.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.676 I perplexity: tokenizing the input ..
0.00.534.719 I perplexity: tokenization took 7.04 ms
0.00.534.734 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.544 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.857 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.870 I llama_perf_context_print:        load time =     518.62 ms
0.00.667.871 I llama_perf_context_print: prompt eval time =     131.52 ms /   128 tokens (    1.03 ms per token,   973.24 tokens per second)
0.00.667.872 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.872 I llama_perf_context_print:       total time =     140.29 ms /   129 tokens
0.00.668.256 I ggml_metal_free: deallocating

real	0m0.682s
user	0m0.080s
sys	0m0.128s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.008.998 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.600 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.119 I llama_model_loader: - type  f32:  194 tensors
0.00.025.119 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.119 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.119 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.120 I print_info: file format = GGUF V3 (latest)
0.00.025.121 I print_info: file type   = Q4_K - Medium
0.00.025.121 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.833 I load: special tokens cache size = 25
0.00.038.981 I load: token to piece cache size = 0.2984 MB
0.00.038.985 I print_info: arch             = gptneox
0.00.038.985 I print_info: vocab_only       = 0
0.00.038.985 I print_info: n_ctx_train      = 2048
0.00.038.985 I print_info: n_embd           = 2048
0.00.038.985 I print_info: n_layer          = 24
0.00.038.988 I print_info: n_head           = 16
0.00.038.989 I print_info: n_head_kv        = 16
0.00.038.989 I print_info: n_rot            = 32
0.00.038.991 I print_info: n_swa            = 0
0.00.038.991 I print_info: n_embd_head_k    = 128
0.00.038.991 I print_info: n_embd_head_v    = 128
0.00.038.992 I print_info: n_gqa            = 1
0.00.038.993 I print_info: n_embd_k_gqa     = 2048
0.00.038.998 I print_info: n_embd_v_gqa     = 2048
0.00.038.999 I print_info: f_norm_eps       = 1.0e-05
0.00.038.999 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.999 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.999 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.999 I print_info: f_logit_scale    = 0.0e+00
0.00.039.000 I print_info: n_ff             = 8192
0.00.039.000 I print_info: n_expert         = 0
0.00.039.001 I print_info: n_expert_used    = 0
0.00.039.001 I print_info: causal attn      = 1
0.00.039.001 I print_info: pooling type     = 0
0.00.039.001 I print_info: rope type        = 2
0.00.039.004 I print_info: rope scaling     = linear
0.00.039.005 I print_info: freq_base_train  = 10000.0
0.00.039.005 I print_info: freq_scale_train = 1
0.00.039.005 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.006 I print_info: rope_finetuned   = unknown
0.00.039.009 I print_info: ssm_d_conv       = 0
0.00.039.009 I print_info: ssm_d_inner      = 0
0.00.039.009 I print_info: ssm_d_state      = 0
0.00.039.009 I print_info: ssm_dt_rank      = 0
0.00.039.009 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.009 I print_info: model type       = 1.4B
0.00.039.010 I print_info: model params     = 1.41 B
0.00.039.010 I print_info: general.name     = 1.4B
0.00.039.011 I print_info: vocab type       = BPE
0.00.039.011 I print_info: n_vocab          = 50304
0.00.039.011 I print_info: n_merges         = 50009
0.00.039.011 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.011 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.011 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.011 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.012 I print_info: LF token         = 187 ''
0.00.039.012 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.012 I print_info: max token length = 1024
0.00.039.012 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.814 I load_tensors: offloading output layer to GPU
0.00.538.815 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.849 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.850 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.339 I llama_init_from_model: n_seq_max     = 1
0.00.540.342 I llama_init_from_model: n_ctx         = 2048
0.00.540.343 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.540.343 I llama_init_from_model: n_batch       = 2048
0.00.540.343 I llama_init_from_model: n_ubatch      = 512
0.00.540.344 I llama_init_from_model: flash_attn    = 0
0.00.540.346 I llama_init_from_model: freq_base     = 10000.0
0.00.540.346 I llama_init_from_model: freq_scale    = 1
0.00.540.349 I ggml_metal_init: allocating
0.00.540.428 I ggml_metal_init: found device: Apple M4
0.00.540.440 I ggml_metal_init: picking default device: Apple M4
0.00.542.278 I ggml_metal_init: using embedded metal library
0.00.548.459 I ggml_metal_init: GPU name:   Apple M4
0.00.548.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.466 I ggml_metal_init: simdgroup reduction   = true
0.00.548.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.467 I ggml_metal_init: has residency sets    = true
0.00.548.467 I ggml_metal_init: has bfloat            = true
0.00.548.467 I ggml_metal_init: use bfloat            = true
0.00.548.469 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.470 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.918 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.652 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.627.660 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.627.683 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.916 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.631.919 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.631.919 I llama_init_from_model: graph nodes  = 967
0.00.631.919 I llama_init_from_model: graph splits = 2
0.00.631.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.632.058 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.632.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.619 I main: llama threadpool init, n_threads = 4
0.00.690.667 I 
0.00.690.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.690 I 
0.00.690.844 I sampler seed: 1234
0.00.690.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.893 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.896 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.896 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.446.658 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47651.01 tokens per second)
0.01.446.659 I llama_perf_context_print:        load time =     680.93 ms
0.01.446.660 I llama_perf_context_print: prompt eval time =      51.75 ms /     7 tokens (    7.39 ms per token,   135.27 tokens per second)
0.01.446.660 I llama_perf_context_print:        eval time =     700.93 ms /    63 runs   (   11.13 ms per token,    89.88 tokens per second)
0.01.446.661 I llama_perf_context_print:       total time =     756.73 ms /    70 tokens
0.01.446.906 I ggml_metal_free: deallocating

real	0m1.463s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.395 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.396 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.397 I llama_model_loader: - type  f32:  194 tensors
0.00.024.398 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.400 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.400 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.401 I print_info: file format = GGUF V3 (latest)
0.00.024.401 I print_info: file type   = Q4_K - Medium
0.00.024.403 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.520 I load: special tokens cache size = 25
0.00.038.619 I load: token to piece cache size = 0.2984 MB
0.00.038.623 I print_info: arch             = gptneox
0.00.038.623 I print_info: vocab_only       = 0
0.00.038.623 I print_info: n_ctx_train      = 2048
0.00.038.624 I print_info: n_embd           = 2048
0.00.038.624 I print_info: n_layer          = 24
0.00.038.628 I print_info: n_head           = 16
0.00.038.628 I print_info: n_head_kv        = 16
0.00.038.629 I print_info: n_rot            = 32
0.00.038.629 I print_info: n_swa            = 0
0.00.038.629 I print_info: n_embd_head_k    = 128
0.00.038.629 I print_info: n_embd_head_v    = 128
0.00.038.630 I print_info: n_gqa            = 1
0.00.038.631 I print_info: n_embd_k_gqa     = 2048
0.00.038.631 I print_info: n_embd_v_gqa     = 2048
0.00.038.632 I print_info: f_norm_eps       = 1.0e-05
0.00.038.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.633 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.633 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.633 I print_info: f_logit_scale    = 0.0e+00
0.00.038.634 I print_info: n_ff             = 8192
0.00.038.634 I print_info: n_expert         = 0
0.00.038.634 I print_info: n_expert_used    = 0
0.00.038.634 I print_info: causal attn      = 1
0.00.038.634 I print_info: pooling type     = 0
0.00.038.634 I print_info: rope type        = 2
0.00.038.634 I print_info: rope scaling     = linear
0.00.038.635 I print_info: freq_base_train  = 10000.0
0.00.038.635 I print_info: freq_scale_train = 1
0.00.038.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.636 I print_info: rope_finetuned   = unknown
0.00.038.636 I print_info: ssm_d_conv       = 0
0.00.038.636 I print_info: ssm_d_inner      = 0
0.00.038.636 I print_info: ssm_d_state      = 0
0.00.038.636 I print_info: ssm_dt_rank      = 0
0.00.038.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.639 I print_info: model type       = 1.4B
0.00.038.639 I print_info: model params     = 1.41 B
0.00.038.639 I print_info: general.name     = 1.4B
0.00.038.640 I print_info: vocab type       = BPE
0.00.038.640 I print_info: n_vocab          = 50304
0.00.038.640 I print_info: n_merges         = 50009
0.00.038.640 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.641 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.641 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.641 I print_info: LF token         = 187 ''
0.00.038.642 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.642 I print_info: max token length = 1024
0.00.038.642 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.523.606 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.623 I load_tensors: offloading output layer to GPU
0.00.523.624 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.658 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.659 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.525.337 I llama_init_from_model: n_seq_max     = 1
0.00.525.343 I llama_init_from_model: n_ctx         = 128
0.00.525.343 I llama_init_from_model: n_ctx_per_seq = 128
0.00.525.344 I llama_init_from_model: n_batch       = 128
0.00.525.344 I llama_init_from_model: n_ubatch      = 128
0.00.525.345 I llama_init_from_model: flash_attn    = 0
0.00.525.347 I llama_init_from_model: freq_base     = 10000.0
0.00.525.347 I llama_init_from_model: freq_scale    = 1
0.00.525.348 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.525.353 I ggml_metal_init: allocating
0.00.525.420 I ggml_metal_init: found device: Apple M4
0.00.525.434 I ggml_metal_init: picking default device: Apple M4
0.00.527.201 I ggml_metal_init: using embedded metal library
0.00.533.182 I ggml_metal_init: GPU name:   Apple M4
0.00.533.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.190 I ggml_metal_init: simdgroup reduction   = true
0.00.533.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.191 I ggml_metal_init: has residency sets    = true
0.00.533.191 I ggml_metal_init: has bfloat            = true
0.00.533.191 I ggml_metal_init: use bfloat            = true
0.00.533.192 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.551.847 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.555.403 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.555.407 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.555.440 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.558.528 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.558.530 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.558.531 I llama_init_from_model: graph nodes  = 967
0.00.558.531 I llama_init_from_model: graph splits = 2
0.00.558.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.558.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.755 I 
0.00.588.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.856 I perplexity: tokenizing the input ..
0.00.596.438 I perplexity: tokenization took 7.579 ms
0.00.596.467 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.331 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.742.671 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.742.689 I llama_perf_context_print:        load time =     579.97 ms
0.00.742.690 I llama_perf_context_print: prompt eval time =     143.96 ms /   128 tokens (    1.12 ms per token,   889.17 tokens per second)
0.00.742.691 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.742.691 I llama_perf_context_print:       total time =     153.94 ms /   129 tokens
0.00.743.075 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.079s
sys	0m0.121s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.063 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.658 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.666 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.667 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.043 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.045 I llama_model_loader: - type  f32:  194 tensors
0.00.027.045 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.045 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.046 I print_info: file format = GGUF V3 (latest)
0.00.027.046 I print_info: file type   = Q5_K - Medium
0.00.027.047 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.184 I load: special tokens cache size = 25
0.00.041.332 I load: token to piece cache size = 0.2984 MB
0.00.041.335 I print_info: arch             = gptneox
0.00.041.335 I print_info: vocab_only       = 0
0.00.041.335 I print_info: n_ctx_train      = 2048
0.00.041.335 I print_info: n_embd           = 2048
0.00.041.335 I print_info: n_layer          = 24
0.00.041.338 I print_info: n_head           = 16
0.00.041.339 I print_info: n_head_kv        = 16
0.00.041.339 I print_info: n_rot            = 32
0.00.041.339 I print_info: n_swa            = 0
0.00.041.340 I print_info: n_embd_head_k    = 128
0.00.041.342 I print_info: n_embd_head_v    = 128
0.00.041.342 I print_info: n_gqa            = 1
0.00.041.343 I print_info: n_embd_k_gqa     = 2048
0.00.041.344 I print_info: n_embd_v_gqa     = 2048
0.00.041.344 I print_info: f_norm_eps       = 1.0e-05
0.00.041.349 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.350 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.350 I print_info: f_logit_scale    = 0.0e+00
0.00.041.351 I print_info: n_ff             = 8192
0.00.041.351 I print_info: n_expert         = 0
0.00.041.351 I print_info: n_expert_used    = 0
0.00.041.352 I print_info: causal attn      = 1
0.00.041.352 I print_info: pooling type     = 0
0.00.041.354 I print_info: rope type        = 2
0.00.041.355 I print_info: rope scaling     = linear
0.00.041.356 I print_info: freq_base_train  = 10000.0
0.00.041.356 I print_info: freq_scale_train = 1
0.00.041.356 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.357 I print_info: rope_finetuned   = unknown
0.00.041.357 I print_info: ssm_d_conv       = 0
0.00.041.357 I print_info: ssm_d_inner      = 0
0.00.041.357 I print_info: ssm_d_state      = 0
0.00.041.357 I print_info: ssm_dt_rank      = 0
0.00.041.357 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.357 I print_info: model type       = 1.4B
0.00.041.358 I print_info: model params     = 1.41 B
0.00.041.359 I print_info: general.name     = 1.4B
0.00.041.359 I print_info: vocab type       = BPE
0.00.041.359 I print_info: n_vocab          = 50304
0.00.041.359 I print_info: n_merges         = 50009
0.00.041.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.360 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.360 I print_info: LF token         = 187 ''
0.00.041.361 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.361 I print_info: max token length = 1024
0.00.041.362 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.687 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.695 I load_tensors: offloading output layer to GPU
0.00.627.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.728 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.627.730 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.629.428 I llama_init_from_model: n_seq_max     = 1
0.00.629.433 I llama_init_from_model: n_ctx         = 2048
0.00.629.433 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.434 I llama_init_from_model: n_batch       = 2048
0.00.629.434 I llama_init_from_model: n_ubatch      = 512
0.00.629.434 I llama_init_from_model: flash_attn    = 0
0.00.629.436 I llama_init_from_model: freq_base     = 10000.0
0.00.629.437 I llama_init_from_model: freq_scale    = 1
0.00.629.439 I ggml_metal_init: allocating
0.00.629.493 I ggml_metal_init: found device: Apple M4
0.00.629.504 I ggml_metal_init: picking default device: Apple M4
0.00.631.055 I ggml_metal_init: using embedded metal library
0.00.637.992 I ggml_metal_init: GPU name:   Apple M4
0.00.637.998 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.000 I ggml_metal_init: simdgroup reduction   = true
0.00.638.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.001 I ggml_metal_init: has residency sets    = true
0.00.638.001 I ggml_metal_init: has bfloat            = true
0.00.638.001 I ggml_metal_init: use bfloat            = true
0.00.638.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.012 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.354 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.566 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.585 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.611 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.895 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.898 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.898 I llama_init_from_model: graph nodes  = 967
0.00.713.898 I llama_init_from_model: graph splits = 2
0.00.713.903 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.635 I main: llama threadpool init, n_threads = 4
0.00.779.676 I 
0.00.779.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.701 I 
0.00.779.869 I sampler seed: 1234
0.00.779.874 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.894 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.895 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.895 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.619.526 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.619.526 I llama_perf_context_print:        load time =     767.88 ms
0.01.619.527 I llama_perf_context_print: prompt eval time =      51.28 ms /     7 tokens (    7.33 ms per token,   136.51 tokens per second)
0.01.619.529 I llama_perf_context_print:        eval time =     785.56 ms /    63 runs   (   12.47 ms per token,    80.20 tokens per second)
0.01.619.529 I llama_perf_context_print:       total time =     840.58 ms /    70 tokens
0.01.619.801 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.109s
sys	0m0.235s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.105 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.111 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.122 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.123 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.755 I llama_model_loader: - type  f32:  194 tensors
0.00.025.755 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.755 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.756 I print_info: file format = GGUF V3 (latest)
0.00.025.756 I print_info: file type   = Q5_K - Medium
0.00.025.757 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.814 I load: special tokens cache size = 25
0.00.039.885 I load: token to piece cache size = 0.2984 MB
0.00.039.888 I print_info: arch             = gptneox
0.00.039.888 I print_info: vocab_only       = 0
0.00.039.888 I print_info: n_ctx_train      = 2048
0.00.039.888 I print_info: n_embd           = 2048
0.00.039.889 I print_info: n_layer          = 24
0.00.039.892 I print_info: n_head           = 16
0.00.039.893 I print_info: n_head_kv        = 16
0.00.039.893 I print_info: n_rot            = 32
0.00.039.894 I print_info: n_swa            = 0
0.00.039.894 I print_info: n_embd_head_k    = 128
0.00.039.894 I print_info: n_embd_head_v    = 128
0.00.039.895 I print_info: n_gqa            = 1
0.00.039.896 I print_info: n_embd_k_gqa     = 2048
0.00.039.896 I print_info: n_embd_v_gqa     = 2048
0.00.039.897 I print_info: f_norm_eps       = 1.0e-05
0.00.039.898 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.898 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.898 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.898 I print_info: f_logit_scale    = 0.0e+00
0.00.039.899 I print_info: n_ff             = 8192
0.00.039.899 I print_info: n_expert         = 0
0.00.039.899 I print_info: n_expert_used    = 0
0.00.039.899 I print_info: causal attn      = 1
0.00.039.900 I print_info: pooling type     = 0
0.00.039.900 I print_info: rope type        = 2
0.00.039.900 I print_info: rope scaling     = linear
0.00.039.900 I print_info: freq_base_train  = 10000.0
0.00.039.901 I print_info: freq_scale_train = 1
0.00.039.901 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.901 I print_info: rope_finetuned   = unknown
0.00.039.901 I print_info: ssm_d_conv       = 0
0.00.039.901 I print_info: ssm_d_inner      = 0
0.00.039.902 I print_info: ssm_d_state      = 0
0.00.039.902 I print_info: ssm_dt_rank      = 0
0.00.039.902 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.902 I print_info: model type       = 1.4B
0.00.039.902 I print_info: model params     = 1.41 B
0.00.039.903 I print_info: general.name     = 1.4B
0.00.039.903 I print_info: vocab type       = BPE
0.00.039.904 I print_info: n_vocab          = 50304
0.00.039.904 I print_info: n_merges         = 50009
0.00.039.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.905 I print_info: LF token         = 187 ''
0.00.039.905 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.905 I print_info: max token length = 1024
0.00.039.906 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.704 I load_tensors: offloading output layer to GPU
0.00.600.705 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.738 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.739 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.602.374 I llama_init_from_model: n_seq_max     = 1
0.00.602.377 I llama_init_from_model: n_ctx         = 128
0.00.602.377 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.378 I llama_init_from_model: n_batch       = 128
0.00.602.378 I llama_init_from_model: n_ubatch      = 128
0.00.602.378 I llama_init_from_model: flash_attn    = 0
0.00.602.380 I llama_init_from_model: freq_base     = 10000.0
0.00.602.380 I llama_init_from_model: freq_scale    = 1
0.00.602.381 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.383 I ggml_metal_init: allocating
0.00.602.398 I ggml_metal_init: found device: Apple M4
0.00.602.408 I ggml_metal_init: picking default device: Apple M4
0.00.603.772 I ggml_metal_init: using embedded metal library
0.00.610.093 I ggml_metal_init: GPU name:   Apple M4
0.00.610.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.099 I ggml_metal_init: simdgroup reduction   = true
0.00.610.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.100 I ggml_metal_init: has residency sets    = true
0.00.610.100 I ggml_metal_init: has bfloat            = true
0.00.610.100 I ggml_metal_init: use bfloat            = true
0.00.610.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.590 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.140 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.144 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.169 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.639 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.641 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.642 I llama_init_from_model: graph nodes  = 967
0.00.633.642 I llama_init_from_model: graph splits = 2
0.00.633.644 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.072 I 
0.00.668.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.168 I perplexity: tokenizing the input ..
0.00.675.241 I perplexity: tokenization took 7.07 ms
0.00.675.259 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.266 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.818.714 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.818.726 I llama_perf_context_print:        load time =     658.01 ms
0.00.818.726 I llama_perf_context_print: prompt eval time =     141.06 ms /   128 tokens (    1.10 ms per token,   907.42 tokens per second)
0.00.818.727 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.727 I llama_perf_context_print:       total time =     150.66 ms /   129 tokens
0.00.819.069 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.078s
sys	0m0.146s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.791 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.378 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.380 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.383 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.384 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.386 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.386 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.386 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.023 I llama_model_loader: - type  f32:  194 tensors
0.00.024.024 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.025 I print_info: file format = GGUF V3 (latest)
0.00.024.025 I print_info: file type   = Q6_K
0.00.024.026 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.135 I load: special tokens cache size = 25
0.00.038.210 I load: token to piece cache size = 0.2984 MB
0.00.038.213 I print_info: arch             = gptneox
0.00.038.213 I print_info: vocab_only       = 0
0.00.038.213 I print_info: n_ctx_train      = 2048
0.00.038.213 I print_info: n_embd           = 2048
0.00.038.214 I print_info: n_layer          = 24
0.00.038.216 I print_info: n_head           = 16
0.00.038.217 I print_info: n_head_kv        = 16
0.00.038.218 I print_info: n_rot            = 32
0.00.038.218 I print_info: n_swa            = 0
0.00.038.218 I print_info: n_embd_head_k    = 128
0.00.038.218 I print_info: n_embd_head_v    = 128
0.00.038.219 I print_info: n_gqa            = 1
0.00.038.220 I print_info: n_embd_k_gqa     = 2048
0.00.038.220 I print_info: n_embd_v_gqa     = 2048
0.00.038.221 I print_info: f_norm_eps       = 1.0e-05
0.00.038.221 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.222 I print_info: f_logit_scale    = 0.0e+00
0.00.038.223 I print_info: n_ff             = 8192
0.00.038.223 I print_info: n_expert         = 0
0.00.038.223 I print_info: n_expert_used    = 0
0.00.038.223 I print_info: causal attn      = 1
0.00.038.223 I print_info: pooling type     = 0
0.00.038.223 I print_info: rope type        = 2
0.00.038.224 I print_info: rope scaling     = linear
0.00.038.224 I print_info: freq_base_train  = 10000.0
0.00.038.225 I print_info: freq_scale_train = 1
0.00.038.225 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.225 I print_info: rope_finetuned   = unknown
0.00.038.225 I print_info: ssm_d_conv       = 0
0.00.038.225 I print_info: ssm_d_inner      = 0
0.00.038.226 I print_info: ssm_d_state      = 0
0.00.038.226 I print_info: ssm_dt_rank      = 0
0.00.038.226 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.226 I print_info: model type       = 1.4B
0.00.038.226 I print_info: model params     = 1.41 B
0.00.038.227 I print_info: general.name     = 1.4B
0.00.038.227 I print_info: vocab type       = BPE
0.00.038.227 I print_info: n_vocab          = 50304
0.00.038.228 I print_info: n_merges         = 50009
0.00.038.228 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.230 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.230 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.230 I print_info: LF token         = 187 ''
0.00.038.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.231 I print_info: max token length = 1024
0.00.038.231 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.675 I load_tensors: offloading 24 repeating layers to GPU
0.00.670.679 I load_tensors: offloading output layer to GPU
0.00.670.680 I load_tensors: offloaded 25/25 layers to GPU
0.00.670.702 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.670.705 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.672.090 I llama_init_from_model: n_seq_max     = 1
0.00.672.092 I llama_init_from_model: n_ctx         = 2048
0.00.672.092 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.672.093 I llama_init_from_model: n_batch       = 2048
0.00.672.093 I llama_init_from_model: n_ubatch      = 512
0.00.672.094 I llama_init_from_model: flash_attn    = 0
0.00.672.095 I llama_init_from_model: freq_base     = 10000.0
0.00.672.095 I llama_init_from_model: freq_scale    = 1
0.00.672.096 I ggml_metal_init: allocating
0.00.672.123 I ggml_metal_init: found device: Apple M4
0.00.672.131 I ggml_metal_init: picking default device: Apple M4
0.00.673.602 I ggml_metal_init: using embedded metal library
0.00.679.630 I ggml_metal_init: GPU name:   Apple M4
0.00.679.634 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.635 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.636 I ggml_metal_init: simdgroup reduction   = true
0.00.679.636 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.637 I ggml_metal_init: has residency sets    = true
0.00.679.637 I ggml_metal_init: has bfloat            = true
0.00.679.637 I ggml_metal_init: use bfloat            = true
0.00.679.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.696.373 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.749.422 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.749.429 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.749.453 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.753.927 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.753.929 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.753.929 I llama_init_from_model: graph nodes  = 967
0.00.753.929 I llama_init_from_model: graph splits = 2
0.00.753.936 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.051 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.410 I main: llama threadpool init, n_threads = 4
0.00.817.454 I 
0.00.817.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.475 I 
0.00.817.652 I sampler seed: 1234
0.00.817.656 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.817.677 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.817.677 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.817.677 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.686.877 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.686.878 I llama_perf_context_print:        load time =     807.93 ms
0.01.686.879 I llama_perf_context_print: prompt eval time =      54.06 ms /     7 tokens (    7.72 ms per token,   129.49 tokens per second)
0.01.686.881 I llama_perf_context_print:        eval time =     812.18 ms /    63 runs   (   12.89 ms per token,    77.57 tokens per second)
0.01.686.881 I llama_perf_context_print:       total time =     870.16 ms /    70 tokens
0.01.687.133 I ggml_metal_free: deallocating

real	0m1.705s
user	0m0.107s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4684 (507f9174) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.173 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.760 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.776 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.776 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.448 I llama_model_loader: - type  f32:  194 tensors
0.00.024.448 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.449 I print_info: file format = GGUF V3 (latest)
0.00.024.449 I print_info: file type   = Q6_K
0.00.024.450 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.253 I load: special tokens cache size = 25
0.00.038.395 I load: token to piece cache size = 0.2984 MB
0.00.038.398 I print_info: arch             = gptneox
0.00.038.399 I print_info: vocab_only       = 0
0.00.038.399 I print_info: n_ctx_train      = 2048
0.00.038.399 I print_info: n_embd           = 2048
0.00.038.399 I print_info: n_layer          = 24
0.00.038.403 I print_info: n_head           = 16
0.00.038.404 I print_info: n_head_kv        = 16
0.00.038.404 I print_info: n_rot            = 32
0.00.038.405 I print_info: n_swa            = 0
0.00.038.406 I print_info: n_embd_head_k    = 128
0.00.038.406 I print_info: n_embd_head_v    = 128
0.00.038.407 I print_info: n_gqa            = 1
0.00.038.408 I print_info: n_embd_k_gqa     = 2048
0.00.038.408 I print_info: n_embd_v_gqa     = 2048
0.00.038.409 I print_info: f_norm_eps       = 1.0e-05
0.00.038.409 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.410 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.410 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.410 I print_info: f_logit_scale    = 0.0e+00
0.00.038.411 I print_info: n_ff             = 8192
0.00.038.411 I print_info: n_expert         = 0
0.00.038.411 I print_info: n_expert_used    = 0
0.00.038.411 I print_info: causal attn      = 1
0.00.038.411 I print_info: pooling type     = 0
0.00.038.411 I print_info: rope type        = 2
0.00.038.412 I print_info: rope scaling     = linear
0.00.038.414 I print_info: freq_base_train  = 10000.0
0.00.038.414 I print_info: freq_scale_train = 1
0.00.038.414 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.415 I print_info: rope_finetuned   = unknown
0.00.038.415 I print_info: ssm_d_conv       = 0
0.00.038.415 I print_info: ssm_d_inner      = 0
0.00.038.415 I print_info: ssm_d_state      = 0
0.00.038.415 I print_info: ssm_dt_rank      = 0
0.00.038.415 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.415 I print_info: model type       = 1.4B
0.00.038.416 I print_info: model params     = 1.41 B
0.00.038.416 I print_info: general.name     = 1.4B
0.00.038.416 I print_info: vocab type       = BPE
0.00.038.420 I print_info: n_vocab          = 50304
0.00.038.421 I print_info: n_merges         = 50009
0.00.038.421 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.421 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.421 I print_info: LF token         = 187 ''
0.00.038.422 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.422 I print_info: max token length = 1024
0.00.038.424 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.416.305 I load_tensors: offloading 24 repeating layers to GPU
0.00.416.312 I load_tensors: offloading output layer to GPU
0.00.416.313 I load_tensors: offloaded 25/25 layers to GPU
0.00.416.338 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.416.341 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.417.887 I llama_init_from_model: n_seq_max     = 1
0.00.417.889 I llama_init_from_model: n_ctx         = 128
0.00.417.889 I llama_init_from_model: n_ctx_per_seq = 128
0.00.417.890 I llama_init_from_model: n_batch       = 128
0.00.417.890 I llama_init_from_model: n_ubatch      = 128
0.00.417.890 I llama_init_from_model: flash_attn    = 0
0.00.417.891 I llama_init_from_model: freq_base     = 10000.0
0.00.417.892 I llama_init_from_model: freq_scale    = 1
0.00.417.893 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.417.894 I ggml_metal_init: allocating
0.00.417.928 I ggml_metal_init: found device: Apple M4
0.00.417.939 I ggml_metal_init: picking default device: Apple M4
0.00.419.459 I ggml_metal_init: using embedded metal library
0.00.425.506 I ggml_metal_init: GPU name:   Apple M4
0.00.425.510 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.425.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.425.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.425.513 I ggml_metal_init: simdgroup reduction   = true
0.00.425.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.425.513 I ggml_metal_init: has residency sets    = true
0.00.425.513 I ggml_metal_init: has bfloat            = true
0.00.425.514 I ggml_metal_init: use bfloat            = true
0.00.425.515 I ggml_metal_init: hasUnifiedMemory      = true
0.00.425.516 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.442.621 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.446.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.446.159 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.446.212 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.449.345 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.449.347 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.449.347 I llama_init_from_model: graph nodes  = 967
0.00.449.348 I llama_init_from_model: graph splits = 2
0.00.449.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.449.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.485.221 I 
0.00.485.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.485.333 I perplexity: tokenizing the input ..
0.00.492.655 I perplexity: tokenization took 7.317 ms
0.00.492.681 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.416 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.634.759 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.634.775 I llama_perf_context_print:        load time =     476.04 ms
0.00.634.776 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.72 tokens per second)
0.00.634.777 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.777 I llama_perf_context_print:       total time =     149.56 ms /   129 tokens
0.00.635.133 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.078s
sys	0m0.117s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4684 (507f9174)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ae080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ae087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ae08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ae09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ae098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ae09ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ae0a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ae0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ae0afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ae0b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ae0b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ae0beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ae0c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ae0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ae0d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ae0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ae0e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ae0eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ae0f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ae0fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ae10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ae10c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ae11340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ae11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ae12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ae125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ae12bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ae13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ae13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ae14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ae144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ae147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ae15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ae15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ae15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ae15cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ae16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ae16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ae16ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ae16f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ae173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ae17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ae17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ae181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ae18490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ae18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ae190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ae199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ae19fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ae1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ae1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ae1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ae1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ae1be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ae1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ae1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ae1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ae1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ae1d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ae1e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ae1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ae1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ae1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ae1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ae1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ae1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ae1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ae20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ae207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ae20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ae21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ae215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ae21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ae21fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ae22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ae22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ae22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ae234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ae23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ae23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ae244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ae24a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ae24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ae254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ae25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ae25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ae264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ae26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ae26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ae274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ae27a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ae27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ae284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ae289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ae28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ae29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ae299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ae196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ae29e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ae2a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ae2ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ae2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ae2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ae2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ae2c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ae2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ae2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ae2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ae2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ae2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ae2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ae2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ae2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ae2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ae2f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ae2f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ae2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ae30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ae306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ae30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ae31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ae314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ae31950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ae31df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ae32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ae32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ae32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ae33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ae33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ae339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ae33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ae342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ae34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ae34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ae350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ae35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ae35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ae35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ae36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ae367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ae36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ae37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ae375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ae37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ae37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ae383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ae38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ae38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ae39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ae39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ae39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ae39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ae3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ae3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ae3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ae3b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ae3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ae3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ae3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ae3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ae3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ae3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ae3d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ae3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ae3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ae3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ae3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ae3e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ae3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ae3f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ae3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ae3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ae40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ae40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ae409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ae40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ae41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ae417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ae41c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ae420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ae42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ae42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ae42ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ae43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ae43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ae43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ae44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ae445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ae44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ae44f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ae453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ae45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ae45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ae46260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ae467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ae46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ae47250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ae47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ae47b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ae48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ae48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ae48f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ae493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ae49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ae49ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ae4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ae4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ae4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ae4b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ae4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ae4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ae4c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ae4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ae4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ae4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ae4dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ae4e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ae4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ae4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ae4f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ae4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ae4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ae4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ae50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ae50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ae50fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ae51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ae51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ae51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ae52520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ae52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ae52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ae53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ae53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ae53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ae54500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ae54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ae54fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ae554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ae55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ae55f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ae564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ae56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ae56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ae574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ae57a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ae57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ae584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ae58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ae58f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ae594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ae59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ae59f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ae5a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ae5a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ae5af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ae5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ae5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ae5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ae5c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ae5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ae5cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ae5d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ae5d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ae5df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ae5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ae5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ae5ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ae5f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ae5f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ae5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ae600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ae60570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ae60a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ae60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ae61350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ae617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ae61c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ae62130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ae625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ae62a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ae62f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ae63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ae63b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ae642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ae649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ae650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ae653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ae65b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ae65e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ae66460 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.737.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108708b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108708ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108709870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108709dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10870a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10870a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10870adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10870b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10870b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10870bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10870c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10870c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10870cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10870d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10870dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10870e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10870eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10870f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10870f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108710aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1087111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1087118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108712000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108712720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1087129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108712ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108713600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108713c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108714400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1087148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108714b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1087153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108715930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108715bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108716090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108716530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1087169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108716e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108717310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1087177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108717c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1087180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108718590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108718850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108718e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108719470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108719a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10871a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10871a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10871acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10871b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10871b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10871bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10871c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10871cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10871d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10871d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10871d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10871e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10871e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10871ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10871eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10871f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10871f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10871fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108720130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1087205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108720a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1087213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108721850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108721cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x108722240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x108722790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x108722ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x108723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x108723780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x108723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x108724220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x108724770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x108724cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x108725210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x108725760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x108725cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x108726200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x108726750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108726ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1087271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x108727740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108727c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1087281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108728730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108728c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1087291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108729720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108729c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10872a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10872a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10872ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10872b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10872b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10872bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10872c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10872c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10872cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10872d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10872d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10872dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10872e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10872e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10872ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10872f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10872f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10872fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10872ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1087303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108730d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1087311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108731670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108731b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108731fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108732450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1087328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108732d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1087336d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108733b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108734010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1087344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108734950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108734df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108735290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108735730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108735bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108736070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108736510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1087369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108736e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1087372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108737790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108737c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1087380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108738570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108738a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108738eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108739350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1087397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108739c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10873a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10873a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10873aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10873af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10873b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10873b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10873bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10873c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10873c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10873cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10873cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10873d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10873d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10873dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10873e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10873e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10873eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10873efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10873f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10873f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10873fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108740250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1087406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108740b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108741030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1087414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108741970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108741e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1087422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108742750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108742bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108743090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1087439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108743e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108744310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1087447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108744c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1087450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108745590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108745a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108746370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1087468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108746e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108747360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1087478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108747b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108748180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108748790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108748da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ae049f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ae04cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ae05120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ae05590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ae05a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ae05e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ae063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ae06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ae06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ae07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ae07ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ae07da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ae08210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ae08680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ae08af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ae08f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ae093d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ae09840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ae09cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ae0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ae0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ae0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ae0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ae0b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ae0b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ae0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ae0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ae0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ae0c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ae0cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ae0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ae0d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ae0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ae0df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ae0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ae0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ae0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ae0f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ae0f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ae0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ae0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ae102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ae10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ae10ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ae11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ae11480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ae118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ae11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ae121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ae12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ae12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ae12f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ae13390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ae13800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ae13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ae140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ae14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ae149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ae14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ae152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ae15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ae15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ae15ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ae16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ae168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ae16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ae171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ae17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ae17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ae17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ae18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ae187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ae18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ae190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ae19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ae199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ae19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ae1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ae1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ae1ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ae1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ae1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ae1beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ae1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ae1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ae1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ae1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ae1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ae1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ae1e750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ae1de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ae1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ae1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ae06f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ae1ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ae1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ae1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ae1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ae1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ae1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ae1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ae1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ae20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ae209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ae21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ae21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ae21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ae21fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ae22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ae22cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ae23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ae23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ae23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ae241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ae24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ae24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ae24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ae251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ae25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ae25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ae25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ae25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ae25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ae26250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ae26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ae267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ae26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ae26d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ae27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ae272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ae27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ae27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ae27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ae27dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ae28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ae28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ae28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ae288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ae28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ae28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ae29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ae293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ae29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ae29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ae29c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ae29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ae2a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ae2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ae2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ae2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ae2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ae2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ae2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ae2b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ae2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ae2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ae2bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ae2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ae2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ae2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ae2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ae2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ae2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ae2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ae2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ae2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ae2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ae2db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ae2de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ae2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ae2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ae2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ae2eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ae2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ae2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ae2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ae300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ae30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ae30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ae310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ae31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ae31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ae320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ae32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ae32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ae330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ae33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ae33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ae340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ae345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ae34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ae35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ae355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ae35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ae35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ae361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ae36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ae36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ae36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ae373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ae37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ae37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ae38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ae38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ae38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ae38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ae393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ae39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ae39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ae3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ae3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ae3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ae3ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ae3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ae3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ae3bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ae3c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ae3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ae3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ae3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ae3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ae3d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ae3dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ae3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ae3e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ae3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ae3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ae3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ae3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ae3f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ae3fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ae402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ae40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ae40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ae41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ae41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ae418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ae41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ae421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ae42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ae42aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ae42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ae43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ae437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ae43c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ae440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ae44540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ae449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ae44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ae45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ae45700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ae45b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ae45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ae46450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ae468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ae46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ae471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ae47610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ae47a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ae47ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ae48360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ae487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ae48c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ae490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ae49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ae49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ae49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ae4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ae4a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ae4ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ae4afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ae4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ae4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ae4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ae4c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ae4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ae4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ae4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ae4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ae4d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ae4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ae4e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ae4e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ae4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ae4ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ae4f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ae4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ae4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ae4ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ae50410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ae50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ae50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ae51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ae51790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ae51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ae52070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ae524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ae52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ae52e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ae53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ae53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ae541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ae54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ae54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ae552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ae558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ae55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ae56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ae569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ae56fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ae57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ae57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ae580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ae586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ae58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ae59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ae597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ae59db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ae5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ae5a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ae5aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ae5b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ae5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ae5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ae5c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ae5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ae5d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ae5d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ae5dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ae5e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ae5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ae5ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ae5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ae5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ae5ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ae60530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ae60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ae610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ae61670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ae61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ae621f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ae627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ae62d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ae63330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ae638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ae63eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ae64470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ae64a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ae64ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ae655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ae65b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ae66130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ae666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ae66cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ae67270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ae67830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ae67df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ae683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ae688b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ae68db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ae692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ae697b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ae69cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ae6a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ae6a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ae6abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ae6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ae6b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ae6bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ae6bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ae6c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ae6c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ae6ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ae6d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ae6dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ae6e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ae6ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ae6f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ae6f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ae6fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ae701a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.797s
user	0m0.280s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4684 (507f9174)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141e0b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141e0b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141e0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141e0c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141e0ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141e0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141e0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141e0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141e0e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141e0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141e0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141e0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141e0fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141e102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141e10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141e11200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141e11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141e12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141e12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141e12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141e13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141e14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141e14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141e15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141e15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141e15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141e16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141e16ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141e17630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141e178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141e18180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141e186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141e18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141e18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141e192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141e19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141e1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141e1a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141e1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141e1ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141e1b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141e1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141e1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141e1c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141e1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141e1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141e1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141e1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141e1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141e1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141e1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141e1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141e1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141e200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141e20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141e20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141e21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141e21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141e218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141e21d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141e22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141e226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141e22b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141e22ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141e23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141e23930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141e23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141e24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141e24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141e24bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141e25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141e25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141e25ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141e260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141e26640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141e26b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141e270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141e27630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141e27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141e280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141e28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141e28b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141e290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141e29610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141e29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141e2a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141e2a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141e2ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141e2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141e2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141e2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141e2c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141e2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141e2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141e1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141e2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141e2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141e2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141e2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141e2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141e2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141e2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141e2f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141e2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141e301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141e30720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141e30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141e311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141e31710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141e31c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141e32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141e325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141e32a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141e32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141e33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141e33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141e33cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141e34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141e34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141e34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141e34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141e353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141e35880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141e35d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141e361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141e36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141e36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141e36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141e37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141e378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141e37d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141e38220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141e386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141e38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141e39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141e394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141e39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141e3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141e3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141e3abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141e3b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141e3b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141e3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141e3be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141e3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141e3c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141e3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141e3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141e3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141e3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141e3dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141e3e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141e3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141e3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141e3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141e3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141e3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141e3ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141e403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141e40840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141e40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141e41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141e41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141e41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141e41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141e42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141e428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141e42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141e431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141e43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141e43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141e43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141e44460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141e44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141e44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141e45240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141e456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141e45b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141e46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141e464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141e46960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141e46e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141e472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141e47740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141e47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141e48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141e48520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141e489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141e48e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141e493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141e49900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141e49e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141e4a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141e4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141e4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141e4b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141e4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141e4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141e4c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141e4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141e4cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141e4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141e4dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141e4e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141e4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141e4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141e4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141e4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141e50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141e506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141e50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141e51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141e516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141e51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141e52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141e526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141e52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141e53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141e53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141e53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141e54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141e54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141e54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141e55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141e55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141e55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141e56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141e56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141e56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141e57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141e57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141e57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141e580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141e58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141e58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141e590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141e59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141e59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141e5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141e5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141e5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141e5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141e5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141e5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141e5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141e5c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141e5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141e5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141e5d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141e5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141e5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141e5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141e5eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141e5f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141e5f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141e5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141e60070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141e605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141e60b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141e61060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141e615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141e61b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141e61fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141e62440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141e628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141e62d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141e63220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141e636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141e63b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141e64000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141e644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141e64940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141e64de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141e65280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141e65720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141e65bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141e66060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141e665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141e66cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141e673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141e67b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141e68230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141e684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141e68ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141e68fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141e695b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.998 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143806200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143806670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143806ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143806f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1438073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143807830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143807ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143808110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143808580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1438089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143808e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143809530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14380a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14380a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14380b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14380b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14380be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14380c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14380cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14380d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14380db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14380e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14380e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14380f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14380f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14380fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14380fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1438101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143810660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143810ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143810f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143811470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1438118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143811ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143812010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143812480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1438128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143812d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1438131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143813640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143813ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143813f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143814390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143814800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143814c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1438150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143815550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1438159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143815e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1438162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143816710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143816b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143816ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143817460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1438178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143817d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1438182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1438187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143818c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143819090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143819500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143819970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143819de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14381a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14381a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14381ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14381afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14381b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14381b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14381bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14381c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14381c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14381ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14381ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14381d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14381d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14381dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14381e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14381e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14381e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14381edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14381f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14381f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14381fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14381ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1438203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143820860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143820cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143821140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1438215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143821a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143821e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143822300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143822770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143822be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143823050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1438234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143823930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143823da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143824210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143824680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143824af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143824f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1438253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143825840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143825cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143826120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143826590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143826a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143826e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1438272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143827750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143827bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143828030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1438284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143828910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143828d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1438291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143829660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143829ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143829f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14382a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14382a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14382ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14382b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14382b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14382b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14382be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14382c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14382c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14382cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14382d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14382d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14382d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14382dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14382e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14382e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14382eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14382ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14382f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14382f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14382fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1438300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143830550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1438309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143830e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1438312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143831710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143831b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143831ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143832460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1438328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143832d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1438331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143833620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143833a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143833f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143834370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1438347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143834c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1438350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143835530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1438359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143835e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143836280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1438366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143837320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1438375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1438378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1438385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1438397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14383a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14383a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14383a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14383ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14383b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14383b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14383bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14383bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14383c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14383c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14383ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14383d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14383d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14383da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14383deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14383e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14383e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14383ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14383f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14383f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14383f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14383fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1438406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143841070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143841580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1438419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143841e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1438422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143842740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143842c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143843170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143843ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143843fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143844560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143844b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1438450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1438456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143845c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143846220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1438467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143846da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143847360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143847920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143847ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1438484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143848a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143849020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1438495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143849ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14384a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14384a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14384ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14384b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14384b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14384be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14384c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14384c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14384cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14384d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14384dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14384e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14384e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14384ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14384f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14384f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14384fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143850320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1438508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143850ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143851460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143851a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143851fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1438525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143852b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143853120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1438536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143853ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143854260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143854820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143854de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1438553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143855960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143855f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1438564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143856aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143857060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143857620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143857be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1438581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1438586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143858ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1438590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1438595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143859aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143859fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14385a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14385a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14385aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14385b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14385b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14385bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14385c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14385c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14385cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14385d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14385ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14385e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14385ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14385eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14385f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14385f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14385ff90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14385cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14384dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14384cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1438498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1438567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143853f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143851ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14384fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143847be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1438453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14384a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14384b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143850ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14384d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143855660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1438481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1438492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143852860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143851720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14384e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143848d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143856d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143845f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143844820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143844de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143846aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143857320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14384c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143854ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14384a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14384d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14384afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143851160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143848760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143852e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143847620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143855c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1438533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14384eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143857ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1438464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1438578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143845960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1438561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143850020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1438522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1438550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1438539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14384bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143843430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14385f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143809120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143860250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143860670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143860930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143860bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143861130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1438613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1438616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143861970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143861c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143861ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1438621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143862470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141f099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141f09f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141f0a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141f0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141f0ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141f0b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141f0b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141f0b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141f0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141f0c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141f0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141f0cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141f0d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141f0d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141f0d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141f0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141f0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141f0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141f0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141f0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141f10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141f109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141f10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141f112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141f11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141f11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141f11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141f12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141f128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141f12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141f131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141f13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141f13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141f14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141f144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141f14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141f14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141f15200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141f15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141f15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141f15f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141f163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141f16830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141f16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141f17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141f17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141f179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141f17e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141f182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141f18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141f18bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141f19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141f19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141f19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141f19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141f1a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141f1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141f1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141f1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141f1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141f1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141f1bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141f1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141f1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141f1c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141f1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141f1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141f1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141f1db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141f1e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141f1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141f1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141f1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141f1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141f1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141f1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141f1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141f20380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141f207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141f20c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141f210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141f21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141f219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141f21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141f22290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141f22700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141f22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141f22fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141f23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141f238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141f23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141f241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141f24610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141f24a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141f24ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141f25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141f257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141f25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141f260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141f26520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141f26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141f26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141f27270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141f276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141f27b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141f27fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141f28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141f288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141f28d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141f29180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141f295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141f29a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141f29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141f2a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141f2a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141f2ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141f2b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141f2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141f2b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141f2bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141f2c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141f2c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141f2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141f2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141f2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141f2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141f2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141f2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141f2e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141f2ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141f2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141f2f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141f2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141f2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141f303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141f309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141f311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141f31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141f31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141f31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141f32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141f32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141f33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141f33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141f33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141f34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141f34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141f34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141f351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141f35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141f35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141f361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141f36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141f36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141f371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141f37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141f37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141f381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141f38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141f38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141f391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141f39700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141f39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141f3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141f3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141f3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141f3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141f3b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141f3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141f3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141f3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141f3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141f3d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141f3d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141f3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141f3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141f3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141f3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141f3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141f3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141f3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141f40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141f40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141f40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141f41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141f41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141f41bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141f42120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141f42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141f43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141f43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141f43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141f44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141f44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141f44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141f450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141f45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141f45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141f45ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141f46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141f46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141f46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141f47150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141f475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141f47a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141f47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141f483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141f48870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141f48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141f491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141f49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141f49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141f4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141f4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141f4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141f4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141f4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141f4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141f4c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141f4cba0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.237s
sys	0m0.199s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.42 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.47 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.89 sec*proc (2 tests)

Total Test time (real) =   1.91 sec
        1.94 real         0.50 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.25 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.37 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.63 sec*proc (2 tests)

Total Test time (real) =   0.72 sec
        0.73 real         0.14 user         0.09 sys
```
