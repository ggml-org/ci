### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.12 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.05 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.87 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    3.23 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.27 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.88 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.80 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 254.89 sec*proc (29 tests)

Total Test time (real) = 254.90 sec

real	4m15.006s
user	8m38.645s
sys	0m7.231s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.79 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    2.46 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.74 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  56.60 sec*proc (29 tests)

Total Test time (real) =  56.61 sec

real	0m56.624s
user	1m17.001s
sys	0m6.311s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.208 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.503 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.063 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.073 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.075 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.075 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.076 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.078 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.079 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.079 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.080 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.081 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.084 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.084 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.085 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.086 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.086 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.087 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.088 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.987 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.989 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.990 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.990 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.991 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.991 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.992 I llama_model_loader: - type  f32:  124 tensors
0.00.030.992 I llama_model_loader: - type  f16:   73 tensors
0.00.030.993 I print_info: file format = GGUF V3 (latest)
0.00.030.994 I print_info: file type   = F16
0.00.030.995 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.035.481 I load: special tokens cache size = 5
0.00.037.602 I load: token to piece cache size = 0.2032 MB
0.00.037.606 I print_info: arch             = bert
0.00.037.606 I print_info: vocab_only       = 0
0.00.037.607 I print_info: n_ctx_train      = 512
0.00.037.607 I print_info: n_embd           = 384
0.00.037.607 I print_info: n_layer          = 12
0.00.037.610 I print_info: n_head           = 12
0.00.037.611 I print_info: n_head_kv        = 12
0.00.037.611 I print_info: n_rot            = 32
0.00.037.612 I print_info: n_swa            = 0
0.00.037.612 I print_info: n_embd_head_k    = 32
0.00.037.615 I print_info: n_embd_head_v    = 32
0.00.037.616 I print_info: n_gqa            = 1
0.00.037.617 I print_info: n_embd_k_gqa     = 384
0.00.037.617 I print_info: n_embd_v_gqa     = 384
0.00.037.618 I print_info: f_norm_eps       = 1.0e-12
0.00.037.625 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.626 I print_info: f_logit_scale    = 0.0e+00
0.00.037.627 I print_info: n_ff             = 1536
0.00.037.627 I print_info: n_expert         = 0
0.00.037.627 I print_info: n_expert_used    = 0
0.00.037.628 I print_info: causal attn      = 0
0.00.037.628 I print_info: pooling type     = 2
0.00.037.628 I print_info: rope type        = 2
0.00.037.628 I print_info: rope scaling     = linear
0.00.037.629 I print_info: freq_base_train  = 10000.0
0.00.037.634 I print_info: freq_scale_train = 1
0.00.037.637 I print_info: n_ctx_orig_yarn  = 512
0.00.037.637 I print_info: rope_finetuned   = unknown
0.00.037.637 I print_info: ssm_d_conv       = 0
0.00.037.638 I print_info: ssm_d_inner      = 0
0.00.037.638 I print_info: ssm_d_state      = 0
0.00.037.638 I print_info: ssm_dt_rank      = 0
0.00.037.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.638 I print_info: model type       = 33M
0.00.037.639 I print_info: model params     = 33.21 M
0.00.037.639 I print_info: general.name     = Bge Small
0.00.037.640 I print_info: vocab type       = WPM
0.00.037.640 I print_info: n_vocab          = 30522
0.00.037.640 I print_info: n_merges         = 0
0.00.037.641 I print_info: BOS token        = 101 '[CLS]'
0.00.037.641 I print_info: UNK token        = 100 '[UNK]'
0.00.037.641 I print_info: SEP token        = 102 '[SEP]'
0.00.037.646 I print_info: PAD token        = 0 '[PAD]'
0.00.037.646 I print_info: MASK token       = 103 '[MASK]'
0.00.037.647 I print_info: LF token         = 0 '[PAD]'
0.00.037.649 I print_info: max token length = 21
0.00.037.649 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.040.897 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.898 I load_tensors: offloading output layer to GPU
0.00.040.899 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.925 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.927 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.234 I llama_init_from_model: n_seq_max     = 1
0.00.041.235 I llama_init_from_model: n_ctx         = 512
0.00.041.235 I llama_init_from_model: n_ctx_per_seq = 512
0.00.041.236 I llama_init_from_model: n_batch       = 2048
0.00.041.236 I llama_init_from_model: n_ubatch      = 2048
0.00.041.236 I llama_init_from_model: flash_attn    = 0
0.00.041.237 I llama_init_from_model: freq_base     = 10000.0
0.00.041.237 I llama_init_from_model: freq_scale    = 1
0.00.041.238 I ggml_metal_init: allocating
0.00.041.243 I ggml_metal_init: found device: Apple M4
0.00.041.250 I ggml_metal_init: picking default device: Apple M4
0.00.041.976 I ggml_metal_init: using embedded metal library
0.00.046.028 I ggml_metal_init: GPU name:   Apple M4
0.00.046.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.032 I ggml_metal_init: simdgroup reduction   = true
0.00.046.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.032 I ggml_metal_init: has residency sets    = true
0.00.046.032 I ggml_metal_init: has bfloat            = true
0.00.046.033 I ggml_metal_init: use bfloat            = true
0.00.046.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.631 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.351 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.354 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.355 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.621 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.622 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.623 I llama_init_from_model: graph nodes  = 429
0.00.060.623 I llama_init_from_model: graph splits = 2
0.00.060.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.224 I 
0.00.066.250 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.927 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.070.787 I llama_perf_context_print:        load time =      46.72 ms
0.00.070.790 I llama_perf_context_print: prompt eval time =       3.73 ms /     9 tokens (    0.41 ms per token,  2416.11 tokens per second)
0.00.070.791 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.070.791 I llama_perf_context_print:       total time =       4.56 ms /    10 tokens
0.00.070.933 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.051s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.352 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.114 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.120 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.123 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.123 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.123 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.124 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.124 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.125 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.125 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.125 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.127 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.128 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.128 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.128 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.129 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.129 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.553 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.181 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.182 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.182 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.182 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.183 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.183 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.183 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.184 I llama_model_loader: - type  f32:  124 tensors
0.00.015.184 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.185 I print_info: file format = GGUF V3 (latest)
0.00.015.185 I print_info: file type   = Q8_0
0.00.015.186 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.669 I load: special tokens cache size = 5
0.00.018.947 I load: token to piece cache size = 0.2032 MB
0.00.018.950 I print_info: arch             = bert
0.00.018.950 I print_info: vocab_only       = 0
0.00.018.950 I print_info: n_ctx_train      = 512
0.00.018.950 I print_info: n_embd           = 384
0.00.018.950 I print_info: n_layer          = 12
0.00.018.954 I print_info: n_head           = 12
0.00.018.954 I print_info: n_head_kv        = 12
0.00.018.955 I print_info: n_rot            = 32
0.00.018.955 I print_info: n_swa            = 0
0.00.018.957 I print_info: n_embd_head_k    = 32
0.00.018.957 I print_info: n_embd_head_v    = 32
0.00.018.957 I print_info: n_gqa            = 1
0.00.018.958 I print_info: n_embd_k_gqa     = 384
0.00.018.959 I print_info: n_embd_v_gqa     = 384
0.00.018.964 I print_info: f_norm_eps       = 1.0e-12
0.00.018.964 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.965 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.967 I print_info: f_logit_scale    = 0.0e+00
0.00.018.967 I print_info: n_ff             = 1536
0.00.018.967 I print_info: n_expert         = 0
0.00.018.968 I print_info: n_expert_used    = 0
0.00.018.968 I print_info: causal attn      = 0
0.00.018.968 I print_info: pooling type     = 2
0.00.018.968 I print_info: rope type        = 2
0.00.018.968 I print_info: rope scaling     = linear
0.00.018.969 I print_info: freq_base_train  = 10000.0
0.00.018.969 I print_info: freq_scale_train = 1
0.00.018.969 I print_info: n_ctx_orig_yarn  = 512
0.00.018.969 I print_info: rope_finetuned   = unknown
0.00.018.969 I print_info: ssm_d_conv       = 0
0.00.018.969 I print_info: ssm_d_inner      = 0
0.00.018.970 I print_info: ssm_d_state      = 0
0.00.018.970 I print_info: ssm_dt_rank      = 0
0.00.018.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.970 I print_info: model type       = 33M
0.00.018.970 I print_info: model params     = 33.21 M
0.00.018.970 I print_info: general.name     = Bge Small
0.00.018.971 I print_info: vocab type       = WPM
0.00.018.971 I print_info: n_vocab          = 30522
0.00.018.971 I print_info: n_merges         = 0
0.00.018.972 I print_info: BOS token        = 101 '[CLS]'
0.00.018.972 I print_info: UNK token        = 100 '[UNK]'
0.00.018.972 I print_info: SEP token        = 102 '[SEP]'
0.00.018.972 I print_info: PAD token        = 0 '[PAD]'
0.00.018.972 I print_info: MASK token       = 103 '[MASK]'
0.00.018.972 I print_info: LF token         = 0 '[PAD]'
0.00.018.973 I print_info: max token length = 21
0.00.018.973 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.809 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.810 I load_tensors: offloading output layer to GPU
0.00.020.811 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.817 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.818 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.064 I llama_init_from_model: n_seq_max     = 1
0.00.021.065 I llama_init_from_model: n_ctx         = 512
0.00.021.065 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.065 I llama_init_from_model: n_batch       = 2048
0.00.021.066 I llama_init_from_model: n_ubatch      = 2048
0.00.021.066 I llama_init_from_model: flash_attn    = 0
0.00.021.066 I llama_init_from_model: freq_base     = 10000.0
0.00.021.066 I llama_init_from_model: freq_scale    = 1
0.00.021.067 I ggml_metal_init: allocating
0.00.021.072 I ggml_metal_init: found device: Apple M4
0.00.021.076 I ggml_metal_init: picking default device: Apple M4
0.00.021.597 I ggml_metal_init: using embedded metal library
0.00.024.073 I ggml_metal_init: GPU name:   Apple M4
0.00.024.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.076 I ggml_metal_init: simdgroup reduction   = true
0.00.024.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.076 I ggml_metal_init: has residency sets    = true
0.00.024.076 I ggml_metal_init: has bfloat            = true
0.00.024.077 I ggml_metal_init: use bfloat            = true
0.00.024.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.649 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.240 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.243 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.249 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.312 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.313 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.313 I llama_init_from_model: graph nodes  = 429
0.00.036.313 I llama_init_from_model: graph splits = 2
0.00.036.314 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.846 I 
0.00.039.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.396 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.631 I llama_perf_context_print:        load time =      30.49 ms
0.00.043.632 I llama_perf_context_print: prompt eval time =       3.11 ms /     9 tokens (    0.35 ms per token,  2896.68 tokens per second)
0.00.043.632 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.633 I llama_perf_context_print:       total time =       3.78 ms /    10 tokens
0.00.043.794 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.272 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.338 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.174 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.181 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.041.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.183 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.041.183 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.041.184 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.041.185 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.041.186 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.041.187 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.041.187 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.041.188 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.041.191 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.041.192 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.041.192 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.041.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.193 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.048.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.050.797 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.055.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.055.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.055.660 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.055.660 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.055.661 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.055.661 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.055.662 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.055.662 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.055.662 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.055.663 I llama_model_loader: - type  f32:   40 tensors
0.00.055.663 I llama_model_loader: - type  f16:   30 tensors
0.00.055.664 I print_info: file format = GGUF V3 (latest)
0.00.055.665 I print_info: file type   = F16
0.00.055.666 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.060.020 W load: empty token at index 5
0.00.065.257 W load: model vocab missing newline token, using special_pad_id instead
0.00.066.806 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.066.843 I load: special tokens cache size = 5
0.00.332.941 I load: token to piece cache size = 1.5060 MB
0.00.332.948 I print_info: arch             = jina-bert-v2
0.00.332.948 I print_info: vocab_only       = 0
0.00.332.948 I print_info: n_ctx_train      = 8192
0.00.332.949 I print_info: n_embd           = 384
0.00.332.949 I print_info: n_layer          = 4
0.00.332.956 I print_info: n_head           = 12
0.00.332.957 I print_info: n_head_kv        = 12
0.00.332.957 I print_info: n_rot            = 32
0.00.332.957 I print_info: n_swa            = 0
0.00.332.957 I print_info: n_embd_head_k    = 32
0.00.332.964 I print_info: n_embd_head_v    = 32
0.00.332.965 I print_info: n_gqa            = 1
0.00.332.968 I print_info: n_embd_k_gqa     = 384
0.00.332.969 I print_info: n_embd_v_gqa     = 384
0.00.332.970 I print_info: f_norm_eps       = 1.0e-12
0.00.332.971 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.332.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.332.971 I print_info: f_max_alibi_bias = 8.0e+00
0.00.332.971 I print_info: f_logit_scale    = 0.0e+00
0.00.332.971 I print_info: n_ff             = 1536
0.00.332.972 I print_info: n_expert         = 0
0.00.332.972 I print_info: n_expert_used    = 0
0.00.332.972 I print_info: causal attn      = 0
0.00.332.972 I print_info: pooling type     = -1
0.00.332.972 I print_info: rope type        = -1
0.00.332.973 I print_info: rope scaling     = linear
0.00.332.973 I print_info: freq_base_train  = 10000.0
0.00.332.973 I print_info: freq_scale_train = 1
0.00.332.973 I print_info: n_ctx_orig_yarn  = 8192
0.00.332.973 I print_info: rope_finetuned   = unknown
0.00.332.973 I print_info: ssm_d_conv       = 0
0.00.332.973 I print_info: ssm_d_inner      = 0
0.00.332.974 I print_info: ssm_d_state      = 0
0.00.332.974 I print_info: ssm_dt_rank      = 0
0.00.332.974 I print_info: ssm_dt_b_c_rms   = 0
0.00.332.974 I print_info: model type       = 33M
0.00.332.974 I print_info: model params     = 32.90 M
0.00.332.974 I print_info: general.name     = Jina Bert Implementation
0.00.332.975 I print_info: vocab type       = BPE
0.00.332.976 I print_info: n_vocab          = 61056
0.00.332.976 I print_info: n_merges         = 39382
0.00.332.976 I print_info: BOS token        = 0 '<s>'
0.00.332.976 I print_info: EOS token        = 2 '</s>'
0.00.332.976 I print_info: UNK token        = 3 '<unk>'
0.00.332.976 I print_info: SEP token        = 2 '</s>'
0.00.332.976 I print_info: PAD token        = 1 '<pad>'
0.00.332.977 I print_info: MASK token       = 4 '<mask>'
0.00.332.977 I print_info: EOG token        = 2 '</s>'
0.00.332.977 I print_info: max token length = 45
0.00.332.978 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.335.254 I load_tensors: offloading 4 repeating layers to GPU
0.00.335.255 I load_tensors: offloading output layer to GPU
0.00.335.256 I load_tensors: offloaded 5/5 layers to GPU
0.00.335.278 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.280 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.335.648 I llama_init_from_model: n_seq_max     = 1
0.00.335.649 I llama_init_from_model: n_ctx         = 8192
0.00.335.649 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.335.649 I llama_init_from_model: n_batch       = 2048
0.00.335.649 I llama_init_from_model: n_ubatch      = 2048
0.00.335.649 I llama_init_from_model: flash_attn    = 0
0.00.335.650 I llama_init_from_model: freq_base     = 10000.0
0.00.335.650 I llama_init_from_model: freq_scale    = 1
0.00.335.651 I ggml_metal_init: allocating
0.00.335.661 I ggml_metal_init: found device: Apple M4
0.00.335.665 I ggml_metal_init: picking default device: Apple M4
0.00.336.384 I ggml_metal_init: using embedded metal library
0.00.339.265 I ggml_metal_init: GPU name:   Apple M4
0.00.339.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.268 I ggml_metal_init: simdgroup reduction   = true
0.00.339.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.268 I ggml_metal_init: has residency sets    = true
0.00.339.269 I ggml_metal_init: has bfloat            = true
0.00.339.269 I ggml_metal_init: use bfloat            = true
0.00.339.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.686 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.351.679 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.351.680 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.682 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.357.906 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.357.907 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.357.907 I llama_init_from_model: graph nodes  = 154
0.00.357.907 I llama_init_from_model: graph splits = 2
0.00.357.909 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.267 I 
0.00.365.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.644 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.644 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.658 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.658 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.664 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.664 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.203 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.922 I llama_perf_context_print:        load time =     337.92 ms
0.00.369.923 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16707.09 tokens per second)
0.00.369.926 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.926 I llama_perf_context_print:       total time =       4.65 ms /    63 tokens
0.00.370.182 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.336s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.232 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.416 I main: llama backend init
0.00.000.422 I main: load the model and apply lora adapter, if any
0.00.047.559 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.064.380 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.391 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.397 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.404 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.404 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.405 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.076.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.083.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.083.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.083.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.083.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.083.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.083.549 I llama_model_loader: - type  f32:  194 tensors
0.00.083.549 I llama_model_loader: - type  f16:   98 tensors
0.00.083.550 I print_info: file format = GGUF V3 (latest)
0.00.083.552 I print_info: file type   = all F32 (guessed)
0.00.083.554 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.253 I load: special tokens cache size = 25
0.00.106.844 I load: token to piece cache size = 0.2984 MB
0.00.106.848 I print_info: arch             = gptneox
0.00.106.848 I print_info: vocab_only       = 0
0.00.106.848 I print_info: n_ctx_train      = 2048
0.00.106.848 I print_info: n_embd           = 2048
0.00.106.848 I print_info: n_layer          = 24
0.00.106.851 I print_info: n_head           = 16
0.00.106.853 I print_info: n_head_kv        = 16
0.00.106.853 I print_info: n_rot            = 32
0.00.106.853 I print_info: n_swa            = 0
0.00.106.853 I print_info: n_embd_head_k    = 128
0.00.106.854 I print_info: n_embd_head_v    = 128
0.00.106.854 I print_info: n_gqa            = 1
0.00.106.855 I print_info: n_embd_k_gqa     = 2048
0.00.106.856 I print_info: n_embd_v_gqa     = 2048
0.00.106.857 I print_info: f_norm_eps       = 1.0e-05
0.00.106.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.106.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.106.858 I print_info: f_max_alibi_bias = 0.0e+00
0.00.106.858 I print_info: f_logit_scale    = 0.0e+00
0.00.106.858 I print_info: n_ff             = 8192
0.00.106.859 I print_info: n_expert         = 0
0.00.106.859 I print_info: n_expert_used    = 0
0.00.106.859 I print_info: causal attn      = 1
0.00.106.859 I print_info: pooling type     = 0
0.00.106.859 I print_info: rope type        = 2
0.00.106.860 I print_info: rope scaling     = linear
0.00.106.860 I print_info: freq_base_train  = 10000.0
0.00.106.862 I print_info: freq_scale_train = 1
0.00.106.862 I print_info: n_ctx_orig_yarn  = 2048
0.00.106.862 I print_info: rope_finetuned   = unknown
0.00.106.863 I print_info: ssm_d_conv       = 0
0.00.106.863 I print_info: ssm_d_inner      = 0
0.00.106.863 I print_info: ssm_d_state      = 0
0.00.106.863 I print_info: ssm_dt_rank      = 0
0.00.106.863 I print_info: ssm_dt_b_c_rms   = 0
0.00.106.866 I print_info: model type       = 1.4B
0.00.106.866 I print_info: model params     = 1.41 B
0.00.106.866 I print_info: general.name     = 1.4B
0.00.106.867 I print_info: vocab type       = BPE
0.00.106.867 I print_info: n_vocab          = 50304
0.00.106.867 I print_info: n_merges         = 50009
0.00.106.867 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.106.868 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.106.868 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.106.868 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.106.868 I print_info: LF token         = 187 ''
0.00.106.875 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.106.876 I print_info: max token length = 1024
0.00.106.877 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.138.778 I load_tensors: offloading 24 repeating layers to GPU
0.00.138.782 I load_tensors: offloading output layer to GPU
0.00.138.783 I load_tensors: offloaded 25/25 layers to GPU
0.00.138.804 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.138.806 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.139.208 I llama_init_from_model: n_seq_max     = 1
0.00.139.209 I llama_init_from_model: n_ctx         = 2048
0.00.139.209 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.139.209 I llama_init_from_model: n_batch       = 2048
0.00.139.209 I llama_init_from_model: n_ubatch      = 512
0.00.139.210 I llama_init_from_model: flash_attn    = 0
0.00.139.210 I llama_init_from_model: freq_base     = 10000.0
0.00.139.210 I llama_init_from_model: freq_scale    = 1
0.00.139.211 I ggml_metal_init: allocating
0.00.139.247 I ggml_metal_init: found device: Apple M4
0.00.139.254 I ggml_metal_init: picking default device: Apple M4
0.00.139.887 I ggml_metal_init: using embedded metal library
0.00.155.315 I ggml_metal_init: GPU name:   Apple M4
0.00.155.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.155.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.155.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.155.318 I ggml_metal_init: simdgroup reduction   = true
0.00.155.318 I ggml_metal_init: simdgroup matrix mul. = true
0.00.155.318 I ggml_metal_init: has residency sets    = true
0.00.155.319 I ggml_metal_init: has bfloat            = true
0.00.155.319 I ggml_metal_init: use bfloat            = true
0.00.155.319 I ggml_metal_init: hasUnifiedMemory      = true
0.00.155.320 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.344.760 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.378.892 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.378.899 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.378.923 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.382.591 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.382.596 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.382.597 I llama_init_from_model: graph nodes  = 967
0.00.382.597 I llama_init_from_model: graph splits = 2
0.00.382.600 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.382.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.382.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.436.697 I main: llama threadpool init, n_threads = 4
0.00.436.740 I 
0.00.436.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.436.779 I 
0.00.436.826 I sampler seed: 1234
0.00.436.832 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.436.861 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.436.863 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.436.863 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.245.410 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.02.245.411 I llama_perf_context_print:        load time =     387.90 ms
0.02.245.411 I llama_perf_context_print: prompt eval time =      44.42 ms /     7 tokens (    6.35 ms per token,   157.59 tokens per second)
0.02.245.412 I llama_perf_context_print:        eval time =    1761.22 ms /    63 runs   (   27.96 ms per token,    35.77 tokens per second)
0.02.245.412 I llama_perf_context_print:       total time =    1809.94 ms /    70 tokens
0.02.245.633 I ggml_metal_free: deallocating

real	0m2.552s
user	0m0.153s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.780 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.326 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.082 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.088 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.090 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.091 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.092 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.099 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.099 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.100 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.103 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.103 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.662 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.663 I llama_model_loader: - type  f32:  194 tensors
0.00.056.664 I llama_model_loader: - type  f16:   98 tensors
0.00.056.665 I print_info: file format = GGUF V3 (latest)
0.00.056.666 I print_info: file type   = all F32 (guessed)
0.00.056.667 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.880 I load: special tokens cache size = 25
0.00.076.858 I load: token to piece cache size = 0.2984 MB
0.00.076.861 I print_info: arch             = gptneox
0.00.076.862 I print_info: vocab_only       = 0
0.00.076.862 I print_info: n_ctx_train      = 2048
0.00.076.862 I print_info: n_embd           = 2048
0.00.076.862 I print_info: n_layer          = 24
0.00.076.865 I print_info: n_head           = 16
0.00.076.866 I print_info: n_head_kv        = 16
0.00.076.866 I print_info: n_rot            = 32
0.00.076.867 I print_info: n_swa            = 0
0.00.076.867 I print_info: n_embd_head_k    = 128
0.00.076.867 I print_info: n_embd_head_v    = 128
0.00.076.868 I print_info: n_gqa            = 1
0.00.076.869 I print_info: n_embd_k_gqa     = 2048
0.00.076.870 I print_info: n_embd_v_gqa     = 2048
0.00.076.871 I print_info: f_norm_eps       = 1.0e-05
0.00.076.871 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.873 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.873 I print_info: f_logit_scale    = 0.0e+00
0.00.076.874 I print_info: n_ff             = 8192
0.00.076.874 I print_info: n_expert         = 0
0.00.076.874 I print_info: n_expert_used    = 0
0.00.076.874 I print_info: causal attn      = 1
0.00.076.875 I print_info: pooling type     = 0
0.00.076.875 I print_info: rope type        = 2
0.00.076.877 I print_info: rope scaling     = linear
0.00.076.877 I print_info: freq_base_train  = 10000.0
0.00.076.878 I print_info: freq_scale_train = 1
0.00.076.878 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.878 I print_info: rope_finetuned   = unknown
0.00.076.878 I print_info: ssm_d_conv       = 0
0.00.076.878 I print_info: ssm_d_inner      = 0
0.00.076.878 I print_info: ssm_d_state      = 0
0.00.076.879 I print_info: ssm_dt_rank      = 0
0.00.076.879 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.879 I print_info: model type       = 1.4B
0.00.076.879 I print_info: model params     = 1.41 B
0.00.076.879 I print_info: general.name     = 1.4B
0.00.076.880 I print_info: vocab type       = BPE
0.00.076.881 I print_info: n_vocab          = 50304
0.00.076.881 I print_info: n_merges         = 50009
0.00.076.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.888 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.888 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.889 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.889 I print_info: LF token         = 187 ''
0.00.076.890 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.890 I print_info: max token length = 1024
0.00.076.891 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.059.458 I load_tensors: offloading 24 repeating layers to GPU
0.01.059.463 I load_tensors: offloading output layer to GPU
0.01.059.463 I load_tensors: offloaded 25/25 layers to GPU
0.01.059.491 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.059.493 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.060.270 I llama_init_from_model: n_seq_max     = 1
0.01.060.271 I llama_init_from_model: n_ctx         = 128
0.01.060.272 I llama_init_from_model: n_ctx_per_seq = 128
0.01.060.272 I llama_init_from_model: n_batch       = 128
0.01.060.272 I llama_init_from_model: n_ubatch      = 128
0.01.060.272 I llama_init_from_model: flash_attn    = 0
0.01.060.273 I llama_init_from_model: freq_base     = 10000.0
0.01.060.273 I llama_init_from_model: freq_scale    = 1
0.01.060.273 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.060.274 I ggml_metal_init: allocating
0.01.060.327 I ggml_metal_init: found device: Apple M4
0.01.060.333 I ggml_metal_init: picking default device: Apple M4
0.01.061.410 I ggml_metal_init: using embedded metal library
0.01.065.006 I ggml_metal_init: GPU name:   Apple M4
0.01.065.008 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.065.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.065.009 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.065.009 I ggml_metal_init: simdgroup reduction   = true
0.01.065.009 I ggml_metal_init: simdgroup matrix mul. = true
0.01.065.010 I ggml_metal_init: has residency sets    = true
0.01.065.010 I ggml_metal_init: has bfloat            = true
0.01.065.010 I ggml_metal_init: use bfloat            = true
0.01.065.010 I ggml_metal_init: hasUnifiedMemory      = true
0.01.065.011 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.076.740 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.078.455 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.078.459 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.078.473 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.080.019 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.080.020 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.080.020 I llama_init_from_model: graph nodes  = 967
0.01.080.021 I llama_init_from_model: graph splits = 2
0.01.080.022 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.080.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.114.177 I 
0.01.114.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.114.234 I perplexity: tokenizing the input ..
0.01.118.776 I perplexity: tokenization took 4.54 ms
0.01.118.795 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.250.402 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.251.672 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.251.694 I llama_perf_context_print:        load time =    1089.85 ms
0.01.251.695 I llama_perf_context_print: prompt eval time =     131.27 ms /   128 tokens (    1.03 ms per token,   975.07 tokens per second)
0.01.251.696 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.251.696 I llama_perf_context_print:       total time =     137.52 ms /   129 tokens
0.01.252.137 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.101s
sys	0m0.278s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.015.312 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.033.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.905 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.042.916 I llama_model_loader: - type  f32:  194 tensors
0.00.042.916 I llama_model_loader: - type q8_0:   98 tensors
0.00.042.917 I print_info: file format = GGUF V3 (latest)
0.00.042.918 I print_info: file type   = Q8_0
0.00.042.919 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.052.164 I load: special tokens cache size = 25
0.00.060.222 I load: token to piece cache size = 0.2984 MB
0.00.060.227 I print_info: arch             = gptneox
0.00.060.227 I print_info: vocab_only       = 0
0.00.060.227 I print_info: n_ctx_train      = 2048
0.00.060.227 I print_info: n_embd           = 2048
0.00.060.228 I print_info: n_layer          = 24
0.00.060.233 I print_info: n_head           = 16
0.00.060.234 I print_info: n_head_kv        = 16
0.00.060.234 I print_info: n_rot            = 32
0.00.060.234 I print_info: n_swa            = 0
0.00.060.236 I print_info: n_embd_head_k    = 128
0.00.060.236 I print_info: n_embd_head_v    = 128
0.00.060.237 I print_info: n_gqa            = 1
0.00.060.238 I print_info: n_embd_k_gqa     = 2048
0.00.060.239 I print_info: n_embd_v_gqa     = 2048
0.00.060.240 I print_info: f_norm_eps       = 1.0e-05
0.00.060.240 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.240 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.243 I print_info: f_logit_scale    = 0.0e+00
0.00.060.244 I print_info: n_ff             = 8192
0.00.060.244 I print_info: n_expert         = 0
0.00.060.244 I print_info: n_expert_used    = 0
0.00.060.244 I print_info: causal attn      = 1
0.00.060.245 I print_info: pooling type     = 0
0.00.060.245 I print_info: rope type        = 2
0.00.060.245 I print_info: rope scaling     = linear
0.00.060.246 I print_info: freq_base_train  = 10000.0
0.00.060.246 I print_info: freq_scale_train = 1
0.00.060.246 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.246 I print_info: rope_finetuned   = unknown
0.00.060.247 I print_info: ssm_d_conv       = 0
0.00.060.247 I print_info: ssm_d_inner      = 0
0.00.060.247 I print_info: ssm_d_state      = 0
0.00.060.248 I print_info: ssm_dt_rank      = 0
0.00.060.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.249 I print_info: model type       = 1.4B
0.00.060.249 I print_info: model params     = 1.41 B
0.00.060.249 I print_info: general.name     = 1.4B
0.00.060.250 I print_info: vocab type       = BPE
0.00.060.250 I print_info: n_vocab          = 50304
0.00.060.250 I print_info: n_merges         = 50009
0.00.060.251 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.251 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.253 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.253 I print_info: LF token         = 187 ''
0.00.060.253 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.253 I print_info: max token length = 1024
0.00.060.254 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.264.388 I load_tensors: offloading 24 repeating layers to GPU
0.01.264.394 I load_tensors: offloading output layer to GPU
0.01.264.395 I load_tensors: offloaded 25/25 layers to GPU
0.01.264.418 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.264.419 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.265.084 I llama_init_from_model: n_seq_max     = 1
0.01.265.085 I llama_init_from_model: n_ctx         = 2048
0.01.265.085 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.265.086 I llama_init_from_model: n_batch       = 2048
0.01.265.086 I llama_init_from_model: n_ubatch      = 512
0.01.265.086 I llama_init_from_model: flash_attn    = 0
0.01.265.087 I llama_init_from_model: freq_base     = 10000.0
0.01.265.087 I llama_init_from_model: freq_scale    = 1
0.01.265.088 I ggml_metal_init: allocating
0.01.265.094 I ggml_metal_init: found device: Apple M4
0.01.265.099 I ggml_metal_init: picking default device: Apple M4
0.01.266.181 I ggml_metal_init: using embedded metal library
0.01.271.028 I ggml_metal_init: GPU name:   Apple M4
0.01.271.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.271.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.271.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.271.033 I ggml_metal_init: simdgroup reduction   = true
0.01.271.033 I ggml_metal_init: simdgroup matrix mul. = true
0.01.271.033 I ggml_metal_init: has residency sets    = true
0.01.271.033 I ggml_metal_init: has bfloat            = true
0.01.271.034 I ggml_metal_init: use bfloat            = true
0.01.271.034 I ggml_metal_init: hasUnifiedMemory      = true
0.01.271.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.284.057 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.315.231 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.315.237 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.315.260 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.320.295 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.320.298 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.320.298 I llama_init_from_model: graph nodes  = 967
0.01.320.298 I llama_init_from_model: graph splits = 2
0.01.320.304 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.320.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.320.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.367.075 I main: llama threadpool init, n_threads = 4
0.01.367.122 I 
0.01.367.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.367.144 I 
0.01.367.276 I sampler seed: 1234
0.01.367.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.367.291 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.367.292 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.367.292 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.460.793 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.02.460.794 I llama_perf_context_print:        load time =    1351.02 ms
0.02.460.797 I llama_perf_context_print: prompt eval time =      49.44 ms /     7 tokens (    7.06 ms per token,   141.60 tokens per second)
0.02.460.797 I llama_perf_context_print:        eval time =    1040.88 ms /    63 runs   (   16.52 ms per token,    60.53 tokens per second)
0.02.460.798 I llama_perf_context_print:       total time =    1094.46 ms /    70 tokens
0.02.461.016 I ggml_metal_free: deallocating

real	0m2.480s
user	0m0.108s
sys	0m0.316s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.544 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.569 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.208 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.213 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.214 I llama_model_loader: - type  f32:  194 tensors
0.00.025.214 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.215 I print_info: file format = GGUF V3 (latest)
0.00.025.215 I print_info: file type   = Q8_0
0.00.025.216 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.523 I load: special tokens cache size = 25
0.00.039.739 I load: token to piece cache size = 0.2984 MB
0.00.039.743 I print_info: arch             = gptneox
0.00.039.744 I print_info: vocab_only       = 0
0.00.039.744 I print_info: n_ctx_train      = 2048
0.00.039.744 I print_info: n_embd           = 2048
0.00.039.744 I print_info: n_layer          = 24
0.00.039.748 I print_info: n_head           = 16
0.00.039.749 I print_info: n_head_kv        = 16
0.00.039.749 I print_info: n_rot            = 32
0.00.039.750 I print_info: n_swa            = 0
0.00.039.750 I print_info: n_embd_head_k    = 128
0.00.039.750 I print_info: n_embd_head_v    = 128
0.00.039.751 I print_info: n_gqa            = 1
0.00.039.751 I print_info: n_embd_k_gqa     = 2048
0.00.039.752 I print_info: n_embd_v_gqa     = 2048
0.00.039.753 I print_info: f_norm_eps       = 1.0e-05
0.00.039.753 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.753 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.756 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.757 I print_info: f_logit_scale    = 0.0e+00
0.00.039.757 I print_info: n_ff             = 8192
0.00.039.757 I print_info: n_expert         = 0
0.00.039.757 I print_info: n_expert_used    = 0
0.00.039.757 I print_info: causal attn      = 1
0.00.039.758 I print_info: pooling type     = 0
0.00.039.758 I print_info: rope type        = 2
0.00.039.758 I print_info: rope scaling     = linear
0.00.039.758 I print_info: freq_base_train  = 10000.0
0.00.039.759 I print_info: freq_scale_train = 1
0.00.039.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.760 I print_info: rope_finetuned   = unknown
0.00.039.760 I print_info: ssm_d_conv       = 0
0.00.039.760 I print_info: ssm_d_inner      = 0
0.00.039.761 I print_info: ssm_d_state      = 0
0.00.039.761 I print_info: ssm_dt_rank      = 0
0.00.039.761 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.761 I print_info: model type       = 1.4B
0.00.039.761 I print_info: model params     = 1.41 B
0.00.039.762 I print_info: general.name     = 1.4B
0.00.039.762 I print_info: vocab type       = BPE
0.00.039.762 I print_info: n_vocab          = 50304
0.00.039.762 I print_info: n_merges         = 50009
0.00.039.762 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: LF token         = 187 ''
0.00.039.764 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: max token length = 1024
0.00.039.765 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.009.462 I load_tensors: offloading 24 repeating layers to GPU
0.01.009.469 I load_tensors: offloading output layer to GPU
0.01.009.469 I load_tensors: offloaded 25/25 layers to GPU
0.01.009.500 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.009.502 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.010.450 I llama_init_from_model: n_seq_max     = 1
0.01.010.452 I llama_init_from_model: n_ctx         = 128
0.01.010.452 I llama_init_from_model: n_ctx_per_seq = 128
0.01.010.452 I llama_init_from_model: n_batch       = 128
0.01.010.452 I llama_init_from_model: n_ubatch      = 128
0.01.010.453 I llama_init_from_model: flash_attn    = 0
0.01.010.453 I llama_init_from_model: freq_base     = 10000.0
0.01.010.454 I llama_init_from_model: freq_scale    = 1
0.01.010.454 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.010.457 I ggml_metal_init: allocating
0.01.010.521 I ggml_metal_init: found device: Apple M4
0.01.010.529 I ggml_metal_init: picking default device: Apple M4
0.01.011.775 I ggml_metal_init: using embedded metal library
0.01.016.235 I ggml_metal_init: GPU name:   Apple M4
0.01.016.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.016.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.016.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.016.240 I ggml_metal_init: simdgroup reduction   = true
0.01.016.240 I ggml_metal_init: simdgroup matrix mul. = true
0.01.016.241 I ggml_metal_init: has residency sets    = true
0.01.016.241 I ggml_metal_init: has bfloat            = true
0.01.016.241 I ggml_metal_init: use bfloat            = true
0.01.016.242 I ggml_metal_init: hasUnifiedMemory      = true
0.01.016.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.029.285 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.031.093 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.031.097 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.031.114 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.032.824 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.032.825 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.032.826 I llama_init_from_model: graph nodes  = 967
0.01.032.826 I llama_init_from_model: graph splits = 2
0.01.032.827 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.032.827 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.058.108 I 
0.01.058.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.058.161 I perplexity: tokenizing the input ..
0.01.063.091 I perplexity: tokenization took 4.928 ms
0.01.063.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.197.576 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.198.825 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.198.840 I llama_perf_context_print:        load time =    1048.56 ms
0.01.198.841 I llama_perf_context_print: prompt eval time =     134.25 ms /   128 tokens (    1.05 ms per token,   953.42 tokens per second)
0.01.198.842 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.198.842 I llama_perf_context_print:       total time =     140.73 ms /   129 tokens
0.01.199.238 I ggml_metal_free: deallocating

real	0m1.213s
user	0m0.071s
sys	0m0.249s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.013.289 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.214 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.218 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.219 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.349 I llama_model_loader: - type  f32:  194 tensors
0.00.034.349 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.349 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.350 I print_info: file format = GGUF V3 (latest)
0.00.034.351 I print_info: file type   = Q4_0
0.00.034.351 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.782 I load: special tokens cache size = 25
0.00.053.370 I load: token to piece cache size = 0.2984 MB
0.00.053.373 I print_info: arch             = gptneox
0.00.053.374 I print_info: vocab_only       = 0
0.00.053.374 I print_info: n_ctx_train      = 2048
0.00.053.374 I print_info: n_embd           = 2048
0.00.053.375 I print_info: n_layer          = 24
0.00.053.379 I print_info: n_head           = 16
0.00.053.380 I print_info: n_head_kv        = 16
0.00.053.381 I print_info: n_rot            = 32
0.00.053.381 I print_info: n_swa            = 0
0.00.053.384 I print_info: n_embd_head_k    = 128
0.00.053.384 I print_info: n_embd_head_v    = 128
0.00.053.385 I print_info: n_gqa            = 1
0.00.053.386 I print_info: n_embd_k_gqa     = 2048
0.00.053.387 I print_info: n_embd_v_gqa     = 2048
0.00.053.387 I print_info: f_norm_eps       = 1.0e-05
0.00.053.388 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.388 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.388 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.388 I print_info: f_logit_scale    = 0.0e+00
0.00.053.389 I print_info: n_ff             = 8192
0.00.053.389 I print_info: n_expert         = 0
0.00.053.390 I print_info: n_expert_used    = 0
0.00.053.390 I print_info: causal attn      = 1
0.00.053.390 I print_info: pooling type     = 0
0.00.053.390 I print_info: rope type        = 2
0.00.053.396 I print_info: rope scaling     = linear
0.00.053.396 I print_info: freq_base_train  = 10000.0
0.00.053.397 I print_info: freq_scale_train = 1
0.00.053.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.399 I print_info: rope_finetuned   = unknown
0.00.053.399 I print_info: ssm_d_conv       = 0
0.00.053.399 I print_info: ssm_d_inner      = 0
0.00.053.399 I print_info: ssm_d_state      = 0
0.00.053.399 I print_info: ssm_dt_rank      = 0
0.00.053.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.400 I print_info: model type       = 1.4B
0.00.053.400 I print_info: model params     = 1.41 B
0.00.053.401 I print_info: general.name     = 1.4B
0.00.053.401 I print_info: vocab type       = BPE
0.00.053.401 I print_info: n_vocab          = 50304
0.00.053.402 I print_info: n_merges         = 50009
0.00.053.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.403 I print_info: LF token         = 187 ''
0.00.053.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.404 I print_info: max token length = 1024
0.00.053.404 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.421.211 I load_tensors: offloading 24 repeating layers to GPU
0.01.421.223 I load_tensors: offloading output layer to GPU
0.01.421.223 I load_tensors: offloaded 25/25 layers to GPU
0.01.421.251 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.01.421.252 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.01.422.487 I llama_init_from_model: n_seq_max     = 1
0.01.422.489 I llama_init_from_model: n_ctx         = 2048
0.01.422.490 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.422.490 I llama_init_from_model: n_batch       = 2048
0.01.422.491 I llama_init_from_model: n_ubatch      = 512
0.01.422.491 I llama_init_from_model: flash_attn    = 0
0.01.422.493 I llama_init_from_model: freq_base     = 10000.0
0.01.422.493 I llama_init_from_model: freq_scale    = 1
0.01.422.497 I ggml_metal_init: allocating
0.01.422.559 I ggml_metal_init: found device: Apple M4
0.01.422.570 I ggml_metal_init: picking default device: Apple M4
0.01.424.228 I ggml_metal_init: using embedded metal library
0.01.430.981 I ggml_metal_init: GPU name:   Apple M4
0.01.430.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.430.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.430.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.430.990 I ggml_metal_init: simdgroup reduction   = true
0.01.430.990 I ggml_metal_init: simdgroup matrix mul. = true
0.01.430.990 I ggml_metal_init: has residency sets    = true
0.01.430.990 I ggml_metal_init: has bfloat            = true
0.01.430.991 I ggml_metal_init: use bfloat            = true
0.01.430.992 I ggml_metal_init: hasUnifiedMemory      = true
0.01.430.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.448.566 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.502.166 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.502.173 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.502.197 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.506.905 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.506.907 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.506.908 I llama_init_from_model: graph nodes  = 967
0.01.506.908 I llama_init_from_model: graph splits = 2
0.01.506.914 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.507.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.507.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.555.609 I main: llama threadpool init, n_threads = 4
0.01.555.664 I 
0.01.555.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.555.684 I 
0.01.555.820 I sampler seed: 1234
0.01.555.825 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.555.859 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.555.861 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.555.861 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.02.246.749 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51824.82 tokens per second)
0.02.246.750 I llama_perf_context_print:        load time =    1541.62 ms
0.02.246.750 I llama_perf_context_print: prompt eval time =      50.32 ms /     7 tokens (    7.19 ms per token,   139.10 tokens per second)
0.02.246.751 I llama_perf_context_print:        eval time =     637.56 ms /    63 runs   (   10.12 ms per token,    98.81 tokens per second)
0.02.246.751 I llama_perf_context_print:       total time =     691.84 ms /    70 tokens
0.02.246.979 I ggml_metal_free: deallocating

real	0m2.278s
user	0m0.117s
sys	0m0.239s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.959 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.965 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.965 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.966 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.966 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.967 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.967 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.967 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.968 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.969 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.971 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.971 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.972 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.741 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.597 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.600 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.600 I llama_model_loader: - type  f32:  194 tensors
0.00.025.601 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.601 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.602 I print_info: file format = GGUF V3 (latest)
0.00.025.602 I print_info: file type   = Q4_0
0.00.025.603 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.456 I load: special tokens cache size = 25
0.00.039.791 I load: token to piece cache size = 0.2984 MB
0.00.039.796 I print_info: arch             = gptneox
0.00.039.796 I print_info: vocab_only       = 0
0.00.039.796 I print_info: n_ctx_train      = 2048
0.00.039.797 I print_info: n_embd           = 2048
0.00.039.797 I print_info: n_layer          = 24
0.00.039.801 I print_info: n_head           = 16
0.00.039.802 I print_info: n_head_kv        = 16
0.00.039.802 I print_info: n_rot            = 32
0.00.039.802 I print_info: n_swa            = 0
0.00.039.803 I print_info: n_embd_head_k    = 128
0.00.039.803 I print_info: n_embd_head_v    = 128
0.00.039.806 I print_info: n_gqa            = 1
0.00.039.807 I print_info: n_embd_k_gqa     = 2048
0.00.039.807 I print_info: n_embd_v_gqa     = 2048
0.00.039.808 I print_info: f_norm_eps       = 1.0e-05
0.00.039.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.809 I print_info: f_logit_scale    = 0.0e+00
0.00.039.809 I print_info: n_ff             = 8192
0.00.039.811 I print_info: n_expert         = 0
0.00.039.811 I print_info: n_expert_used    = 0
0.00.039.811 I print_info: causal attn      = 1
0.00.039.811 I print_info: pooling type     = 0
0.00.039.811 I print_info: rope type        = 2
0.00.039.811 I print_info: rope scaling     = linear
0.00.039.812 I print_info: freq_base_train  = 10000.0
0.00.039.812 I print_info: freq_scale_train = 1
0.00.039.812 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.812 I print_info: rope_finetuned   = unknown
0.00.039.813 I print_info: ssm_d_conv       = 0
0.00.039.813 I print_info: ssm_d_inner      = 0
0.00.039.813 I print_info: ssm_d_state      = 0
0.00.039.813 I print_info: ssm_dt_rank      = 0
0.00.039.813 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.813 I print_info: model type       = 1.4B
0.00.039.814 I print_info: model params     = 1.41 B
0.00.039.814 I print_info: general.name     = 1.4B
0.00.039.814 I print_info: vocab type       = BPE
0.00.039.815 I print_info: n_vocab          = 50304
0.00.039.815 I print_info: n_merges         = 50009
0.00.039.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.817 I print_info: LF token         = 187 ''
0.00.039.817 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.817 I print_info: max token length = 1024
0.00.039.817 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.475 I load_tensors: offloading 24 repeating layers to GPU
0.00.670.480 I load_tensors: offloading output layer to GPU
0.00.670.481 I load_tensors: offloaded 25/25 layers to GPU
0.00.670.505 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.670.506 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.671.638 I llama_init_from_model: n_seq_max     = 1
0.00.671.640 I llama_init_from_model: n_ctx         = 128
0.00.671.641 I llama_init_from_model: n_ctx_per_seq = 128
0.00.671.641 I llama_init_from_model: n_batch       = 128
0.00.671.642 I llama_init_from_model: n_ubatch      = 128
0.00.671.642 I llama_init_from_model: flash_attn    = 0
0.00.671.643 I llama_init_from_model: freq_base     = 10000.0
0.00.671.643 I llama_init_from_model: freq_scale    = 1
0.00.671.644 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.671.645 I ggml_metal_init: allocating
0.00.671.688 I ggml_metal_init: found device: Apple M4
0.00.671.700 I ggml_metal_init: picking default device: Apple M4
0.00.673.076 I ggml_metal_init: using embedded metal library
0.00.678.908 I ggml_metal_init: GPU name:   Apple M4
0.00.678.913 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.914 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.915 I ggml_metal_init: simdgroup reduction   = true
0.00.678.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.915 I ggml_metal_init: has residency sets    = true
0.00.678.915 I ggml_metal_init: has bfloat            = true
0.00.678.916 I ggml_metal_init: use bfloat            = true
0.00.678.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.016 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.400 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.698.404 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.698.431 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.520 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.701.523 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.701.523 I llama_init_from_model: graph nodes  = 967
0.00.701.523 I llama_init_from_model: graph splits = 2
0.00.701.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.701.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.166 I 
0.00.728.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.265 I perplexity: tokenizing the input ..
0.00.735.388 I perplexity: tokenization took 7.12 ms
0.00.735.406 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.743 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.868.000 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.868.011 I llama_perf_context_print:        load time =     718.42 ms
0.00.868.012 I llama_perf_context_print: prompt eval time =     130.43 ms /   128 tokens (    1.02 ms per token,   981.37 tokens per second)
0.00.868.014 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.014 I llama_perf_context_print:       total time =     139.85 ms /   129 tokens
0.00.868.441 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.078s
sys	0m0.171s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.016.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.084 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.098 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.099 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.108 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.249 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.251 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.252 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.253 I llama_model_loader: - type  f32:  194 tensors
0.00.037.253 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.253 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.254 I print_info: file format = GGUF V3 (latest)
0.00.037.254 I print_info: file type   = Q4_1
0.00.037.255 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.660 I load: special tokens cache size = 25
0.00.058.143 I load: token to piece cache size = 0.2984 MB
0.00.058.147 I print_info: arch             = gptneox
0.00.058.147 I print_info: vocab_only       = 0
0.00.058.148 I print_info: n_ctx_train      = 2048
0.00.058.148 I print_info: n_embd           = 2048
0.00.058.148 I print_info: n_layer          = 24
0.00.058.151 I print_info: n_head           = 16
0.00.058.152 I print_info: n_head_kv        = 16
0.00.058.153 I print_info: n_rot            = 32
0.00.058.153 I print_info: n_swa            = 0
0.00.058.154 I print_info: n_embd_head_k    = 128
0.00.058.155 I print_info: n_embd_head_v    = 128
0.00.058.158 I print_info: n_gqa            = 1
0.00.058.159 I print_info: n_embd_k_gqa     = 2048
0.00.058.160 I print_info: n_embd_v_gqa     = 2048
0.00.058.160 I print_info: f_norm_eps       = 1.0e-05
0.00.058.161 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.161 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.161 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.162 I print_info: f_logit_scale    = 0.0e+00
0.00.058.162 I print_info: n_ff             = 8192
0.00.058.163 I print_info: n_expert         = 0
0.00.058.163 I print_info: n_expert_used    = 0
0.00.058.163 I print_info: causal attn      = 1
0.00.058.163 I print_info: pooling type     = 0
0.00.058.163 I print_info: rope type        = 2
0.00.058.164 I print_info: rope scaling     = linear
0.00.058.164 I print_info: freq_base_train  = 10000.0
0.00.058.165 I print_info: freq_scale_train = 1
0.00.058.165 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.165 I print_info: rope_finetuned   = unknown
0.00.058.165 I print_info: ssm_d_conv       = 0
0.00.058.165 I print_info: ssm_d_inner      = 0
0.00.058.167 I print_info: ssm_d_state      = 0
0.00.058.167 I print_info: ssm_dt_rank      = 0
0.00.058.167 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.168 I print_info: model type       = 1.4B
0.00.058.168 I print_info: model params     = 1.41 B
0.00.058.168 I print_info: general.name     = 1.4B
0.00.058.169 I print_info: vocab type       = BPE
0.00.058.169 I print_info: n_vocab          = 50304
0.00.058.169 I print_info: n_merges         = 50009
0.00.058.170 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.170 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.170 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.172 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.172 I print_info: LF token         = 187 ''
0.00.058.172 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.173 I print_info: max token length = 1024
0.00.058.173 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.728.208 I load_tensors: offloading 24 repeating layers to GPU
0.00.728.212 I load_tensors: offloading output layer to GPU
0.00.728.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.728.237 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.728.239 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.729.294 I llama_init_from_model: n_seq_max     = 1
0.00.729.296 I llama_init_from_model: n_ctx         = 2048
0.00.729.297 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.729.297 I llama_init_from_model: n_batch       = 2048
0.00.729.298 I llama_init_from_model: n_ubatch      = 512
0.00.729.298 I llama_init_from_model: flash_attn    = 0
0.00.729.299 I llama_init_from_model: freq_base     = 10000.0
0.00.729.300 I llama_init_from_model: freq_scale    = 1
0.00.729.301 I ggml_metal_init: allocating
0.00.729.310 I ggml_metal_init: found device: Apple M4
0.00.729.318 I ggml_metal_init: picking default device: Apple M4
0.00.730.801 I ggml_metal_init: using embedded metal library
0.00.737.040 I ggml_metal_init: GPU name:   Apple M4
0.00.737.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.737.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.737.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.737.045 I ggml_metal_init: simdgroup reduction   = true
0.00.737.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.737.046 I ggml_metal_init: has residency sets    = true
0.00.737.046 I ggml_metal_init: has bfloat            = true
0.00.737.046 I ggml_metal_init: use bfloat            = true
0.00.737.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.737.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.753.672 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.807.434 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.807.441 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.807.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.812.358 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.812.361 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.812.361 I llama_init_from_model: graph nodes  = 967
0.00.812.361 I llama_init_from_model: graph splits = 2
0.00.812.366 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.812.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.812.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.763 I main: llama threadpool init, n_threads = 4
0.00.860.815 I 
0.00.860.838 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.838 I 
0.00.860.959 I sampler seed: 1234
0.00.860.964 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.860.973 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.860.975 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.860.975 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.590.672 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.590.673 I llama_perf_context_print:        load time =     843.88 ms
0.01.590.674 I llama_perf_context_print: prompt eval time =      46.13 ms /     7 tokens (    6.59 ms per token,   151.73 tokens per second)
0.01.590.674 I llama_perf_context_print:        eval time =     680.75 ms /    63 runs   (   10.81 ms per token,    92.55 tokens per second)
0.01.590.674 I llama_perf_context_print:       total time =     730.61 ms /    70 tokens
0.01.590.916 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.119s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.766 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.471 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.478 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.484 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.485 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.485 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.021 I llama_model_loader: - type  f32:  194 tensors
0.00.025.021 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.021 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.022 I print_info: file format = GGUF V3 (latest)
0.00.025.022 I print_info: file type   = Q4_1
0.00.025.024 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.876 I load: special tokens cache size = 25
0.00.039.024 I load: token to piece cache size = 0.2984 MB
0.00.039.028 I print_info: arch             = gptneox
0.00.039.029 I print_info: vocab_only       = 0
0.00.039.029 I print_info: n_ctx_train      = 2048
0.00.039.029 I print_info: n_embd           = 2048
0.00.039.029 I print_info: n_layer          = 24
0.00.039.035 I print_info: n_head           = 16
0.00.039.036 I print_info: n_head_kv        = 16
0.00.039.036 I print_info: n_rot            = 32
0.00.039.037 I print_info: n_swa            = 0
0.00.039.037 I print_info: n_embd_head_k    = 128
0.00.039.037 I print_info: n_embd_head_v    = 128
0.00.039.038 I print_info: n_gqa            = 1
0.00.039.038 I print_info: n_embd_k_gqa     = 2048
0.00.039.039 I print_info: n_embd_v_gqa     = 2048
0.00.039.040 I print_info: f_norm_eps       = 1.0e-05
0.00.039.041 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.041 I print_info: f_logit_scale    = 0.0e+00
0.00.039.042 I print_info: n_ff             = 8192
0.00.039.042 I print_info: n_expert         = 0
0.00.039.042 I print_info: n_expert_used    = 0
0.00.039.043 I print_info: causal attn      = 1
0.00.039.045 I print_info: pooling type     = 0
0.00.039.045 I print_info: rope type        = 2
0.00.039.045 I print_info: rope scaling     = linear
0.00.039.046 I print_info: freq_base_train  = 10000.0
0.00.039.046 I print_info: freq_scale_train = 1
0.00.039.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.046 I print_info: rope_finetuned   = unknown
0.00.039.047 I print_info: ssm_d_conv       = 0
0.00.039.047 I print_info: ssm_d_inner      = 0
0.00.039.047 I print_info: ssm_d_state      = 0
0.00.039.047 I print_info: ssm_dt_rank      = 0
0.00.039.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.049 I print_info: model type       = 1.4B
0.00.039.049 I print_info: model params     = 1.41 B
0.00.039.049 I print_info: general.name     = 1.4B
0.00.039.050 I print_info: vocab type       = BPE
0.00.039.051 I print_info: n_vocab          = 50304
0.00.039.051 I print_info: n_merges         = 50009
0.00.039.051 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.051 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: LF token         = 187 ''
0.00.039.053 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: max token length = 1024
0.00.039.053 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.698.613 I load_tensors: offloading 24 repeating layers to GPU
0.00.698.621 I load_tensors: offloading output layer to GPU
0.00.698.621 I load_tensors: offloaded 25/25 layers to GPU
0.00.698.651 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.698.654 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.699.979 I llama_init_from_model: n_seq_max     = 1
0.00.699.981 I llama_init_from_model: n_ctx         = 128
0.00.699.982 I llama_init_from_model: n_ctx_per_seq = 128
0.00.699.982 I llama_init_from_model: n_batch       = 128
0.00.699.983 I llama_init_from_model: n_ubatch      = 128
0.00.699.983 I llama_init_from_model: flash_attn    = 0
0.00.699.985 I llama_init_from_model: freq_base     = 10000.0
0.00.699.986 I llama_init_from_model: freq_scale    = 1
0.00.699.986 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.699.988 I ggml_metal_init: allocating
0.00.700.054 I ggml_metal_init: found device: Apple M4
0.00.700.066 I ggml_metal_init: picking default device: Apple M4
0.00.701.655 I ggml_metal_init: using embedded metal library
0.00.707.757 I ggml_metal_init: GPU name:   Apple M4
0.00.707.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.763 I ggml_metal_init: simdgroup reduction   = true
0.00.707.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.764 I ggml_metal_init: has residency sets    = true
0.00.707.764 I ggml_metal_init: has bfloat            = true
0.00.707.764 I ggml_metal_init: use bfloat            = true
0.00.707.766 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.008 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.728.289 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.728.294 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.728.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.430 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.731.432 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.731.432 I llama_init_from_model: graph nodes  = 967
0.00.731.433 I llama_init_from_model: graph splits = 2
0.00.731.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.731.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.752 I 
0.00.758.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.841 I perplexity: tokenizing the input ..
0.00.764.341 I perplexity: tokenization took 5.498 ms
0.00.764.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.897.321 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.898.575 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.898.592 I llama_perf_context_print:        load time =     748.98 ms
0.00.898.593 I llama_perf_context_print: prompt eval time =     132.74 ms /   128 tokens (    1.04 ms per token,   964.26 tokens per second)
0.00.898.594 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.898.594 I llama_perf_context_print:       total time =     139.84 ms /   129 tokens
0.00.898.987 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.077s
sys	0m0.170s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.237 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.144 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.149 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.157 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.060 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.899 I llama_model_loader: - type  f32:  194 tensors
0.00.027.899 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.899 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.900 I print_info: file format = GGUF V3 (latest)
0.00.027.900 I print_info: file type   = Q5_0
0.00.027.901 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.862 I load: special tokens cache size = 25
0.00.042.131 I load: token to piece cache size = 0.2984 MB
0.00.042.134 I print_info: arch             = gptneox
0.00.042.135 I print_info: vocab_only       = 0
0.00.042.135 I print_info: n_ctx_train      = 2048
0.00.042.135 I print_info: n_embd           = 2048
0.00.042.135 I print_info: n_layer          = 24
0.00.042.138 I print_info: n_head           = 16
0.00.042.139 I print_info: n_head_kv        = 16
0.00.042.139 I print_info: n_rot            = 32
0.00.042.139 I print_info: n_swa            = 0
0.00.042.139 I print_info: n_embd_head_k    = 128
0.00.042.139 I print_info: n_embd_head_v    = 128
0.00.042.140 I print_info: n_gqa            = 1
0.00.042.141 I print_info: n_embd_k_gqa     = 2048
0.00.042.143 I print_info: n_embd_v_gqa     = 2048
0.00.042.144 I print_info: f_norm_eps       = 1.0e-05
0.00.042.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.144 I print_info: f_logit_scale    = 0.0e+00
0.00.042.149 I print_info: n_ff             = 8192
0.00.042.150 I print_info: n_expert         = 0
0.00.042.150 I print_info: n_expert_used    = 0
0.00.042.156 I print_info: causal attn      = 1
0.00.042.156 I print_info: pooling type     = 0
0.00.042.158 I print_info: rope type        = 2
0.00.042.158 I print_info: rope scaling     = linear
0.00.042.159 I print_info: freq_base_train  = 10000.0
0.00.042.159 I print_info: freq_scale_train = 1
0.00.042.159 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.160 I print_info: rope_finetuned   = unknown
0.00.042.160 I print_info: ssm_d_conv       = 0
0.00.042.161 I print_info: ssm_d_inner      = 0
0.00.042.161 I print_info: ssm_d_state      = 0
0.00.042.161 I print_info: ssm_dt_rank      = 0
0.00.042.161 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.161 I print_info: model type       = 1.4B
0.00.042.162 I print_info: model params     = 1.41 B
0.00.042.162 I print_info: general.name     = 1.4B
0.00.042.162 I print_info: vocab type       = BPE
0.00.042.163 I print_info: n_vocab          = 50304
0.00.042.163 I print_info: n_merges         = 50009
0.00.042.163 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.163 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.163 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.163 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.164 I print_info: LF token         = 187 ''
0.00.042.164 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.164 I print_info: max token length = 1024
0.00.042.164 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.748.210 I load_tensors: offloading 24 repeating layers to GPU
0.00.748.215 I load_tensors: offloading output layer to GPU
0.00.748.216 I load_tensors: offloaded 25/25 layers to GPU
0.00.748.237 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.748.239 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.749.180 I llama_init_from_model: n_seq_max     = 1
0.00.749.181 I llama_init_from_model: n_ctx         = 2048
0.00.749.182 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.749.182 I llama_init_from_model: n_batch       = 2048
0.00.749.183 I llama_init_from_model: n_ubatch      = 512
0.00.749.183 I llama_init_from_model: flash_attn    = 0
0.00.749.184 I llama_init_from_model: freq_base     = 10000.0
0.00.749.185 I llama_init_from_model: freq_scale    = 1
0.00.749.186 I ggml_metal_init: allocating
0.00.749.198 I ggml_metal_init: found device: Apple M4
0.00.749.205 I ggml_metal_init: picking default device: Apple M4
0.00.750.531 I ggml_metal_init: using embedded metal library
0.00.756.315 I ggml_metal_init: GPU name:   Apple M4
0.00.756.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.756.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.756.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.756.320 I ggml_metal_init: simdgroup reduction   = true
0.00.756.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.756.320 I ggml_metal_init: has residency sets    = true
0.00.756.321 I ggml_metal_init: has bfloat            = true
0.00.756.321 I ggml_metal_init: use bfloat            = true
0.00.756.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.756.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.772.249 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.828.735 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.828.744 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.828.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.834.169 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.834.171 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.834.172 I llama_init_from_model: graph nodes  = 967
0.00.834.172 I llama_init_from_model: graph splits = 2
0.00.834.177 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.834.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.834.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.880.061 I main: llama threadpool init, n_threads = 4
0.00.880.105 I 
0.00.880.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.880.131 I 
0.00.880.253 I sampler seed: 1234
0.00.880.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.880.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.880.268 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.880.269 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.659.909 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.659.910 I llama_perf_context_print:        load time =     870.10 ms
0.01.659.911 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.51 tokens per second)
0.01.659.912 I llama_perf_context_print:        eval time =     733.87 ms /    63 runs   (   11.65 ms per token,    85.85 tokens per second)
0.01.659.912 I llama_perf_context_print:       total time =     780.57 ms /    70 tokens
0.01.660.128 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.107s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.576 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.567 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.567 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.568 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.569 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.569 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.570 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.570 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.570 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.571 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.573 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.573 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.574 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.272 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.984 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.987 I llama_model_loader: - type  f32:  194 tensors
0.00.025.988 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.988 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.988 I print_info: file format = GGUF V3 (latest)
0.00.025.989 I print_info: file type   = Q5_0
0.00.025.990 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.806 I load: special tokens cache size = 25
0.00.039.904 I load: token to piece cache size = 0.2984 MB
0.00.039.908 I print_info: arch             = gptneox
0.00.039.908 I print_info: vocab_only       = 0
0.00.039.908 I print_info: n_ctx_train      = 2048
0.00.039.909 I print_info: n_embd           = 2048
0.00.039.909 I print_info: n_layer          = 24
0.00.039.913 I print_info: n_head           = 16
0.00.039.913 I print_info: n_head_kv        = 16
0.00.039.914 I print_info: n_rot            = 32
0.00.039.914 I print_info: n_swa            = 0
0.00.039.914 I print_info: n_embd_head_k    = 128
0.00.039.914 I print_info: n_embd_head_v    = 128
0.00.039.915 I print_info: n_gqa            = 1
0.00.039.916 I print_info: n_embd_k_gqa     = 2048
0.00.039.916 I print_info: n_embd_v_gqa     = 2048
0.00.039.917 I print_info: f_norm_eps       = 1.0e-05
0.00.039.917 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.918 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.918 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.918 I print_info: f_logit_scale    = 0.0e+00
0.00.039.919 I print_info: n_ff             = 8192
0.00.039.919 I print_info: n_expert         = 0
0.00.039.919 I print_info: n_expert_used    = 0
0.00.039.919 I print_info: causal attn      = 1
0.00.039.919 I print_info: pooling type     = 0
0.00.039.919 I print_info: rope type        = 2
0.00.039.919 I print_info: rope scaling     = linear
0.00.039.920 I print_info: freq_base_train  = 10000.0
0.00.039.923 I print_info: freq_scale_train = 1
0.00.039.923 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.923 I print_info: rope_finetuned   = unknown
0.00.039.923 I print_info: ssm_d_conv       = 0
0.00.039.923 I print_info: ssm_d_inner      = 0
0.00.039.923 I print_info: ssm_d_state      = 0
0.00.039.923 I print_info: ssm_dt_rank      = 0
0.00.039.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.925 I print_info: model type       = 1.4B
0.00.039.925 I print_info: model params     = 1.41 B
0.00.039.925 I print_info: general.name     = 1.4B
0.00.039.926 I print_info: vocab type       = BPE
0.00.039.926 I print_info: n_vocab          = 50304
0.00.039.926 I print_info: n_merges         = 50009
0.00.039.926 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.927 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: LF token         = 187 ''
0.00.039.928 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: max token length = 1024
0.00.039.929 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.748.978 I load_tensors: offloading 24 repeating layers to GPU
0.00.748.981 I load_tensors: offloading output layer to GPU
0.00.748.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.749.001 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.749.006 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.750.006 I llama_init_from_model: n_seq_max     = 1
0.00.750.008 I llama_init_from_model: n_ctx         = 128
0.00.750.008 I llama_init_from_model: n_ctx_per_seq = 128
0.00.750.009 I llama_init_from_model: n_batch       = 128
0.00.750.009 I llama_init_from_model: n_ubatch      = 128
0.00.750.009 I llama_init_from_model: flash_attn    = 0
0.00.750.010 I llama_init_from_model: freq_base     = 10000.0
0.00.750.011 I llama_init_from_model: freq_scale    = 1
0.00.750.011 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.750.013 I ggml_metal_init: allocating
0.00.750.033 I ggml_metal_init: found device: Apple M4
0.00.750.041 I ggml_metal_init: picking default device: Apple M4
0.00.751.363 I ggml_metal_init: using embedded metal library
0.00.756.846 I ggml_metal_init: GPU name:   Apple M4
0.00.756.849 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.756.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.756.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.756.854 I ggml_metal_init: simdgroup reduction   = true
0.00.756.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.756.855 I ggml_metal_init: has residency sets    = true
0.00.756.855 I ggml_metal_init: has bfloat            = true
0.00.756.855 I ggml_metal_init: use bfloat            = true
0.00.756.856 I ggml_metal_init: hasUnifiedMemory      = true
0.00.756.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.772.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.775.594 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.775.598 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.775.625 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.778.614 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.778.616 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.778.617 I llama_init_from_model: graph nodes  = 967
0.00.778.617 I llama_init_from_model: graph splits = 2
0.00.778.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.778.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.301 I 
0.00.804.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.398 I perplexity: tokenizing the input ..
0.00.810.347 I perplexity: tokenization took 5.947 ms
0.00.810.358 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.944.776 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.946.048 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.946.076 I llama_perf_context_print:        load time =     793.72 ms
0.00.946.076 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.83 tokens per second)
0.00.946.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.946.077 I llama_perf_context_print:       total time =     141.78 ms /   129 tokens
0.00.946.480 I ggml_metal_free: deallocating

real	0m0.961s
user	0m0.074s
sys	0m0.198s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.980 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.983 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.985 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.986 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.594 I llama_model_loader: - type  f32:  194 tensors
0.00.025.594 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.595 I print_info: file format = GGUF V3 (latest)
0.00.025.596 I print_info: file type   = Q5_1
0.00.025.596 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.614 I load: special tokens cache size = 25
0.00.039.650 I load: token to piece cache size = 0.2984 MB
0.00.039.654 I print_info: arch             = gptneox
0.00.039.654 I print_info: vocab_only       = 0
0.00.039.654 I print_info: n_ctx_train      = 2048
0.00.039.654 I print_info: n_embd           = 2048
0.00.039.654 I print_info: n_layer          = 24
0.00.039.657 I print_info: n_head           = 16
0.00.039.658 I print_info: n_head_kv        = 16
0.00.039.658 I print_info: n_rot            = 32
0.00.039.659 I print_info: n_swa            = 0
0.00.039.659 I print_info: n_embd_head_k    = 128
0.00.039.659 I print_info: n_embd_head_v    = 128
0.00.039.660 I print_info: n_gqa            = 1
0.00.039.660 I print_info: n_embd_k_gqa     = 2048
0.00.039.661 I print_info: n_embd_v_gqa     = 2048
0.00.039.662 I print_info: f_norm_eps       = 1.0e-05
0.00.039.662 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.662 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.662 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.662 I print_info: f_logit_scale    = 0.0e+00
0.00.039.664 I print_info: n_ff             = 8192
0.00.039.664 I print_info: n_expert         = 0
0.00.039.665 I print_info: n_expert_used    = 0
0.00.039.665 I print_info: causal attn      = 1
0.00.039.665 I print_info: pooling type     = 0
0.00.039.665 I print_info: rope type        = 2
0.00.039.667 I print_info: rope scaling     = linear
0.00.039.667 I print_info: freq_base_train  = 10000.0
0.00.039.668 I print_info: freq_scale_train = 1
0.00.039.668 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.668 I print_info: rope_finetuned   = unknown
0.00.039.668 I print_info: ssm_d_conv       = 0
0.00.039.668 I print_info: ssm_d_inner      = 0
0.00.039.668 I print_info: ssm_d_state      = 0
0.00.039.668 I print_info: ssm_dt_rank      = 0
0.00.039.669 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.670 I print_info: model type       = 1.4B
0.00.039.671 I print_info: model params     = 1.41 B
0.00.039.671 I print_info: general.name     = 1.4B
0.00.039.671 I print_info: vocab type       = BPE
0.00.039.672 I print_info: n_vocab          = 50304
0.00.039.672 I print_info: n_merges         = 50009
0.00.039.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.672 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: LF token         = 187 ''
0.00.039.677 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.678 I print_info: max token length = 1024
0.00.039.678 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.693.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.102 I load_tensors: offloading output layer to GPU
0.00.693.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.123 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.693.128 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.694.300 I llama_init_from_model: n_seq_max     = 1
0.00.694.302 I llama_init_from_model: n_ctx         = 2048
0.00.694.302 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.694.303 I llama_init_from_model: n_batch       = 2048
0.00.694.303 I llama_init_from_model: n_ubatch      = 512
0.00.694.304 I llama_init_from_model: flash_attn    = 0
0.00.694.305 I llama_init_from_model: freq_base     = 10000.0
0.00.694.305 I llama_init_from_model: freq_scale    = 1
0.00.694.306 I ggml_metal_init: allocating
0.00.694.328 I ggml_metal_init: found device: Apple M4
0.00.694.340 I ggml_metal_init: picking default device: Apple M4
0.00.695.700 I ggml_metal_init: using embedded metal library
0.00.701.094 I ggml_metal_init: GPU name:   Apple M4
0.00.701.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.099 I ggml_metal_init: simdgroup reduction   = true
0.00.701.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.100 I ggml_metal_init: has residency sets    = true
0.00.701.100 I ggml_metal_init: has bfloat            = true
0.00.701.100 I ggml_metal_init: use bfloat            = true
0.00.701.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.716.776 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.772.779 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.772.787 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.772.810 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.778.369 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.778.372 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.778.372 I llama_init_from_model: graph nodes  = 967
0.00.778.373 I llama_init_from_model: graph splits = 2
0.00.778.378 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.778.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.778.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.685 I main: llama threadpool init, n_threads = 4
0.00.829.731 I 
0.00.829.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.759 I 
0.00.829.875 I sampler seed: 1234
0.00.829.880 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.889 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.889 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.673.197 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.673.197 I llama_perf_context_print:        load time =     819.31 ms
0.01.673.198 I llama_perf_context_print: prompt eval time =      52.54 ms /     7 tokens (    7.51 ms per token,   133.24 tokens per second)
0.01.673.199 I llama_perf_context_print:        eval time =     787.69 ms /    63 runs   (   12.50 ms per token,    79.98 tokens per second)
0.01.673.199 I llama_perf_context_print:       total time =     844.26 ms /    70 tokens
0.01.673.433 I ggml_metal_free: deallocating

real	0m1.691s
user	0m0.106s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.572 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.316 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.752 I llama_model_loader: - type  f32:  194 tensors
0.00.023.752 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.753 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.754 I print_info: file format = GGUF V3 (latest)
0.00.023.754 I print_info: file type   = Q5_1
0.00.023.756 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.020 I load: special tokens cache size = 25
0.00.038.218 I load: token to piece cache size = 0.2984 MB
0.00.038.225 I print_info: arch             = gptneox
0.00.038.225 I print_info: vocab_only       = 0
0.00.038.225 I print_info: n_ctx_train      = 2048
0.00.038.226 I print_info: n_embd           = 2048
0.00.038.226 I print_info: n_layer          = 24
0.00.038.230 I print_info: n_head           = 16
0.00.038.231 I print_info: n_head_kv        = 16
0.00.038.231 I print_info: n_rot            = 32
0.00.038.231 I print_info: n_swa            = 0
0.00.038.231 I print_info: n_embd_head_k    = 128
0.00.038.231 I print_info: n_embd_head_v    = 128
0.00.038.232 I print_info: n_gqa            = 1
0.00.038.233 I print_info: n_embd_k_gqa     = 2048
0.00.038.233 I print_info: n_embd_v_gqa     = 2048
0.00.038.234 I print_info: f_norm_eps       = 1.0e-05
0.00.038.240 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.240 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.242 I print_info: f_logit_scale    = 0.0e+00
0.00.038.243 I print_info: n_ff             = 8192
0.00.038.243 I print_info: n_expert         = 0
0.00.038.243 I print_info: n_expert_used    = 0
0.00.038.243 I print_info: causal attn      = 1
0.00.038.243 I print_info: pooling type     = 0
0.00.038.244 I print_info: rope type        = 2
0.00.038.246 I print_info: rope scaling     = linear
0.00.038.247 I print_info: freq_base_train  = 10000.0
0.00.038.247 I print_info: freq_scale_train = 1
0.00.038.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.247 I print_info: rope_finetuned   = unknown
0.00.038.248 I print_info: ssm_d_conv       = 0
0.00.038.248 I print_info: ssm_d_inner      = 0
0.00.038.249 I print_info: ssm_d_state      = 0
0.00.038.249 I print_info: ssm_dt_rank      = 0
0.00.038.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.249 I print_info: model type       = 1.4B
0.00.038.250 I print_info: model params     = 1.41 B
0.00.038.250 I print_info: general.name     = 1.4B
0.00.038.250 I print_info: vocab type       = BPE
0.00.038.250 I print_info: n_vocab          = 50304
0.00.038.251 I print_info: n_merges         = 50009
0.00.038.251 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.251 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.251 I print_info: LF token         = 187 ''
0.00.038.252 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.252 I print_info: max token length = 1024
0.00.038.252 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.677.790 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.797 I load_tensors: offloading output layer to GPU
0.00.677.797 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.821 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.677.823 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.678.672 I llama_init_from_model: n_seq_max     = 1
0.00.678.673 I llama_init_from_model: n_ctx         = 128
0.00.678.674 I llama_init_from_model: n_ctx_per_seq = 128
0.00.678.674 I llama_init_from_model: n_batch       = 128
0.00.678.675 I llama_init_from_model: n_ubatch      = 128
0.00.678.675 I llama_init_from_model: flash_attn    = 0
0.00.678.677 I llama_init_from_model: freq_base     = 10000.0
0.00.678.677 I llama_init_from_model: freq_scale    = 1
0.00.678.678 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.678.678 I ggml_metal_init: allocating
0.00.678.705 I ggml_metal_init: found device: Apple M4
0.00.678.716 I ggml_metal_init: picking default device: Apple M4
0.00.680.101 I ggml_metal_init: using embedded metal library
0.00.685.566 I ggml_metal_init: GPU name:   Apple M4
0.00.685.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.685.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.685.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.685.572 I ggml_metal_init: simdgroup reduction   = true
0.00.685.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.685.573 I ggml_metal_init: has residency sets    = true
0.00.685.573 I ggml_metal_init: has bfloat            = true
0.00.685.573 I ggml_metal_init: use bfloat            = true
0.00.685.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.685.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.701.460 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.727 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.704.730 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.704.756 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.795 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.707.797 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.707.797 I llama_init_from_model: graph nodes  = 967
0.00.707.798 I llama_init_from_model: graph splits = 2
0.00.707.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.707.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.031 I 
0.00.738.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.118 I perplexity: tokenizing the input ..
0.00.745.170 I perplexity: tokenization took 7.047 ms
0.00.745.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.211 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.889.508 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.523 I llama_perf_context_print:        load time =     729.45 ms
0.00.889.524 I llama_perf_context_print: prompt eval time =     142.18 ms /   128 tokens (    1.11 ms per token,   900.28 tokens per second)
0.00.889.525 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.525 I llama_perf_context_print:       total time =     151.50 ms /   129 tokens
0.00.889.870 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.077s
sys	0m0.187s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.016 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.237 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.904 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.905 I llama_model_loader: - type  f32:  194 tensors
0.00.024.905 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.905 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.905 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.906 I print_info: file format = GGUF V3 (latest)
0.00.024.906 I print_info: file type   = Q2_K - Medium
0.00.024.907 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.868 I load: special tokens cache size = 25
0.00.039.130 I load: token to piece cache size = 0.2984 MB
0.00.039.135 I print_info: arch             = gptneox
0.00.039.135 I print_info: vocab_only       = 0
0.00.039.136 I print_info: n_ctx_train      = 2048
0.00.039.136 I print_info: n_embd           = 2048
0.00.039.136 I print_info: n_layer          = 24
0.00.039.139 I print_info: n_head           = 16
0.00.039.139 I print_info: n_head_kv        = 16
0.00.039.139 I print_info: n_rot            = 32
0.00.039.140 I print_info: n_swa            = 0
0.00.039.140 I print_info: n_embd_head_k    = 128
0.00.039.140 I print_info: n_embd_head_v    = 128
0.00.039.141 I print_info: n_gqa            = 1
0.00.039.141 I print_info: n_embd_k_gqa     = 2048
0.00.039.142 I print_info: n_embd_v_gqa     = 2048
0.00.039.143 I print_info: f_norm_eps       = 1.0e-05
0.00.039.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.143 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.144 I print_info: f_logit_scale    = 0.0e+00
0.00.039.147 I print_info: n_ff             = 8192
0.00.039.147 I print_info: n_expert         = 0
0.00.039.147 I print_info: n_expert_used    = 0
0.00.039.147 I print_info: causal attn      = 1
0.00.039.147 I print_info: pooling type     = 0
0.00.039.148 I print_info: rope type        = 2
0.00.039.148 I print_info: rope scaling     = linear
0.00.039.148 I print_info: freq_base_train  = 10000.0
0.00.039.149 I print_info: freq_scale_train = 1
0.00.039.149 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.149 I print_info: rope_finetuned   = unknown
0.00.039.149 I print_info: ssm_d_conv       = 0
0.00.039.150 I print_info: ssm_d_inner      = 0
0.00.039.150 I print_info: ssm_d_state      = 0
0.00.039.151 I print_info: ssm_dt_rank      = 0
0.00.039.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.151 I print_info: model type       = 1.4B
0.00.039.151 I print_info: model params     = 1.41 B
0.00.039.152 I print_info: general.name     = 1.4B
0.00.039.152 I print_info: vocab type       = BPE
0.00.039.152 I print_info: n_vocab          = 50304
0.00.039.152 I print_info: n_merges         = 50009
0.00.039.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.155 I print_info: LF token         = 187 ''
0.00.039.155 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.155 I print_info: max token length = 1024
0.00.039.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.379.760 I load_tensors: offloading 24 repeating layers to GPU
0.00.379.773 I load_tensors: offloading output layer to GPU
0.00.379.773 I load_tensors: offloaded 25/25 layers to GPU
0.00.379.803 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.379.804 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.381.171 I llama_init_from_model: n_seq_max     = 1
0.00.381.174 I llama_init_from_model: n_ctx         = 2048
0.00.381.175 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.381.176 I llama_init_from_model: n_batch       = 2048
0.00.381.176 I llama_init_from_model: n_ubatch      = 512
0.00.381.177 I llama_init_from_model: flash_attn    = 0
0.00.381.178 I llama_init_from_model: freq_base     = 10000.0
0.00.381.179 I llama_init_from_model: freq_scale    = 1
0.00.381.182 I ggml_metal_init: allocating
0.00.381.241 I ggml_metal_init: found device: Apple M4
0.00.381.253 I ggml_metal_init: picking default device: Apple M4
0.00.383.036 I ggml_metal_init: using embedded metal library
0.00.389.323 I ggml_metal_init: GPU name:   Apple M4
0.00.389.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.389.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.389.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.389.331 I ggml_metal_init: simdgroup reduction   = true
0.00.389.331 I ggml_metal_init: simdgroup matrix mul. = true
0.00.389.331 I ggml_metal_init: has residency sets    = true
0.00.389.332 I ggml_metal_init: has bfloat            = true
0.00.389.332 I ggml_metal_init: use bfloat            = true
0.00.389.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.389.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.407.734 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.463.927 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.463.935 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.464.010 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.469.025 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.469.027 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.469.028 I llama_init_from_model: graph nodes  = 967
0.00.469.028 I llama_init_from_model: graph splits = 2
0.00.469.034 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.469.150 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.469.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.536 I main: llama threadpool init, n_threads = 4
0.00.521.582 I 
0.00.521.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.608 I 
0.00.521.733 I sampler seed: 1234
0.00.521.737 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.521.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.521.755 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.521.756 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.208.926 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.208.926 I llama_perf_context_print:        load time =     510.81 ms
0.01.208.927 I llama_perf_context_print: prompt eval time =      41.74 ms /     7 tokens (    5.96 ms per token,   167.70 tokens per second)
0.01.208.928 I llama_perf_context_print:        eval time =     642.57 ms /    63 runs   (   10.20 ms per token,    98.04 tokens per second)
0.01.208.928 I llama_perf_context_print:       total time =     688.10 ms /    70 tokens
0.01.209.204 I ggml_metal_free: deallocating

real	0m1.226s
user	0m0.109s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.701 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.468 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.196 I llama_model_loader: - type  f32:  194 tensors
0.00.026.196 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.196 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.198 I print_info: file format = GGUF V3 (latest)
0.00.026.198 I print_info: file type   = Q2_K - Medium
0.00.026.199 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.372 I load: special tokens cache size = 25
0.00.040.297 I load: token to piece cache size = 0.2984 MB
0.00.040.302 I print_info: arch             = gptneox
0.00.040.302 I print_info: vocab_only       = 0
0.00.040.302 I print_info: n_ctx_train      = 2048
0.00.040.303 I print_info: n_embd           = 2048
0.00.040.303 I print_info: n_layer          = 24
0.00.040.307 I print_info: n_head           = 16
0.00.040.308 I print_info: n_head_kv        = 16
0.00.040.308 I print_info: n_rot            = 32
0.00.040.308 I print_info: n_swa            = 0
0.00.040.312 I print_info: n_embd_head_k    = 128
0.00.040.312 I print_info: n_embd_head_v    = 128
0.00.040.312 I print_info: n_gqa            = 1
0.00.040.313 I print_info: n_embd_k_gqa     = 2048
0.00.040.314 I print_info: n_embd_v_gqa     = 2048
0.00.040.314 I print_info: f_norm_eps       = 1.0e-05
0.00.040.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.316 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.317 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.317 I print_info: f_logit_scale    = 0.0e+00
0.00.040.317 I print_info: n_ff             = 8192
0.00.040.318 I print_info: n_expert         = 0
0.00.040.318 I print_info: n_expert_used    = 0
0.00.040.318 I print_info: causal attn      = 1
0.00.040.318 I print_info: pooling type     = 0
0.00.040.318 I print_info: rope type        = 2
0.00.040.319 I print_info: rope scaling     = linear
0.00.040.319 I print_info: freq_base_train  = 10000.0
0.00.040.319 I print_info: freq_scale_train = 1
0.00.040.320 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.320 I print_info: rope_finetuned   = unknown
0.00.040.320 I print_info: ssm_d_conv       = 0
0.00.040.320 I print_info: ssm_d_inner      = 0
0.00.040.320 I print_info: ssm_d_state      = 0
0.00.040.320 I print_info: ssm_dt_rank      = 0
0.00.040.321 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.321 I print_info: model type       = 1.4B
0.00.040.321 I print_info: model params     = 1.41 B
0.00.040.321 I print_info: general.name     = 1.4B
0.00.040.322 I print_info: vocab type       = BPE
0.00.040.322 I print_info: n_vocab          = 50304
0.00.040.322 I print_info: n_merges         = 50009
0.00.040.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: LF token         = 187 ''
0.00.040.325 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.325 I print_info: max token length = 1024
0.00.040.325 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.388.415 I load_tensors: offloading 24 repeating layers to GPU
0.00.388.427 I load_tensors: offloading output layer to GPU
0.00.388.428 I load_tensors: offloaded 25/25 layers to GPU
0.00.388.456 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.388.457 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.389.993 I llama_init_from_model: n_seq_max     = 1
0.00.389.996 I llama_init_from_model: n_ctx         = 128
0.00.389.997 I llama_init_from_model: n_ctx_per_seq = 128
0.00.389.997 I llama_init_from_model: n_batch       = 128
0.00.389.998 I llama_init_from_model: n_ubatch      = 128
0.00.389.998 I llama_init_from_model: flash_attn    = 0
0.00.390.000 I llama_init_from_model: freq_base     = 10000.0
0.00.390.001 I llama_init_from_model: freq_scale    = 1
0.00.390.001 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.390.004 I ggml_metal_init: allocating
0.00.390.061 I ggml_metal_init: found device: Apple M4
0.00.390.075 I ggml_metal_init: picking default device: Apple M4
0.00.392.034 I ggml_metal_init: using embedded metal library
0.00.397.949 I ggml_metal_init: GPU name:   Apple M4
0.00.397.954 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.397.955 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.397.956 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.397.956 I ggml_metal_init: simdgroup reduction   = true
0.00.397.957 I ggml_metal_init: simdgroup matrix mul. = true
0.00.397.957 I ggml_metal_init: has residency sets    = true
0.00.397.957 I ggml_metal_init: has bfloat            = true
0.00.397.958 I ggml_metal_init: use bfloat            = true
0.00.397.959 I ggml_metal_init: hasUnifiedMemory      = true
0.00.397.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.418.135 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.421.671 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.421.675 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.421.701 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.425.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.425.090 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.425.091 I llama_init_from_model: graph nodes  = 967
0.00.425.091 I llama_init_from_model: graph splits = 2
0.00.425.095 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.425.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.453.079 I 
0.00.453.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.453.176 I perplexity: tokenizing the input ..
0.00.459.768 I perplexity: tokenization took 6.59 ms
0.00.459.785 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.592.592 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.593.842 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.593.856 I llama_perf_context_print:        load time =     442.21 ms
0.00.593.857 I llama_perf_context_print: prompt eval time =     132.27 ms /   128 tokens (    1.03 ms per token,   967.71 tokens per second)
0.00.593.857 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.858 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.594.237 I ggml_metal_free: deallocating

real	0m0.609s
user	0m0.081s
sys	0m0.131s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.920 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.720 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.741 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.741 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.742 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.334 I llama_model_loader: - type  f32:  194 tensors
0.00.025.335 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.335 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.335 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.335 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.336 I print_info: file format = GGUF V3 (latest)
0.00.025.337 I print_info: file type   = Q3_K - Medium
0.00.025.337 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.419 I load: special tokens cache size = 25
0.00.039.467 I load: token to piece cache size = 0.2984 MB
0.00.039.470 I print_info: arch             = gptneox
0.00.039.471 I print_info: vocab_only       = 0
0.00.039.471 I print_info: n_ctx_train      = 2048
0.00.039.471 I print_info: n_embd           = 2048
0.00.039.471 I print_info: n_layer          = 24
0.00.039.473 I print_info: n_head           = 16
0.00.039.474 I print_info: n_head_kv        = 16
0.00.039.474 I print_info: n_rot            = 32
0.00.039.475 I print_info: n_swa            = 0
0.00.039.475 I print_info: n_embd_head_k    = 128
0.00.039.475 I print_info: n_embd_head_v    = 128
0.00.039.476 I print_info: n_gqa            = 1
0.00.039.478 I print_info: n_embd_k_gqa     = 2048
0.00.039.479 I print_info: n_embd_v_gqa     = 2048
0.00.039.479 I print_info: f_norm_eps       = 1.0e-05
0.00.039.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.486 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.488 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.488 I print_info: f_logit_scale    = 0.0e+00
0.00.039.495 I print_info: n_ff             = 8192
0.00.039.496 I print_info: n_expert         = 0
0.00.039.497 I print_info: n_expert_used    = 0
0.00.039.498 I print_info: causal attn      = 1
0.00.039.499 I print_info: pooling type     = 0
0.00.039.499 I print_info: rope type        = 2
0.00.039.499 I print_info: rope scaling     = linear
0.00.039.499 I print_info: freq_base_train  = 10000.0
0.00.039.499 I print_info: freq_scale_train = 1
0.00.039.500 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.500 I print_info: rope_finetuned   = unknown
0.00.039.500 I print_info: ssm_d_conv       = 0
0.00.039.500 I print_info: ssm_d_inner      = 0
0.00.039.500 I print_info: ssm_d_state      = 0
0.00.039.500 I print_info: ssm_dt_rank      = 0
0.00.039.500 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.501 I print_info: model type       = 1.4B
0.00.039.501 I print_info: model params     = 1.41 B
0.00.039.501 I print_info: general.name     = 1.4B
0.00.039.502 I print_info: vocab type       = BPE
0.00.039.502 I print_info: n_vocab          = 50304
0.00.039.503 I print_info: n_merges         = 50009
0.00.039.503 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.503 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.504 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.504 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.504 I print_info: LF token         = 187 ''
0.00.039.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.505 I print_info: max token length = 1024
0.00.039.505 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.487.190 I load_tensors: offloading 24 repeating layers to GPU
0.00.487.201 I load_tensors: offloading output layer to GPU
0.00.487.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.487.232 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.487.233 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.488.540 I llama_init_from_model: n_seq_max     = 1
0.00.488.542 I llama_init_from_model: n_ctx         = 2048
0.00.488.542 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.488.543 I llama_init_from_model: n_batch       = 2048
0.00.488.543 I llama_init_from_model: n_ubatch      = 512
0.00.488.543 I llama_init_from_model: flash_attn    = 0
0.00.488.546 I llama_init_from_model: freq_base     = 10000.0
0.00.488.546 I llama_init_from_model: freq_scale    = 1
0.00.488.548 I ggml_metal_init: allocating
0.00.488.603 I ggml_metal_init: found device: Apple M4
0.00.488.614 I ggml_metal_init: picking default device: Apple M4
0.00.490.425 I ggml_metal_init: using embedded metal library
0.00.497.412 I ggml_metal_init: GPU name:   Apple M4
0.00.497.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.497.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.497.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.497.419 I ggml_metal_init: simdgroup reduction   = true
0.00.497.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.497.419 I ggml_metal_init: has residency sets    = true
0.00.497.419 I ggml_metal_init: has bfloat            = true
0.00.497.419 I ggml_metal_init: use bfloat            = true
0.00.497.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.497.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.515.171 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.569.619 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.569.626 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.569.649 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.586.778 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.586.781 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.586.781 I llama_init_from_model: graph nodes  = 967
0.00.586.781 I llama_init_from_model: graph splits = 2
0.00.586.786 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.586.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.586.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.265 I main: llama threadpool init, n_threads = 4
0.00.633.308 I 
0.00.633.335 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.335 I 
0.00.633.444 I sampler seed: 1234
0.00.633.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.633.458 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.633.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.633.462 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.371.274 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.371.274 I llama_perf_context_print:        load time =     623.65 ms
0.01.371.275 I llama_perf_context_print: prompt eval time =      40.21 ms /     7 tokens (    5.74 ms per token,   174.09 tokens per second)
0.01.371.276 I llama_perf_context_print:        eval time =     694.58 ms /    63 runs   (   11.03 ms per token,    90.70 tokens per second)
0.01.371.276 I llama_perf_context_print:       total time =     738.70 ms /    70 tokens
0.01.371.530 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.110s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.690 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.323 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.153 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.155 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.155 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.156 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.156 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.156 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.157 I llama_model_loader: - type  f32:  194 tensors
0.00.025.157 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.158 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.158 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.158 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.159 I print_info: file format = GGUF V3 (latest)
0.00.025.159 I print_info: file type   = Q3_K - Medium
0.00.025.160 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.223 I load: special tokens cache size = 25
0.00.039.220 I load: token to piece cache size = 0.2984 MB
0.00.039.224 I print_info: arch             = gptneox
0.00.039.224 I print_info: vocab_only       = 0
0.00.039.225 I print_info: n_ctx_train      = 2048
0.00.039.225 I print_info: n_embd           = 2048
0.00.039.225 I print_info: n_layer          = 24
0.00.039.230 I print_info: n_head           = 16
0.00.039.230 I print_info: n_head_kv        = 16
0.00.039.230 I print_info: n_rot            = 32
0.00.039.231 I print_info: n_swa            = 0
0.00.039.231 I print_info: n_embd_head_k    = 128
0.00.039.231 I print_info: n_embd_head_v    = 128
0.00.039.232 I print_info: n_gqa            = 1
0.00.039.232 I print_info: n_embd_k_gqa     = 2048
0.00.039.233 I print_info: n_embd_v_gqa     = 2048
0.00.039.234 I print_info: f_norm_eps       = 1.0e-05
0.00.039.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.234 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.235 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.235 I print_info: f_logit_scale    = 0.0e+00
0.00.039.236 I print_info: n_ff             = 8192
0.00.039.236 I print_info: n_expert         = 0
0.00.039.236 I print_info: n_expert_used    = 0
0.00.039.236 I print_info: causal attn      = 1
0.00.039.236 I print_info: pooling type     = 0
0.00.039.236 I print_info: rope type        = 2
0.00.039.237 I print_info: rope scaling     = linear
0.00.039.237 I print_info: freq_base_train  = 10000.0
0.00.039.237 I print_info: freq_scale_train = 1
0.00.039.237 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.238 I print_info: rope_finetuned   = unknown
0.00.039.238 I print_info: ssm_d_conv       = 0
0.00.039.238 I print_info: ssm_d_inner      = 0
0.00.039.238 I print_info: ssm_d_state      = 0
0.00.039.238 I print_info: ssm_dt_rank      = 0
0.00.039.241 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.241 I print_info: model type       = 1.4B
0.00.039.242 I print_info: model params     = 1.41 B
0.00.039.242 I print_info: general.name     = 1.4B
0.00.039.243 I print_info: vocab type       = BPE
0.00.039.243 I print_info: n_vocab          = 50304
0.00.039.243 I print_info: n_merges         = 50009
0.00.039.243 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.243 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.244 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: LF token         = 187 ''
0.00.039.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.246 I print_info: max token length = 1024
0.00.039.246 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.483.709 I load_tensors: offloading 24 repeating layers to GPU
0.00.483.721 I load_tensors: offloading output layer to GPU
0.00.483.722 I load_tensors: offloaded 25/25 layers to GPU
0.00.483.751 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.483.756 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.485.063 I llama_init_from_model: n_seq_max     = 1
0.00.485.065 I llama_init_from_model: n_ctx         = 128
0.00.485.066 I llama_init_from_model: n_ctx_per_seq = 128
0.00.485.066 I llama_init_from_model: n_batch       = 128
0.00.485.067 I llama_init_from_model: n_ubatch      = 128
0.00.485.067 I llama_init_from_model: flash_attn    = 0
0.00.485.068 I llama_init_from_model: freq_base     = 10000.0
0.00.485.069 I llama_init_from_model: freq_scale    = 1
0.00.485.069 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.485.071 I ggml_metal_init: allocating
0.00.485.147 I ggml_metal_init: found device: Apple M4
0.00.485.161 I ggml_metal_init: picking default device: Apple M4
0.00.487.034 I ggml_metal_init: using embedded metal library
0.00.493.695 I ggml_metal_init: GPU name:   Apple M4
0.00.493.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.493.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.493.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.493.703 I ggml_metal_init: simdgroup reduction   = true
0.00.493.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.493.703 I ggml_metal_init: has residency sets    = true
0.00.493.704 I ggml_metal_init: has bfloat            = true
0.00.493.704 I ggml_metal_init: use bfloat            = true
0.00.493.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.493.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.510.998 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.514.337 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.514.343 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.514.382 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.517.470 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.517.471 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.517.472 I llama_init_from_model: graph nodes  = 967
0.00.517.472 I llama_init_from_model: graph splits = 2
0.00.517.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.517.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.747 I 
0.00.543.820 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.836 I perplexity: tokenizing the input ..
0.00.550.743 I perplexity: tokenization took 6.904 ms
0.00.550.763 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.009 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.694.296 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.694.313 I llama_perf_context_print:        load time =     534.05 ms
0.00.694.314 I llama_perf_context_print: prompt eval time =     141.29 ms /   128 tokens (    1.10 ms per token,   905.94 tokens per second)
0.00.694.314 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.694.315 I llama_perf_context_print:       total time =     150.57 ms /   129 tokens
0.00.694.727 I ggml_metal_free: deallocating

real	0m0.708s
user	0m0.080s
sys	0m0.139s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.505 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.320 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.329 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.330 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.331 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.331 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.332 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.335 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.017 I llama_model_loader: - type  f32:  194 tensors
0.00.026.017 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.017 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.017 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.018 I print_info: file format = GGUF V3 (latest)
0.00.026.018 I print_info: file type   = Q4_K - Medium
0.00.026.019 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.683 I load: special tokens cache size = 25
0.00.039.796 I load: token to piece cache size = 0.2984 MB
0.00.039.799 I print_info: arch             = gptneox
0.00.039.800 I print_info: vocab_only       = 0
0.00.039.800 I print_info: n_ctx_train      = 2048
0.00.039.800 I print_info: n_embd           = 2048
0.00.039.800 I print_info: n_layer          = 24
0.00.039.803 I print_info: n_head           = 16
0.00.039.804 I print_info: n_head_kv        = 16
0.00.039.804 I print_info: n_rot            = 32
0.00.039.804 I print_info: n_swa            = 0
0.00.039.805 I print_info: n_embd_head_k    = 128
0.00.039.805 I print_info: n_embd_head_v    = 128
0.00.039.805 I print_info: n_gqa            = 1
0.00.039.806 I print_info: n_embd_k_gqa     = 2048
0.00.039.807 I print_info: n_embd_v_gqa     = 2048
0.00.039.808 I print_info: f_norm_eps       = 1.0e-05
0.00.039.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.811 I print_info: f_logit_scale    = 0.0e+00
0.00.039.811 I print_info: n_ff             = 8192
0.00.039.812 I print_info: n_expert         = 0
0.00.039.812 I print_info: n_expert_used    = 0
0.00.039.812 I print_info: causal attn      = 1
0.00.039.814 I print_info: pooling type     = 0
0.00.039.816 I print_info: rope type        = 2
0.00.039.816 I print_info: rope scaling     = linear
0.00.039.817 I print_info: freq_base_train  = 10000.0
0.00.039.817 I print_info: freq_scale_train = 1
0.00.039.819 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.819 I print_info: rope_finetuned   = unknown
0.00.039.819 I print_info: ssm_d_conv       = 0
0.00.039.819 I print_info: ssm_d_inner      = 0
0.00.039.819 I print_info: ssm_d_state      = 0
0.00.039.819 I print_info: ssm_dt_rank      = 0
0.00.039.820 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.820 I print_info: model type       = 1.4B
0.00.039.820 I print_info: model params     = 1.41 B
0.00.039.820 I print_info: general.name     = 1.4B
0.00.039.822 I print_info: vocab type       = BPE
0.00.039.822 I print_info: n_vocab          = 50304
0.00.039.822 I print_info: n_merges         = 50009
0.00.039.822 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.822 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.822 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.823 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.823 I print_info: LF token         = 187 ''
0.00.039.823 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.823 I print_info: max token length = 1024
0.00.039.824 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.580.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.580.693 I load_tensors: offloading output layer to GPU
0.00.580.695 I load_tensors: offloaded 25/25 layers to GPU
0.00.580.719 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.580.720 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.581.917 I llama_init_from_model: n_seq_max     = 1
0.00.581.919 I llama_init_from_model: n_ctx         = 2048
0.00.581.920 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.581.920 I llama_init_from_model: n_batch       = 2048
0.00.581.921 I llama_init_from_model: n_ubatch      = 512
0.00.581.921 I llama_init_from_model: flash_attn    = 0
0.00.581.922 I llama_init_from_model: freq_base     = 10000.0
0.00.581.923 I llama_init_from_model: freq_scale    = 1
0.00.581.924 I ggml_metal_init: allocating
0.00.581.938 I ggml_metal_init: found device: Apple M4
0.00.581.947 I ggml_metal_init: picking default device: Apple M4
0.00.583.374 I ggml_metal_init: using embedded metal library
0.00.589.389 I ggml_metal_init: GPU name:   Apple M4
0.00.589.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.589.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.589.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.589.395 I ggml_metal_init: simdgroup reduction   = true
0.00.589.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.589.396 I ggml_metal_init: has residency sets    = true
0.00.589.396 I ggml_metal_init: has bfloat            = true
0.00.589.396 I ggml_metal_init: use bfloat            = true
0.00.589.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.589.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.606.875 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.561 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.569 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.598 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.675.832 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.675.834 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.675.834 I llama_init_from_model: graph nodes  = 967
0.00.675.835 I llama_init_from_model: graph splits = 2
0.00.675.840 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.675.974 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.675.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.684 I main: llama threadpool init, n_threads = 4
0.00.725.731 I 
0.00.725.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.757 I 
0.00.725.887 I sampler seed: 1234
0.00.725.891 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.725.901 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.725.901 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.725.902 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.489.244 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.489.246 I llama_perf_context_print:        load time =     715.50 ms
0.01.489.247 I llama_perf_context_print: prompt eval time =      57.95 ms /     7 tokens (    8.28 ms per token,   120.78 tokens per second)
0.01.489.248 I llama_perf_context_print:        eval time =     702.32 ms /    63 runs   (   11.15 ms per token,    89.70 tokens per second)
0.01.489.248 I llama_perf_context_print:       total time =     764.24 ms /    70 tokens
0.01.489.502 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.109s
sys	0m0.245s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.591 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.361 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.187 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.163 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.012 I llama_model_loader: - type  f32:  194 tensors
0.00.026.013 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.013 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.013 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.014 I print_info: file format = GGUF V3 (latest)
0.00.026.017 I print_info: file type   = Q4_K - Medium
0.00.026.018 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.058 I load: special tokens cache size = 25
0.00.039.946 I load: token to piece cache size = 0.2984 MB
0.00.039.949 I print_info: arch             = gptneox
0.00.039.949 I print_info: vocab_only       = 0
0.00.039.949 I print_info: n_ctx_train      = 2048
0.00.039.949 I print_info: n_embd           = 2048
0.00.039.950 I print_info: n_layer          = 24
0.00.039.953 I print_info: n_head           = 16
0.00.039.953 I print_info: n_head_kv        = 16
0.00.039.954 I print_info: n_rot            = 32
0.00.039.954 I print_info: n_swa            = 0
0.00.039.954 I print_info: n_embd_head_k    = 128
0.00.039.954 I print_info: n_embd_head_v    = 128
0.00.039.955 I print_info: n_gqa            = 1
0.00.039.956 I print_info: n_embd_k_gqa     = 2048
0.00.039.956 I print_info: n_embd_v_gqa     = 2048
0.00.039.957 I print_info: f_norm_eps       = 1.0e-05
0.00.039.957 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.957 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.958 I print_info: f_logit_scale    = 0.0e+00
0.00.039.958 I print_info: n_ff             = 8192
0.00.039.958 I print_info: n_expert         = 0
0.00.039.959 I print_info: n_expert_used    = 0
0.00.039.959 I print_info: causal attn      = 1
0.00.039.959 I print_info: pooling type     = 0
0.00.039.959 I print_info: rope type        = 2
0.00.039.959 I print_info: rope scaling     = linear
0.00.039.960 I print_info: freq_base_train  = 10000.0
0.00.039.960 I print_info: freq_scale_train = 1
0.00.039.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.960 I print_info: rope_finetuned   = unknown
0.00.039.962 I print_info: ssm_d_conv       = 0
0.00.039.962 I print_info: ssm_d_inner      = 0
0.00.039.962 I print_info: ssm_d_state      = 0
0.00.039.962 I print_info: ssm_dt_rank      = 0
0.00.039.962 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.962 I print_info: model type       = 1.4B
0.00.039.963 I print_info: model params     = 1.41 B
0.00.039.963 I print_info: general.name     = 1.4B
0.00.039.964 I print_info: vocab type       = BPE
0.00.039.964 I print_info: n_vocab          = 50304
0.00.039.964 I print_info: n_merges         = 50009
0.00.039.964 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.965 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.965 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.965 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.965 I print_info: LF token         = 187 ''
0.00.039.966 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.966 I print_info: max token length = 1024
0.00.039.966 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.581.115 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.122 I load_tensors: offloading output layer to GPU
0.00.581.122 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.151 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.581.153 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.582.291 I llama_init_from_model: n_seq_max     = 1
0.00.582.293 I llama_init_from_model: n_ctx         = 128
0.00.582.293 I llama_init_from_model: n_ctx_per_seq = 128
0.00.582.294 I llama_init_from_model: n_batch       = 128
0.00.582.294 I llama_init_from_model: n_ubatch      = 128
0.00.582.294 I llama_init_from_model: flash_attn    = 0
0.00.582.295 I llama_init_from_model: freq_base     = 10000.0
0.00.582.296 I llama_init_from_model: freq_scale    = 1
0.00.582.297 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.582.298 I ggml_metal_init: allocating
0.00.582.344 I ggml_metal_init: found device: Apple M4
0.00.582.354 I ggml_metal_init: picking default device: Apple M4
0.00.583.741 I ggml_metal_init: using embedded metal library
0.00.589.654 I ggml_metal_init: GPU name:   Apple M4
0.00.589.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.589.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.589.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.589.659 I ggml_metal_init: simdgroup reduction   = true
0.00.589.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.589.660 I ggml_metal_init: has residency sets    = true
0.00.589.660 I ggml_metal_init: has bfloat            = true
0.00.589.660 I ggml_metal_init: use bfloat            = true
0.00.589.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.589.663 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.972 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.609.347 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.609.372 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.297 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.612.298 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.612.299 I llama_init_from_model: graph nodes  = 967
0.00.612.299 I llama_init_from_model: graph splits = 2
0.00.612.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.612.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.385 I 
0.00.642.455 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.474 I perplexity: tokenizing the input ..
0.00.649.672 I perplexity: tokenization took 7.195 ms
0.00.649.693 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.030 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.800.339 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.800.354 I llama_perf_context_print:        load time =     632.79 ms
0.00.800.355 I llama_perf_context_print: prompt eval time =     148.46 ms /   128 tokens (    1.16 ms per token,   862.17 tokens per second)
0.00.800.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.356 I llama_perf_context_print:       total time =     157.97 ms /   129 tokens
0.00.800.767 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.078s
sys	0m0.175s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.578 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.584 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.988 I llama_model_loader: - type  f32:  194 tensors
0.00.024.989 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.989 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.989 I print_info: file format = GGUF V3 (latest)
0.00.024.990 I print_info: file type   = Q5_K - Medium
0.00.024.991 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.612 I load: special tokens cache size = 25
0.00.038.713 I load: token to piece cache size = 0.2984 MB
0.00.038.715 I print_info: arch             = gptneox
0.00.038.715 I print_info: vocab_only       = 0
0.00.038.715 I print_info: n_ctx_train      = 2048
0.00.038.716 I print_info: n_embd           = 2048
0.00.038.716 I print_info: n_layer          = 24
0.00.038.718 I print_info: n_head           = 16
0.00.038.719 I print_info: n_head_kv        = 16
0.00.038.719 I print_info: n_rot            = 32
0.00.038.719 I print_info: n_swa            = 0
0.00.038.719 I print_info: n_embd_head_k    = 128
0.00.038.719 I print_info: n_embd_head_v    = 128
0.00.038.720 I print_info: n_gqa            = 1
0.00.038.720 I print_info: n_embd_k_gqa     = 2048
0.00.038.721 I print_info: n_embd_v_gqa     = 2048
0.00.038.722 I print_info: f_norm_eps       = 1.0e-05
0.00.038.723 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.724 I print_info: f_logit_scale    = 0.0e+00
0.00.038.724 I print_info: n_ff             = 8192
0.00.038.725 I print_info: n_expert         = 0
0.00.038.725 I print_info: n_expert_used    = 0
0.00.038.725 I print_info: causal attn      = 1
0.00.038.725 I print_info: pooling type     = 0
0.00.038.725 I print_info: rope type        = 2
0.00.038.726 I print_info: rope scaling     = linear
0.00.038.726 I print_info: freq_base_train  = 10000.0
0.00.038.726 I print_info: freq_scale_train = 1
0.00.038.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.727 I print_info: rope_finetuned   = unknown
0.00.038.727 I print_info: ssm_d_conv       = 0
0.00.038.727 I print_info: ssm_d_inner      = 0
0.00.038.727 I print_info: ssm_d_state      = 0
0.00.038.727 I print_info: ssm_dt_rank      = 0
0.00.038.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.729 I print_info: model type       = 1.4B
0.00.038.729 I print_info: model params     = 1.41 B
0.00.038.729 I print_info: general.name     = 1.4B
0.00.038.730 I print_info: vocab type       = BPE
0.00.038.730 I print_info: n_vocab          = 50304
0.00.038.730 I print_info: n_merges         = 50009
0.00.038.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: LF token         = 187 ''
0.00.038.732 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.732 I print_info: max token length = 1024
0.00.038.732 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.156 I load_tensors: offloading 24 repeating layers to GPU
0.00.670.159 I load_tensors: offloading output layer to GPU
0.00.670.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.670.182 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.670.184 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.671.309 I llama_init_from_model: n_seq_max     = 1
0.00.671.310 I llama_init_from_model: n_ctx         = 2048
0.00.671.311 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.671.311 I llama_init_from_model: n_batch       = 2048
0.00.671.311 I llama_init_from_model: n_ubatch      = 512
0.00.671.312 I llama_init_from_model: flash_attn    = 0
0.00.671.313 I llama_init_from_model: freq_base     = 10000.0
0.00.671.313 I llama_init_from_model: freq_scale    = 1
0.00.671.314 I ggml_metal_init: allocating
0.00.671.345 I ggml_metal_init: found device: Apple M4
0.00.671.355 I ggml_metal_init: picking default device: Apple M4
0.00.672.680 I ggml_metal_init: using embedded metal library
0.00.678.032 I ggml_metal_init: GPU name:   Apple M4
0.00.678.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.037 I ggml_metal_init: simdgroup reduction   = true
0.00.678.037 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.038 I ggml_metal_init: has residency sets    = true
0.00.678.038 I ggml_metal_init: has bfloat            = true
0.00.678.038 I ggml_metal_init: use bfloat            = true
0.00.678.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.734 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.499 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.506 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.745.530 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.507 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.509 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.509 I llama_init_from_model: graph nodes  = 967
0.00.750.510 I llama_init_from_model: graph splits = 2
0.00.750.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.650 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.051 I main: llama threadpool init, n_threads = 4
0.00.807.094 I 
0.00.807.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.121 I 
0.00.807.263 I sampler seed: 1234
0.00.807.268 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.324 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.326 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.326 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.647.566 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48233.70 tokens per second)
0.01.647.567 I llama_perf_context_print:        load time =     797.61 ms
0.01.647.568 I llama_perf_context_print: prompt eval time =      51.82 ms /     7 tokens (    7.40 ms per token,   135.08 tokens per second)
0.01.647.569 I llama_perf_context_print:        eval time =     785.61 ms /    63 runs   (   12.47 ms per token,    80.19 tokens per second)
0.01.647.570 I llama_perf_context_print:       total time =     841.25 ms /    70 tokens
0.01.647.798 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.105s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.472 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.573 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.939 I llama_model_loader: - type  f32:  194 tensors
0.00.025.939 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.940 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.940 I print_info: file format = GGUF V3 (latest)
0.00.025.941 I print_info: file type   = Q5_K - Medium
0.00.025.942 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.607 I load: special tokens cache size = 25
0.00.039.742 I load: token to piece cache size = 0.2984 MB
0.00.039.745 I print_info: arch             = gptneox
0.00.039.746 I print_info: vocab_only       = 0
0.00.039.746 I print_info: n_ctx_train      = 2048
0.00.039.746 I print_info: n_embd           = 2048
0.00.039.746 I print_info: n_layer          = 24
0.00.039.750 I print_info: n_head           = 16
0.00.039.751 I print_info: n_head_kv        = 16
0.00.039.751 I print_info: n_rot            = 32
0.00.039.751 I print_info: n_swa            = 0
0.00.039.752 I print_info: n_embd_head_k    = 128
0.00.039.753 I print_info: n_embd_head_v    = 128
0.00.039.754 I print_info: n_gqa            = 1
0.00.039.754 I print_info: n_embd_k_gqa     = 2048
0.00.039.755 I print_info: n_embd_v_gqa     = 2048
0.00.039.756 I print_info: f_norm_eps       = 1.0e-05
0.00.039.756 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.756 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.756 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.757 I print_info: f_logit_scale    = 0.0e+00
0.00.039.757 I print_info: n_ff             = 8192
0.00.039.757 I print_info: n_expert         = 0
0.00.039.758 I print_info: n_expert_used    = 0
0.00.039.758 I print_info: causal attn      = 1
0.00.039.758 I print_info: pooling type     = 0
0.00.039.760 I print_info: rope type        = 2
0.00.039.760 I print_info: rope scaling     = linear
0.00.039.761 I print_info: freq_base_train  = 10000.0
0.00.039.761 I print_info: freq_scale_train = 1
0.00.039.761 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.761 I print_info: rope_finetuned   = unknown
0.00.039.761 I print_info: ssm_d_conv       = 0
0.00.039.761 I print_info: ssm_d_inner      = 0
0.00.039.762 I print_info: ssm_d_state      = 0
0.00.039.762 I print_info: ssm_dt_rank      = 0
0.00.039.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.762 I print_info: model type       = 1.4B
0.00.039.762 I print_info: model params     = 1.41 B
0.00.039.763 I print_info: general.name     = 1.4B
0.00.039.763 I print_info: vocab type       = BPE
0.00.039.763 I print_info: n_vocab          = 50304
0.00.039.763 I print_info: n_merges         = 50009
0.00.039.764 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.769 I print_info: LF token         = 187 ''
0.00.039.769 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.769 I print_info: max token length = 1024
0.00.039.769 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.684.690 I load_tensors: offloading 24 repeating layers to GPU
0.00.684.695 I load_tensors: offloading output layer to GPU
0.00.684.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.684.717 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.684.719 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.685.697 I llama_init_from_model: n_seq_max     = 1
0.00.685.699 I llama_init_from_model: n_ctx         = 128
0.00.685.699 I llama_init_from_model: n_ctx_per_seq = 128
0.00.685.699 I llama_init_from_model: n_batch       = 128
0.00.685.700 I llama_init_from_model: n_ubatch      = 128
0.00.685.700 I llama_init_from_model: flash_attn    = 0
0.00.685.701 I llama_init_from_model: freq_base     = 10000.0
0.00.685.702 I llama_init_from_model: freq_scale    = 1
0.00.685.702 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.685.704 I ggml_metal_init: allocating
0.00.685.717 I ggml_metal_init: found device: Apple M4
0.00.685.727 I ggml_metal_init: picking default device: Apple M4
0.00.687.003 I ggml_metal_init: using embedded metal library
0.00.692.390 I ggml_metal_init: GPU name:   Apple M4
0.00.692.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.396 I ggml_metal_init: simdgroup reduction   = true
0.00.692.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.396 I ggml_metal_init: has residency sets    = true
0.00.692.397 I ggml_metal_init: has bfloat            = true
0.00.692.397 I ggml_metal_init: use bfloat            = true
0.00.692.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.167 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.474 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.711.482 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.711.538 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.853 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.714.855 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.714.855 I llama_init_from_model: graph nodes  = 967
0.00.714.856 I llama_init_from_model: graph splits = 2
0.00.714.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.714.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.766 I 
0.00.749.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.863 I perplexity: tokenizing the input ..
0.00.756.657 I perplexity: tokenization took 6.791 ms
0.00.756.675 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.119 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.907.423 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.907.437 I llama_perf_context_print:        load time =     739.29 ms
0.00.907.438 I llama_perf_context_print: prompt eval time =     148.53 ms /   128 tokens (    1.16 ms per token,   861.78 tokens per second)
0.00.907.439 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.439 I llama_perf_context_print:       total time =     157.67 ms /   129 tokens
0.00.907.799 I ggml_metal_free: deallocating

real	0m0.922s
user	0m0.076s
sys	0m0.197s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.143 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.704 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.716 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.716 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.734 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.729 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.499 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.500 I llama_model_loader: - type  f32:  194 tensors
0.00.026.500 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.501 I print_info: file format = GGUF V3 (latest)
0.00.026.501 I print_info: file type   = Q6_K
0.00.026.502 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.616 I load: special tokens cache size = 25
0.00.040.713 I load: token to piece cache size = 0.2984 MB
0.00.040.716 I print_info: arch             = gptneox
0.00.040.716 I print_info: vocab_only       = 0
0.00.040.717 I print_info: n_ctx_train      = 2048
0.00.040.717 I print_info: n_embd           = 2048
0.00.040.717 I print_info: n_layer          = 24
0.00.040.719 I print_info: n_head           = 16
0.00.040.720 I print_info: n_head_kv        = 16
0.00.040.720 I print_info: n_rot            = 32
0.00.040.720 I print_info: n_swa            = 0
0.00.040.721 I print_info: n_embd_head_k    = 128
0.00.040.721 I print_info: n_embd_head_v    = 128
0.00.040.721 I print_info: n_gqa            = 1
0.00.040.722 I print_info: n_embd_k_gqa     = 2048
0.00.040.723 I print_info: n_embd_v_gqa     = 2048
0.00.040.723 I print_info: f_norm_eps       = 1.0e-05
0.00.040.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.726 I print_info: f_logit_scale    = 0.0e+00
0.00.040.727 I print_info: n_ff             = 8192
0.00.040.727 I print_info: n_expert         = 0
0.00.040.727 I print_info: n_expert_used    = 0
0.00.040.727 I print_info: causal attn      = 1
0.00.040.727 I print_info: pooling type     = 0
0.00.040.727 I print_info: rope type        = 2
0.00.040.728 I print_info: rope scaling     = linear
0.00.040.728 I print_info: freq_base_train  = 10000.0
0.00.040.728 I print_info: freq_scale_train = 1
0.00.040.728 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.729 I print_info: rope_finetuned   = unknown
0.00.040.729 I print_info: ssm_d_conv       = 0
0.00.040.729 I print_info: ssm_d_inner      = 0
0.00.040.730 I print_info: ssm_d_state      = 0
0.00.040.730 I print_info: ssm_dt_rank      = 0
0.00.040.730 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.731 I print_info: model type       = 1.4B
0.00.040.731 I print_info: model params     = 1.41 B
0.00.040.731 I print_info: general.name     = 1.4B
0.00.040.732 I print_info: vocab type       = BPE
0.00.040.732 I print_info: n_vocab          = 50304
0.00.040.732 I print_info: n_merges         = 50009
0.00.040.732 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.732 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.733 I print_info: LF token         = 187 ''
0.00.040.734 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.736 I print_info: max token length = 1024
0.00.040.736 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.738.826 I load_tensors: offloading 24 repeating layers to GPU
0.00.738.831 I load_tensors: offloading output layer to GPU
0.00.738.832 I load_tensors: offloaded 25/25 layers to GPU
0.00.738.856 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.738.858 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.739.842 I llama_init_from_model: n_seq_max     = 1
0.00.739.843 I llama_init_from_model: n_ctx         = 2048
0.00.739.843 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.739.844 I llama_init_from_model: n_batch       = 2048
0.00.739.845 I llama_init_from_model: n_ubatch      = 512
0.00.739.845 I llama_init_from_model: flash_attn    = 0
0.00.739.846 I llama_init_from_model: freq_base     = 10000.0
0.00.739.846 I llama_init_from_model: freq_scale    = 1
0.00.739.847 I ggml_metal_init: allocating
0.00.739.856 I ggml_metal_init: found device: Apple M4
0.00.739.863 I ggml_metal_init: picking default device: Apple M4
0.00.741.195 I ggml_metal_init: using embedded metal library
0.00.746.673 I ggml_metal_init: GPU name:   Apple M4
0.00.746.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.679 I ggml_metal_init: simdgroup reduction   = true
0.00.746.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.680 I ggml_metal_init: has residency sets    = true
0.00.746.680 I ggml_metal_init: has bfloat            = true
0.00.746.680 I ggml_metal_init: use bfloat            = true
0.00.746.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.762.405 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.819.696 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.819.702 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.819.725 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.824.820 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.824.822 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.824.823 I llama_init_from_model: graph nodes  = 967
0.00.824.823 I llama_init_from_model: graph splits = 2
0.00.824.829 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.824.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.824.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.880.996 I main: llama threadpool init, n_threads = 4
0.00.881.042 I 
0.00.881.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.881.067 I 
0.00.881.176 I sampler seed: 1234
0.00.881.180 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.881.198 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.881.199 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.881.199 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.748.867 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54741.71 tokens per second)
0.01.748.868 I llama_perf_context_print:        load time =     870.16 ms
0.01.748.869 I llama_perf_context_print: prompt eval time =      54.00 ms /     7 tokens (    7.71 ms per token,   129.64 tokens per second)
0.01.748.869 I llama_perf_context_print:        eval time =     810.81 ms /    63 runs   (   12.87 ms per token,    77.70 tokens per second)
0.01.748.870 I llama_perf_context_print:       total time =     868.56 ms /    70 tokens
0.01.749.107 I ggml_metal_free: deallocating

real	0m1.767s
user	0m0.106s
sys	0m0.277s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4687 (b9ab0a4d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.636 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.432 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.365 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.507 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.509 I llama_model_loader: - type  f32:  194 tensors
0.00.026.509 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.510 I print_info: file format = GGUF V3 (latest)
0.00.026.510 I print_info: file type   = Q6_K
0.00.026.511 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.406 I load: special tokens cache size = 25
0.00.040.400 I load: token to piece cache size = 0.2984 MB
0.00.040.403 I print_info: arch             = gptneox
0.00.040.403 I print_info: vocab_only       = 0
0.00.040.404 I print_info: n_ctx_train      = 2048
0.00.040.404 I print_info: n_embd           = 2048
0.00.040.404 I print_info: n_layer          = 24
0.00.040.407 I print_info: n_head           = 16
0.00.040.408 I print_info: n_head_kv        = 16
0.00.040.408 I print_info: n_rot            = 32
0.00.040.408 I print_info: n_swa            = 0
0.00.040.408 I print_info: n_embd_head_k    = 128
0.00.040.408 I print_info: n_embd_head_v    = 128
0.00.040.409 I print_info: n_gqa            = 1
0.00.040.410 I print_info: n_embd_k_gqa     = 2048
0.00.040.411 I print_info: n_embd_v_gqa     = 2048
0.00.040.411 I print_info: f_norm_eps       = 1.0e-05
0.00.040.411 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.412 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.412 I print_info: f_logit_scale    = 0.0e+00
0.00.040.413 I print_info: n_ff             = 8192
0.00.040.415 I print_info: n_expert         = 0
0.00.040.415 I print_info: n_expert_used    = 0
0.00.040.415 I print_info: causal attn      = 1
0.00.040.415 I print_info: pooling type     = 0
0.00.040.415 I print_info: rope type        = 2
0.00.040.416 I print_info: rope scaling     = linear
0.00.040.416 I print_info: freq_base_train  = 10000.0
0.00.040.416 I print_info: freq_scale_train = 1
0.00.040.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.417 I print_info: rope_finetuned   = unknown
0.00.040.419 I print_info: ssm_d_conv       = 0
0.00.040.419 I print_info: ssm_d_inner      = 0
0.00.040.419 I print_info: ssm_d_state      = 0
0.00.040.419 I print_info: ssm_dt_rank      = 0
0.00.040.419 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.419 I print_info: model type       = 1.4B
0.00.040.420 I print_info: model params     = 1.41 B
0.00.040.420 I print_info: general.name     = 1.4B
0.00.040.420 I print_info: vocab type       = BPE
0.00.040.421 I print_info: n_vocab          = 50304
0.00.040.421 I print_info: n_merges         = 50009
0.00.040.421 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.422 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: LF token         = 187 ''
0.00.040.427 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.428 I print_info: max token length = 1024
0.00.040.428 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.261.532 I load_tensors: offloading 24 repeating layers to GPU
0.00.261.538 I load_tensors: offloading output layer to GPU
0.00.261.539 I load_tensors: offloaded 25/25 layers to GPU
0.00.261.560 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.261.562 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.262.562 I llama_init_from_model: n_seq_max     = 1
0.00.262.563 I llama_init_from_model: n_ctx         = 128
0.00.262.564 I llama_init_from_model: n_ctx_per_seq = 128
0.00.262.564 I llama_init_from_model: n_batch       = 128
0.00.262.564 I llama_init_from_model: n_ubatch      = 128
0.00.262.565 I llama_init_from_model: flash_attn    = 0
0.00.262.566 I llama_init_from_model: freq_base     = 10000.0
0.00.262.566 I llama_init_from_model: freq_scale    = 1
0.00.262.567 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.262.568 I ggml_metal_init: allocating
0.00.262.599 I ggml_metal_init: found device: Apple M4
0.00.262.608 I ggml_metal_init: picking default device: Apple M4
0.00.263.815 I ggml_metal_init: using embedded metal library
0.00.269.111 I ggml_metal_init: GPU name:   Apple M4
0.00.269.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.269.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.269.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.269.117 I ggml_metal_init: simdgroup reduction   = true
0.00.269.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.269.117 I ggml_metal_init: has residency sets    = true
0.00.269.118 I ggml_metal_init: has bfloat            = true
0.00.269.118 I ggml_metal_init: use bfloat            = true
0.00.269.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.269.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.284.075 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.287.334 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.287.338 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.287.383 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.290.401 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.290.403 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.290.403 I llama_init_from_model: graph nodes  = 967
0.00.290.404 I llama_init_from_model: graph splits = 2
0.00.290.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.290.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.326.724 I 
0.00.326.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.326.829 I perplexity: tokenizing the input ..
0.00.333.703 I perplexity: tokenization took 6.87 ms
0.00.333.728 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.474.621 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.475.890 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.475.903 I llama_perf_context_print:        load time =     316.08 ms
0.00.475.904 I llama_perf_context_print: prompt eval time =     140.00 ms /   128 tokens (    1.09 ms per token,   914.30 tokens per second)
0.00.475.904 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.475.905 I llama_perf_context_print:       total time =     149.18 ms /   129 tokens
0.00.476.245 I ggml_metal_free: deallocating

real	0m0.492s
user	0m0.076s
sys	0m0.106s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4687 (b9ab0a4d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157307ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1573083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157308960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157308f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1573094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157309a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15730a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15730a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15730ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15730b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15730b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15730ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15730c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15730cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15730d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15730dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15730e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15730eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15730f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15730f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1573100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1573107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157310f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1573117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157311ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157312190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1573127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157313410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157313950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157313c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1573140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157314370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157314c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157315140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157315400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1573158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157315d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1573161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157316680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157316b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157316fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157317460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157317900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157317da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157318060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157318670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157318c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1573195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157319bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15731a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15731a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15731ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15731b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15731ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15731c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15731c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15731cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15731cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15731d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15731dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15731deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15731e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15731e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15731ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15731f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15731f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15731fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15731ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1573203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157320850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157320cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157321190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157321630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157321b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1573220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157322620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157322b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1573230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157323610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157323b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1573240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157324600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157324b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1573250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1573255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157325b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157326090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1573265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157326b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157327080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1573275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157327b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157328070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1573285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157328b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157329060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1573295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157319290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157329a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15732a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15732a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15732ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15732b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15732b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15732bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15732c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15732c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15732cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15732d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15732d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15732dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15732e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15732e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15732eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15732f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15732f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15732f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15732fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1573302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157330740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157330be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157331080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157331520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1573319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157331e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157332300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1573327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157332c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1573330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157333580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157333a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157333ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157334360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157334800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157334ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157335140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1573355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157335a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157335f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1573363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157336860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157336d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1573371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157337640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157337ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157337f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157338420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1573388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157338d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157339200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1573396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157339b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157339fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15733a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15733a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15733adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15733b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15733b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15733bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15733c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15733c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15733c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15733ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15733d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15733d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15733dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15733e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15733e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15733e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15733ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15733f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15733f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15733fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157340100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1573405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157340a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157340ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157341380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157341820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157341cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157342160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157342600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157342aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157342f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1573433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157343880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157343d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1573441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157344660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157344b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157344fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157345440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1573458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157345e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157346380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1573468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157346e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1573470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1573476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157347d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157348310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157348b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157348fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157349260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157349870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157349e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15734a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15734ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15734afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15734b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15734bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15734c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15734c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15734cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15734d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15734d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15734dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15734e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15734e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15734ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15734f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15734f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15734fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157350110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157350660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157350bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157351100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157351650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157351ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1573520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157352640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157352b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1573530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157353630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157353b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1573540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157354620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157354b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1573550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157355610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157355b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1573560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157356600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157356b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1573570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1573575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157357b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157358090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1573585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157358b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157359080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1573595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157359b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15735a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15735a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15735ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15735b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15735b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15735bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15735c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15735c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15735caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15735d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15735d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15735dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15735e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15735e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15735ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15735eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15735f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15735f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15735fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157360140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1573605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157360a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157360f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1573613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157361860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157361d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1573621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157362640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157362ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157363030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157363750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157363e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157364590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157364cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157364f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157365760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157365a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157366030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.784.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.784.119 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157365ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1573479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1573473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157347fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15731b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15731aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15731d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157349b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157312450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157318f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157319860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157319e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157318320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15731a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157311450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15731d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157329ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157365230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157314630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1573148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15734a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1573485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157312a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157312d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157312fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157366490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157366750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157366a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157366cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157366f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157367250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157367510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1573677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157367a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157367d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157368010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1573682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157368590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157368850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157368b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157368dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157369090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157369350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157369610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1573698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157369b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157369e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15736a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15736a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15736a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15736a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15736ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15736aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15736b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15736b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15736b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15736b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15736bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15736bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15736c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15736c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15736c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15736ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15736cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15736cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15736d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15736d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15736d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15736dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15736dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15736e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15736e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15736e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15736e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15736eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15736ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15736f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15736f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15736f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15736f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15736fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15736fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157370150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157370410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1573706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157370990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157370c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157370f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1573711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157371490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157371750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157371a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157371cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157371f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157372250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157372510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1573727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157372a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157372d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157373010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1573732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157373590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157373850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157373b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157373dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157374090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157374350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157374610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1573748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157374b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157374e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157375110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1573753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157375690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157375950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157375c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157375ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157376190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157376450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157376710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1573769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157376c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157376f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157377210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1573774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157377790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157377a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157377d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157377fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157378290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157378550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157378810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157378ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157378d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157379050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157379310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1573795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157379890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157379b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157379e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15737a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15737a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15737a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15737a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15737abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15737ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15737b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15737b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15737b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15737b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15737bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15737bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15737c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15737c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15737c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15737ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15737ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15737cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15737d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15737d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15737d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15737da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15737dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15737e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15737e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15737e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15737e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15737eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15737edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15737f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15737f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15737f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15737f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15737fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15737fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157380110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1573803d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157380690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157380950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157380c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157380ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157381190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157381450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157381710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1573819d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157381c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157381f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157382210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1573824d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157382790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157382a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157382d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157382fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157383290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157383550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157383810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157383ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157383d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157384050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157384310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1573845d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157384890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157384b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157384e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157385350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157385890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157385b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157385ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157386490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157386930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1573870e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1573873a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157387660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157387ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157387f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1573883b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157388820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157388c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157389100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157389570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1573899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157389e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15738a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15738a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15738aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15738b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15738b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15738b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15738bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15738c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15738c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15738cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15738cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15738d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15738d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15738dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15738e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15738e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15738e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15738ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15738f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15738f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15738fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15738fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157390460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1573908d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157390d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1573911b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157391620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157391a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157391f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157392370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1573927e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157392c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1573930c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157393530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1573939a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157393e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157394280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1573946f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157394b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157394fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157395440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1573958b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157395d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157396190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157396600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157396a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157396ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157397350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1573977c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157397c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1573980a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157398510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157398980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157398df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157399260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1573996d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157399b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157399fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15739a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15739a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15739ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15739b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15739be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15739c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15739ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15739cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15739d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15739da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15739e050 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157408350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1574087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157408c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1574090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157409510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157409980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157409df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15740a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15740a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15740ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15740afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15740b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15740c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15740c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15740d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15740d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15740dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15740e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15740ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15740f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15740fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1574103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157410ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1574111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157411900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157411bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157411e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1574122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157412760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157412bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1574130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1574135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157413a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157413d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157414180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1574145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157414b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157415050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157415550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157415a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157415f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157416450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157416950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157416e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157417350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1574177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157417c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1574180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157418510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157418980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157418df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157419260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1574196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157419b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157419fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15741a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15741ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15741aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15741b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15741bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15741c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15741c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15741cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15741cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15741d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15741d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15741dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15741e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15741e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15741eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15741efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15741f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15741f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15741fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1574203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1574208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157420e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157421390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1574218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157421e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157422380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1574228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157422e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157423370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1574238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157423e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157424360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1574248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157424e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157425350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1574258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157425df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157426340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157426890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157426de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157427330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157427880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157427dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157428320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157428870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157428dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157429310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157429860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157429db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15742a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15742a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15742ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15742b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15742b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15742bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15742c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15742c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15742cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15742d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15742d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15742db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15742e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15742e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15742e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15742ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15742f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15742f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15742fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157430060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157430500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1574309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157430e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1574312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157431780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157431c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1574320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157432560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157432a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157432ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157433340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1574337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157433c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157434120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1574345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157434a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157434f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1574353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157435840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157435ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157436180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157436620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157436ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157436f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157437400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1574378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157437d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1574381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157438680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157438b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157438fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157439460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157439900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157439da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15743a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15743a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15743ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15743b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15743b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15743b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15743be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15743c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15743c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15743cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15743d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15743d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15743d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15743de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15743e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15743e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15743ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15743f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15743f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15743fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15743fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157440360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157440800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157440ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157441140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1574415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157441a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157441f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1574423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157442860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157442d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1574431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157443640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157443ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157443f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1574444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157444a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157444f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1574454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157445780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157445d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1574463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1574469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1574471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157447640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157447900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157447f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157448520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157448d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1574491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157449650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157449af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15744a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15744a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15744ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15744b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15744b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15744bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15744c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15744c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15744cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15744d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15744d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15744dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15744e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15744e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15744ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15744f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15744f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15744fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157450240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157450790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157450ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157451230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157451780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157451cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157452220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157452770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157452cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157453210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157453760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157453cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157454200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157454750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157454ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1574551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157455740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157455c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1574561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157456730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157456c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1574571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157457720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157457c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1574581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157458710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157458c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1574591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157459700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157459c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15745a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15745a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15745ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15745b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15745b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15745bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15745c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15745c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15745cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15745d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15745d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15745da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15745dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15745e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15745e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15745ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15745f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15745f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15745fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15745ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1574603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157460840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157460ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157461180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1574616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157461df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157462510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157462c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157463350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157463610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157463e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1574640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1574646d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.835s
user	0m0.281s
sys	0m0.329s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4687 (b9ab0a4d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf10460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf10b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf13340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf13d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf14240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf14d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf16b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf1cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf1e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf22370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf24e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf26670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf28230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf28b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf2a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf2e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf2eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf2f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf2f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf32990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf33980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf34420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf34970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf34ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf38120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf38f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf3a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf3aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf3cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf3cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf3e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf3f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf40740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf41520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf43ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf45140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf4adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf4b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf4c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf4f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf52e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf55e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf59370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf5a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf5ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf5b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf5c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf5ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf5edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf60300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf60850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf62830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf62d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf63820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf63d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf64810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf64d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf65800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf65d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf66d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf67b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf67fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf68460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf68900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf68da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf69b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf6a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf6a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf6ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf6b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf6b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf6bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf6c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf6cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf6df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf6e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf6e7f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d804b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d805000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d805470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d8058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d805d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d8061c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d806630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d806aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d806f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d807380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d8077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d807ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d808a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d8091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d8099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d80a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d80a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d80af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d80b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d80bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d80c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d80cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d80d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d80d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d80e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d80e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d80e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d80eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d80ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d80f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d80f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d80fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d8101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d8104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d810920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d810d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d811200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d811670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d811ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d811f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d8123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d812830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d812ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d813110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d813580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d8139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d813e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d8142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d814740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d815020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d815490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d815900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d815d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d8161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d816650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d816bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d8170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d817530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d8179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d817e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d818280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d8186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d818b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d818fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d819440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d8198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d819d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d81a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d81a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d81aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d81aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d81b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d81b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d81bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d81c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d81c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d81c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d81cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d81d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d81d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d81db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d81dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d81e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d81e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d81ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d81f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d81f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d81fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d81fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d820330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d8207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d820c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d8214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d821dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d822240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d8226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d822b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d823400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d823870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d823ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d824150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d8245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d824ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d825310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d8264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d826940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d827220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d827f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d8283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d828850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d829130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d8295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d829a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d829e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d82a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d82a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d82abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d82b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d82b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d82b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d82bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d82c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d82c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d82cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d82cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d82d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d82d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d82dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d82e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d82e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d82e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d82ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d82f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d82f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d82fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d830490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d830900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d830d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d8311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d831650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d831ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d831f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d8323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d832810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d832c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d8330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d8339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d8342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d834720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d834b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d835000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d835c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d835ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d8361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d836620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d836a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d836f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d837370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d8377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d837c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d8380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d838530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d8389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d838e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d839280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d8396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d839b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d839fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d83a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d83a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d83ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d83b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d83b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d83ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d83bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d83c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d83c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d83cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d83d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d83d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d83d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d83ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d83e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d83e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d83eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d83efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d83f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d83f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d83fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d840300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d840770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d840be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d841050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d841570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d841a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d8425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d8428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d842e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d843430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d8439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d843fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d844570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d8450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d8456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d845c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d846230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d8467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d846db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d847370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d847930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d847ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d8484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d848a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d849030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d8495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d849bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d84a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d84a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d84acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d84b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d84b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d84be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d84c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d84c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d84cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d84d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d84daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d84e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d84e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d84ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d84f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d84f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d84fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d850330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d8508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d850eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d851470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d851a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d851ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d8525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d852b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d853130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d8536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d853cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d854270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d854830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d854df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d8553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d855970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d855f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d8564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d856ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d856fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d8574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d8579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d857eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d8583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d8588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d858db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d8597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d859cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d85a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d85a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d85abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d85b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d85b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d85bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d85c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d85ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d85d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d85d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d85dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d85e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d85e8a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf6e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf50780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf21700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf22020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf22630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf1cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf52900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf1b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf6ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf6ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf6f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf6f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf6f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf6fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf6fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf6ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf70250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf70510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf707d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf70a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf70d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf71010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf712d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf71590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf71850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf71b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf71dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf72090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf72350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf72610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf728d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf72b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf72e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf73110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf733d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf73690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf73950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf73c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf73ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf74190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf74450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf74710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf749d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf74c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf74f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf75210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf754d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf75790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf75a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf75d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf75fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf76290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf76550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf76810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf76ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf76d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf77050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf77310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf775d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf77890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf77b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf77e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf780d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf78390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf78650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf78910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf78bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf78e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf79150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf79410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf796d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf79990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf79c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf79f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf7a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf7a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf7a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf7aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf7acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf7af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf7b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf7b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf7b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf7ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf7bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf7c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf7c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf7c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf7c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf7cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf7cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf7d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf7d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf7d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf7d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf7db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf7de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf7e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf7e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf7e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf7e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf7ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf7eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf7f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf7f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf7f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf7f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf7fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf7ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf80210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf804d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf80790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf80a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf80d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf80fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf81290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf81550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf81810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf81ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf81d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf82050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf82310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf825d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf82890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf82b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf82e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf830d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf83390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf83650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf83910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf83bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf83e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf84150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf84410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf846d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf84990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf84c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf84f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf851d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf85490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf85750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf85a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf85cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf85f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf86250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf86510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf867d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf86a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf86d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf87010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf872d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf87590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf87850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf87b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf87dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf88090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf88350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf88610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf888d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf88b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf88e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf89110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf893d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf89690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf89950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf89c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf89ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf8a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf8a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf8a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf8a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf8ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf8af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf8b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf8b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf8b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf8ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf8bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf8bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf8c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf8c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf8c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf8cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf8cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf8d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf8d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf8d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf8d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf8db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf8de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf8e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf8e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf8e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf8f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf8f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf8f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf8fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf8ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf90860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf90cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf91140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf915b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf91a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf91e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf92300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf92770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf92be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf93050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf934c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf93930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf93da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf94210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf94680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf94af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf94f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf953d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf95840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf95cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf96120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf96590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf96a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf96e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf972e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf97750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf97bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf98030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf984a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf98910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf98d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf991f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf99660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf99ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf99f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf9a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf9a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf9ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf9b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf9b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf9b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf9be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf9c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf9c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf9cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf9d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf9d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf9d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf9dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf9e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf9e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf9eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf9ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf9f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf9f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf9fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bfa00e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bfa0550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bfa09c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bfa0e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bfa12a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bfa1710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bfa1b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bfa1ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bfa2460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bfa28d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bfa2d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bfa37b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bfa3ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bfa45f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bfa4d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bfa4fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bfa57c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bfa5a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bfa6090 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.946s
user	0m0.228s
sys	0m0.185s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.82 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.25 sec*proc (2 tests)

Total Test time (real) =   2.26 sec
        2.29 real         0.51 user         0.28 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.32 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.56 sec*proc (2 tests)

Total Test time (real) =   0.57 sec
        0.57 real         0.13 user         0.08 sys
```
