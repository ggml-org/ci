### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.37 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.56 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.96 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.87 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.45 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.68 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.33 sec*proc (29 tests)

Total Test time (real) = 165.34 sec

real	2m45.473s
user	4m40.205s
sys	0m5.755s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.88 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.78 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.51 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.28 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.39 sec*proc (29 tests)

Total Test time (real) =  48.41 sec

real	0m48.417s
user	0m54.827s
sys	0m5.131s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.093 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.414 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.887 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.017.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.892 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.017.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.893 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.017.893 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.017.894 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.017.894 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.017.895 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.017.895 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.017.895 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.017.896 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.017.898 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.017.898 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.017.899 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.017.899 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.017.900 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.017.900 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.017.900 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.019.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.020.637 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.020.638 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.020.638 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.020.639 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.020.639 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.020.639 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.020.640 I llama_model_loader: - type  f32:  124 tensors
0.00.020.640 I llama_model_loader: - type  f16:   73 tensors
0.00.020.640 I print_info: file format = GGUF V3 (latest)
0.00.020.641 I print_info: file type   = F16
0.00.020.642 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.022.906 I load: special tokens cache size = 5
0.00.024.118 I load: token to piece cache size = 0.2032 MB
0.00.024.121 I print_info: arch             = bert
0.00.024.121 I print_info: vocab_only       = 0
0.00.024.121 I print_info: n_ctx_train      = 512
0.00.024.121 I print_info: n_embd           = 384
0.00.024.122 I print_info: n_layer          = 12
0.00.024.124 I print_info: n_head           = 12
0.00.024.125 I print_info: n_head_kv        = 12
0.00.024.125 I print_info: n_rot            = 32
0.00.024.125 I print_info: n_swa            = 0
0.00.024.125 I print_info: n_embd_head_k    = 32
0.00.024.126 I print_info: n_embd_head_v    = 32
0.00.024.126 I print_info: n_gqa            = 1
0.00.024.127 I print_info: n_embd_k_gqa     = 384
0.00.024.129 I print_info: n_embd_v_gqa     = 384
0.00.024.130 I print_info: f_norm_eps       = 1.0e-12
0.00.024.130 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.024.130 I print_info: f_clamp_kqv      = 0.0e+00
0.00.024.130 I print_info: f_max_alibi_bias = 0.0e+00
0.00.024.131 I print_info: f_logit_scale    = 0.0e+00
0.00.024.131 I print_info: n_ff             = 1536
0.00.024.131 I print_info: n_expert         = 0
0.00.024.132 I print_info: n_expert_used    = 0
0.00.024.133 I print_info: causal attn      = 0
0.00.024.133 I print_info: pooling type     = 2
0.00.024.133 I print_info: rope type        = 2
0.00.024.133 I print_info: rope scaling     = linear
0.00.024.134 I print_info: freq_base_train  = 10000.0
0.00.024.134 I print_info: freq_scale_train = 1
0.00.024.134 I print_info: n_ctx_orig_yarn  = 512
0.00.024.135 I print_info: rope_finetuned   = unknown
0.00.024.135 I print_info: ssm_d_conv       = 0
0.00.024.135 I print_info: ssm_d_inner      = 0
0.00.024.135 I print_info: ssm_d_state      = 0
0.00.024.135 I print_info: ssm_dt_rank      = 0
0.00.024.135 I print_info: ssm_dt_b_c_rms   = 0
0.00.024.135 I print_info: model type       = 33M
0.00.024.136 I print_info: model params     = 33.21 M
0.00.024.136 I print_info: general.name     = Bge Small
0.00.024.137 I print_info: vocab type       = WPM
0.00.024.137 I print_info: n_vocab          = 30522
0.00.024.137 I print_info: n_merges         = 0
0.00.024.137 I print_info: BOS token        = 101 '[CLS]'
0.00.024.137 I print_info: UNK token        = 100 '[UNK]'
0.00.024.138 I print_info: SEP token        = 102 '[SEP]'
0.00.024.141 I print_info: PAD token        = 0 '[PAD]'
0.00.024.142 I print_info: MASK token       = 103 '[MASK]'
0.00.024.142 I print_info: LF token         = 0 '[PAD]'
0.00.024.142 I print_info: max token length = 21
0.00.024.142 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.026.124 I load_tensors: offloading 12 repeating layers to GPU
0.00.026.125 I load_tensors: offloading output layer to GPU
0.00.026.125 I load_tensors: offloaded 13/13 layers to GPU
0.00.026.141 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.026.142 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.026.314 I llama_init_from_model: n_seq_max     = 1
0.00.026.315 I llama_init_from_model: n_ctx         = 512
0.00.026.315 I llama_init_from_model: n_ctx_per_seq = 512
0.00.026.315 I llama_init_from_model: n_batch       = 2048
0.00.026.316 I llama_init_from_model: n_ubatch      = 2048
0.00.026.316 I llama_init_from_model: flash_attn    = 0
0.00.026.316 I llama_init_from_model: freq_base     = 10000.0
0.00.026.317 I llama_init_from_model: freq_scale    = 1
0.00.026.317 I ggml_metal_init: allocating
0.00.026.321 I ggml_metal_init: found device: Apple M4
0.00.026.326 I ggml_metal_init: picking default device: Apple M4
0.00.026.869 I ggml_metal_init: using embedded metal library
0.00.029.393 I ggml_metal_init: GPU name:   Apple M4
0.00.029.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.029.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.029.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.029.396 I ggml_metal_init: simdgroup reduction   = true
0.00.029.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.029.396 I ggml_metal_init: has residency sets    = true
0.00.029.396 I ggml_metal_init: has bfloat            = true
0.00.029.396 I ggml_metal_init: use bfloat            = true
0.00.029.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.029.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.039.927 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.040.521 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.040.523 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.040.546 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.041.587 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.041.588 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.041.589 I llama_init_from_model: graph nodes  = 429
0.00.041.589 I llama_init_from_model: graph splits = 2
0.00.041.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.041.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.046.012 I 
0.00.046.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.046.581 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.050.933 I llama_perf_context_print:        load time =      30.59 ms
0.00.050.934 I llama_perf_context_print: prompt eval time =       4.23 ms /     9 tokens (    0.47 ms per token,  2129.17 tokens per second)
0.00.050.935 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.050.935 I llama_perf_context_print:       total time =       4.92 ms /    10 tokens
0.00.051.146 I ggml_metal_free: deallocating

real	0m0.250s
user	0m0.035s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.332 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.337 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.340 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.341 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.341 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.342 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.343 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.343 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.343 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.344 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.346 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.346 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.347 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.347 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.347 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.348 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.425 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.052 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.053 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.053 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.054 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.054 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.054 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.054 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.055 I llama_model_loader: - type  f32:  124 tensors
0.00.014.055 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.056 I print_info: file format = GGUF V3 (latest)
0.00.014.056 I print_info: file type   = Q8_0
0.00.014.057 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.264 I load: special tokens cache size = 5
0.00.017.404 I load: token to piece cache size = 0.2032 MB
0.00.017.407 I print_info: arch             = bert
0.00.017.407 I print_info: vocab_only       = 0
0.00.017.407 I print_info: n_ctx_train      = 512
0.00.017.407 I print_info: n_embd           = 384
0.00.017.408 I print_info: n_layer          = 12
0.00.017.411 I print_info: n_head           = 12
0.00.017.412 I print_info: n_head_kv        = 12
0.00.017.412 I print_info: n_rot            = 32
0.00.017.412 I print_info: n_swa            = 0
0.00.017.412 I print_info: n_embd_head_k    = 32
0.00.017.412 I print_info: n_embd_head_v    = 32
0.00.017.413 I print_info: n_gqa            = 1
0.00.017.413 I print_info: n_embd_k_gqa     = 384
0.00.017.414 I print_info: n_embd_v_gqa     = 384
0.00.017.415 I print_info: f_norm_eps       = 1.0e-12
0.00.017.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.416 I print_info: f_logit_scale    = 0.0e+00
0.00.017.416 I print_info: n_ff             = 1536
0.00.017.416 I print_info: n_expert         = 0
0.00.017.417 I print_info: n_expert_used    = 0
0.00.017.417 I print_info: causal attn      = 0
0.00.017.417 I print_info: pooling type     = 2
0.00.017.417 I print_info: rope type        = 2
0.00.017.417 I print_info: rope scaling     = linear
0.00.017.418 I print_info: freq_base_train  = 10000.0
0.00.017.418 I print_info: freq_scale_train = 1
0.00.017.419 I print_info: n_ctx_orig_yarn  = 512
0.00.017.419 I print_info: rope_finetuned   = unknown
0.00.017.421 I print_info: ssm_d_conv       = 0
0.00.017.421 I print_info: ssm_d_inner      = 0
0.00.017.421 I print_info: ssm_d_state      = 0
0.00.017.421 I print_info: ssm_dt_rank      = 0
0.00.017.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.422 I print_info: model type       = 33M
0.00.017.422 I print_info: model params     = 33.21 M
0.00.017.447 I print_info: general.name     = Bge Small
0.00.017.450 I print_info: vocab type       = WPM
0.00.017.450 I print_info: n_vocab          = 30522
0.00.017.450 I print_info: n_merges         = 0
0.00.017.450 I print_info: BOS token        = 101 '[CLS]'
0.00.017.451 I print_info: UNK token        = 100 '[UNK]'
0.00.017.451 I print_info: SEP token        = 102 '[SEP]'
0.00.017.451 I print_info: PAD token        = 0 '[PAD]'
0.00.017.451 I print_info: MASK token       = 103 '[MASK]'
0.00.017.452 I print_info: LF token         = 0 '[PAD]'
0.00.017.453 I print_info: max token length = 21
0.00.017.453 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.063 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.064 I load_tensors: offloading output layer to GPU
0.00.019.064 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.070 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.070 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.255 I llama_init_from_model: n_seq_max     = 1
0.00.019.256 I llama_init_from_model: n_ctx         = 512
0.00.019.256 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.256 I llama_init_from_model: n_batch       = 2048
0.00.019.256 I llama_init_from_model: n_ubatch      = 2048
0.00.019.256 I llama_init_from_model: flash_attn    = 0
0.00.019.257 I llama_init_from_model: freq_base     = 10000.0
0.00.019.257 I llama_init_from_model: freq_scale    = 1
0.00.019.258 I ggml_metal_init: allocating
0.00.019.264 I ggml_metal_init: found device: Apple M4
0.00.019.269 I ggml_metal_init: picking default device: Apple M4
0.00.019.799 I ggml_metal_init: using embedded metal library
0.00.022.257 I ggml_metal_init: GPU name:   Apple M4
0.00.022.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.260 I ggml_metal_init: simdgroup reduction   = true
0.00.022.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.260 I ggml_metal_init: has residency sets    = true
0.00.022.260 I ggml_metal_init: has bfloat            = true
0.00.022.261 I ggml_metal_init: use bfloat            = true
0.00.022.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.948 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.579 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.581 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.596 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.653 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.654 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.655 I llama_init_from_model: graph nodes  = 429
0.00.034.655 I llama_init_from_model: graph splits = 2
0.00.034.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.833 I 
0.00.038.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.390 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.828 I llama_perf_context_print:        load time =      29.94 ms
0.00.043.829 I llama_perf_context_print: prompt eval time =       4.32 ms /     9 tokens (    0.48 ms per token,  2082.37 tokens per second)
0.00.043.830 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.830 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.044.015 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.261 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.473 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.516 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.524 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.526 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.526 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.527 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.528 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.529 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.530 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.531 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.531 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.534 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.535 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.536 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.033 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.034 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.034 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.034 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.035 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.035 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.035 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.036 I llama_model_loader: - type  f32:   40 tensors
0.00.050.036 I llama_model_loader: - type  f16:   30 tensors
0.00.050.037 I print_info: file format = GGUF V3 (latest)
0.00.050.037 I print_info: file type   = F16
0.00.050.039 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.378 W load: empty token at index 5
0.00.059.488 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.988 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.023 I load: special tokens cache size = 5
0.00.319.237 I load: token to piece cache size = 1.5060 MB
0.00.319.244 I print_info: arch             = jina-bert-v2
0.00.319.244 I print_info: vocab_only       = 0
0.00.319.244 I print_info: n_ctx_train      = 8192
0.00.319.244 I print_info: n_embd           = 384
0.00.319.245 I print_info: n_layer          = 4
0.00.319.249 I print_info: n_head           = 12
0.00.319.250 I print_info: n_head_kv        = 12
0.00.319.250 I print_info: n_rot            = 32
0.00.319.250 I print_info: n_swa            = 0
0.00.319.250 I print_info: n_embd_head_k    = 32
0.00.319.250 I print_info: n_embd_head_v    = 32
0.00.319.251 I print_info: n_gqa            = 1
0.00.319.251 I print_info: n_embd_k_gqa     = 384
0.00.319.252 I print_info: n_embd_v_gqa     = 384
0.00.319.253 I print_info: f_norm_eps       = 1.0e-12
0.00.319.254 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.319.259 I print_info: f_clamp_kqv      = 0.0e+00
0.00.319.259 I print_info: f_max_alibi_bias = 8.0e+00
0.00.319.259 I print_info: f_logit_scale    = 0.0e+00
0.00.319.260 I print_info: n_ff             = 1536
0.00.319.261 I print_info: n_expert         = 0
0.00.319.261 I print_info: n_expert_used    = 0
0.00.319.261 I print_info: causal attn      = 0
0.00.319.261 I print_info: pooling type     = -1
0.00.319.261 I print_info: rope type        = -1
0.00.319.262 I print_info: rope scaling     = linear
0.00.319.262 I print_info: freq_base_train  = 10000.0
0.00.319.264 I print_info: freq_scale_train = 1
0.00.319.264 I print_info: n_ctx_orig_yarn  = 8192
0.00.319.264 I print_info: rope_finetuned   = unknown
0.00.319.264 I print_info: ssm_d_conv       = 0
0.00.319.264 I print_info: ssm_d_inner      = 0
0.00.319.265 I print_info: ssm_d_state      = 0
0.00.319.265 I print_info: ssm_dt_rank      = 0
0.00.319.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.319.265 I print_info: model type       = 33M
0.00.319.265 I print_info: model params     = 32.90 M
0.00.319.266 I print_info: general.name     = Jina Bert Implementation
0.00.319.266 I print_info: vocab type       = BPE
0.00.319.272 I print_info: n_vocab          = 61056
0.00.319.272 I print_info: n_merges         = 39382
0.00.319.272 I print_info: BOS token        = 0 '<s>'
0.00.319.278 I print_info: EOS token        = 2 '</s>'
0.00.319.279 I print_info: UNK token        = 3 '<unk>'
0.00.319.279 I print_info: SEP token        = 2 '</s>'
0.00.319.279 I print_info: PAD token        = 1 '<pad>'
0.00.319.279 I print_info: MASK token       = 4 '<mask>'
0.00.319.280 I print_info: EOG token        = 2 '</s>'
0.00.319.280 I print_info: max token length = 45
0.00.319.280 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.358 I load_tensors: offloading 4 repeating layers to GPU
0.00.321.359 I load_tensors: offloading output layer to GPU
0.00.321.360 I load_tensors: offloaded 5/5 layers to GPU
0.00.321.384 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.321.385 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.321.700 I llama_init_from_model: n_seq_max     = 1
0.00.321.701 I llama_init_from_model: n_ctx         = 8192
0.00.321.701 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.321.702 I llama_init_from_model: n_batch       = 2048
0.00.321.702 I llama_init_from_model: n_ubatch      = 2048
0.00.321.702 I llama_init_from_model: flash_attn    = 0
0.00.321.702 I llama_init_from_model: freq_base     = 10000.0
0.00.321.703 I llama_init_from_model: freq_scale    = 1
0.00.321.703 I ggml_metal_init: allocating
0.00.321.708 I ggml_metal_init: found device: Apple M4
0.00.321.711 I ggml_metal_init: picking default device: Apple M4
0.00.322.619 I ggml_metal_init: using embedded metal library
0.00.325.426 I ggml_metal_init: GPU name:   Apple M4
0.00.325.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.325.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.325.428 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.325.428 I ggml_metal_init: simdgroup reduction   = true
0.00.325.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.325.429 I ggml_metal_init: has residency sets    = true
0.00.325.429 I ggml_metal_init: has bfloat            = true
0.00.325.429 I ggml_metal_init: use bfloat            = true
0.00.325.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.325.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.335.173 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.338.152 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.338.154 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.338.175 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.344.019 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.344.021 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.344.021 I llama_init_from_model: graph nodes  = 154
0.00.344.021 I llama_init_from_model: graph splits = 2
0.00.344.022 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.344.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.460 I 
0.00.351.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.351.592 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.351.593 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.351.596 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.351.596 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.351.599 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.351.600 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.352.089 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.355.680 I llama_perf_context_print:        load time =     326.98 ms
0.00.355.681 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17303.94 tokens per second)
0.00.355.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.355.682 I llama_perf_context_print:       total time =       4.22 ms /    63 tokens
0.00.355.892 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.325s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.190 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.364 I main: llama backend init
0.00.000.370 I main: load the model and apply lora adapter, if any
0.00.056.311 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.068.970 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.068.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.998 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.069.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.069.014 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.069.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.069.015 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.069.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.069.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.069.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.069.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.069.022 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.076.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.087.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.607 I llama_model_loader: - type  f32:  194 tensors
0.00.087.607 I llama_model_loader: - type  f16:   98 tensors
0.00.087.609 I print_info: file format = GGUF V3 (latest)
0.00.087.614 I print_info: file type   = all F32 (guessed)
0.00.087.616 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.103.058 I load: special tokens cache size = 25
0.00.111.946 I load: token to piece cache size = 0.2984 MB
0.00.111.951 I print_info: arch             = gptneox
0.00.111.951 I print_info: vocab_only       = 0
0.00.111.951 I print_info: n_ctx_train      = 2048
0.00.111.952 I print_info: n_embd           = 2048
0.00.111.952 I print_info: n_layer          = 24
0.00.111.956 I print_info: n_head           = 16
0.00.111.957 I print_info: n_head_kv        = 16
0.00.111.957 I print_info: n_rot            = 32
0.00.111.957 I print_info: n_swa            = 0
0.00.111.958 I print_info: n_embd_head_k    = 128
0.00.111.958 I print_info: n_embd_head_v    = 128
0.00.111.959 I print_info: n_gqa            = 1
0.00.111.959 I print_info: n_embd_k_gqa     = 2048
0.00.111.960 I print_info: n_embd_v_gqa     = 2048
0.00.111.961 I print_info: f_norm_eps       = 1.0e-05
0.00.111.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.111.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.111.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.111.962 I print_info: f_logit_scale    = 0.0e+00
0.00.111.963 I print_info: n_ff             = 8192
0.00.111.963 I print_info: n_expert         = 0
0.00.111.963 I print_info: n_expert_used    = 0
0.00.111.964 I print_info: causal attn      = 1
0.00.111.964 I print_info: pooling type     = 0
0.00.111.964 I print_info: rope type        = 2
0.00.111.964 I print_info: rope scaling     = linear
0.00.111.965 I print_info: freq_base_train  = 10000.0
0.00.111.965 I print_info: freq_scale_train = 1
0.00.111.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.111.966 I print_info: rope_finetuned   = unknown
0.00.111.966 I print_info: ssm_d_conv       = 0
0.00.111.966 I print_info: ssm_d_inner      = 0
0.00.111.966 I print_info: ssm_d_state      = 0
0.00.111.966 I print_info: ssm_dt_rank      = 0
0.00.111.967 I print_info: ssm_dt_b_c_rms   = 0
0.00.111.967 I print_info: model type       = 1.4B
0.00.111.967 I print_info: model params     = 1.41 B
0.00.111.968 I print_info: general.name     = 1.4B
0.00.111.968 I print_info: vocab type       = BPE
0.00.111.969 I print_info: n_vocab          = 50304
0.00.111.969 I print_info: n_merges         = 50009
0.00.111.969 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.111.969 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.111.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.111.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.111.970 I print_info: LF token         = 187 ''
0.00.111.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.111.971 I print_info: max token length = 1024
0.00.111.971 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.178.835 I load_tensors: offloading 24 repeating layers to GPU
0.00.178.838 I load_tensors: offloading output layer to GPU
0.00.178.838 I load_tensors: offloaded 25/25 layers to GPU
0.00.178.864 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.178.865 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.179.342 I llama_init_from_model: n_seq_max     = 1
0.00.179.343 I llama_init_from_model: n_ctx         = 2048
0.00.179.343 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.179.343 I llama_init_from_model: n_batch       = 2048
0.00.179.344 I llama_init_from_model: n_ubatch      = 512
0.00.179.344 I llama_init_from_model: flash_attn    = 0
0.00.179.344 I llama_init_from_model: freq_base     = 10000.0
0.00.179.345 I llama_init_from_model: freq_scale    = 1
0.00.179.345 I ggml_metal_init: allocating
0.00.179.398 I ggml_metal_init: found device: Apple M4
0.00.179.406 I ggml_metal_init: picking default device: Apple M4
0.00.180.069 I ggml_metal_init: using embedded metal library
0.00.189.776 I ggml_metal_init: GPU name:   Apple M4
0.00.189.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.189.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.189.779 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.189.779 I ggml_metal_init: simdgroup reduction   = true
0.00.189.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.189.779 I ggml_metal_init: has residency sets    = true
0.00.189.779 I ggml_metal_init: has bfloat            = true
0.00.189.780 I ggml_metal_init: use bfloat            = true
0.00.189.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.189.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.217.671 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.246.839 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.246.847 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.246.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.250.617 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.250.620 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.250.621 I llama_init_from_model: graph nodes  = 967
0.00.250.621 I llama_init_from_model: graph splits = 2
0.00.250.626 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.250.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.250.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.316.311 I main: llama threadpool init, n_threads = 4
0.00.316.356 I 
0.00.316.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.316.392 I 
0.00.316.568 I sampler seed: 1234
0.00.316.573 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.316.598 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.316.600 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.316.600 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.143.537 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.02.143.538 I llama_perf_context_print:        load time =     259.11 ms
0.02.143.538 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.43 tokens per second)
0.02.143.540 I llama_perf_context_print:        eval time =    1780.38 ms /    63 runs   (   28.26 ms per token,    35.39 tokens per second)
0.02.143.540 I llama_perf_context_print:       total time =    1828.10 ms /    70 tokens
0.02.143.754 I ggml_metal_free: deallocating

real	0m2.542s
user	0m0.133s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.714 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.106 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.129 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.024 I llama_model_loader: - type  f32:  194 tensors
0.00.057.025 I llama_model_loader: - type  f16:   98 tensors
0.00.057.025 I print_info: file format = GGUF V3 (latest)
0.00.057.026 I print_info: file type   = all F32 (guessed)
0.00.057.032 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.920 I load: special tokens cache size = 25
0.00.075.362 I load: token to piece cache size = 0.2984 MB
0.00.075.365 I print_info: arch             = gptneox
0.00.075.365 I print_info: vocab_only       = 0
0.00.075.366 I print_info: n_ctx_train      = 2048
0.00.075.366 I print_info: n_embd           = 2048
0.00.075.366 I print_info: n_layer          = 24
0.00.075.369 I print_info: n_head           = 16
0.00.075.370 I print_info: n_head_kv        = 16
0.00.075.371 I print_info: n_rot            = 32
0.00.075.371 I print_info: n_swa            = 0
0.00.075.371 I print_info: n_embd_head_k    = 128
0.00.075.371 I print_info: n_embd_head_v    = 128
0.00.075.372 I print_info: n_gqa            = 1
0.00.075.372 I print_info: n_embd_k_gqa     = 2048
0.00.075.373 I print_info: n_embd_v_gqa     = 2048
0.00.075.374 I print_info: f_norm_eps       = 1.0e-05
0.00.075.378 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.378 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.379 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.379 I print_info: f_logit_scale    = 0.0e+00
0.00.075.381 I print_info: n_ff             = 8192
0.00.075.381 I print_info: n_expert         = 0
0.00.075.381 I print_info: n_expert_used    = 0
0.00.075.381 I print_info: causal attn      = 1
0.00.075.382 I print_info: pooling type     = 0
0.00.075.382 I print_info: rope type        = 2
0.00.075.382 I print_info: rope scaling     = linear
0.00.075.382 I print_info: freq_base_train  = 10000.0
0.00.075.383 I print_info: freq_scale_train = 1
0.00.075.383 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.383 I print_info: rope_finetuned   = unknown
0.00.075.383 I print_info: ssm_d_conv       = 0
0.00.075.383 I print_info: ssm_d_inner      = 0
0.00.075.386 I print_info: ssm_d_state      = 0
0.00.075.386 I print_info: ssm_dt_rank      = 0
0.00.075.386 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.387 I print_info: model type       = 1.4B
0.00.075.387 I print_info: model params     = 1.41 B
0.00.075.387 I print_info: general.name     = 1.4B
0.00.075.388 I print_info: vocab type       = BPE
0.00.075.388 I print_info: n_vocab          = 50304
0.00.075.388 I print_info: n_merges         = 50009
0.00.075.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.389 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.389 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.389 I print_info: LF token         = 187 ''
0.00.075.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.391 I print_info: max token length = 1024
0.00.075.391 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.479.109 I load_tensors: offloading 24 repeating layers to GPU
0.01.479.114 I load_tensors: offloading output layer to GPU
0.01.479.114 I load_tensors: offloaded 25/25 layers to GPU
0.01.479.136 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.479.138 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.480.026 I llama_init_from_model: n_seq_max     = 1
0.01.480.027 I llama_init_from_model: n_ctx         = 128
0.01.480.027 I llama_init_from_model: n_ctx_per_seq = 128
0.01.480.027 I llama_init_from_model: n_batch       = 128
0.01.480.028 I llama_init_from_model: n_ubatch      = 128
0.01.480.028 I llama_init_from_model: flash_attn    = 0
0.01.480.028 I llama_init_from_model: freq_base     = 10000.0
0.01.480.029 I llama_init_from_model: freq_scale    = 1
0.01.480.029 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.480.030 I ggml_metal_init: allocating
0.01.480.070 I ggml_metal_init: found device: Apple M4
0.01.480.076 I ggml_metal_init: picking default device: Apple M4
0.01.481.170 I ggml_metal_init: using embedded metal library
0.01.485.195 I ggml_metal_init: GPU name:   Apple M4
0.01.485.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.485.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.485.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.485.199 I ggml_metal_init: simdgroup reduction   = true
0.01.485.199 I ggml_metal_init: simdgroup matrix mul. = true
0.01.485.199 I ggml_metal_init: has residency sets    = true
0.01.485.200 I ggml_metal_init: has bfloat            = true
0.01.485.200 I ggml_metal_init: use bfloat            = true
0.01.485.200 I ggml_metal_init: hasUnifiedMemory      = true
0.01.485.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.496.565 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.498.394 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.498.397 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.498.421 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.500.074 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.500.075 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.500.075 I llama_init_from_model: graph nodes  = 967
0.01.500.076 I llama_init_from_model: graph splits = 2
0.01.500.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.500.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.535.908 I 
0.01.535.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.535.961 I perplexity: tokenizing the input ..
0.01.541.196 I perplexity: tokenization took 5.234 ms
0.01.541.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.660.899 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.662.257 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.662.294 I llama_perf_context_print:        load time =    1509.11 ms
0.01.662.296 I llama_perf_context_print: prompt eval time =     119.34 ms /   128 tokens (    0.93 ms per token,  1072.61 tokens per second)
0.01.662.296 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.662.297 I llama_perf_context_print:       total time =     126.39 ms /   129 tokens
0.01.662.658 I ggml_metal_free: deallocating

real	0m1.875s
user	0m0.099s
sys	0m0.285s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.373 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.850 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.850 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.851 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.851 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.852 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.853 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.854 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.854 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.854 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.856 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.856 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.665 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.488 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.489 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.491 I llama_model_loader: - type  f32:  194 tensors
0.00.027.491 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.492 I print_info: file format = GGUF V3 (latest)
0.00.027.492 I print_info: file type   = Q8_0
0.00.027.493 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.927 I load: special tokens cache size = 25
0.00.042.034 I load: token to piece cache size = 0.2984 MB
0.00.042.040 I print_info: arch             = gptneox
0.00.042.040 I print_info: vocab_only       = 0
0.00.042.040 I print_info: n_ctx_train      = 2048
0.00.042.041 I print_info: n_embd           = 2048
0.00.042.041 I print_info: n_layer          = 24
0.00.042.047 I print_info: n_head           = 16
0.00.042.048 I print_info: n_head_kv        = 16
0.00.042.048 I print_info: n_rot            = 32
0.00.042.048 I print_info: n_swa            = 0
0.00.042.049 I print_info: n_embd_head_k    = 128
0.00.042.049 I print_info: n_embd_head_v    = 128
0.00.042.049 I print_info: n_gqa            = 1
0.00.042.050 I print_info: n_embd_k_gqa     = 2048
0.00.042.050 I print_info: n_embd_v_gqa     = 2048
0.00.042.051 I print_info: f_norm_eps       = 1.0e-05
0.00.042.051 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.051 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.052 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.052 I print_info: f_logit_scale    = 0.0e+00
0.00.042.053 I print_info: n_ff             = 8192
0.00.042.053 I print_info: n_expert         = 0
0.00.042.053 I print_info: n_expert_used    = 0
0.00.042.053 I print_info: causal attn      = 1
0.00.042.053 I print_info: pooling type     = 0
0.00.042.053 I print_info: rope type        = 2
0.00.042.054 I print_info: rope scaling     = linear
0.00.042.054 I print_info: freq_base_train  = 10000.0
0.00.042.054 I print_info: freq_scale_train = 1
0.00.042.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.055 I print_info: rope_finetuned   = unknown
0.00.042.055 I print_info: ssm_d_conv       = 0
0.00.042.055 I print_info: ssm_d_inner      = 0
0.00.042.055 I print_info: ssm_d_state      = 0
0.00.042.055 I print_info: ssm_dt_rank      = 0
0.00.042.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.056 I print_info: model type       = 1.4B
0.00.042.056 I print_info: model params     = 1.41 B
0.00.042.056 I print_info: general.name     = 1.4B
0.00.042.057 I print_info: vocab type       = BPE
0.00.042.061 I print_info: n_vocab          = 50304
0.00.042.062 I print_info: n_merges         = 50009
0.00.042.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.062 I print_info: LF token         = 187 ''
0.00.042.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.063 I print_info: max token length = 1024
0.00.042.063 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.037.109 I load_tensors: offloading 24 repeating layers to GPU
0.01.037.112 I load_tensors: offloading output layer to GPU
0.01.037.112 I load_tensors: offloaded 25/25 layers to GPU
0.01.037.133 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.037.136 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.037.933 I llama_init_from_model: n_seq_max     = 1
0.01.037.934 I llama_init_from_model: n_ctx         = 2048
0.01.037.935 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.037.935 I llama_init_from_model: n_batch       = 2048
0.01.037.935 I llama_init_from_model: n_ubatch      = 512
0.01.037.936 I llama_init_from_model: flash_attn    = 0
0.01.037.936 I llama_init_from_model: freq_base     = 10000.0
0.01.037.937 I llama_init_from_model: freq_scale    = 1
0.01.037.938 I ggml_metal_init: allocating
0.01.037.953 I ggml_metal_init: found device: Apple M4
0.01.037.961 I ggml_metal_init: picking default device: Apple M4
0.01.039.250 I ggml_metal_init: using embedded metal library
0.01.044.504 I ggml_metal_init: GPU name:   Apple M4
0.01.044.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.044.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.044.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.044.509 I ggml_metal_init: simdgroup reduction   = true
0.01.044.509 I ggml_metal_init: simdgroup matrix mul. = true
0.01.044.509 I ggml_metal_init: has residency sets    = true
0.01.044.509 I ggml_metal_init: has bfloat            = true
0.01.044.510 I ggml_metal_init: use bfloat            = true
0.01.044.510 I ggml_metal_init: hasUnifiedMemory      = true
0.01.044.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.061.573 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.116.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.116.494 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.116.530 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.120.595 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.120.597 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.120.597 I llama_init_from_model: graph nodes  = 967
0.01.120.598 I llama_init_from_model: graph splits = 2
0.01.120.603 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.120.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.120.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.176.258 I main: llama threadpool init, n_threads = 4
0.01.176.302 I 
0.01.176.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.176.327 I 
0.01.176.475 I sampler seed: 1234
0.01.176.479 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.176.498 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.176.498 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.176.498 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.263.457 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.02.263.458 I llama_perf_context_print:        load time =    1165.15 ms
0.02.263.459 I llama_perf_context_print: prompt eval time =      49.41 ms /     7 tokens (    7.06 ms per token,   141.67 tokens per second)
0.02.263.460 I llama_perf_context_print:        eval time =    1034.72 ms /    63 runs   (   16.42 ms per token,    60.89 tokens per second)
0.02.263.462 I llama_perf_context_print:       total time =    1087.93 ms /    70 tokens
0.02.263.726 I ggml_metal_free: deallocating

real	0m2.284s
user	0m0.109s
sys	0m0.274s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.282 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.511 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.210 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.184 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.185 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.185 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.186 I llama_model_loader: - type  f32:  194 tensors
0.00.026.186 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.187 I print_info: file format = GGUF V3 (latest)
0.00.026.188 I print_info: file type   = Q8_0
0.00.026.189 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.191 I load: special tokens cache size = 25
0.00.040.160 I load: token to piece cache size = 0.2984 MB
0.00.040.165 I print_info: arch             = gptneox
0.00.040.165 I print_info: vocab_only       = 0
0.00.040.166 I print_info: n_ctx_train      = 2048
0.00.040.166 I print_info: n_embd           = 2048
0.00.040.166 I print_info: n_layer          = 24
0.00.040.170 I print_info: n_head           = 16
0.00.040.171 I print_info: n_head_kv        = 16
0.00.040.171 I print_info: n_rot            = 32
0.00.040.171 I print_info: n_swa            = 0
0.00.040.172 I print_info: n_embd_head_k    = 128
0.00.040.172 I print_info: n_embd_head_v    = 128
0.00.040.173 I print_info: n_gqa            = 1
0.00.040.173 I print_info: n_embd_k_gqa     = 2048
0.00.040.174 I print_info: n_embd_v_gqa     = 2048
0.00.040.175 I print_info: f_norm_eps       = 1.0e-05
0.00.040.175 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.175 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.177 I print_info: f_logit_scale    = 0.0e+00
0.00.040.177 I print_info: n_ff             = 8192
0.00.040.177 I print_info: n_expert         = 0
0.00.040.178 I print_info: n_expert_used    = 0
0.00.040.178 I print_info: causal attn      = 1
0.00.040.178 I print_info: pooling type     = 0
0.00.040.178 I print_info: rope type        = 2
0.00.040.178 I print_info: rope scaling     = linear
0.00.040.179 I print_info: freq_base_train  = 10000.0
0.00.040.179 I print_info: freq_scale_train = 1
0.00.040.179 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.181 I print_info: rope_finetuned   = unknown
0.00.040.181 I print_info: ssm_d_conv       = 0
0.00.040.181 I print_info: ssm_d_inner      = 0
0.00.040.181 I print_info: ssm_d_state      = 0
0.00.040.181 I print_info: ssm_dt_rank      = 0
0.00.040.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.182 I print_info: model type       = 1.4B
0.00.040.182 I print_info: model params     = 1.41 B
0.00.040.182 I print_info: general.name     = 1.4B
0.00.040.183 I print_info: vocab type       = BPE
0.00.040.183 I print_info: n_vocab          = 50304
0.00.040.183 I print_info: n_merges         = 50009
0.00.040.184 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.184 I print_info: LF token         = 187 ''
0.00.040.185 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.185 I print_info: max token length = 1024
0.00.040.185 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.922.530 I load_tensors: offloading 24 repeating layers to GPU
0.00.922.537 I load_tensors: offloading output layer to GPU
0.00.922.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.922.567 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.922.569 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.923.752 I llama_init_from_model: n_seq_max     = 1
0.00.923.754 I llama_init_from_model: n_ctx         = 128
0.00.923.755 I llama_init_from_model: n_ctx_per_seq = 128
0.00.923.755 I llama_init_from_model: n_batch       = 128
0.00.923.755 I llama_init_from_model: n_ubatch      = 128
0.00.923.756 I llama_init_from_model: flash_attn    = 0
0.00.923.757 I llama_init_from_model: freq_base     = 10000.0
0.00.923.757 I llama_init_from_model: freq_scale    = 1
0.00.923.758 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.923.760 I ggml_metal_init: allocating
0.00.923.823 I ggml_metal_init: found device: Apple M4
0.00.923.834 I ggml_metal_init: picking default device: Apple M4
0.00.925.213 I ggml_metal_init: using embedded metal library
0.00.931.030 I ggml_metal_init: GPU name:   Apple M4
0.00.931.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.931.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.931.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.931.035 I ggml_metal_init: simdgroup reduction   = true
0.00.931.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.931.035 I ggml_metal_init: has residency sets    = true
0.00.931.035 I ggml_metal_init: has bfloat            = true
0.00.931.036 I ggml_metal_init: use bfloat            = true
0.00.931.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.931.040 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.947.131 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.950.516 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.950.521 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.950.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.953.650 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.953.652 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.953.652 I llama_init_from_model: graph nodes  = 967
0.00.953.653 I llama_init_from_model: graph splits = 2
0.00.953.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.953.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.980.898 I 
0.00.980.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.980.955 I perplexity: tokenizing the input ..
0.00.987.973 I perplexity: tokenization took 7.014 ms
0.00.987.985 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.127.126 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.128.429 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.128.451 I llama_perf_context_print:        load time =     970.38 ms
0.01.128.452 I llama_perf_context_print: prompt eval time =     138.08 ms /   128 tokens (    1.08 ms per token,   926.99 tokens per second)
0.01.128.453 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.128.453 I llama_perf_context_print:       total time =     147.55 ms /   129 tokens
0.01.128.860 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.078s
sys	0m0.187s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.010.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.119 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.121 I llama_model_loader: - type  f32:  194 tensors
0.00.027.121 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.121 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.122 I print_info: file format = GGUF V3 (latest)
0.00.027.123 I print_info: file type   = Q4_0
0.00.027.124 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.107 I load: special tokens cache size = 25
0.00.040.847 I load: token to piece cache size = 0.2984 MB
0.00.040.851 I print_info: arch             = gptneox
0.00.040.851 I print_info: vocab_only       = 0
0.00.040.851 I print_info: n_ctx_train      = 2048
0.00.040.852 I print_info: n_embd           = 2048
0.00.040.852 I print_info: n_layer          = 24
0.00.040.856 I print_info: n_head           = 16
0.00.040.857 I print_info: n_head_kv        = 16
0.00.040.857 I print_info: n_rot            = 32
0.00.040.857 I print_info: n_swa            = 0
0.00.040.857 I print_info: n_embd_head_k    = 128
0.00.040.860 I print_info: n_embd_head_v    = 128
0.00.040.861 I print_info: n_gqa            = 1
0.00.040.863 I print_info: n_embd_k_gqa     = 2048
0.00.040.863 I print_info: n_embd_v_gqa     = 2048
0.00.040.864 I print_info: f_norm_eps       = 1.0e-05
0.00.040.864 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.864 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.866 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.866 I print_info: f_logit_scale    = 0.0e+00
0.00.040.867 I print_info: n_ff             = 8192
0.00.040.867 I print_info: n_expert         = 0
0.00.040.867 I print_info: n_expert_used    = 0
0.00.040.867 I print_info: causal attn      = 1
0.00.040.867 I print_info: pooling type     = 0
0.00.040.868 I print_info: rope type        = 2
0.00.040.868 I print_info: rope scaling     = linear
0.00.040.868 I print_info: freq_base_train  = 10000.0
0.00.040.868 I print_info: freq_scale_train = 1
0.00.040.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.869 I print_info: rope_finetuned   = unknown
0.00.040.869 I print_info: ssm_d_conv       = 0
0.00.040.869 I print_info: ssm_d_inner      = 0
0.00.040.869 I print_info: ssm_d_state      = 0
0.00.040.869 I print_info: ssm_dt_rank      = 0
0.00.040.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.870 I print_info: model type       = 1.4B
0.00.040.871 I print_info: model params     = 1.41 B
0.00.040.871 I print_info: general.name     = 1.4B
0.00.040.873 I print_info: vocab type       = BPE
0.00.040.873 I print_info: n_vocab          = 50304
0.00.040.873 I print_info: n_merges         = 50009
0.00.040.878 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.879 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.879 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.879 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.880 I print_info: LF token         = 187 ''
0.00.040.882 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.882 I print_info: max token length = 1024
0.00.040.883 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.147 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.163 I load_tensors: offloading output layer to GPU
0.00.600.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.199 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.600.203 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.601.927 I llama_init_from_model: n_seq_max     = 1
0.00.601.930 I llama_init_from_model: n_ctx         = 2048
0.00.601.930 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.931 I llama_init_from_model: n_batch       = 2048
0.00.601.931 I llama_init_from_model: n_ubatch      = 512
0.00.601.932 I llama_init_from_model: flash_attn    = 0
0.00.601.934 I llama_init_from_model: freq_base     = 10000.0
0.00.601.935 I llama_init_from_model: freq_scale    = 1
0.00.601.937 I ggml_metal_init: allocating
0.00.602.016 I ggml_metal_init: found device: Apple M4
0.00.602.029 I ggml_metal_init: picking default device: Apple M4
0.00.603.972 I ggml_metal_init: using embedded metal library
0.00.609.631 I ggml_metal_init: GPU name:   Apple M4
0.00.609.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.638 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.639 I ggml_metal_init: simdgroup reduction   = true
0.00.609.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.640 I ggml_metal_init: has residency sets    = true
0.00.609.640 I ggml_metal_init: has bfloat            = true
0.00.609.641 I ggml_metal_init: use bfloat            = true
0.00.609.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.632 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.183 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.682.190 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.682.226 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.358 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.360 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.361 I llama_init_from_model: graph nodes  = 967
0.00.686.361 I llama_init_from_model: graph splits = 2
0.00.686.367 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.490 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.491 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.572 I main: llama threadpool init, n_threads = 4
0.00.739.616 I 
0.00.739.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.642 I 
0.00.739.820 I sampler seed: 1234
0.00.739.824 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.849 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.850 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.850 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.418.639 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.418.640 I llama_perf_context_print:        load time =     727.93 ms
0.01.418.641 I llama_perf_context_print: prompt eval time =      49.04 ms /     7 tokens (    7.01 ms per token,   142.74 tokens per second)
0.01.418.641 I llama_perf_context_print:        eval time =     626.87 ms /    63 runs   (    9.95 ms per token,   100.50 tokens per second)
0.01.418.642 I llama_perf_context_print:       total time =     679.79 ms /    70 tokens
0.01.418.868 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.292 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.109 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.354 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.148 I llama_model_loader: - type  f32:  194 tensors
0.00.026.148 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.150 I print_info: file format = GGUF V3 (latest)
0.00.026.150 I print_info: file type   = Q4_0
0.00.026.151 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.171 I load: special tokens cache size = 25
0.00.040.140 I load: token to piece cache size = 0.2984 MB
0.00.040.145 I print_info: arch             = gptneox
0.00.040.145 I print_info: vocab_only       = 0
0.00.040.145 I print_info: n_ctx_train      = 2048
0.00.040.145 I print_info: n_embd           = 2048
0.00.040.146 I print_info: n_layer          = 24
0.00.040.150 I print_info: n_head           = 16
0.00.040.151 I print_info: n_head_kv        = 16
0.00.040.151 I print_info: n_rot            = 32
0.00.040.151 I print_info: n_swa            = 0
0.00.040.151 I print_info: n_embd_head_k    = 128
0.00.040.151 I print_info: n_embd_head_v    = 128
0.00.040.152 I print_info: n_gqa            = 1
0.00.040.153 I print_info: n_embd_k_gqa     = 2048
0.00.040.153 I print_info: n_embd_v_gqa     = 2048
0.00.040.154 I print_info: f_norm_eps       = 1.0e-05
0.00.040.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.155 I print_info: f_logit_scale    = 0.0e+00
0.00.040.156 I print_info: n_ff             = 8192
0.00.040.156 I print_info: n_expert         = 0
0.00.040.156 I print_info: n_expert_used    = 0
0.00.040.156 I print_info: causal attn      = 1
0.00.040.156 I print_info: pooling type     = 0
0.00.040.156 I print_info: rope type        = 2
0.00.040.157 I print_info: rope scaling     = linear
0.00.040.157 I print_info: freq_base_train  = 10000.0
0.00.040.157 I print_info: freq_scale_train = 1
0.00.040.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.157 I print_info: rope_finetuned   = unknown
0.00.040.158 I print_info: ssm_d_conv       = 0
0.00.040.158 I print_info: ssm_d_inner      = 0
0.00.040.158 I print_info: ssm_d_state      = 0
0.00.040.158 I print_info: ssm_dt_rank      = 0
0.00.040.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.158 I print_info: model type       = 1.4B
0.00.040.158 I print_info: model params     = 1.41 B
0.00.040.159 I print_info: general.name     = 1.4B
0.00.040.159 I print_info: vocab type       = BPE
0.00.040.159 I print_info: n_vocab          = 50304
0.00.040.159 I print_info: n_merges         = 50009
0.00.040.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: LF token         = 187 ''
0.00.040.161 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: max token length = 1024
0.00.040.161 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.550 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.573 I load_tensors: offloading output layer to GPU
0.00.593.573 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.609 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.593.610 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.594.991 I llama_init_from_model: n_seq_max     = 1
0.00.594.994 I llama_init_from_model: n_ctx         = 128
0.00.594.995 I llama_init_from_model: n_ctx_per_seq = 128
0.00.594.995 I llama_init_from_model: n_batch       = 128
0.00.594.995 I llama_init_from_model: n_ubatch      = 128
0.00.594.996 I llama_init_from_model: flash_attn    = 0
0.00.594.998 I llama_init_from_model: freq_base     = 10000.0
0.00.594.999 I llama_init_from_model: freq_scale    = 1
0.00.594.999 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.002 I ggml_metal_init: allocating
0.00.595.088 I ggml_metal_init: found device: Apple M4
0.00.595.102 I ggml_metal_init: picking default device: Apple M4
0.00.597.037 I ggml_metal_init: using embedded metal library
0.00.602.941 I ggml_metal_init: GPU name:   Apple M4
0.00.602.967 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.968 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.969 I ggml_metal_init: simdgroup reduction   = true
0.00.602.969 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.970 I ggml_metal_init: has residency sets    = true
0.00.602.970 I ggml_metal_init: has bfloat            = true
0.00.602.971 I ggml_metal_init: use bfloat            = true
0.00.602.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.987 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.530 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.536 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.578 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.860 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.630.862 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.630.863 I llama_init_from_model: graph nodes  = 967
0.00.630.863 I llama_init_from_model: graph splits = 2
0.00.630.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.522 I 
0.00.654.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.584 I perplexity: tokenizing the input ..
0.00.660.908 I perplexity: tokenization took 6.322 ms
0.00.660.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.207 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.794.554 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.794.581 I llama_perf_context_print:        load time =     644.40 ms
0.00.794.582 I llama_perf_context_print: prompt eval time =     131.94 ms /   128 tokens (    1.03 ms per token,   970.14 tokens per second)
0.00.794.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.583 I llama_perf_context_print:       total time =     140.06 ms /   129 tokens
0.00.794.959 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.081s
sys	0m0.132s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.877 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.459 I llama_model_loader: - type  f32:  194 tensors
0.00.025.459 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.460 I print_info: file format = GGUF V3 (latest)
0.00.025.460 I print_info: file type   = Q4_1
0.00.025.461 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.228 I load: special tokens cache size = 25
0.00.039.183 I load: token to piece cache size = 0.2984 MB
0.00.039.186 I print_info: arch             = gptneox
0.00.039.186 I print_info: vocab_only       = 0
0.00.039.186 I print_info: n_ctx_train      = 2048
0.00.039.186 I print_info: n_embd           = 2048
0.00.039.187 I print_info: n_layer          = 24
0.00.039.189 I print_info: n_head           = 16
0.00.039.190 I print_info: n_head_kv        = 16
0.00.039.190 I print_info: n_rot            = 32
0.00.039.191 I print_info: n_swa            = 0
0.00.039.191 I print_info: n_embd_head_k    = 128
0.00.039.191 I print_info: n_embd_head_v    = 128
0.00.039.192 I print_info: n_gqa            = 1
0.00.039.192 I print_info: n_embd_k_gqa     = 2048
0.00.039.193 I print_info: n_embd_v_gqa     = 2048
0.00.039.194 I print_info: f_norm_eps       = 1.0e-05
0.00.039.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.195 I print_info: f_logit_scale    = 0.0e+00
0.00.039.195 I print_info: n_ff             = 8192
0.00.039.196 I print_info: n_expert         = 0
0.00.039.196 I print_info: n_expert_used    = 0
0.00.039.196 I print_info: causal attn      = 1
0.00.039.196 I print_info: pooling type     = 0
0.00.039.196 I print_info: rope type        = 2
0.00.039.197 I print_info: rope scaling     = linear
0.00.039.197 I print_info: freq_base_train  = 10000.0
0.00.039.197 I print_info: freq_scale_train = 1
0.00.039.198 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.198 I print_info: rope_finetuned   = unknown
0.00.039.198 I print_info: ssm_d_conv       = 0
0.00.039.199 I print_info: ssm_d_inner      = 0
0.00.039.199 I print_info: ssm_d_state      = 0
0.00.039.199 I print_info: ssm_dt_rank      = 0
0.00.039.199 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.200 I print_info: model type       = 1.4B
0.00.039.200 I print_info: model params     = 1.41 B
0.00.039.200 I print_info: general.name     = 1.4B
0.00.039.201 I print_info: vocab type       = BPE
0.00.039.201 I print_info: n_vocab          = 50304
0.00.039.201 I print_info: n_merges         = 50009
0.00.039.201 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.202 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.202 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.202 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.202 I print_info: LF token         = 187 ''
0.00.039.202 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.203 I print_info: max token length = 1024
0.00.039.203 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.293 I load_tensors: offloading output layer to GPU
0.00.648.294 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.323 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.648.324 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.649.713 I llama_init_from_model: n_seq_max     = 1
0.00.649.719 I llama_init_from_model: n_ctx         = 2048
0.00.649.719 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.720 I llama_init_from_model: n_batch       = 2048
0.00.649.721 I llama_init_from_model: n_ubatch      = 512
0.00.649.721 I llama_init_from_model: flash_attn    = 0
0.00.649.722 I llama_init_from_model: freq_base     = 10000.0
0.00.649.722 I llama_init_from_model: freq_scale    = 1
0.00.649.724 I ggml_metal_init: allocating
0.00.649.776 I ggml_metal_init: found device: Apple M4
0.00.649.789 I ggml_metal_init: picking default device: Apple M4
0.00.651.607 I ggml_metal_init: using embedded metal library
0.00.657.933 I ggml_metal_init: GPU name:   Apple M4
0.00.657.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.941 I ggml_metal_init: simdgroup reduction   = true
0.00.657.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.942 I ggml_metal_init: has residency sets    = true
0.00.657.942 I ggml_metal_init: has bfloat            = true
0.00.657.943 I ggml_metal_init: use bfloat            = true
0.00.657.944 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.144 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.739.835 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.871 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.744.592 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.744.594 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.744.594 I llama_init_from_model: graph nodes  = 967
0.00.744.595 I llama_init_from_model: graph splits = 2
0.00.744.601 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.407 I main: llama threadpool init, n_threads = 4
0.00.800.450 I 
0.00.800.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.476 I 
0.00.800.628 I sampler seed: 1234
0.00.800.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.664 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.666 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.666 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.525.967 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.525.968 I llama_perf_context_print:        load time =     790.75 ms
0.01.525.969 I llama_perf_context_print: prompt eval time =      48.87 ms /     7 tokens (    6.98 ms per token,   143.23 tokens per second)
0.01.525.970 I llama_perf_context_print:        eval time =     674.11 ms /    63 runs   (   10.70 ms per token,    93.46 tokens per second)
0.01.525.970 I llama_perf_context_print:       total time =     726.30 ms /    70 tokens
0.01.526.243 I ggml_metal_free: deallocating

real	0m1.543s
user	0m0.113s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.397 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.642 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.642 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.643 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.645 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.645 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.646 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.648 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.648 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.648 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.581 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.300 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.301 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.303 I llama_model_loader: - type  f32:  194 tensors
0.00.025.303 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.304 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.305 I print_info: file format = GGUF V3 (latest)
0.00.025.305 I print_info: file type   = Q4_1
0.00.025.306 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.313 I load: special tokens cache size = 25
0.00.039.371 I load: token to piece cache size = 0.2984 MB
0.00.039.375 I print_info: arch             = gptneox
0.00.039.375 I print_info: vocab_only       = 0
0.00.039.376 I print_info: n_ctx_train      = 2048
0.00.039.376 I print_info: n_embd           = 2048
0.00.039.376 I print_info: n_layer          = 24
0.00.039.380 I print_info: n_head           = 16
0.00.039.381 I print_info: n_head_kv        = 16
0.00.039.381 I print_info: n_rot            = 32
0.00.039.382 I print_info: n_swa            = 0
0.00.039.382 I print_info: n_embd_head_k    = 128
0.00.039.384 I print_info: n_embd_head_v    = 128
0.00.039.385 I print_info: n_gqa            = 1
0.00.039.385 I print_info: n_embd_k_gqa     = 2048
0.00.039.388 I print_info: n_embd_v_gqa     = 2048
0.00.039.388 I print_info: f_norm_eps       = 1.0e-05
0.00.039.389 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.389 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.389 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.389 I print_info: f_logit_scale    = 0.0e+00
0.00.039.390 I print_info: n_ff             = 8192
0.00.039.390 I print_info: n_expert         = 0
0.00.039.390 I print_info: n_expert_used    = 0
0.00.039.390 I print_info: causal attn      = 1
0.00.039.390 I print_info: pooling type     = 0
0.00.039.391 I print_info: rope type        = 2
0.00.039.392 I print_info: rope scaling     = linear
0.00.039.392 I print_info: freq_base_train  = 10000.0
0.00.039.392 I print_info: freq_scale_train = 1
0.00.039.392 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.393 I print_info: rope_finetuned   = unknown
0.00.039.393 I print_info: ssm_d_conv       = 0
0.00.039.393 I print_info: ssm_d_inner      = 0
0.00.039.393 I print_info: ssm_d_state      = 0
0.00.039.393 I print_info: ssm_dt_rank      = 0
0.00.039.393 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.394 I print_info: model type       = 1.4B
0.00.039.394 I print_info: model params     = 1.41 B
0.00.039.394 I print_info: general.name     = 1.4B
0.00.039.394 I print_info: vocab type       = BPE
0.00.039.395 I print_info: n_vocab          = 50304
0.00.039.395 I print_info: n_merges         = 50009
0.00.039.395 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.396 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.396 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.396 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.396 I print_info: LF token         = 187 ''
0.00.039.397 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.397 I print_info: max token length = 1024
0.00.039.397 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.334 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.354 I load_tensors: offloading output layer to GPU
0.00.645.355 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.391 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.645.393 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.646.871 I llama_init_from_model: n_seq_max     = 1
0.00.646.875 I llama_init_from_model: n_ctx         = 128
0.00.646.875 I llama_init_from_model: n_ctx_per_seq = 128
0.00.646.876 I llama_init_from_model: n_batch       = 128
0.00.646.876 I llama_init_from_model: n_ubatch      = 128
0.00.646.877 I llama_init_from_model: flash_attn    = 0
0.00.646.879 I llama_init_from_model: freq_base     = 10000.0
0.00.646.880 I llama_init_from_model: freq_scale    = 1
0.00.646.881 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.646.883 I ggml_metal_init: allocating
0.00.646.967 I ggml_metal_init: found device: Apple M4
0.00.646.982 I ggml_metal_init: picking default device: Apple M4
0.00.648.879 I ggml_metal_init: using embedded metal library
0.00.654.666 I ggml_metal_init: GPU name:   Apple M4
0.00.654.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.681 I ggml_metal_init: simdgroup reduction   = true
0.00.654.681 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.682 I ggml_metal_init: has residency sets    = true
0.00.654.682 I ggml_metal_init: has bfloat            = true
0.00.654.682 I ggml_metal_init: use bfloat            = true
0.00.654.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.689 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.415 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.906 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.677.914 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.956 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.263 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.681.265 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.681.266 I llama_init_from_model: graph nodes  = 967
0.00.681.266 I llama_init_from_model: graph splits = 2
0.00.681.270 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.681.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.652 I 
0.00.707.715 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.721 I perplexity: tokenizing the input ..
0.00.714.607 I perplexity: tokenization took 6.883 ms
0.00.714.618 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.983 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.847.347 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.847.371 I llama_perf_context_print:        load time =     698.25 ms
0.00.847.373 I llama_perf_context_print: prompt eval time =     130.37 ms /   128 tokens (    1.02 ms per token,   981.86 tokens per second)
0.00.847.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.374 I llama_perf_context_print:       total time =     139.72 ms /   129 tokens
0.00.847.744 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.081s
sys	0m0.132s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.112 I main: llama backend init
0.00.000.114 I main: load the model and apply lora adapter, if any
0.00.010.738 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.520 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.523 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.524 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.524 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.524 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.525 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.238 I llama_model_loader: - type  f32:  194 tensors
0.00.029.238 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.238 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.239 I print_info: file format = GGUF V3 (latest)
0.00.029.240 I print_info: file type   = Q5_0
0.00.029.241 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.465 I load: special tokens cache size = 25
0.00.043.670 I load: token to piece cache size = 0.2984 MB
0.00.043.675 I print_info: arch             = gptneox
0.00.043.675 I print_info: vocab_only       = 0
0.00.043.675 I print_info: n_ctx_train      = 2048
0.00.043.675 I print_info: n_embd           = 2048
0.00.043.676 I print_info: n_layer          = 24
0.00.043.680 I print_info: n_head           = 16
0.00.043.681 I print_info: n_head_kv        = 16
0.00.043.681 I print_info: n_rot            = 32
0.00.043.681 I print_info: n_swa            = 0
0.00.043.684 I print_info: n_embd_head_k    = 128
0.00.043.684 I print_info: n_embd_head_v    = 128
0.00.043.684 I print_info: n_gqa            = 1
0.00.043.686 I print_info: n_embd_k_gqa     = 2048
0.00.043.686 I print_info: n_embd_v_gqa     = 2048
0.00.043.687 I print_info: f_norm_eps       = 1.0e-05
0.00.043.687 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.687 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.688 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.688 I print_info: f_logit_scale    = 0.0e+00
0.00.043.688 I print_info: n_ff             = 8192
0.00.043.689 I print_info: n_expert         = 0
0.00.043.689 I print_info: n_expert_used    = 0
0.00.043.689 I print_info: causal attn      = 1
0.00.043.689 I print_info: pooling type     = 0
0.00.043.691 I print_info: rope type        = 2
0.00.043.692 I print_info: rope scaling     = linear
0.00.043.693 I print_info: freq_base_train  = 10000.0
0.00.043.693 I print_info: freq_scale_train = 1
0.00.043.693 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.693 I print_info: rope_finetuned   = unknown
0.00.043.693 I print_info: ssm_d_conv       = 0
0.00.043.693 I print_info: ssm_d_inner      = 0
0.00.043.694 I print_info: ssm_d_state      = 0
0.00.043.694 I print_info: ssm_dt_rank      = 0
0.00.043.694 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.694 I print_info: model type       = 1.4B
0.00.043.694 I print_info: model params     = 1.41 B
0.00.043.695 I print_info: general.name     = 1.4B
0.00.043.695 I print_info: vocab type       = BPE
0.00.043.695 I print_info: n_vocab          = 50304
0.00.043.695 I print_info: n_merges         = 50009
0.00.043.696 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.696 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.696 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.696 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.696 I print_info: LF token         = 187 ''
0.00.043.696 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.697 I print_info: max token length = 1024
0.00.043.697 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.723.598 I load_tensors: offloading 24 repeating layers to GPU
0.00.723.607 I load_tensors: offloading output layer to GPU
0.00.723.608 I load_tensors: offloaded 25/25 layers to GPU
0.00.723.628 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.723.629 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.724.534 I llama_init_from_model: n_seq_max     = 1
0.00.724.540 I llama_init_from_model: n_ctx         = 2048
0.00.724.540 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.724.540 I llama_init_from_model: n_batch       = 2048
0.00.724.541 I llama_init_from_model: n_ubatch      = 512
0.00.724.541 I llama_init_from_model: flash_attn    = 0
0.00.724.542 I llama_init_from_model: freq_base     = 10000.0
0.00.724.543 I llama_init_from_model: freq_scale    = 1
0.00.724.544 I ggml_metal_init: allocating
0.00.724.580 I ggml_metal_init: found device: Apple M4
0.00.724.588 I ggml_metal_init: picking default device: Apple M4
0.00.725.703 I ggml_metal_init: using embedded metal library
0.00.730.096 I ggml_metal_init: GPU name:   Apple M4
0.00.730.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.730.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.730.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.730.102 I ggml_metal_init: simdgroup reduction   = true
0.00.730.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.730.102 I ggml_metal_init: has residency sets    = true
0.00.730.102 I ggml_metal_init: has bfloat            = true
0.00.730.103 I ggml_metal_init: use bfloat            = true
0.00.730.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.730.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.745.205 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.776.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.776.638 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.776.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.781.373 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.781.375 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.781.376 I llama_init_from_model: graph nodes  = 967
0.00.781.376 I llama_init_from_model: graph splits = 2
0.00.781.385 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.781.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.781.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.839.957 I main: llama threadpool init, n_threads = 4
0.00.840.001 I 
0.00.840.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.840.023 I 
0.00.840.176 I sampler seed: 1234
0.00.840.181 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.840.220 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.840.223 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.840.223 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.620.536 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.620.537 I llama_perf_context_print:        load time =     828.51 ms
0.01.620.538 I llama_perf_context_print: prompt eval time =      42.99 ms /     7 tokens (    6.14 ms per token,   162.82 tokens per second)
0.01.620.538 I llama_perf_context_print:        eval time =     734.66 ms /    63 runs   (   11.66 ms per token,    85.75 tokens per second)
0.01.620.539 I llama_perf_context_print:       total time =     781.29 ms /    70 tokens
0.01.620.744 I ggml_metal_free: deallocating

real	0m1.639s
user	0m0.106s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.048 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.051 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.051 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.052 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.052 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.054 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.054 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.055 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.693 I llama_model_loader: - type  f32:  194 tensors
0.00.027.693 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.693 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.694 I print_info: file format = GGUF V3 (latest)
0.00.027.695 I print_info: file type   = Q5_0
0.00.027.696 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.732 I load: special tokens cache size = 25
0.00.041.744 I load: token to piece cache size = 0.2984 MB
0.00.041.750 I print_info: arch             = gptneox
0.00.041.750 I print_info: vocab_only       = 0
0.00.041.750 I print_info: n_ctx_train      = 2048
0.00.041.750 I print_info: n_embd           = 2048
0.00.041.750 I print_info: n_layer          = 24
0.00.041.755 I print_info: n_head           = 16
0.00.041.756 I print_info: n_head_kv        = 16
0.00.041.756 I print_info: n_rot            = 32
0.00.041.756 I print_info: n_swa            = 0
0.00.041.756 I print_info: n_embd_head_k    = 128
0.00.041.756 I print_info: n_embd_head_v    = 128
0.00.041.757 I print_info: n_gqa            = 1
0.00.041.758 I print_info: n_embd_k_gqa     = 2048
0.00.041.761 I print_info: n_embd_v_gqa     = 2048
0.00.041.762 I print_info: f_norm_eps       = 1.0e-05
0.00.041.762 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.762 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.762 I print_info: f_logit_scale    = 0.0e+00
0.00.041.763 I print_info: n_ff             = 8192
0.00.041.763 I print_info: n_expert         = 0
0.00.041.763 I print_info: n_expert_used    = 0
0.00.041.763 I print_info: causal attn      = 1
0.00.041.765 I print_info: pooling type     = 0
0.00.041.765 I print_info: rope type        = 2
0.00.041.765 I print_info: rope scaling     = linear
0.00.041.766 I print_info: freq_base_train  = 10000.0
0.00.041.766 I print_info: freq_scale_train = 1
0.00.041.767 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.767 I print_info: rope_finetuned   = unknown
0.00.041.767 I print_info: ssm_d_conv       = 0
0.00.041.767 I print_info: ssm_d_inner      = 0
0.00.041.768 I print_info: ssm_d_state      = 0
0.00.041.768 I print_info: ssm_dt_rank      = 0
0.00.041.768 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.768 I print_info: model type       = 1.4B
0.00.041.768 I print_info: model params     = 1.41 B
0.00.041.768 I print_info: general.name     = 1.4B
0.00.041.769 I print_info: vocab type       = BPE
0.00.041.769 I print_info: n_vocab          = 50304
0.00.041.769 I print_info: n_merges         = 50009
0.00.041.769 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.769 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: LF token         = 187 ''
0.00.041.770 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.770 I print_info: max token length = 1024
0.00.041.774 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.735.297 I load_tensors: offloading 24 repeating layers to GPU
0.00.735.322 I load_tensors: offloading output layer to GPU
0.00.735.323 I load_tensors: offloaded 25/25 layers to GPU
0.00.735.358 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.735.360 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.736.753 I llama_init_from_model: n_seq_max     = 1
0.00.736.760 I llama_init_from_model: n_ctx         = 128
0.00.736.760 I llama_init_from_model: n_ctx_per_seq = 128
0.00.736.761 I llama_init_from_model: n_batch       = 128
0.00.736.761 I llama_init_from_model: n_ubatch      = 128
0.00.736.762 I llama_init_from_model: flash_attn    = 0
0.00.736.764 I llama_init_from_model: freq_base     = 10000.0
0.00.736.765 I llama_init_from_model: freq_scale    = 1
0.00.736.765 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.736.768 I ggml_metal_init: allocating
0.00.736.855 I ggml_metal_init: found device: Apple M4
0.00.736.871 I ggml_metal_init: picking default device: Apple M4
0.00.738.769 I ggml_metal_init: using embedded metal library
0.00.745.804 I ggml_metal_init: GPU name:   Apple M4
0.00.745.818 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.745.819 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.745.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.745.820 I ggml_metal_init: simdgroup reduction   = true
0.00.745.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.745.820 I ggml_metal_init: has residency sets    = true
0.00.745.821 I ggml_metal_init: has bfloat            = true
0.00.745.821 I ggml_metal_init: use bfloat            = true
0.00.745.822 I ggml_metal_init: hasUnifiedMemory      = true
0.00.745.827 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.763.918 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.767.305 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.767.308 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.767.349 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.770.090 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.770.092 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.770.092 I llama_init_from_model: graph nodes  = 967
0.00.770.093 I llama_init_from_model: graph splits = 2
0.00.770.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.770.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.309 I 
0.00.798.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.371 I perplexity: tokenizing the input ..
0.00.805.522 I perplexity: tokenization took 7.146 ms
0.00.805.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.952.314 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.953.650 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.953.691 I llama_perf_context_print:        load time =     786.43 ms
0.00.953.692 I llama_perf_context_print: prompt eval time =     145.79 ms /   128 tokens (    1.14 ms per token,   877.96 tokens per second)
0.00.953.693 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.953.693 I llama_perf_context_print:       total time =     155.38 ms /   129 tokens
0.00.954.182 I ggml_metal_free: deallocating

real	0m0.971s
user	0m0.081s
sys	0m0.157s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.011.692 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.030.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.040.818 I llama_model_loader: - type  f32:  194 tensors
0.00.040.818 I llama_model_loader: - type q5_1:   97 tensors
0.00.040.818 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.819 I print_info: file format = GGUF V3 (latest)
0.00.040.819 I print_info: file type   = Q5_1
0.00.040.820 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.050.216 I load: special tokens cache size = 25
0.00.057.967 I load: token to piece cache size = 0.2984 MB
0.00.057.970 I print_info: arch             = gptneox
0.00.057.970 I print_info: vocab_only       = 0
0.00.057.970 I print_info: n_ctx_train      = 2048
0.00.057.971 I print_info: n_embd           = 2048
0.00.057.971 I print_info: n_layer          = 24
0.00.057.973 I print_info: n_head           = 16
0.00.057.974 I print_info: n_head_kv        = 16
0.00.057.975 I print_info: n_rot            = 32
0.00.057.975 I print_info: n_swa            = 0
0.00.057.975 I print_info: n_embd_head_k    = 128
0.00.057.975 I print_info: n_embd_head_v    = 128
0.00.057.976 I print_info: n_gqa            = 1
0.00.057.977 I print_info: n_embd_k_gqa     = 2048
0.00.057.978 I print_info: n_embd_v_gqa     = 2048
0.00.057.978 I print_info: f_norm_eps       = 1.0e-05
0.00.057.979 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.979 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.979 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.979 I print_info: f_logit_scale    = 0.0e+00
0.00.057.980 I print_info: n_ff             = 8192
0.00.057.980 I print_info: n_expert         = 0
0.00.057.980 I print_info: n_expert_used    = 0
0.00.057.980 I print_info: causal attn      = 1
0.00.057.981 I print_info: pooling type     = 0
0.00.057.982 I print_info: rope type        = 2
0.00.057.984 I print_info: rope scaling     = linear
0.00.057.985 I print_info: freq_base_train  = 10000.0
0.00.057.985 I print_info: freq_scale_train = 1
0.00.057.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.986 I print_info: rope_finetuned   = unknown
0.00.057.986 I print_info: ssm_d_conv       = 0
0.00.057.986 I print_info: ssm_d_inner      = 0
0.00.057.986 I print_info: ssm_d_state      = 0
0.00.057.986 I print_info: ssm_dt_rank      = 0
0.00.057.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.987 I print_info: model type       = 1.4B
0.00.057.987 I print_info: model params     = 1.41 B
0.00.057.987 I print_info: general.name     = 1.4B
0.00.057.988 I print_info: vocab type       = BPE
0.00.057.988 I print_info: n_vocab          = 50304
0.00.057.988 I print_info: n_merges         = 50009
0.00.057.988 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.989 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.990 I print_info: LF token         = 187 ''
0.00.057.991 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.991 I print_info: max token length = 1024
0.00.057.991 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.820.077 I load_tensors: offloading 24 repeating layers to GPU
0.00.820.089 I load_tensors: offloading output layer to GPU
0.00.820.090 I load_tensors: offloaded 25/25 layers to GPU
0.00.820.125 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.820.127 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.821.731 I llama_init_from_model: n_seq_max     = 1
0.00.821.738 I llama_init_from_model: n_ctx         = 2048
0.00.821.738 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.821.739 I llama_init_from_model: n_batch       = 2048
0.00.821.739 I llama_init_from_model: n_ubatch      = 512
0.00.821.740 I llama_init_from_model: flash_attn    = 0
0.00.821.741 I llama_init_from_model: freq_base     = 10000.0
0.00.821.741 I llama_init_from_model: freq_scale    = 1
0.00.821.744 I ggml_metal_init: allocating
0.00.821.792 I ggml_metal_init: found device: Apple M4
0.00.821.802 I ggml_metal_init: picking default device: Apple M4
0.00.823.717 I ggml_metal_init: using embedded metal library
0.00.830.230 I ggml_metal_init: GPU name:   Apple M4
0.00.830.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.830.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.830.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.830.237 I ggml_metal_init: simdgroup reduction   = true
0.00.830.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.830.237 I ggml_metal_init: has residency sets    = true
0.00.830.238 I ggml_metal_init: has bfloat            = true
0.00.830.238 I ggml_metal_init: use bfloat            = true
0.00.830.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.830.240 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.847.646 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.902.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.902.936 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.902.976 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.907.491 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.907.493 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.907.494 I llama_init_from_model: graph nodes  = 967
0.00.907.494 I llama_init_from_model: graph splits = 2
0.00.907.499 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.907.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.907.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.008 I main: llama threadpool init, n_threads = 4
0.00.965.050 I 
0.00.965.072 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.072 I 
0.00.965.216 I sampler seed: 1234
0.00.965.220 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.965.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.965.239 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.965.239 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.795.235 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.795.235 I llama_perf_context_print:        load time =     952.61 ms
0.01.795.237 I llama_perf_context_print: prompt eval time =      42.04 ms /     7 tokens (    6.01 ms per token,   166.52 tokens per second)
0.01.795.238 I llama_perf_context_print:        eval time =     785.10 ms /    63 runs   (   12.46 ms per token,    80.24 tokens per second)
0.01.795.238 I llama_perf_context_print:       total time =     830.93 ms /    70 tokens
0.01.795.502 I ggml_metal_free: deallocating

real	0m1.820s
user	0m0.114s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.378 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.566 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.120 I llama_model_loader: - type  f32:  194 tensors
0.00.025.120 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.121 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.121 I print_info: file format = GGUF V3 (latest)
0.00.025.122 I print_info: file type   = Q5_1
0.00.025.124 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.169 I load: special tokens cache size = 25
0.00.039.105 I load: token to piece cache size = 0.2984 MB
0.00.039.110 I print_info: arch             = gptneox
0.00.039.110 I print_info: vocab_only       = 0
0.00.039.111 I print_info: n_ctx_train      = 2048
0.00.039.111 I print_info: n_embd           = 2048
0.00.039.111 I print_info: n_layer          = 24
0.00.039.115 I print_info: n_head           = 16
0.00.039.116 I print_info: n_head_kv        = 16
0.00.039.116 I print_info: n_rot            = 32
0.00.039.116 I print_info: n_swa            = 0
0.00.039.116 I print_info: n_embd_head_k    = 128
0.00.039.116 I print_info: n_embd_head_v    = 128
0.00.039.117 I print_info: n_gqa            = 1
0.00.039.118 I print_info: n_embd_k_gqa     = 2048
0.00.039.119 I print_info: n_embd_v_gqa     = 2048
0.00.039.119 I print_info: f_norm_eps       = 1.0e-05
0.00.039.120 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.120 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.120 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.120 I print_info: f_logit_scale    = 0.0e+00
0.00.039.124 I print_info: n_ff             = 8192
0.00.039.124 I print_info: n_expert         = 0
0.00.039.124 I print_info: n_expert_used    = 0
0.00.039.124 I print_info: causal attn      = 1
0.00.039.124 I print_info: pooling type     = 0
0.00.039.124 I print_info: rope type        = 2
0.00.039.124 I print_info: rope scaling     = linear
0.00.039.125 I print_info: freq_base_train  = 10000.0
0.00.039.125 I print_info: freq_scale_train = 1
0.00.039.125 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.125 I print_info: rope_finetuned   = unknown
0.00.039.126 I print_info: ssm_d_conv       = 0
0.00.039.126 I print_info: ssm_d_inner      = 0
0.00.039.126 I print_info: ssm_d_state      = 0
0.00.039.127 I print_info: ssm_dt_rank      = 0
0.00.039.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.127 I print_info: model type       = 1.4B
0.00.039.127 I print_info: model params     = 1.41 B
0.00.039.128 I print_info: general.name     = 1.4B
0.00.039.128 I print_info: vocab type       = BPE
0.00.039.128 I print_info: n_vocab          = 50304
0.00.039.128 I print_info: n_merges         = 50009
0.00.039.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: LF token         = 187 ''
0.00.039.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: max token length = 1024
0.00.039.130 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.976 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.998 I load_tensors: offloading output layer to GPU
0.00.650.999 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.038 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.651.040 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.652.514 I llama_init_from_model: n_seq_max     = 1
0.00.652.524 I llama_init_from_model: n_ctx         = 128
0.00.652.524 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.525 I llama_init_from_model: n_batch       = 128
0.00.652.525 I llama_init_from_model: n_ubatch      = 128
0.00.652.526 I llama_init_from_model: flash_attn    = 0
0.00.652.529 I llama_init_from_model: freq_base     = 10000.0
0.00.652.529 I llama_init_from_model: freq_scale    = 1
0.00.652.530 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.532 I ggml_metal_init: allocating
0.00.652.630 I ggml_metal_init: found device: Apple M4
0.00.652.645 I ggml_metal_init: picking default device: Apple M4
0.00.654.572 I ggml_metal_init: using embedded metal library
0.00.661.555 I ggml_metal_init: GPU name:   Apple M4
0.00.661.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.567 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.568 I ggml_metal_init: simdgroup reduction   = true
0.00.661.568 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.568 I ggml_metal_init: has residency sets    = true
0.00.661.568 I ggml_metal_init: has bfloat            = true
0.00.661.569 I ggml_metal_init: use bfloat            = true
0.00.661.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.625 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.034 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.683.039 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.683.079 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.864 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.685.866 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.685.866 I llama_init_from_model: graph nodes  = 967
0.00.685.866 I llama_init_from_model: graph splits = 2
0.00.685.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.762 I 
0.00.713.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.829 I perplexity: tokenizing the input ..
0.00.721.751 I perplexity: tokenization took 7.919 ms
0.00.721.767 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.474 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.858.815 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.858.847 I llama_perf_context_print:        load time =     704.38 ms
0.00.858.849 I llama_perf_context_print: prompt eval time =     134.72 ms /   128 tokens (    1.05 ms per token,   950.13 tokens per second)
0.00.858.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.854 I llama_perf_context_print:       total time =     145.09 ms /   129 tokens
0.00.859.357 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.082s
sys	0m0.166s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.680 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.560 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.280 I llama_model_loader: - type  f32:  194 tensors
0.00.025.280 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.280 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.281 I print_info: file format = GGUF V3 (latest)
0.00.025.282 I print_info: file type   = Q2_K - Medium
0.00.025.283 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.365 I load: special tokens cache size = 25
0.00.039.175 I load: token to piece cache size = 0.2984 MB
0.00.039.178 I print_info: arch             = gptneox
0.00.039.178 I print_info: vocab_only       = 0
0.00.039.178 I print_info: n_ctx_train      = 2048
0.00.039.178 I print_info: n_embd           = 2048
0.00.039.179 I print_info: n_layer          = 24
0.00.039.181 I print_info: n_head           = 16
0.00.039.182 I print_info: n_head_kv        = 16
0.00.039.182 I print_info: n_rot            = 32
0.00.039.182 I print_info: n_swa            = 0
0.00.039.183 I print_info: n_embd_head_k    = 128
0.00.039.185 I print_info: n_embd_head_v    = 128
0.00.039.186 I print_info: n_gqa            = 1
0.00.039.187 I print_info: n_embd_k_gqa     = 2048
0.00.039.187 I print_info: n_embd_v_gqa     = 2048
0.00.039.192 I print_info: f_norm_eps       = 1.0e-05
0.00.039.193 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.194 I print_info: f_logit_scale    = 0.0e+00
0.00.039.195 I print_info: n_ff             = 8192
0.00.039.195 I print_info: n_expert         = 0
0.00.039.195 I print_info: n_expert_used    = 0
0.00.039.195 I print_info: causal attn      = 1
0.00.039.195 I print_info: pooling type     = 0
0.00.039.196 I print_info: rope type        = 2
0.00.039.196 I print_info: rope scaling     = linear
0.00.039.196 I print_info: freq_base_train  = 10000.0
0.00.039.197 I print_info: freq_scale_train = 1
0.00.039.197 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.197 I print_info: rope_finetuned   = unknown
0.00.039.199 I print_info: ssm_d_conv       = 0
0.00.039.199 I print_info: ssm_d_inner      = 0
0.00.039.199 I print_info: ssm_d_state      = 0
0.00.039.199 I print_info: ssm_dt_rank      = 0
0.00.039.199 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.199 I print_info: model type       = 1.4B
0.00.039.200 I print_info: model params     = 1.41 B
0.00.039.200 I print_info: general.name     = 1.4B
0.00.039.200 I print_info: vocab type       = BPE
0.00.039.201 I print_info: n_vocab          = 50304
0.00.039.201 I print_info: n_merges         = 50009
0.00.039.201 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.201 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.201 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.201 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.204 I print_info: LF token         = 187 ''
0.00.039.204 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.205 I print_info: max token length = 1024
0.00.039.205 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.346.997 I load_tensors: offloading 24 repeating layers to GPU
0.00.347.013 I load_tensors: offloading output layer to GPU
0.00.347.013 I load_tensors: offloaded 25/25 layers to GPU
0.00.347.047 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.347.049 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.348.647 I llama_init_from_model: n_seq_max     = 1
0.00.348.655 I llama_init_from_model: n_ctx         = 2048
0.00.348.655 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.348.656 I llama_init_from_model: n_batch       = 2048
0.00.348.656 I llama_init_from_model: n_ubatch      = 512
0.00.348.656 I llama_init_from_model: flash_attn    = 0
0.00.348.658 I llama_init_from_model: freq_base     = 10000.0
0.00.348.659 I llama_init_from_model: freq_scale    = 1
0.00.348.664 I ggml_metal_init: allocating
0.00.348.766 I ggml_metal_init: found device: Apple M4
0.00.348.780 I ggml_metal_init: picking default device: Apple M4
0.00.350.700 I ggml_metal_init: using embedded metal library
0.00.356.407 I ggml_metal_init: GPU name:   Apple M4
0.00.356.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.427 I ggml_metal_init: simdgroup reduction   = true
0.00.356.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.428 I ggml_metal_init: has residency sets    = true
0.00.356.428 I ggml_metal_init: has bfloat            = true
0.00.356.429 I ggml_metal_init: use bfloat            = true
0.00.356.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.828 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.435.535 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.435.542 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.435.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.819 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.439.821 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.439.822 I llama_init_from_model: graph nodes  = 967
0.00.439.822 I llama_init_from_model: graph splits = 2
0.00.439.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.439.951 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.439.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.579 I main: llama threadpool init, n_threads = 4
0.00.497.642 I 
0.00.497.666 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.668 I 
0.00.497.843 I sampler seed: 1234
0.00.497.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.497.868 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.497.868 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.497.868 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.171.877 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.171.878 I llama_perf_context_print:        load time =     486.92 ms
0.01.171.879 I llama_perf_context_print: prompt eval time =      35.49 ms /     7 tokens (    5.07 ms per token,   197.25 tokens per second)
0.01.171.881 I llama_perf_context_print:        eval time =     635.73 ms /    63 runs   (   10.09 ms per token,    99.10 tokens per second)
0.01.171.883 I llama_perf_context_print:       total time =     675.03 ms /    70 tokens
0.01.172.072 I ggml_metal_free: deallocating

real	0m1.192s
user	0m0.112s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.126 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.532 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.428 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.149 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.149 I llama_model_loader: - type  f32:  194 tensors
0.00.027.150 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.150 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.150 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.151 I print_info: file format = GGUF V3 (latest)
0.00.027.151 I print_info: file type   = Q2_K - Medium
0.00.027.153 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.228 I load: special tokens cache size = 25
0.00.041.309 I load: token to piece cache size = 0.2984 MB
0.00.041.314 I print_info: arch             = gptneox
0.00.041.314 I print_info: vocab_only       = 0
0.00.041.314 I print_info: n_ctx_train      = 2048
0.00.041.314 I print_info: n_embd           = 2048
0.00.041.314 I print_info: n_layer          = 24
0.00.041.318 I print_info: n_head           = 16
0.00.041.319 I print_info: n_head_kv        = 16
0.00.041.319 I print_info: n_rot            = 32
0.00.041.320 I print_info: n_swa            = 0
0.00.041.320 I print_info: n_embd_head_k    = 128
0.00.041.320 I print_info: n_embd_head_v    = 128
0.00.041.321 I print_info: n_gqa            = 1
0.00.041.321 I print_info: n_embd_k_gqa     = 2048
0.00.041.324 I print_info: n_embd_v_gqa     = 2048
0.00.041.326 I print_info: f_norm_eps       = 1.0e-05
0.00.041.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.327 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.327 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.328 I print_info: f_logit_scale    = 0.0e+00
0.00.041.329 I print_info: n_ff             = 8192
0.00.041.329 I print_info: n_expert         = 0
0.00.041.330 I print_info: n_expert_used    = 0
0.00.041.330 I print_info: causal attn      = 1
0.00.041.330 I print_info: pooling type     = 0
0.00.041.330 I print_info: rope type        = 2
0.00.041.330 I print_info: rope scaling     = linear
0.00.041.331 I print_info: freq_base_train  = 10000.0
0.00.041.332 I print_info: freq_scale_train = 1
0.00.041.332 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.332 I print_info: rope_finetuned   = unknown
0.00.041.332 I print_info: ssm_d_conv       = 0
0.00.041.332 I print_info: ssm_d_inner      = 0
0.00.041.332 I print_info: ssm_d_state      = 0
0.00.041.332 I print_info: ssm_dt_rank      = 0
0.00.041.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.333 I print_info: model type       = 1.4B
0.00.041.333 I print_info: model params     = 1.41 B
0.00.041.334 I print_info: general.name     = 1.4B
0.00.041.335 I print_info: vocab type       = BPE
0.00.041.335 I print_info: n_vocab          = 50304
0.00.041.335 I print_info: n_merges         = 50009
0.00.041.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.336 I print_info: LF token         = 187 ''
0.00.041.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.336 I print_info: max token length = 1024
0.00.041.337 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.357.309 I load_tensors: offloading 24 repeating layers to GPU
0.00.357.333 I load_tensors: offloading output layer to GPU
0.00.357.333 I load_tensors: offloaded 25/25 layers to GPU
0.00.357.369 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.357.370 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.359.004 I llama_init_from_model: n_seq_max     = 1
0.00.359.013 I llama_init_from_model: n_ctx         = 128
0.00.359.013 I llama_init_from_model: n_ctx_per_seq = 128
0.00.359.014 I llama_init_from_model: n_batch       = 128
0.00.359.015 I llama_init_from_model: n_ubatch      = 128
0.00.359.015 I llama_init_from_model: flash_attn    = 0
0.00.359.018 I llama_init_from_model: freq_base     = 10000.0
0.00.359.018 I llama_init_from_model: freq_scale    = 1
0.00.359.019 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.359.022 I ggml_metal_init: allocating
0.00.359.109 I ggml_metal_init: found device: Apple M4
0.00.359.125 I ggml_metal_init: picking default device: Apple M4
0.00.361.025 I ggml_metal_init: using embedded metal library
0.00.366.954 I ggml_metal_init: GPU name:   Apple M4
0.00.366.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.366.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.366.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.366.980 I ggml_metal_init: simdgroup reduction   = true
0.00.366.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.366.981 I ggml_metal_init: has residency sets    = true
0.00.366.981 I ggml_metal_init: has bfloat            = true
0.00.366.981 I ggml_metal_init: use bfloat            = true
0.00.366.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.366.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.305 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.394.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.394.012 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.394.060 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.396.896 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.396.898 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.396.899 I llama_init_from_model: graph nodes  = 967
0.00.396.899 I llama_init_from_model: graph splits = 2
0.00.396.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.396.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.652 I 
0.00.422.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.720 I perplexity: tokenizing the input ..
0.00.429.888 I perplexity: tokenization took 7.165 ms
0.00.429.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.002 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.328 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.364 I llama_perf_context_print:        load time =     411.11 ms
0.00.564.365 I llama_perf_context_print: prompt eval time =     132.08 ms /   128 tokens (    1.03 ms per token,   969.13 tokens per second)
0.00.564.366 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.367 I llama_perf_context_print:       total time =     141.72 ms /   129 tokens
0.00.564.846 I ggml_metal_free: deallocating

real	0m0.582s
user	0m0.084s
sys	0m0.102s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.741 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.491 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.497 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.503 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.509 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.244 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.060 I llama_model_loader: - type  f32:  194 tensors
0.00.025.060 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.060 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.060 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.060 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.061 I print_info: file format = GGUF V3 (latest)
0.00.025.062 I print_info: file type   = Q3_K - Medium
0.00.025.064 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.916 I load: special tokens cache size = 25
0.00.038.905 I load: token to piece cache size = 0.2984 MB
0.00.038.908 I print_info: arch             = gptneox
0.00.038.908 I print_info: vocab_only       = 0
0.00.038.909 I print_info: n_ctx_train      = 2048
0.00.038.909 I print_info: n_embd           = 2048
0.00.038.909 I print_info: n_layer          = 24
0.00.038.912 I print_info: n_head           = 16
0.00.038.912 I print_info: n_head_kv        = 16
0.00.038.913 I print_info: n_rot            = 32
0.00.038.913 I print_info: n_swa            = 0
0.00.038.913 I print_info: n_embd_head_k    = 128
0.00.038.913 I print_info: n_embd_head_v    = 128
0.00.038.914 I print_info: n_gqa            = 1
0.00.038.914 I print_info: n_embd_k_gqa     = 2048
0.00.038.915 I print_info: n_embd_v_gqa     = 2048
0.00.038.916 I print_info: f_norm_eps       = 1.0e-05
0.00.038.916 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.916 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.916 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.916 I print_info: f_logit_scale    = 0.0e+00
0.00.038.917 I print_info: n_ff             = 8192
0.00.038.917 I print_info: n_expert         = 0
0.00.038.917 I print_info: n_expert_used    = 0
0.00.038.918 I print_info: causal attn      = 1
0.00.038.918 I print_info: pooling type     = 0
0.00.038.918 I print_info: rope type        = 2
0.00.038.920 I print_info: rope scaling     = linear
0.00.038.920 I print_info: freq_base_train  = 10000.0
0.00.038.920 I print_info: freq_scale_train = 1
0.00.038.921 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.921 I print_info: rope_finetuned   = unknown
0.00.038.921 I print_info: ssm_d_conv       = 0
0.00.038.922 I print_info: ssm_d_inner      = 0
0.00.038.922 I print_info: ssm_d_state      = 0
0.00.038.922 I print_info: ssm_dt_rank      = 0
0.00.038.922 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.923 I print_info: model type       = 1.4B
0.00.038.923 I print_info: model params     = 1.41 B
0.00.038.923 I print_info: general.name     = 1.4B
0.00.038.924 I print_info: vocab type       = BPE
0.00.038.924 I print_info: n_vocab          = 50304
0.00.038.924 I print_info: n_merges         = 50009
0.00.038.924 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: LF token         = 187 ''
0.00.038.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.928 I print_info: max token length = 1024
0.00.038.928 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.241 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.251 I load_tensors: offloading output layer to GPU
0.00.445.252 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.285 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.289 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.446.790 I llama_init_from_model: n_seq_max     = 1
0.00.446.795 I llama_init_from_model: n_ctx         = 2048
0.00.446.796 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.797 I llama_init_from_model: n_batch       = 2048
0.00.446.797 I llama_init_from_model: n_ubatch      = 512
0.00.446.797 I llama_init_from_model: flash_attn    = 0
0.00.446.799 I llama_init_from_model: freq_base     = 10000.0
0.00.446.800 I llama_init_from_model: freq_scale    = 1
0.00.446.801 I ggml_metal_init: allocating
0.00.446.877 I ggml_metal_init: found device: Apple M4
0.00.446.891 I ggml_metal_init: picking default device: Apple M4
0.00.448.773 I ggml_metal_init: using embedded metal library
0.00.454.345 I ggml_metal_init: GPU name:   Apple M4
0.00.454.350 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.352 I ggml_metal_init: simdgroup reduction   = true
0.00.454.352 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.352 I ggml_metal_init: has residency sets    = true
0.00.454.353 I ggml_metal_init: has bfloat            = true
0.00.454.353 I ggml_metal_init: use bfloat            = true
0.00.454.354 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.850 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.534.597 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.534.605 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.534.640 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.539.249 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.539.252 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.539.252 I llama_init_from_model: graph nodes  = 967
0.00.539.252 I llama_init_from_model: graph splits = 2
0.00.539.259 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.539.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.539.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.755 I main: llama threadpool init, n_threads = 4
0.00.597.800 I 
0.00.597.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.826 I 
0.00.597.988 I sampler seed: 1234
0.00.597.993 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.598.042 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.598.057 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.598.057 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.838 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51824.82 tokens per second)
0.01.342.839 I llama_perf_context_print:        load time =     588.28 ms
0.01.342.840 I llama_perf_context_print: prompt eval time =      45.83 ms /     7 tokens (    6.55 ms per token,   152.74 tokens per second)
0.01.342.840 I llama_perf_context_print:        eval time =     696.02 ms /    63 runs   (   11.05 ms per token,    90.52 tokens per second)
0.01.342.841 I llama_perf_context_print:       total time =     745.82 ms /    70 tokens
0.01.343.067 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.109s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.534 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.204 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.205 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.062 I llama_model_loader: - type  f32:  194 tensors
0.00.026.062 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.062 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.063 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.063 I print_info: file format = GGUF V3 (latest)
0.00.026.064 I print_info: file type   = Q3_K - Medium
0.00.026.065 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.445 I load: special tokens cache size = 25
0.00.040.440 I load: token to piece cache size = 0.2984 MB
0.00.040.444 I print_info: arch             = gptneox
0.00.040.444 I print_info: vocab_only       = 0
0.00.040.445 I print_info: n_ctx_train      = 2048
0.00.040.445 I print_info: n_embd           = 2048
0.00.040.445 I print_info: n_layer          = 24
0.00.040.450 I print_info: n_head           = 16
0.00.040.451 I print_info: n_head_kv        = 16
0.00.040.451 I print_info: n_rot            = 32
0.00.040.454 I print_info: n_swa            = 0
0.00.040.454 I print_info: n_embd_head_k    = 128
0.00.040.454 I print_info: n_embd_head_v    = 128
0.00.040.455 I print_info: n_gqa            = 1
0.00.040.457 I print_info: n_embd_k_gqa     = 2048
0.00.040.458 I print_info: n_embd_v_gqa     = 2048
0.00.040.458 I print_info: f_norm_eps       = 1.0e-05
0.00.040.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.463 I print_info: f_logit_scale    = 0.0e+00
0.00.040.464 I print_info: n_ff             = 8192
0.00.040.464 I print_info: n_expert         = 0
0.00.040.464 I print_info: n_expert_used    = 0
0.00.040.464 I print_info: causal attn      = 1
0.00.040.466 I print_info: pooling type     = 0
0.00.040.466 I print_info: rope type        = 2
0.00.040.466 I print_info: rope scaling     = linear
0.00.040.467 I print_info: freq_base_train  = 10000.0
0.00.040.467 I print_info: freq_scale_train = 1
0.00.040.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.467 I print_info: rope_finetuned   = unknown
0.00.040.468 I print_info: ssm_d_conv       = 0
0.00.040.468 I print_info: ssm_d_inner      = 0
0.00.040.468 I print_info: ssm_d_state      = 0
0.00.040.468 I print_info: ssm_dt_rank      = 0
0.00.040.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.468 I print_info: model type       = 1.4B
0.00.040.469 I print_info: model params     = 1.41 B
0.00.040.469 I print_info: general.name     = 1.4B
0.00.040.470 I print_info: vocab type       = BPE
0.00.040.470 I print_info: n_vocab          = 50304
0.00.040.470 I print_info: n_merges         = 50009
0.00.040.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: LF token         = 187 ''
0.00.040.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: max token length = 1024
0.00.040.474 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.458.379 I load_tensors: offloading 24 repeating layers to GPU
0.00.458.398 I load_tensors: offloading output layer to GPU
0.00.458.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.458.432 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.458.434 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.459.814 I llama_init_from_model: n_seq_max     = 1
0.00.459.820 I llama_init_from_model: n_ctx         = 128
0.00.459.821 I llama_init_from_model: n_ctx_per_seq = 128
0.00.459.821 I llama_init_from_model: n_batch       = 128
0.00.459.822 I llama_init_from_model: n_ubatch      = 128
0.00.459.822 I llama_init_from_model: flash_attn    = 0
0.00.459.825 I llama_init_from_model: freq_base     = 10000.0
0.00.459.825 I llama_init_from_model: freq_scale    = 1
0.00.459.826 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.459.828 I ggml_metal_init: allocating
0.00.459.946 I ggml_metal_init: found device: Apple M4
0.00.459.960 I ggml_metal_init: picking default device: Apple M4
0.00.461.924 I ggml_metal_init: using embedded metal library
0.00.467.962 I ggml_metal_init: GPU name:   Apple M4
0.00.467.980 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.980 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.981 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.982 I ggml_metal_init: simdgroup reduction   = true
0.00.467.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.983 I ggml_metal_init: has residency sets    = true
0.00.467.983 I ggml_metal_init: has bfloat            = true
0.00.467.984 I ggml_metal_init: use bfloat            = true
0.00.467.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.468.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.489.257 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.492.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.492.759 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.492.804 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.495.999 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.496.001 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.496.002 I llama_init_from_model: graph nodes  = 967
0.00.496.002 I llama_init_from_model: graph splits = 2
0.00.496.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.496.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.100 I 
0.00.522.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.168 I perplexity: tokenizing the input ..
0.00.529.411 I perplexity: tokenization took 7.24 ms
0.00.529.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.671.694 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.673.035 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.673.063 I llama_perf_context_print:        load time =     512.56 ms
0.00.673.064 I llama_perf_context_print: prompt eval time =     141.29 ms /   128 tokens (    1.10 ms per token,   905.96 tokens per second)
0.00.673.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.673.065 I llama_perf_context_print:       total time =     150.97 ms /   129 tokens
0.00.673.438 I ggml_metal_free: deallocating

real	0m0.687s
user	0m0.083s
sys	0m0.119s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.620 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.623 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.623 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.624 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.220 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.220 I llama_model_loader: - type  f32:  194 tensors
0.00.028.221 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.221 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.221 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.221 I print_info: file format = GGUF V3 (latest)
0.00.028.222 I print_info: file type   = Q4_K - Medium
0.00.028.223 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.400 I load: special tokens cache size = 25
0.00.042.422 I load: token to piece cache size = 0.2984 MB
0.00.042.425 I print_info: arch             = gptneox
0.00.042.425 I print_info: vocab_only       = 0
0.00.042.425 I print_info: n_ctx_train      = 2048
0.00.042.425 I print_info: n_embd           = 2048
0.00.042.425 I print_info: n_layer          = 24
0.00.042.428 I print_info: n_head           = 16
0.00.042.429 I print_info: n_head_kv        = 16
0.00.042.429 I print_info: n_rot            = 32
0.00.042.429 I print_info: n_swa            = 0
0.00.042.430 I print_info: n_embd_head_k    = 128
0.00.042.430 I print_info: n_embd_head_v    = 128
0.00.042.430 I print_info: n_gqa            = 1
0.00.042.431 I print_info: n_embd_k_gqa     = 2048
0.00.042.432 I print_info: n_embd_v_gqa     = 2048
0.00.042.432 I print_info: f_norm_eps       = 1.0e-05
0.00.042.433 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.433 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.433 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.433 I print_info: f_logit_scale    = 0.0e+00
0.00.042.434 I print_info: n_ff             = 8192
0.00.042.434 I print_info: n_expert         = 0
0.00.042.434 I print_info: n_expert_used    = 0
0.00.042.434 I print_info: causal attn      = 1
0.00.042.435 I print_info: pooling type     = 0
0.00.042.435 I print_info: rope type        = 2
0.00.042.435 I print_info: rope scaling     = linear
0.00.042.435 I print_info: freq_base_train  = 10000.0
0.00.042.436 I print_info: freq_scale_train = 1
0.00.042.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.444 I print_info: rope_finetuned   = unknown
0.00.042.446 I print_info: ssm_d_conv       = 0
0.00.042.447 I print_info: ssm_d_inner      = 0
0.00.042.447 I print_info: ssm_d_state      = 0
0.00.042.447 I print_info: ssm_dt_rank      = 0
0.00.042.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.447 I print_info: model type       = 1.4B
0.00.042.448 I print_info: model params     = 1.41 B
0.00.042.448 I print_info: general.name     = 1.4B
0.00.042.449 I print_info: vocab type       = BPE
0.00.042.449 I print_info: n_vocab          = 50304
0.00.042.449 I print_info: n_merges         = 50009
0.00.042.449 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.449 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.449 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.450 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.450 I print_info: LF token         = 187 ''
0.00.042.452 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.452 I print_info: max token length = 1024
0.00.042.452 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.545.593 I load_tensors: offloading 24 repeating layers to GPU
0.00.545.611 I load_tensors: offloading output layer to GPU
0.00.545.611 I load_tensors: offloaded 25/25 layers to GPU
0.00.545.648 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.545.649 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.547.148 I llama_init_from_model: n_seq_max     = 1
0.00.547.152 I llama_init_from_model: n_ctx         = 2048
0.00.547.153 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.547.154 I llama_init_from_model: n_batch       = 2048
0.00.547.154 I llama_init_from_model: n_ubatch      = 512
0.00.547.154 I llama_init_from_model: flash_attn    = 0
0.00.547.156 I llama_init_from_model: freq_base     = 10000.0
0.00.547.157 I llama_init_from_model: freq_scale    = 1
0.00.547.159 I ggml_metal_init: allocating
0.00.547.243 I ggml_metal_init: found device: Apple M4
0.00.547.256 I ggml_metal_init: picking default device: Apple M4
0.00.549.166 I ggml_metal_init: using embedded metal library
0.00.556.000 I ggml_metal_init: GPU name:   Apple M4
0.00.556.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.556.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.556.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.556.006 I ggml_metal_init: simdgroup reduction   = true
0.00.556.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.556.007 I ggml_metal_init: has residency sets    = true
0.00.556.007 I ggml_metal_init: has bfloat            = true
0.00.556.008 I ggml_metal_init: use bfloat            = true
0.00.556.009 I ggml_metal_init: hasUnifiedMemory      = true
0.00.556.011 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.574.312 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.486 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.630.494 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.630.535 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.146 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.635.148 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.635.148 I llama_init_from_model: graph nodes  = 967
0.00.635.148 I llama_init_from_model: graph splits = 2
0.00.635.154 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.635.280 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.635.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.763 I main: llama threadpool init, n_threads = 4
0.00.692.810 I 
0.00.692.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.835 I 
0.00.692.991 I sampler seed: 1234
0.00.692.995 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.007 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.007 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.007 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.446.283 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.446.284 I llama_perf_context_print:        load time =     680.09 ms
0.01.446.286 I llama_perf_context_print: prompt eval time =      50.15 ms /     7 tokens (    7.16 ms per token,   139.58 tokens per second)
0.01.446.287 I llama_perf_context_print:        eval time =     700.20 ms /    63 runs   (   11.11 ms per token,    89.97 tokens per second)
0.01.446.287 I llama_perf_context_print:       total time =     754.24 ms /    70 tokens
0.01.446.533 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.110s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.912 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.917 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.918 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.918 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.555 I llama_model_loader: - type  f32:  194 tensors
0.00.027.556 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.556 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.556 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.557 I print_info: file format = GGUF V3 (latest)
0.00.027.557 I print_info: file type   = Q4_K - Medium
0.00.027.559 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.597 I load: special tokens cache size = 25
0.00.041.408 I load: token to piece cache size = 0.2984 MB
0.00.041.412 I print_info: arch             = gptneox
0.00.041.412 I print_info: vocab_only       = 0
0.00.041.413 I print_info: n_ctx_train      = 2048
0.00.041.413 I print_info: n_embd           = 2048
0.00.041.413 I print_info: n_layer          = 24
0.00.041.417 I print_info: n_head           = 16
0.00.041.418 I print_info: n_head_kv        = 16
0.00.041.418 I print_info: n_rot            = 32
0.00.041.418 I print_info: n_swa            = 0
0.00.041.418 I print_info: n_embd_head_k    = 128
0.00.041.420 I print_info: n_embd_head_v    = 128
0.00.041.421 I print_info: n_gqa            = 1
0.00.041.421 I print_info: n_embd_k_gqa     = 2048
0.00.041.422 I print_info: n_embd_v_gqa     = 2048
0.00.041.423 I print_info: f_norm_eps       = 1.0e-05
0.00.041.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.423 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.425 I print_info: f_logit_scale    = 0.0e+00
0.00.041.426 I print_info: n_ff             = 8192
0.00.041.426 I print_info: n_expert         = 0
0.00.041.426 I print_info: n_expert_used    = 0
0.00.041.426 I print_info: causal attn      = 1
0.00.041.426 I print_info: pooling type     = 0
0.00.041.426 I print_info: rope type        = 2
0.00.041.427 I print_info: rope scaling     = linear
0.00.041.427 I print_info: freq_base_train  = 10000.0
0.00.041.427 I print_info: freq_scale_train = 1
0.00.041.427 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.428 I print_info: rope_finetuned   = unknown
0.00.041.431 I print_info: ssm_d_conv       = 0
0.00.041.431 I print_info: ssm_d_inner      = 0
0.00.041.432 I print_info: ssm_d_state      = 0
0.00.041.432 I print_info: ssm_dt_rank      = 0
0.00.041.432 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.433 I print_info: model type       = 1.4B
0.00.041.433 I print_info: model params     = 1.41 B
0.00.041.433 I print_info: general.name     = 1.4B
0.00.041.434 I print_info: vocab type       = BPE
0.00.041.434 I print_info: n_vocab          = 50304
0.00.041.434 I print_info: n_merges         = 50009
0.00.041.434 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.435 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.435 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.435 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.435 I print_info: LF token         = 187 ''
0.00.041.435 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: max token length = 1024
0.00.041.436 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.561.234 I load_tensors: offloading 24 repeating layers to GPU
0.00.561.258 I load_tensors: offloading output layer to GPU
0.00.561.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.561.296 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.561.297 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.562.734 I llama_init_from_model: n_seq_max     = 1
0.00.562.737 I llama_init_from_model: n_ctx         = 128
0.00.562.738 I llama_init_from_model: n_ctx_per_seq = 128
0.00.562.739 I llama_init_from_model: n_batch       = 128
0.00.562.739 I llama_init_from_model: n_ubatch      = 128
0.00.562.739 I llama_init_from_model: flash_attn    = 0
0.00.562.742 I llama_init_from_model: freq_base     = 10000.0
0.00.562.742 I llama_init_from_model: freq_scale    = 1
0.00.562.743 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.562.745 I ggml_metal_init: allocating
0.00.562.835 I ggml_metal_init: found device: Apple M4
0.00.562.853 I ggml_metal_init: picking default device: Apple M4
0.00.564.742 I ggml_metal_init: using embedded metal library
0.00.570.602 I ggml_metal_init: GPU name:   Apple M4
0.00.570.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.570.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.570.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.570.626 I ggml_metal_init: simdgroup reduction   = true
0.00.570.626 I ggml_metal_init: simdgroup matrix mul. = true
0.00.570.626 I ggml_metal_init: has residency sets    = true
0.00.570.627 I ggml_metal_init: has bfloat            = true
0.00.570.627 I ggml_metal_init: use bfloat            = true
0.00.570.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.570.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.591.407 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.594.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.594.971 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.595.017 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.597.887 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.597.888 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.597.889 I llama_init_from_model: graph nodes  = 967
0.00.597.890 I llama_init_from_model: graph splits = 2
0.00.597.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.597.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.258 I 
0.00.627.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.337 I perplexity: tokenizing the input ..
0.00.632.961 I perplexity: tokenization took 5.622 ms
0.00.632.966 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.896 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.767.230 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.767.265 I llama_perf_context_print:        load time =     615.43 ms
0.00.767.266 I llama_perf_context_print: prompt eval time =     132.60 ms /   128 tokens (    1.04 ms per token,   965.28 tokens per second)
0.00.767.267 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.267 I llama_perf_context_print:       total time =     140.01 ms /   129 tokens
0.00.767.704 I ggml_metal_free: deallocating

real	0m0.784s
user	0m0.079s
sys	0m0.154s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.014.229 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.449 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.021.454 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.029.870 I llama_model_loader: - type  f32:  194 tensors
0.00.029.870 I llama_model_loader: - type q5_K:   61 tensors
0.00.029.871 I llama_model_loader: - type q6_K:   37 tensors
0.00.029.871 I print_info: file format = GGUF V3 (latest)
0.00.029.872 I print_info: file type   = Q5_K - Medium
0.00.029.873 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.037.711 I load: special tokens cache size = 25
0.00.043.626 I load: token to piece cache size = 0.2984 MB
0.00.043.628 I print_info: arch             = gptneox
0.00.043.629 I print_info: vocab_only       = 0
0.00.043.629 I print_info: n_ctx_train      = 2048
0.00.043.629 I print_info: n_embd           = 2048
0.00.043.629 I print_info: n_layer          = 24
0.00.043.632 I print_info: n_head           = 16
0.00.043.633 I print_info: n_head_kv        = 16
0.00.043.633 I print_info: n_rot            = 32
0.00.043.633 I print_info: n_swa            = 0
0.00.043.634 I print_info: n_embd_head_k    = 128
0.00.043.634 I print_info: n_embd_head_v    = 128
0.00.043.635 I print_info: n_gqa            = 1
0.00.043.635 I print_info: n_embd_k_gqa     = 2048
0.00.043.636 I print_info: n_embd_v_gqa     = 2048
0.00.043.636 I print_info: f_norm_eps       = 1.0e-05
0.00.043.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.637 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.637 I print_info: f_logit_scale    = 0.0e+00
0.00.043.638 I print_info: n_ff             = 8192
0.00.043.638 I print_info: n_expert         = 0
0.00.043.638 I print_info: n_expert_used    = 0
0.00.043.638 I print_info: causal attn      = 1
0.00.043.638 I print_info: pooling type     = 0
0.00.043.639 I print_info: rope type        = 2
0.00.043.640 I print_info: rope scaling     = linear
0.00.043.640 I print_info: freq_base_train  = 10000.0
0.00.043.641 I print_info: freq_scale_train = 1
0.00.043.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.641 I print_info: rope_finetuned   = unknown
0.00.043.641 I print_info: ssm_d_conv       = 0
0.00.043.642 I print_info: ssm_d_inner      = 0
0.00.043.642 I print_info: ssm_d_state      = 0
0.00.043.642 I print_info: ssm_dt_rank      = 0
0.00.043.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.642 I print_info: model type       = 1.4B
0.00.043.643 I print_info: model params     = 1.41 B
0.00.043.643 I print_info: general.name     = 1.4B
0.00.043.643 I print_info: vocab type       = BPE
0.00.043.644 I print_info: n_vocab          = 50304
0.00.043.644 I print_info: n_merges         = 50009
0.00.043.644 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.644 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.645 I print_info: LF token         = 187 ''
0.00.043.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.646 I print_info: max token length = 1024
0.00.043.647 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.983 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.006 I load_tensors: offloading output layer to GPU
0.00.626.007 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.043 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.626.044 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.627.511 I llama_init_from_model: n_seq_max     = 1
0.00.627.515 I llama_init_from_model: n_ctx         = 2048
0.00.627.515 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.627.516 I llama_init_from_model: n_batch       = 2048
0.00.627.516 I llama_init_from_model: n_ubatch      = 512
0.00.627.517 I llama_init_from_model: flash_attn    = 0
0.00.627.519 I llama_init_from_model: freq_base     = 10000.0
0.00.627.520 I llama_init_from_model: freq_scale    = 1
0.00.627.523 I ggml_metal_init: allocating
0.00.627.596 I ggml_metal_init: found device: Apple M4
0.00.627.617 I ggml_metal_init: picking default device: Apple M4
0.00.629.197 I ggml_metal_init: using embedded metal library
0.00.635.646 I ggml_metal_init: GPU name:   Apple M4
0.00.635.650 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.651 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.652 I ggml_metal_init: simdgroup reduction   = true
0.00.635.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.653 I ggml_metal_init: has residency sets    = true
0.00.635.653 I ggml_metal_init: has bfloat            = true
0.00.635.653 I ggml_metal_init: use bfloat            = true
0.00.635.654 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.569 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.727 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.713.736 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.771 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.127 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.718.129 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.718.130 I llama_init_from_model: graph nodes  = 967
0.00.718.130 I llama_init_from_model: graph splits = 2
0.00.718.136 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.718.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.265 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.423 I main: llama threadpool init, n_threads = 4
0.00.773.467 I 
0.00.773.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.494 I 
0.00.773.634 I sampler seed: 1234
0.00.773.638 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.680 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.684 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.684 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.651.830 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.651.831 I llama_perf_context_print:        load time =     758.48 ms
0.01.651.836 I llama_perf_context_print: prompt eval time =      53.09 ms /     7 tokens (    7.58 ms per token,   131.84 tokens per second)
0.01.651.837 I llama_perf_context_print:        eval time =     822.10 ms /    63 runs   (   13.05 ms per token,    76.63 tokens per second)
0.01.651.837 I llama_perf_context_print:       total time =     879.12 ms /    70 tokens
0.01.652.116 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.111s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.369 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.381 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.382 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.382 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.383 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.384 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.384 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.385 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.385 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.389 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.389 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.167 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.896 I llama_model_loader: - type  f32:  194 tensors
0.00.024.897 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.897 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.898 I print_info: file format = GGUF V3 (latest)
0.00.024.900 I print_info: file type   = Q5_K - Medium
0.00.024.901 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.889 I load: special tokens cache size = 25
0.00.039.003 I load: token to piece cache size = 0.2984 MB
0.00.039.008 I print_info: arch             = gptneox
0.00.039.008 I print_info: vocab_only       = 0
0.00.039.008 I print_info: n_ctx_train      = 2048
0.00.039.009 I print_info: n_embd           = 2048
0.00.039.009 I print_info: n_layer          = 24
0.00.039.014 I print_info: n_head           = 16
0.00.039.014 I print_info: n_head_kv        = 16
0.00.039.015 I print_info: n_rot            = 32
0.00.039.015 I print_info: n_swa            = 0
0.00.039.015 I print_info: n_embd_head_k    = 128
0.00.039.015 I print_info: n_embd_head_v    = 128
0.00.039.016 I print_info: n_gqa            = 1
0.00.039.016 I print_info: n_embd_k_gqa     = 2048
0.00.039.017 I print_info: n_embd_v_gqa     = 2048
0.00.039.018 I print_info: f_norm_eps       = 1.0e-05
0.00.039.018 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.018 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.018 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.018 I print_info: f_logit_scale    = 0.0e+00
0.00.039.019 I print_info: n_ff             = 8192
0.00.039.019 I print_info: n_expert         = 0
0.00.039.019 I print_info: n_expert_used    = 0
0.00.039.019 I print_info: causal attn      = 1
0.00.039.023 I print_info: pooling type     = 0
0.00.039.023 I print_info: rope type        = 2
0.00.039.024 I print_info: rope scaling     = linear
0.00.039.024 I print_info: freq_base_train  = 10000.0
0.00.039.027 I print_info: freq_scale_train = 1
0.00.039.028 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.028 I print_info: rope_finetuned   = unknown
0.00.039.028 I print_info: ssm_d_conv       = 0
0.00.039.028 I print_info: ssm_d_inner      = 0
0.00.039.028 I print_info: ssm_d_state      = 0
0.00.039.028 I print_info: ssm_dt_rank      = 0
0.00.039.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.029 I print_info: model type       = 1.4B
0.00.039.030 I print_info: model params     = 1.41 B
0.00.039.030 I print_info: general.name     = 1.4B
0.00.039.031 I print_info: vocab type       = BPE
0.00.039.031 I print_info: n_vocab          = 50304
0.00.039.031 I print_info: n_merges         = 50009
0.00.039.031 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.031 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: LF token         = 187 ''
0.00.039.032 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: max token length = 1024
0.00.039.033 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.629.901 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.919 I load_tensors: offloading output layer to GPU
0.00.629.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.951 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.629.953 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.631.567 I llama_init_from_model: n_seq_max     = 1
0.00.631.573 I llama_init_from_model: n_ctx         = 128
0.00.631.573 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.574 I llama_init_from_model: n_batch       = 128
0.00.631.574 I llama_init_from_model: n_ubatch      = 128
0.00.631.575 I llama_init_from_model: flash_attn    = 0
0.00.631.576 I llama_init_from_model: freq_base     = 10000.0
0.00.631.576 I llama_init_from_model: freq_scale    = 1
0.00.631.577 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.579 I ggml_metal_init: allocating
0.00.631.632 I ggml_metal_init: found device: Apple M4
0.00.631.646 I ggml_metal_init: picking default device: Apple M4
0.00.633.448 I ggml_metal_init: using embedded metal library
0.00.640.187 I ggml_metal_init: GPU name:   Apple M4
0.00.640.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.194 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.194 I ggml_metal_init: simdgroup reduction   = true
0.00.640.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.195 I ggml_metal_init: has residency sets    = true
0.00.640.195 I ggml_metal_init: has bfloat            = true
0.00.640.195 I ggml_metal_init: use bfloat            = true
0.00.640.196 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.823 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.661.557 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.967 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.664.969 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.664.969 I llama_init_from_model: graph nodes  = 967
0.00.664.970 I llama_init_from_model: graph splits = 2
0.00.664.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.275 I 
0.00.702.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.358 I perplexity: tokenizing the input ..
0.00.708.420 I perplexity: tokenization took 6.061 ms
0.00.708.427 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.854 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.846.192 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.846.215 I llama_perf_context_print:        load time =     692.90 ms
0.00.846.216 I llama_perf_context_print: prompt eval time =     136.18 ms /   128 tokens (    1.06 ms per token,   939.92 tokens per second)
0.00.846.217 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.217 I llama_perf_context_print:       total time =     143.94 ms /   129 tokens
0.00.846.594 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.078s
sys	0m0.157s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.311 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.334 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.061 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.880 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.880 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.882 I llama_model_loader: - type  f32:  194 tensors
0.00.025.882 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.882 I print_info: file format = GGUF V3 (latest)
0.00.025.883 I print_info: file type   = Q6_K
0.00.025.884 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.623 I load: special tokens cache size = 25
0.00.039.335 I load: token to piece cache size = 0.2984 MB
0.00.039.338 I print_info: arch             = gptneox
0.00.039.339 I print_info: vocab_only       = 0
0.00.039.339 I print_info: n_ctx_train      = 2048
0.00.039.339 I print_info: n_embd           = 2048
0.00.039.339 I print_info: n_layer          = 24
0.00.039.342 I print_info: n_head           = 16
0.00.039.343 I print_info: n_head_kv        = 16
0.00.039.343 I print_info: n_rot            = 32
0.00.039.343 I print_info: n_swa            = 0
0.00.039.343 I print_info: n_embd_head_k    = 128
0.00.039.344 I print_info: n_embd_head_v    = 128
0.00.039.344 I print_info: n_gqa            = 1
0.00.039.345 I print_info: n_embd_k_gqa     = 2048
0.00.039.346 I print_info: n_embd_v_gqa     = 2048
0.00.039.346 I print_info: f_norm_eps       = 1.0e-05
0.00.039.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.351 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.353 I print_info: f_logit_scale    = 0.0e+00
0.00.039.354 I print_info: n_ff             = 8192
0.00.039.354 I print_info: n_expert         = 0
0.00.039.355 I print_info: n_expert_used    = 0
0.00.039.355 I print_info: causal attn      = 1
0.00.039.355 I print_info: pooling type     = 0
0.00.039.355 I print_info: rope type        = 2
0.00.039.355 I print_info: rope scaling     = linear
0.00.039.356 I print_info: freq_base_train  = 10000.0
0.00.039.356 I print_info: freq_scale_train = 1
0.00.039.356 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.356 I print_info: rope_finetuned   = unknown
0.00.039.357 I print_info: ssm_d_conv       = 0
0.00.039.357 I print_info: ssm_d_inner      = 0
0.00.039.357 I print_info: ssm_d_state      = 0
0.00.039.357 I print_info: ssm_dt_rank      = 0
0.00.039.358 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.358 I print_info: model type       = 1.4B
0.00.039.359 I print_info: model params     = 1.41 B
0.00.039.359 I print_info: general.name     = 1.4B
0.00.039.359 I print_info: vocab type       = BPE
0.00.039.360 I print_info: n_vocab          = 50304
0.00.039.360 I print_info: n_merges         = 50009
0.00.039.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.361 I print_info: LF token         = 187 ''
0.00.039.361 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.361 I print_info: max token length = 1024
0.00.039.362 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.659.332 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.352 I load_tensors: offloading output layer to GPU
0.00.659.352 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.396 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.659.397 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.660.840 I llama_init_from_model: n_seq_max     = 1
0.00.660.844 I llama_init_from_model: n_ctx         = 2048
0.00.660.845 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.845 I llama_init_from_model: n_batch       = 2048
0.00.660.845 I llama_init_from_model: n_ubatch      = 512
0.00.660.846 I llama_init_from_model: flash_attn    = 0
0.00.660.848 I llama_init_from_model: freq_base     = 10000.0
0.00.660.848 I llama_init_from_model: freq_scale    = 1
0.00.660.850 I ggml_metal_init: allocating
0.00.660.941 I ggml_metal_init: found device: Apple M4
0.00.660.956 I ggml_metal_init: picking default device: Apple M4
0.00.662.868 I ggml_metal_init: using embedded metal library
0.00.669.410 I ggml_metal_init: GPU name:   Apple M4
0.00.669.414 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.415 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.416 I ggml_metal_init: simdgroup reduction   = true
0.00.669.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.416 I ggml_metal_init: has residency sets    = true
0.00.669.416 I ggml_metal_init: has bfloat            = true
0.00.669.417 I ggml_metal_init: use bfloat            = true
0.00.669.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.713 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.940 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.948 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.988 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.238 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.747.240 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.747.241 I llama_init_from_model: graph nodes  = 967
0.00.747.241 I llama_init_from_model: graph splits = 2
0.00.747.246 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.605 I main: llama threadpool init, n_threads = 4
0.00.806.652 I 
0.00.806.675 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.676 I 
0.00.806.807 I sampler seed: 1234
0.00.806.812 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.831 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.832 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.728.295 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.728.296 I llama_perf_context_print:        load time =     796.56 ms
0.01.728.297 I llama_perf_context_print: prompt eval time =      57.55 ms /     7 tokens (    8.22 ms per token,   121.62 tokens per second)
0.01.728.298 I llama_perf_context_print:        eval time =     860.97 ms /    63 runs   (   13.67 ms per token,    73.17 tokens per second)
0.01.728.299 I llama_perf_context_print:       total time =     922.42 ms /    70 tokens
0.01.728.610 I ggml_metal_free: deallocating

real	0m1.747s
user	0m0.110s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.285 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.153 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.920 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.690 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.692 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.692 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.692 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.693 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.693 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.694 I llama_model_loader: - type  f32:  194 tensors
0.00.025.694 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.695 I print_info: file format = GGUF V3 (latest)
0.00.025.696 I print_info: file type   = Q6_K
0.00.025.697 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.998 I load: special tokens cache size = 25
0.00.039.794 I load: token to piece cache size = 0.2984 MB
0.00.039.798 I print_info: arch             = gptneox
0.00.039.798 I print_info: vocab_only       = 0
0.00.039.798 I print_info: n_ctx_train      = 2048
0.00.039.798 I print_info: n_embd           = 2048
0.00.039.798 I print_info: n_layer          = 24
0.00.039.802 I print_info: n_head           = 16
0.00.039.803 I print_info: n_head_kv        = 16
0.00.039.804 I print_info: n_rot            = 32
0.00.039.804 I print_info: n_swa            = 0
0.00.039.804 I print_info: n_embd_head_k    = 128
0.00.039.806 I print_info: n_embd_head_v    = 128
0.00.039.807 I print_info: n_gqa            = 1
0.00.039.811 I print_info: n_embd_k_gqa     = 2048
0.00.039.811 I print_info: n_embd_v_gqa     = 2048
0.00.039.812 I print_info: f_norm_eps       = 1.0e-05
0.00.039.812 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.812 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.813 I print_info: f_logit_scale    = 0.0e+00
0.00.039.813 I print_info: n_ff             = 8192
0.00.039.813 I print_info: n_expert         = 0
0.00.039.814 I print_info: n_expert_used    = 0
0.00.039.815 I print_info: causal attn      = 1
0.00.039.815 I print_info: pooling type     = 0
0.00.039.815 I print_info: rope type        = 2
0.00.039.815 I print_info: rope scaling     = linear
0.00.039.816 I print_info: freq_base_train  = 10000.0
0.00.039.816 I print_info: freq_scale_train = 1
0.00.039.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.816 I print_info: rope_finetuned   = unknown
0.00.039.816 I print_info: ssm_d_conv       = 0
0.00.039.816 I print_info: ssm_d_inner      = 0
0.00.039.816 I print_info: ssm_d_state      = 0
0.00.039.817 I print_info: ssm_dt_rank      = 0
0.00.039.817 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.817 I print_info: model type       = 1.4B
0.00.039.818 I print_info: model params     = 1.41 B
0.00.039.818 I print_info: general.name     = 1.4B
0.00.039.818 I print_info: vocab type       = BPE
0.00.039.818 I print_info: n_vocab          = 50304
0.00.039.818 I print_info: n_merges         = 50009
0.00.039.819 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.819 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.819 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.819 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.819 I print_info: LF token         = 187 ''
0.00.039.820 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.820 I print_info: max token length = 1024
0.00.039.822 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.137 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.144 I load_tensors: offloading output layer to GPU
0.00.604.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.171 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.604.174 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.605.796 I llama_init_from_model: n_seq_max     = 1
0.00.605.798 I llama_init_from_model: n_ctx         = 128
0.00.605.798 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.798 I llama_init_from_model: n_batch       = 128
0.00.605.799 I llama_init_from_model: n_ubatch      = 128
0.00.605.799 I llama_init_from_model: flash_attn    = 0
0.00.605.800 I llama_init_from_model: freq_base     = 10000.0
0.00.605.801 I llama_init_from_model: freq_scale    = 1
0.00.605.801 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.803 I ggml_metal_init: allocating
0.00.605.847 I ggml_metal_init: found device: Apple M4
0.00.605.857 I ggml_metal_init: picking default device: Apple M4
0.00.607.155 I ggml_metal_init: using embedded metal library
0.00.613.215 I ggml_metal_init: GPU name:   Apple M4
0.00.613.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.221 I ggml_metal_init: simdgroup reduction   = true
0.00.613.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.222 I ggml_metal_init: has residency sets    = true
0.00.613.222 I ggml_metal_init: has bfloat            = true
0.00.613.222 I ggml_metal_init: use bfloat            = true
0.00.613.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.227 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.083 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.535 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.546 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.591 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.713 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.714 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.715 I llama_init_from_model: graph nodes  = 967
0.00.636.715 I llama_init_from_model: graph splits = 2
0.00.636.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.061 I 
0.00.673.147 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.153 I perplexity: tokenizing the input ..
0.00.680.090 I perplexity: tokenization took 6.935 ms
0.00.680.099 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.414 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.812.752 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.812.775 I llama_perf_context_print:        load time =     662.77 ms
0.00.812.776 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.46 tokens per second)
0.00.812.777 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.777 I llama_perf_context_print:       total time =     139.72 ms /   129 tokens
0.00.813.136 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.077s
sys	0m0.133s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4784 (b95c8af3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10de08200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10de08900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10de08eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10de09460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10de09a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10de09fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10de0a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10de0ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10de0b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10de0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10de0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10de0bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10de0caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10de0d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10de0dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10de0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10de0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10de0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10de0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10de0ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10de10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10de10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10de11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10de11d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10de12420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10de126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10de12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10de13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10de13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10de14160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10de14600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10de148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10de15150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10de15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10de15950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10de15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10de16290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10de16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10de16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10de17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10de17510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10de179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10de17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10de182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10de185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10de18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10de191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10de19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10de1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10de1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10de1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10de1b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10de1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10de1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10de1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10de1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10de1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10de1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10de1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10de1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10de1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10de1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10de1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10de1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10de1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10de1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10de1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10de20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10de20900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10de20da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10de21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10de216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10de21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10de220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10de22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10de22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10de230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10de23610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10de23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10de240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10de24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10de24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10de250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10de255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10de25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10de26090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10de265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10de26b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10de27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10de275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10de27b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10de28070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10de285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10de28b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10de29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10de295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10de29b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10de197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10de29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10de2a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10de2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10de2b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10de2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10de2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10de2c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10de2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10de2cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10de2d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10de2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10de2dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10de2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10de2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10de2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10de2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10de2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10de2fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10de2feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10de30350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10de307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10de30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10de31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10de315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10de31a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10de31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10de323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10de32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10de32cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10de33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10de33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10de33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10de33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10de34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10de348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10de34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10de351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10de35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10de35b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10de35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10de36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10de36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10de36db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10de37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10de376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10de37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10de38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10de384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10de38970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10de38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10de392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10de39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10de39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10de3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10de3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10de3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10de3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10de3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10de3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10de3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10de3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10de3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10de3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10de3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10de3d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10de3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10de3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10de3e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10de3e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10de3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10de3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10de3f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10de3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10de3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10de401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10de40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10de40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10de40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10de41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10de418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10de41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10de42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10de426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10de42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10de42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10de43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10de43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10de43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10de44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10de44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10de44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10de45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10de454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10de45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10de45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10de46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10de468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10de46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10de47370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10de47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10de47c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10de48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10de48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10de49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10de494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10de497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10de49dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10de4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10de4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10de4b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10de4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10de4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10de4c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10de4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10de4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10de4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10de4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10de4dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10de4e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10de4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10de4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10de4f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10de4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10de4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10de50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10de50660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10de50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10de51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10de51650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10de51ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10de520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10de52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10de52b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10de530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10de53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10de53b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10de540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10de54620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10de54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10de550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10de55610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10de55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10de560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10de56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10de56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10de570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10de575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10de57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10de58090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10de585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10de58b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10de59080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10de595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10de59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10de5a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10de5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10de5ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10de5b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10de5b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10de5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10de5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10de5c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10de5caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10de5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10de5d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10de5dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10de5e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10de5e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10de5ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10de5ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10de5f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10de5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10de5fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10de601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10de60690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10de60b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10de60fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10de61470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10de61910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10de61db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10de62250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10de626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10de62b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10de63030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10de634d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10de63970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10de63e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10de642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10de64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10de64bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10de65090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10de65530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10de659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10de65e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10de663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10de66ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10de67200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10de67920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10de68040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10de68300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10de68af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10de68db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10de693c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.717.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d204dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d205240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d2056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d205b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d205f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d206400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d206870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d206ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d207150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d2075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d207a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d208120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d208c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d2093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d209c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d20a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d20aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d20b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d20b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d20bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d20c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d20cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d20d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d20dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d20e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d20e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d20e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d20ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d20f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d20f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d20fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d20ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d210430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d2106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d210b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d210fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d211440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d2118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d211d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d212190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d212600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d212a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d212ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d213350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d2137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d213c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d2140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d214510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d214980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d214df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d215260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d2156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d215b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d215fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d216420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d216890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d216e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d217300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d217770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d217be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d218050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d2184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d218930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d218da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d219210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d219680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d219af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d219f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d21a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d21a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d21acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d21b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d21b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d21ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d21be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d21c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d21c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d21cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d21d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d21d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d21d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d21dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d21e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d21e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d21ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d21ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d21f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d21f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d21fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d220100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d220570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d2209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d220e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d2212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d221730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d221ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d222010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d222480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d2228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d222d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d2231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d223640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d223ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d223f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d224390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d224800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d224c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d2250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d225550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d2259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d225e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d2262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d226710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d226b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d226ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d227460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d2278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d227d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d2281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d228620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d228a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d228f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d229370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d2297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d229c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d22a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d22a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d22a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d22ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d22b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d22b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d22bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d22bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d22c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d22c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d22cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d22d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d22d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d22da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d22dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d22e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d22e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d22ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d22f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d22f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d22f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d22fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d230260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d2306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d230b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d230fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d231420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d231890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d231d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d232170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d2325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d232a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d232ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d233330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d2337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d233c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d234080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d2344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d234960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d234dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d235240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d235e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d236130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d2363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d236860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d236cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d237140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d2375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d237a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d237e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d238300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d238770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d238be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d239050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d2394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d239930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d239da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d23a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d23a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d23aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d23af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d23b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d23b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d23bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d23c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d23c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d23ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d23ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d23d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d23d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d23dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d23e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d23e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d23e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d23ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d23f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d23f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d23fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d2400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d240540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d2409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d240e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d241290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d2417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d241cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d242830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d242af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d2430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d243670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d243c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d2441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d2447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d244d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d245330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d2458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d245eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d246470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d246a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d246ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d2475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d247b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d248130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d2486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d248cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d249270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d249830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d249df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d24a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d24a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d24af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d24b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d24bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d24c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d24c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d24cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d24d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d24d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d24dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d24e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d24e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d24ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d24f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d24f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d24ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d250570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d250b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d2510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d2516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d251c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d252230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d2527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d252db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d253370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d253930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d253ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d2544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d254a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d255030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d2555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d255bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d256170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d256730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d256cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d2571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d2576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d257bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d2580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d2585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d258af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d258ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d2594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d2599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d259ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d25a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d25a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d25adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d25b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10d25b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10d25bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10d25c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10d25c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10d25cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10d25d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10d25d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10d25daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10d25dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10d25e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d25e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d25f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d25fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d260240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d260960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d260c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d261410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d2616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d261ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1197044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x119704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1197056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1197063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x119706cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119707140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119707860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119708380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x119708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119709340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x119709a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11970a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11970a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11970afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11970b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11970be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11970c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11970cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11970d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11970da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11970dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11970e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11970e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11970e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11970ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11970f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11970f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11970fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11970fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1197102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119711460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1197118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1197121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119712620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119712a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1197137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119713c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1197140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119714530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1197149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x119714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1197156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119715b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119715fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119716a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119716eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119717320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x119717c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119718070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1197184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119718950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119718dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119719230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1197196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x119719b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x119719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11971a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11971a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11971acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11971b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11971b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11971ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11971be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11971c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11971c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11971cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11971d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11971d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11971d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11971dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11971e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11971e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11971eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11971ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11971f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11971f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11971fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119720a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119720e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1197212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119721750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1197234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x119723a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119724000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1197245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119724b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119725110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1197256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119725c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x119726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1197267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119726d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119727330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1197278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119727de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1197282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1197287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1197291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1197296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11972a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11972a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11972aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11972afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11972b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11972b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11972bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11972c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11972c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11972cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11972d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11972d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11972dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11972e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11972e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11972ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11972f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11972f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11972fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11972ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1197304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1197309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x119730ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1197313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1197318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x119731de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1197322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1197327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1197331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1197336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119733be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1197340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1197345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x119734ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1197354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1197359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x119735ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1197363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1197368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x119736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1197372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1197377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119737ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1197381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1197386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119738be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1197390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1197395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119739ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11973a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11973a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11973aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11973b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11973b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11973bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11973c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11973c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11973cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11973d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11973d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11973dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11973e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11973e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11973eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11973efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11973f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11973f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11973fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1197403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1197408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x119740e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x119741440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1197419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x119741fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1197425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1197431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1197439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x119743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x119744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x119744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119744d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119745530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1197459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119745e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x119746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119746ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119747ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119748aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119748ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119749540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119749a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x119749fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11974a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11974aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11974afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11974b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11974ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11974bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11974c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11974ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11974cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11974d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11974da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11974dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11974e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11974ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11974ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11974f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11974fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11974ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1197504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x119750a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119750f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1197514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119751a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x119751f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1197524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119752a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1197534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1197539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119753f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x119754490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1197549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x119754f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x119755480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1197559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x119755f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x119756470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1197569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x119756f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119757460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1197579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119757f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1197589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119758ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119759440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1197598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119759d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11975a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11975a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11975ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11975b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11975b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11975b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11975bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11975c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11975c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11975cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11975d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11975d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11975d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11975de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11975e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11975e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11975ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11975f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11975f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11975fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11975fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x119760340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1197607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119760d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119761450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x119762290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1197629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119762c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119763460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119763720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119763d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.781s
user	0m0.281s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4784 (b95c8af3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe090e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe09750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe09bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe0a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe0a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe0a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe0b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe0b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe0bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe0c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe0f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe11400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe12ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe15120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe15590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe17ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe18010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe18d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe1a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe1a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe1b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe1ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe23e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe25540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe25af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe26650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe27d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe28870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe19380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe29b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe29fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe2b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe2c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe2cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe2d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe2ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe2f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe30c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe32a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe34300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe38400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe3de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe3e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe40b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe41a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe43800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe43d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe44700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe46000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe46500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe46f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe47900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe4a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe4c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe4d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe4ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe4f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe50a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe50fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe51a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe58f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe59f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10fe04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10fe04630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10fe04aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10fe04f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10fe05380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10fe057f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10fe05c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10fe060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10fe06540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10fe069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10fe06e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10fe07290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10fe07700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10fe07b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10fe07fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10fe08450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10fe088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10fe08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10fe09270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10fe096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10fe09b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10fe09fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10fe0a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10fe0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10fe0ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10fe0b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10fe0b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10fe0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10fe0bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10fe0c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10fe0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10fe0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10fe0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10fe0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10fe0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10fe0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10fe0e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10fe0e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10fe0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10fe0efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10fe0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10fe0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10fe0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10fe10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10fe10e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10fe11570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10fe11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10fe11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10fe12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10fe12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10fe13010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.105.645 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ff0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ff0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ff0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ff0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ff0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ff0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ff0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ff0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ff0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ff0dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ff0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ff0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ff0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ff0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ff101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ff108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ff10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ff11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ff11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ff12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ff12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ff13440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ff13b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ff14280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ff149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ff14c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ff14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ff15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ff15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ff15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ff160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ff16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ff16a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ff16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ff171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ff17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ff17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ff17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ff18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ff187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ff18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ff190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ff19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ff199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ff19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ff1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ff1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ff1ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ff1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ff1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ff1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ff1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ff1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ff1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ff1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ff1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ff1d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ff1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ff1ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ff1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ff1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ff1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ff1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ff1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ff1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ff1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ff20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ff205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ff20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ff20e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ff21300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ff21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ff21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ff22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ff224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ff22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ff22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ff23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ff23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ff23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ff23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ff243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ff24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ff24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131805b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131805e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1318060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131806540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1318069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131806e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131807290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131807700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131807b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131807fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131808450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1318088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131808d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1318091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131809610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131809a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131809ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13180a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13180a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13180ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13180b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13180b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13180b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13180be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13180c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13180c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13180cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13180cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13180d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13180d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13180dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13180e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13180e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13180ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13180eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13180f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13180f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13180fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131810090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131810500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131810970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131810de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131811250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1318116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131811b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131811fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131812410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131812880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131812cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131813160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1318135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131813a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131813eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131814320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131814790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131814c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131815070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1318154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131815950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131815dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131816230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1318166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131816b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131816f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1318173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131817860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131817cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131818140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1318185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131818a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131818e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131819300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131819770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131819be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13181a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13181a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13181a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13181ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13181b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13181b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13181baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13181bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13181cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13181ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13181d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13181d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13181d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13181de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13181e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13181e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13181eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13181f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13181f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13181f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13181fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1318201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131820630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131820aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131820f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131821380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1318217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131821c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1318220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131822540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1318229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131822e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131823290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131823700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131823b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131823fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131824450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1318248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131824d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1318251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131825610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131825a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131825ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131826360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1318268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131826dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131827240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1318276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131827f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1318284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1318289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131829530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1318297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131829db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13182a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13182a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13182aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13182b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13182ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13182c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13182c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13182cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13182d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13182d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13182dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13182e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13182e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13182ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13182f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13182f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13182ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131830530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1318310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131831670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131831c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1318321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1318327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131832d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131833330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1318338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131833eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131834470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131834a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131834ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1318355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131835b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131836130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1318366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131836cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131837270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131837830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131837df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1318383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131838970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131838f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1318394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131839ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13183a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13183a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13183abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13183b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13183b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13183bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13183c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13183c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13183ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13183d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13183d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13183def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13183e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13183e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13183edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13183f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13183f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13183fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1318401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1318406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131840bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1318410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1318415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131841af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131841ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1318424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1318429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x131842ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1318433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1318438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x131843df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1318442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1318447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x131844cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1318451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1318456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131846100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131846820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131846f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131847660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131847920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131848110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1318483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1318489e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1318335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1318324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13182f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13182c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13183bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1318397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131837530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1318352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13182d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13182abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13182fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131830db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1318363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131833030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13183aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13182d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131835e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1318307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131836f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131834170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13182f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131839d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131834cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13182a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13183cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131831ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13183a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131830230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131832a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1318369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13182dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131838670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131829ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13182ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13183b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131838c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131834730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13183d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13182bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13183d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13182b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13183ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131835870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131837af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13183a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1318391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131831370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131847be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1318490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131849380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131849640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131849900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131849bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131849e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13184a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13184a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13184a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13184a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13184ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13184af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13184b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13184b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13184b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13184ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13184bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13184bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13184c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13184c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13184c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13184ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13184cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13184d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13184d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13184d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13184d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13184db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13184ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13184e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13184e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13184e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13184e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13184eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13184ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13184f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13184f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13184f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13184f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13184fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13184fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131850180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131850440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131850700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1318509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131850c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131850f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131851200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1318514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131851780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131851a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131851d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131851fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131852280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131852540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131852800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131852d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131853040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131853300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1318535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131853880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131853b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131853e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1318540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131854380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131854640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131854900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131854bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131854e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131855140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131855400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1318556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131855980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131855c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131855f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1318561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131856480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131856740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131856a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131856cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131856f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131857240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131857500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1318577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131857a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131857d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131858000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1318582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131858580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131858840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131858b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131858dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131859080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131859340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131859600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131859a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131859cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131859f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13185a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13185a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13185acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13185b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13185b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13185ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13185be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13185c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13185c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13185cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13185d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13185d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13185d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13185dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13185e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13185e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13185eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13185ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13185f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13185f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13185fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131860120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131860590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131860a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131860e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1318612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131861750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131861bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131862030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1318624a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131862910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131862d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1318631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131863660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131863ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131863f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1318643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131864820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131864c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131865100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131865570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1318659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131865e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1318662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131866730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131866ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131867010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131867480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1318678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131867d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1318681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131868640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131868ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131868f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131869390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131869b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131869e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13186a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13186aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13186b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13186b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13186bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13186c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13186c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13186cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13186d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13186d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13186dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13186e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13186e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13186ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13186f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13186f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13186fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131870250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1318707a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131870cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131871240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131871790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131871ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131872230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131872780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131872cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131873220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131873770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131873cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131874210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131874760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131874cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131875200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131875750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131875ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1318761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131876740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131876c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1318771e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131877730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131877c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1318781d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131878720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131878c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1318791c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131879710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131879c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13187a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13187a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13187ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13187b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13187b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13187bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13187c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13187c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13187cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13187d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13187d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13187dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13187e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13187e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13187ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13187f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13187f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13187faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13187ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1318803e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131880880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131880d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1318811c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131881660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131881b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131881fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131882440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1318828e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131882d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131883220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1318836c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x131883b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x131884000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1318844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x131884940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x131884de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x131885280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x131885720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x131885bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x131886060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x131886500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131886a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131887170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131887890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131887fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1318886d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131888990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131889180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131889440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131889a50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.968s
user	0m0.233s
sys	0m0.196s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.71 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.15 sec*proc (2 tests)

Total Test time (real) =   2.16 sec
        2.18 real         0.52 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
```
