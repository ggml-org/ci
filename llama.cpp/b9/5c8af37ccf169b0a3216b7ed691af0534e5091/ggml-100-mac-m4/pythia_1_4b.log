Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.092s
user	0m1.049s
sys	0m1.491s
++ nproc
+ make -j10
[  0%] Generating build details from Git
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Generating build details from Git
[  5%] Built target ggml-base
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 10%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Built target build_info
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-autorelease
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-infill
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-cli
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-passkey
[ 81%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-perplexity
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-save-load-state
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.227s
user	0m6.539s
sys	0m10.188s

main: quantize time =  3247.71 ms
main:    total time =  3247.71 ms

main: quantize time =  1441.51 ms
main:    total time =  1441.51 ms

main: quantize time =  1838.78 ms
main:    total time =  1838.78 ms

main: quantize time =  2860.40 ms
main:    total time =  2860.40 ms

main: quantize time =  2051.20 ms
main:    total time =  2051.20 ms

main: quantize time =  5061.85 ms
main:    total time =  5061.85 ms

main: quantize time =  5995.21 ms
main:    total time =  5995.21 ms

main: quantize time =  6932.72 ms
main:    total time =  6932.72 ms

main: quantize time =  5907.68 ms
main:    total time =  5907.68 ms

main: quantize time =  4577.38 ms
main:    total time =  4577.38 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.203 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.381 I main: llama backend init
0.00.000.388 I main: load the model and apply lora adapter, if any
0.00.027.312 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.963 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.985 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.994 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.591 I llama_model_loader: - type  f32:  194 tensors
0.00.058.591 I llama_model_loader: - type  f16:   98 tensors
0.00.058.593 I print_info: file format = GGUF V3 (latest)
0.00.058.597 I print_info: file type   = all F32 (guessed)
0.00.058.599 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.072.781 I load: special tokens cache size = 25
0.00.081.541 I load: token to piece cache size = 0.2984 MB
0.00.081.544 I print_info: arch             = gptneox
0.00.081.545 I print_info: vocab_only       = 0
0.00.081.545 I print_info: n_ctx_train      = 2048
0.00.081.545 I print_info: n_embd           = 2048
0.00.081.545 I print_info: n_layer          = 24
0.00.081.548 I print_info: n_head           = 16
0.00.081.549 I print_info: n_head_kv        = 16
0.00.081.549 I print_info: n_rot            = 32
0.00.081.549 I print_info: n_swa            = 0
0.00.081.549 I print_info: n_embd_head_k    = 128
0.00.081.550 I print_info: n_embd_head_v    = 128
0.00.081.550 I print_info: n_gqa            = 1
0.00.081.551 I print_info: n_embd_k_gqa     = 2048
0.00.081.552 I print_info: n_embd_v_gqa     = 2048
0.00.081.554 I print_info: f_norm_eps       = 1.0e-05
0.00.081.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.555 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.555 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.555 I print_info: f_logit_scale    = 0.0e+00
0.00.081.556 I print_info: n_ff             = 8192
0.00.081.556 I print_info: n_expert         = 0
0.00.081.556 I print_info: n_expert_used    = 0
0.00.081.557 I print_info: causal attn      = 1
0.00.081.558 I print_info: pooling type     = 0
0.00.081.558 I print_info: rope type        = 2
0.00.081.559 I print_info: rope scaling     = linear
0.00.081.559 I print_info: freq_base_train  = 10000.0
0.00.081.559 I print_info: freq_scale_train = 1
0.00.081.559 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.560 I print_info: rope_finetuned   = unknown
0.00.081.560 I print_info: ssm_d_conv       = 0
0.00.081.560 I print_info: ssm_d_inner      = 0
0.00.081.560 I print_info: ssm_d_state      = 0
0.00.081.560 I print_info: ssm_dt_rank      = 0
0.00.081.560 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.561 I print_info: model type       = 1.4B
0.00.081.561 I print_info: model params     = 1.41 B
0.00.081.561 I print_info: general.name     = 1.4B
0.00.081.562 I print_info: vocab type       = BPE
0.00.081.562 I print_info: n_vocab          = 50304
0.00.081.562 I print_info: n_merges         = 50009
0.00.081.562 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.567 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.567 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.567 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.567 I print_info: LF token         = 187 'Ċ'
0.00.081.568 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.568 I print_info: max token length = 1024
0.00.081.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.158.806 I load_tensors: offloading 24 repeating layers to GPU
0.00.158.810 I load_tensors: offloading output layer to GPU
0.00.158.811 I load_tensors: offloaded 25/25 layers to GPU
0.00.158.838 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.158.839 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.159.308 I llama_init_from_model: n_seq_max     = 1
0.00.159.309 I llama_init_from_model: n_ctx         = 2048
0.00.159.309 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.159.309 I llama_init_from_model: n_batch       = 2048
0.00.159.309 I llama_init_from_model: n_ubatch      = 512
0.00.159.309 I llama_init_from_model: flash_attn    = 0
0.00.159.310 I llama_init_from_model: freq_base     = 10000.0
0.00.159.310 I llama_init_from_model: freq_scale    = 1
0.00.159.311 I ggml_metal_init: allocating
0.00.159.343 I ggml_metal_init: found device: Apple M4
0.00.159.349 I ggml_metal_init: picking default device: Apple M4
0.00.160.097 I ggml_metal_init: using embedded metal library
0.00.178.088 I ggml_metal_init: GPU name:   Apple M4
0.00.178.089 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.178.090 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.178.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.178.090 I ggml_metal_init: simdgroup reduction   = true
0.00.178.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.178.091 I ggml_metal_init: has residency sets    = true
0.00.178.091 I ggml_metal_init: has bfloat            = true
0.00.178.091 I ggml_metal_init: use bfloat            = true
0.00.178.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.178.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.236.955 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.268.126 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.268.132 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.268.177 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.273.090 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.273.092 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.273.092 I llama_init_from_model: graph nodes  = 967
0.00.273.092 I llama_init_from_model: graph splits = 2
0.00.273.098 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.273.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.273.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.339.421 I main: llama threadpool init, n_threads = 4
0.00.339.463 I 
0.00.339.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.339.495 I 
0.00.339.670 I sampler seed: 1234
0.00.339.675 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.339.700 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.339.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.339.702 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.174.052 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.02.174.053 I llama_perf_context_print:        load time =     311.18 ms
0.02.174.054 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.33 tokens per second)
0.02.174.055 I llama_perf_context_print:        eval time =    1787.84 ms /    63 runs   (   28.38 ms per token,    35.24 tokens per second)
0.02.174.055 I llama_perf_context_print:       total time =    1835.55 ms /    70 tokens
0.02.174.256 I ggml_metal_free: deallocating

real	0m2.564s
user	0m0.133s
sys	0m0.166s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.948 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.737 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.739 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.740 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.744 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.772 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.510 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.511 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.512 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.514 I llama_model_loader: - type  f32:  194 tensors
0.00.034.514 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.515 I print_info: file format = GGUF V3 (latest)
0.00.034.516 I print_info: file type   = Q8_0
0.00.034.517 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.359 I load: special tokens cache size = 25
0.00.049.965 I load: token to piece cache size = 0.2984 MB
0.00.049.970 I print_info: arch             = gptneox
0.00.049.970 I print_info: vocab_only       = 0
0.00.049.971 I print_info: n_ctx_train      = 2048
0.00.049.973 I print_info: n_embd           = 2048
0.00.049.973 I print_info: n_layer          = 24
0.00.049.978 I print_info: n_head           = 16
0.00.049.979 I print_info: n_head_kv        = 16
0.00.049.980 I print_info: n_rot            = 32
0.00.049.981 I print_info: n_swa            = 0
0.00.049.981 I print_info: n_embd_head_k    = 128
0.00.049.981 I print_info: n_embd_head_v    = 128
0.00.049.982 I print_info: n_gqa            = 1
0.00.049.983 I print_info: n_embd_k_gqa     = 2048
0.00.049.983 I print_info: n_embd_v_gqa     = 2048
0.00.049.985 I print_info: f_norm_eps       = 1.0e-05
0.00.049.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.987 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.987 I print_info: f_logit_scale    = 0.0e+00
0.00.049.988 I print_info: n_ff             = 8192
0.00.049.989 I print_info: n_expert         = 0
0.00.049.989 I print_info: n_expert_used    = 0
0.00.049.989 I print_info: causal attn      = 1
0.00.049.989 I print_info: pooling type     = 0
0.00.049.990 I print_info: rope type        = 2
0.00.049.990 I print_info: rope scaling     = linear
0.00.049.990 I print_info: freq_base_train  = 10000.0
0.00.049.991 I print_info: freq_scale_train = 1
0.00.049.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.991 I print_info: rope_finetuned   = unknown
0.00.049.991 I print_info: ssm_d_conv       = 0
0.00.049.991 I print_info: ssm_d_inner      = 0
0.00.049.992 I print_info: ssm_d_state      = 0
0.00.049.992 I print_info: ssm_dt_rank      = 0
0.00.049.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.992 I print_info: model type       = 1.4B
0.00.049.992 I print_info: model params     = 1.41 B
0.00.049.992 I print_info: general.name     = 1.4B
0.00.049.993 I print_info: vocab type       = BPE
0.00.049.993 I print_info: n_vocab          = 50304
0.00.049.993 I print_info: n_merges         = 50009
0.00.049.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.994 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.994 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.994 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.994 I print_info: LF token         = 187 'Ċ'
0.00.049.994 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.995 I print_info: max token length = 1024
0.00.049.995 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.237.813 I load_tensors: offloading 24 repeating layers to GPU
0.01.237.818 I load_tensors: offloading output layer to GPU
0.01.237.820 I load_tensors: offloaded 25/25 layers to GPU
0.01.237.843 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.237.845 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.239.015 I llama_init_from_model: n_seq_max     = 1
0.01.239.017 I llama_init_from_model: n_ctx         = 2048
0.01.239.018 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.239.018 I llama_init_from_model: n_batch       = 2048
0.01.239.018 I llama_init_from_model: n_ubatch      = 512
0.01.239.019 I llama_init_from_model: flash_attn    = 0
0.01.239.020 I llama_init_from_model: freq_base     = 10000.0
0.01.239.020 I llama_init_from_model: freq_scale    = 1
0.01.239.021 I ggml_metal_init: allocating
0.01.239.034 I ggml_metal_init: found device: Apple M4
0.01.239.044 I ggml_metal_init: picking default device: Apple M4
0.01.240.345 I ggml_metal_init: using embedded metal library
0.01.246.017 I ggml_metal_init: GPU name:   Apple M4
0.01.246.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.246.020 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.246.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.246.021 I ggml_metal_init: simdgroup reduction   = true
0.01.246.021 I ggml_metal_init: simdgroup matrix mul. = true
0.01.246.022 I ggml_metal_init: has residency sets    = true
0.01.246.022 I ggml_metal_init: has bfloat            = true
0.01.246.022 I ggml_metal_init: use bfloat            = true
0.01.246.023 I ggml_metal_init: hasUnifiedMemory      = true
0.01.246.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.262.014 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.310.236 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.310.243 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.310.278 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.314.424 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.314.426 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.314.426 I llama_init_from_model: graph nodes  = 967
0.01.314.426 I llama_init_from_model: graph splits = 2
0.01.314.433 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.314.564 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.314.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.370.091 I main: llama threadpool init, n_threads = 4
0.01.370.136 I 
0.01.370.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.370.160 I 
0.01.370.339 I sampler seed: 1234
0.01.370.343 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.370.362 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.370.362 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.370.362 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.480.428 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46957.67 tokens per second)
0.02.480.428 I llama_perf_context_print:        load time =    1359.39 ms
0.02.480.429 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.11 tokens per second)
0.02.480.430 I llama_perf_context_print:        eval time =    1058.52 ms /    63 runs   (   16.80 ms per token,    59.52 tokens per second)
0.02.480.431 I llama_perf_context_print:       total time =    1111.08 ms /    70 tokens
0.02.480.710 I ggml_metal_free: deallocating

real	0m2.497s
user	0m0.110s
sys	0m0.286s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.713 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.027.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.436 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.710 I llama_model_loader: - type  f32:  194 tensors
0.00.036.710 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.711 I print_info: file format = GGUF V3 (latest)
0.00.036.712 I print_info: file type   = Q4_0
0.00.036.716 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.423 I load: special tokens cache size = 25
0.00.053.971 I load: token to piece cache size = 0.2984 MB
0.00.053.975 I print_info: arch             = gptneox
0.00.053.975 I print_info: vocab_only       = 0
0.00.053.975 I print_info: n_ctx_train      = 2048
0.00.053.976 I print_info: n_embd           = 2048
0.00.053.976 I print_info: n_layer          = 24
0.00.053.979 I print_info: n_head           = 16
0.00.053.980 I print_info: n_head_kv        = 16
0.00.053.980 I print_info: n_rot            = 32
0.00.053.980 I print_info: n_swa            = 0
0.00.053.981 I print_info: n_embd_head_k    = 128
0.00.053.981 I print_info: n_embd_head_v    = 128
0.00.053.983 I print_info: n_gqa            = 1
0.00.053.984 I print_info: n_embd_k_gqa     = 2048
0.00.053.985 I print_info: n_embd_v_gqa     = 2048
0.00.053.985 I print_info: f_norm_eps       = 1.0e-05
0.00.053.986 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.987 I print_info: f_logit_scale    = 0.0e+00
0.00.053.987 I print_info: n_ff             = 8192
0.00.053.987 I print_info: n_expert         = 0
0.00.053.988 I print_info: n_expert_used    = 0
0.00.053.988 I print_info: causal attn      = 1
0.00.053.988 I print_info: pooling type     = 0
0.00.053.988 I print_info: rope type        = 2
0.00.053.988 I print_info: rope scaling     = linear
0.00.053.989 I print_info: freq_base_train  = 10000.0
0.00.053.989 I print_info: freq_scale_train = 1
0.00.053.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.989 I print_info: rope_finetuned   = unknown
0.00.053.990 I print_info: ssm_d_conv       = 0
0.00.053.990 I print_info: ssm_d_inner      = 0
0.00.053.990 I print_info: ssm_d_state      = 0
0.00.053.992 I print_info: ssm_dt_rank      = 0
0.00.053.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.992 I print_info: model type       = 1.4B
0.00.053.993 I print_info: model params     = 1.41 B
0.00.053.993 I print_info: general.name     = 1.4B
0.00.053.993 I print_info: vocab type       = BPE
0.00.053.993 I print_info: n_vocab          = 50304
0.00.053.993 I print_info: n_merges         = 50009
0.00.053.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.994 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.994 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.994 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.995 I print_info: LF token         = 187 'Ċ'
0.00.053.995 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.995 I print_info: max token length = 1024
0.00.053.996 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.782.441 I load_tensors: offloading 24 repeating layers to GPU
0.00.782.456 I load_tensors: offloading output layer to GPU
0.00.782.457 I load_tensors: offloaded 25/25 layers to GPU
0.00.782.496 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.782.498 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.783.797 I llama_init_from_model: n_seq_max     = 1
0.00.783.800 I llama_init_from_model: n_ctx         = 2048
0.00.783.800 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.783.801 I llama_init_from_model: n_batch       = 2048
0.00.783.802 I llama_init_from_model: n_ubatch      = 512
0.00.783.802 I llama_init_from_model: flash_attn    = 0
0.00.783.804 I llama_init_from_model: freq_base     = 10000.0
0.00.783.804 I llama_init_from_model: freq_scale    = 1
0.00.783.807 I ggml_metal_init: allocating
0.00.783.908 I ggml_metal_init: found device: Apple M4
0.00.783.923 I ggml_metal_init: picking default device: Apple M4
0.00.785.753 I ggml_metal_init: using embedded metal library
0.00.792.795 I ggml_metal_init: GPU name:   Apple M4
0.00.792.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.792.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.792.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.792.806 I ggml_metal_init: simdgroup reduction   = true
0.00.792.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.792.806 I ggml_metal_init: has residency sets    = true
0.00.792.806 I ggml_metal_init: has bfloat            = true
0.00.792.807 I ggml_metal_init: use bfloat            = true
0.00.792.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.792.812 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.811.462 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.865.074 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.865.081 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.865.119 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.869.542 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.869.544 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.869.544 I llama_init_from_model: graph nodes  = 967
0.00.869.544 I llama_init_from_model: graph splits = 2
0.00.869.552 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.869.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.869.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.125 I main: llama threadpool init, n_threads = 4
0.00.928.176 I 
0.00.928.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.201 I 
0.00.928.381 I sampler seed: 1234
0.00.928.386 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.928.429 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.928.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.928.433 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.613.702 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54741.71 tokens per second)
0.01.613.702 I llama_perf_context_print:        load time =     916.64 ms
0.01.613.703 I llama_perf_context_print: prompt eval time =      49.59 ms /     7 tokens (    7.08 ms per token,   141.16 tokens per second)
0.01.613.705 I llama_perf_context_print:        eval time =     632.83 ms /    63 runs   (   10.04 ms per token,    99.55 tokens per second)
0.01.613.705 I llama_perf_context_print:       total time =     686.34 ms /    70 tokens
0.01.613.982 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.115s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.828 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.430 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.431 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.431 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.432 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.433 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.435 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.304 I llama_model_loader: - type  f32:  194 tensors
0.00.036.304 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.304 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.305 I print_info: file format = GGUF V3 (latest)
0.00.036.305 I print_info: file type   = Q4_1
0.00.036.306 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.183 I load: special tokens cache size = 25
0.00.052.198 I load: token to piece cache size = 0.2984 MB
0.00.052.201 I print_info: arch             = gptneox
0.00.052.201 I print_info: vocab_only       = 0
0.00.052.201 I print_info: n_ctx_train      = 2048
0.00.052.201 I print_info: n_embd           = 2048
0.00.052.201 I print_info: n_layer          = 24
0.00.052.204 I print_info: n_head           = 16
0.00.052.205 I print_info: n_head_kv        = 16
0.00.052.205 I print_info: n_rot            = 32
0.00.052.206 I print_info: n_swa            = 0
0.00.052.206 I print_info: n_embd_head_k    = 128
0.00.052.206 I print_info: n_embd_head_v    = 128
0.00.052.207 I print_info: n_gqa            = 1
0.00.052.207 I print_info: n_embd_k_gqa     = 2048
0.00.052.208 I print_info: n_embd_v_gqa     = 2048
0.00.052.208 I print_info: f_norm_eps       = 1.0e-05
0.00.052.209 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.209 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.209 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.209 I print_info: f_logit_scale    = 0.0e+00
0.00.052.210 I print_info: n_ff             = 8192
0.00.052.210 I print_info: n_expert         = 0
0.00.052.212 I print_info: n_expert_used    = 0
0.00.052.212 I print_info: causal attn      = 1
0.00.052.212 I print_info: pooling type     = 0
0.00.052.214 I print_info: rope type        = 2
0.00.052.214 I print_info: rope scaling     = linear
0.00.052.215 I print_info: freq_base_train  = 10000.0
0.00.052.215 I print_info: freq_scale_train = 1
0.00.052.215 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.215 I print_info: rope_finetuned   = unknown
0.00.052.215 I print_info: ssm_d_conv       = 0
0.00.052.215 I print_info: ssm_d_inner      = 0
0.00.052.215 I print_info: ssm_d_state      = 0
0.00.052.216 I print_info: ssm_dt_rank      = 0
0.00.052.216 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.216 I print_info: model type       = 1.4B
0.00.052.216 I print_info: model params     = 1.41 B
0.00.052.216 I print_info: general.name     = 1.4B
0.00.052.217 I print_info: vocab type       = BPE
0.00.052.217 I print_info: n_vocab          = 50304
0.00.052.217 I print_info: n_merges         = 50009
0.00.052.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.221 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.222 I print_info: LF token         = 187 'Ċ'
0.00.052.224 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.224 I print_info: max token length = 1024
0.00.052.224 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.687.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.687.797 I load_tensors: offloading output layer to GPU
0.00.687.798 I load_tensors: offloaded 25/25 layers to GPU
0.00.687.834 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.687.835 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.689.158 I llama_init_from_model: n_seq_max     = 1
0.00.689.162 I llama_init_from_model: n_ctx         = 2048
0.00.689.162 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.689.163 I llama_init_from_model: n_batch       = 2048
0.00.689.163 I llama_init_from_model: n_ubatch      = 512
0.00.689.164 I llama_init_from_model: flash_attn    = 0
0.00.689.167 I llama_init_from_model: freq_base     = 10000.0
0.00.689.167 I llama_init_from_model: freq_scale    = 1
0.00.689.170 I ggml_metal_init: allocating
0.00.689.249 I ggml_metal_init: found device: Apple M4
0.00.689.263 I ggml_metal_init: picking default device: Apple M4
0.00.691.253 I ggml_metal_init: using embedded metal library
0.00.697.317 I ggml_metal_init: GPU name:   Apple M4
0.00.697.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.697.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.697.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.697.335 I ggml_metal_init: simdgroup reduction   = true
0.00.697.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.697.336 I ggml_metal_init: has residency sets    = true
0.00.697.336 I ggml_metal_init: has bfloat            = true
0.00.697.336 I ggml_metal_init: use bfloat            = true
0.00.697.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.697.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.905 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.773.602 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.773.609 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.773.648 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.778.124 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.778.126 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.778.127 I llama_init_from_model: graph nodes  = 967
0.00.778.127 I llama_init_from_model: graph splits = 2
0.00.778.133 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.778.256 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.778.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.318 I main: llama threadpool init, n_threads = 4
0.00.822.369 I 
0.00.822.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.392 I 
0.00.822.516 I sampler seed: 1234
0.00.822.521 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.544 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.545 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.589.565 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.589.566 I llama_perf_context_print:        load time =     812.77 ms
0.01.589.567 I llama_perf_context_print: prompt eval time =      49.78 ms /     7 tokens (    7.11 ms per token,   140.61 tokens per second)
0.01.589.568 I llama_perf_context_print:        eval time =     714.46 ms /    63 runs   (   11.34 ms per token,    88.18 tokens per second)
0.01.589.568 I llama_perf_context_print:       total time =     767.96 ms /    70 tokens
0.01.589.776 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.115s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.013.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.803 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.816 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.816 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.819 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.605 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.608 I llama_model_loader: - type  f32:  194 tensors
0.00.030.608 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.608 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.609 I print_info: file format = GGUF V3 (latest)
0.00.030.609 I print_info: file type   = Q5_0
0.00.030.610 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.038.620 I load: special tokens cache size = 25
0.00.044.681 I load: token to piece cache size = 0.2984 MB
0.00.044.683 I print_info: arch             = gptneox
0.00.044.684 I print_info: vocab_only       = 0
0.00.044.684 I print_info: n_ctx_train      = 2048
0.00.044.684 I print_info: n_embd           = 2048
0.00.044.684 I print_info: n_layer          = 24
0.00.044.687 I print_info: n_head           = 16
0.00.044.687 I print_info: n_head_kv        = 16
0.00.044.688 I print_info: n_rot            = 32
0.00.044.688 I print_info: n_swa            = 0
0.00.044.688 I print_info: n_embd_head_k    = 128
0.00.044.688 I print_info: n_embd_head_v    = 128
0.00.044.689 I print_info: n_gqa            = 1
0.00.044.690 I print_info: n_embd_k_gqa     = 2048
0.00.044.690 I print_info: n_embd_v_gqa     = 2048
0.00.044.691 I print_info: f_norm_eps       = 1.0e-05
0.00.044.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.696 I print_info: f_logit_scale    = 0.0e+00
0.00.044.697 I print_info: n_ff             = 8192
0.00.044.697 I print_info: n_expert         = 0
0.00.044.697 I print_info: n_expert_used    = 0
0.00.044.697 I print_info: causal attn      = 1
0.00.044.697 I print_info: pooling type     = 0
0.00.044.698 I print_info: rope type        = 2
0.00.044.699 I print_info: rope scaling     = linear
0.00.044.699 I print_info: freq_base_train  = 10000.0
0.00.044.700 I print_info: freq_scale_train = 1
0.00.044.700 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.700 I print_info: rope_finetuned   = unknown
0.00.044.700 I print_info: ssm_d_conv       = 0
0.00.044.700 I print_info: ssm_d_inner      = 0
0.00.044.700 I print_info: ssm_d_state      = 0
0.00.044.701 I print_info: ssm_dt_rank      = 0
0.00.044.701 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.701 I print_info: model type       = 1.4B
0.00.044.701 I print_info: model params     = 1.41 B
0.00.044.701 I print_info: general.name     = 1.4B
0.00.044.702 I print_info: vocab type       = BPE
0.00.044.702 I print_info: n_vocab          = 50304
0.00.044.703 I print_info: n_merges         = 50009
0.00.044.704 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.704 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.704 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.704 I print_info: LF token         = 187 'Ċ'
0.00.044.705 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.705 I print_info: max token length = 1024
0.00.044.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.675.387 I load_tensors: offloading 24 repeating layers to GPU
0.00.675.405 I load_tensors: offloading output layer to GPU
0.00.675.406 I load_tensors: offloaded 25/25 layers to GPU
0.00.675.440 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.675.441 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.676.844 I llama_init_from_model: n_seq_max     = 1
0.00.676.847 I llama_init_from_model: n_ctx         = 2048
0.00.676.848 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.676.848 I llama_init_from_model: n_batch       = 2048
0.00.676.848 I llama_init_from_model: n_ubatch      = 512
0.00.676.849 I llama_init_from_model: flash_attn    = 0
0.00.676.851 I llama_init_from_model: freq_base     = 10000.0
0.00.676.852 I llama_init_from_model: freq_scale    = 1
0.00.676.854 I ggml_metal_init: allocating
0.00.676.937 I ggml_metal_init: found device: Apple M4
0.00.676.952 I ggml_metal_init: picking default device: Apple M4
0.00.679.020 I ggml_metal_init: using embedded metal library
0.00.685.948 I ggml_metal_init: GPU name:   Apple M4
0.00.685.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.685.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.685.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.685.956 I ggml_metal_init: simdgroup reduction   = true
0.00.685.956 I ggml_metal_init: simdgroup matrix mul. = true
0.00.685.956 I ggml_metal_init: has residency sets    = true
0.00.685.957 I ggml_metal_init: has bfloat            = true
0.00.685.957 I ggml_metal_init: use bfloat            = true
0.00.685.959 I ggml_metal_init: hasUnifiedMemory      = true
0.00.685.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.382 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.755.701 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.755.709 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.755.746 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.760.269 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.760.272 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.760.272 I llama_init_from_model: graph nodes  = 967
0.00.760.272 I llama_init_from_model: graph splits = 2
0.00.760.278 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.760.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.760.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.888 I main: llama threadpool init, n_threads = 4
0.00.809.935 I 
0.00.809.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.956 I 
0.00.810.076 I sampler seed: 1234
0.00.810.081 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.123 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.127 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.127 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.645.763 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.645.764 I llama_perf_context_print:        load time =     795.22 ms
0.01.645.765 I llama_perf_context_print: prompt eval time =      54.16 ms /     7 tokens (    7.74 ms per token,   129.26 tokens per second)
0.01.645.767 I llama_perf_context_print:        eval time =     778.48 ms /    63 runs   (   12.36 ms per token,    80.93 tokens per second)
0.01.645.767 I llama_perf_context_print:       total time =     836.61 ms /    70 tokens
0.01.646.001 I ggml_metal_free: deallocating

real	0m1.666s
user	0m0.111s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.629 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.162 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.167 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.168 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.170 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.919 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.920 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.921 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.921 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.921 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.922 I llama_model_loader: - type  f32:  194 tensors
0.00.027.922 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.923 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.923 I print_info: file format = GGUF V3 (latest)
0.00.027.924 I print_info: file type   = Q5_1
0.00.027.925 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.833 I load: special tokens cache size = 25
0.00.041.840 I load: token to piece cache size = 0.2984 MB
0.00.041.843 I print_info: arch             = gptneox
0.00.041.844 I print_info: vocab_only       = 0
0.00.041.844 I print_info: n_ctx_train      = 2048
0.00.041.844 I print_info: n_embd           = 2048
0.00.041.844 I print_info: n_layer          = 24
0.00.041.848 I print_info: n_head           = 16
0.00.041.848 I print_info: n_head_kv        = 16
0.00.041.849 I print_info: n_rot            = 32
0.00.041.849 I print_info: n_swa            = 0
0.00.041.849 I print_info: n_embd_head_k    = 128
0.00.041.849 I print_info: n_embd_head_v    = 128
0.00.041.852 I print_info: n_gqa            = 1
0.00.041.853 I print_info: n_embd_k_gqa     = 2048
0.00.041.854 I print_info: n_embd_v_gqa     = 2048
0.00.041.854 I print_info: f_norm_eps       = 1.0e-05
0.00.041.855 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.855 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.855 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.855 I print_info: f_logit_scale    = 0.0e+00
0.00.041.856 I print_info: n_ff             = 8192
0.00.041.856 I print_info: n_expert         = 0
0.00.041.856 I print_info: n_expert_used    = 0
0.00.041.857 I print_info: causal attn      = 1
0.00.041.857 I print_info: pooling type     = 0
0.00.041.858 I print_info: rope type        = 2
0.00.041.859 I print_info: rope scaling     = linear
0.00.041.860 I print_info: freq_base_train  = 10000.0
0.00.041.860 I print_info: freq_scale_train = 1
0.00.041.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.860 I print_info: rope_finetuned   = unknown
0.00.041.861 I print_info: ssm_d_conv       = 0
0.00.041.861 I print_info: ssm_d_inner      = 0
0.00.041.861 I print_info: ssm_d_state      = 0
0.00.041.861 I print_info: ssm_dt_rank      = 0
0.00.041.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.861 I print_info: model type       = 1.4B
0.00.041.862 I print_info: model params     = 1.41 B
0.00.041.862 I print_info: general.name     = 1.4B
0.00.041.862 I print_info: vocab type       = BPE
0.00.041.862 I print_info: n_vocab          = 50304
0.00.041.863 I print_info: n_merges         = 50009
0.00.041.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.863 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.863 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.864 I print_info: LF token         = 187 'Ċ'
0.00.041.864 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.864 I print_info: max token length = 1024
0.00.041.864 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.322 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.344 I load_tensors: offloading output layer to GPU
0.00.645.345 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.381 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.645.382 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.646.782 I llama_init_from_model: n_seq_max     = 1
0.00.646.787 I llama_init_from_model: n_ctx         = 2048
0.00.646.788 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.646.788 I llama_init_from_model: n_batch       = 2048
0.00.646.789 I llama_init_from_model: n_ubatch      = 512
0.00.646.790 I llama_init_from_model: flash_attn    = 0
0.00.646.792 I llama_init_from_model: freq_base     = 10000.0
0.00.646.793 I llama_init_from_model: freq_scale    = 1
0.00.646.795 I ggml_metal_init: allocating
0.00.646.880 I ggml_metal_init: found device: Apple M4
0.00.646.895 I ggml_metal_init: picking default device: Apple M4
0.00.648.876 I ggml_metal_init: using embedded metal library
0.00.655.620 I ggml_metal_init: GPU name:   Apple M4
0.00.655.625 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.628 I ggml_metal_init: simdgroup reduction   = true
0.00.655.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.628 I ggml_metal_init: has residency sets    = true
0.00.655.628 I ggml_metal_init: has bfloat            = true
0.00.655.629 I ggml_metal_init: use bfloat            = true
0.00.655.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.361 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.728.371 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.728.380 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.728.416 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.777 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.780 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.781 I llama_init_from_model: graph nodes  = 967
0.00.734.781 I llama_init_from_model: graph splits = 2
0.00.734.786 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.920 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.555 I main: llama threadpool init, n_threads = 4
0.00.783.602 I 
0.00.783.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.627 I 
0.00.783.730 I sampler seed: 1234
0.00.783.735 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.780 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.783 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.783 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.673.039 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50678.09 tokens per second)
0.01.673.039 I llama_perf_context_print:        load time =     771.19 ms
0.01.673.040 I llama_perf_context_print: prompt eval time =      52.88 ms /     7 tokens (    7.55 ms per token,   132.37 tokens per second)
0.01.673.041 I llama_perf_context_print:        eval time =     833.33 ms /    63 runs   (   13.23 ms per token,    75.60 tokens per second)
0.01.673.042 I llama_perf_context_print:       total time =     890.22 ms /    70 tokens
0.01.673.240 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.114s
sys	0m0.218s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.307 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.256 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.258 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.259 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.260 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.262 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.809 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.810 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.811 I llama_model_loader: - type  f32:  194 tensors
0.00.025.811 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.812 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.812 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.812 I print_info: file format = GGUF V3 (latest)
0.00.025.813 I print_info: file type   = Q2_K - Medium
0.00.025.814 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.921 I load: special tokens cache size = 25
0.00.039.872 I load: token to piece cache size = 0.2984 MB
0.00.039.874 I print_info: arch             = gptneox
0.00.039.874 I print_info: vocab_only       = 0
0.00.039.875 I print_info: n_ctx_train      = 2048
0.00.039.875 I print_info: n_embd           = 2048
0.00.039.875 I print_info: n_layer          = 24
0.00.039.878 I print_info: n_head           = 16
0.00.039.878 I print_info: n_head_kv        = 16
0.00.039.878 I print_info: n_rot            = 32
0.00.039.879 I print_info: n_swa            = 0
0.00.039.879 I print_info: n_embd_head_k    = 128
0.00.039.879 I print_info: n_embd_head_v    = 128
0.00.039.880 I print_info: n_gqa            = 1
0.00.039.881 I print_info: n_embd_k_gqa     = 2048
0.00.039.881 I print_info: n_embd_v_gqa     = 2048
0.00.039.882 I print_info: f_norm_eps       = 1.0e-05
0.00.039.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.883 I print_info: f_logit_scale    = 0.0e+00
0.00.039.883 I print_info: n_ff             = 8192
0.00.039.884 I print_info: n_expert         = 0
0.00.039.884 I print_info: n_expert_used    = 0
0.00.039.884 I print_info: causal attn      = 1
0.00.039.884 I print_info: pooling type     = 0
0.00.039.884 I print_info: rope type        = 2
0.00.039.884 I print_info: rope scaling     = linear
0.00.039.885 I print_info: freq_base_train  = 10000.0
0.00.039.885 I print_info: freq_scale_train = 1
0.00.039.885 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.885 I print_info: rope_finetuned   = unknown
0.00.039.889 I print_info: ssm_d_conv       = 0
0.00.039.889 I print_info: ssm_d_inner      = 0
0.00.039.889 I print_info: ssm_d_state      = 0
0.00.039.889 I print_info: ssm_dt_rank      = 0
0.00.039.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.889 I print_info: model type       = 1.4B
0.00.039.891 I print_info: model params     = 1.41 B
0.00.039.891 I print_info: general.name     = 1.4B
0.00.039.892 I print_info: vocab type       = BPE
0.00.039.892 I print_info: n_vocab          = 50304
0.00.039.892 I print_info: n_merges         = 50009
0.00.039.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: LF token         = 187 'Ċ'
0.00.039.895 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.895 I print_info: max token length = 1024
0.00.039.896 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.351.530 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.554 I load_tensors: offloading output layer to GPU
0.00.351.554 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.603 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.605 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.882 I llama_init_from_model: n_seq_max     = 1
0.00.352.884 I llama_init_from_model: n_ctx         = 2048
0.00.352.885 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.352.886 I llama_init_from_model: n_batch       = 2048
0.00.352.886 I llama_init_from_model: n_ubatch      = 512
0.00.352.887 I llama_init_from_model: flash_attn    = 0
0.00.352.890 I llama_init_from_model: freq_base     = 10000.0
0.00.352.890 I llama_init_from_model: freq_scale    = 1
0.00.352.893 I ggml_metal_init: allocating
0.00.352.995 I ggml_metal_init: found device: Apple M4
0.00.353.009 I ggml_metal_init: picking default device: Apple M4
0.00.355.086 I ggml_metal_init: using embedded metal library
0.00.361.116 I ggml_metal_init: GPU name:   Apple M4
0.00.361.134 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.135 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.135 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.136 I ggml_metal_init: simdgroup reduction   = true
0.00.361.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.137 I ggml_metal_init: has residency sets    = true
0.00.361.137 I ggml_metal_init: has bfloat            = true
0.00.361.138 I ggml_metal_init: use bfloat            = true
0.00.361.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.672 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.440.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.440.143 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.440.179 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.445.245 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.445.247 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.445.247 I llama_init_from_model: graph nodes  = 967
0.00.445.247 I llama_init_from_model: graph splits = 2
0.00.445.252 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.445.385 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.445.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.041 I main: llama threadpool init, n_threads = 4
0.00.494.083 I 
0.00.494.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.106 I 
0.00.494.233 I sampler seed: 1234
0.00.494.238 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.494.257 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.494.258 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.494.258 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.191.789 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.191.790 I llama_perf_context_print:        load time =     482.96 ms
0.01.191.791 I llama_perf_context_print: prompt eval time =      44.81 ms /     7 tokens (    6.40 ms per token,   156.20 tokens per second)
0.01.191.791 I llama_perf_context_print:        eval time =     649.89 ms /    63 runs   (   10.32 ms per token,    96.94 tokens per second)
0.01.191.792 I llama_perf_context_print:       total time =     698.52 ms /    70 tokens
0.01.192.070 I ggml_metal_free: deallocating

real	0m1.210s
user	0m0.114s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.571 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.956 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.959 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.960 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.963 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.964 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.605 I llama_model_loader: - type  f32:  194 tensors
0.00.026.605 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.605 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.605 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.606 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.606 I print_info: file format = GGUF V3 (latest)
0.00.026.607 I print_info: file type   = Q3_K - Medium
0.00.026.608 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.494 I load: special tokens cache size = 25
0.00.040.564 I load: token to piece cache size = 0.2984 MB
0.00.040.567 I print_info: arch             = gptneox
0.00.040.567 I print_info: vocab_only       = 0
0.00.040.567 I print_info: n_ctx_train      = 2048
0.00.040.567 I print_info: n_embd           = 2048
0.00.040.568 I print_info: n_layer          = 24
0.00.040.571 I print_info: n_head           = 16
0.00.040.571 I print_info: n_head_kv        = 16
0.00.040.573 I print_info: n_rot            = 32
0.00.040.575 I print_info: n_swa            = 0
0.00.040.575 I print_info: n_embd_head_k    = 128
0.00.040.575 I print_info: n_embd_head_v    = 128
0.00.040.576 I print_info: n_gqa            = 1
0.00.040.577 I print_info: n_embd_k_gqa     = 2048
0.00.040.577 I print_info: n_embd_v_gqa     = 2048
0.00.040.578 I print_info: f_norm_eps       = 1.0e-05
0.00.040.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.579 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.579 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.579 I print_info: f_logit_scale    = 0.0e+00
0.00.040.580 I print_info: n_ff             = 8192
0.00.040.580 I print_info: n_expert         = 0
0.00.040.580 I print_info: n_expert_used    = 0
0.00.040.581 I print_info: causal attn      = 1
0.00.040.582 I print_info: pooling type     = 0
0.00.040.582 I print_info: rope type        = 2
0.00.040.582 I print_info: rope scaling     = linear
0.00.040.583 I print_info: freq_base_train  = 10000.0
0.00.040.583 I print_info: freq_scale_train = 1
0.00.040.583 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.583 I print_info: rope_finetuned   = unknown
0.00.040.584 I print_info: ssm_d_conv       = 0
0.00.040.584 I print_info: ssm_d_inner      = 0
0.00.040.584 I print_info: ssm_d_state      = 0
0.00.040.584 I print_info: ssm_dt_rank      = 0
0.00.040.584 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.584 I print_info: model type       = 1.4B
0.00.040.585 I print_info: model params     = 1.41 B
0.00.040.585 I print_info: general.name     = 1.4B
0.00.040.585 I print_info: vocab type       = BPE
0.00.040.586 I print_info: n_vocab          = 50304
0.00.040.586 I print_info: n_merges         = 50009
0.00.040.586 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.587 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.587 I print_info: LF token         = 187 'Ċ'
0.00.040.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.587 I print_info: max token length = 1024
0.00.040.588 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.455.958 I load_tensors: offloading output layer to GPU
0.00.455.959 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.992 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.993 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.457.432 I llama_init_from_model: n_seq_max     = 1
0.00.457.436 I llama_init_from_model: n_ctx         = 2048
0.00.457.436 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.457.437 I llama_init_from_model: n_batch       = 2048
0.00.457.437 I llama_init_from_model: n_ubatch      = 512
0.00.457.438 I llama_init_from_model: flash_attn    = 0
0.00.457.440 I llama_init_from_model: freq_base     = 10000.0
0.00.457.441 I llama_init_from_model: freq_scale    = 1
0.00.457.444 I ggml_metal_init: allocating
0.00.457.524 I ggml_metal_init: found device: Apple M4
0.00.457.539 I ggml_metal_init: picking default device: Apple M4
0.00.459.547 I ggml_metal_init: using embedded metal library
0.00.465.813 I ggml_metal_init: GPU name:   Apple M4
0.00.465.833 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.465.834 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.465.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.465.835 I ggml_metal_init: simdgroup reduction   = true
0.00.465.835 I ggml_metal_init: simdgroup matrix mul. = true
0.00.465.836 I ggml_metal_init: has residency sets    = true
0.00.465.836 I ggml_metal_init: has bfloat            = true
0.00.465.837 I ggml_metal_init: use bfloat            = true
0.00.465.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.465.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.487.176 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.541.138 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.541.150 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.541.188 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.782 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.546.785 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.546.785 I llama_init_from_model: graph nodes  = 967
0.00.546.785 I llama_init_from_model: graph splits = 2
0.00.546.796 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.546.925 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.546.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.483 I main: llama threadpool init, n_threads = 4
0.00.595.527 I 
0.00.595.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.549 I 
0.00.595.652 I sampler seed: 1234
0.00.595.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.595.676 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.595.677 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.595.677 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.370.120 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.370.121 I llama_perf_context_print:        load time =     584.18 ms
0.01.370.122 I llama_perf_context_print: prompt eval time =      49.81 ms /     7 tokens (    7.12 ms per token,   140.54 tokens per second)
0.01.370.122 I llama_perf_context_print:        eval time =     721.70 ms /    63 runs   (   11.46 ms per token,    87.29 tokens per second)
0.01.370.124 I llama_perf_context_print:       total time =     775.36 ms /    70 tokens
0.01.370.362 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.114s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.607 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.096 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.097 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.098 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.098 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.099 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.099 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.100 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.101 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.104 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.669 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.670 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.671 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.671 I llama_model_loader: - type  f32:  194 tensors
0.00.027.672 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.672 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.672 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.673 I print_info: file format = GGUF V3 (latest)
0.00.027.673 I print_info: file type   = Q4_K - Medium
0.00.027.674 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.560 I load: special tokens cache size = 25
0.00.041.511 I load: token to piece cache size = 0.2984 MB
0.00.041.514 I print_info: arch             = gptneox
0.00.041.514 I print_info: vocab_only       = 0
0.00.041.514 I print_info: n_ctx_train      = 2048
0.00.041.515 I print_info: n_embd           = 2048
0.00.041.515 I print_info: n_layer          = 24
0.00.041.518 I print_info: n_head           = 16
0.00.041.518 I print_info: n_head_kv        = 16
0.00.041.518 I print_info: n_rot            = 32
0.00.041.519 I print_info: n_swa            = 0
0.00.041.519 I print_info: n_embd_head_k    = 128
0.00.041.519 I print_info: n_embd_head_v    = 128
0.00.041.520 I print_info: n_gqa            = 1
0.00.041.521 I print_info: n_embd_k_gqa     = 2048
0.00.041.521 I print_info: n_embd_v_gqa     = 2048
0.00.041.523 I print_info: f_norm_eps       = 1.0e-05
0.00.041.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.524 I print_info: f_logit_scale    = 0.0e+00
0.00.041.525 I print_info: n_ff             = 8192
0.00.041.525 I print_info: n_expert         = 0
0.00.041.525 I print_info: n_expert_used    = 0
0.00.041.525 I print_info: causal attn      = 1
0.00.041.526 I print_info: pooling type     = 0
0.00.041.528 I print_info: rope type        = 2
0.00.041.528 I print_info: rope scaling     = linear
0.00.041.528 I print_info: freq_base_train  = 10000.0
0.00.041.529 I print_info: freq_scale_train = 1
0.00.041.529 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.529 I print_info: rope_finetuned   = unknown
0.00.041.529 I print_info: ssm_d_conv       = 0
0.00.041.529 I print_info: ssm_d_inner      = 0
0.00.041.529 I print_info: ssm_d_state      = 0
0.00.041.530 I print_info: ssm_dt_rank      = 0
0.00.041.530 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.530 I print_info: model type       = 1.4B
0.00.041.530 I print_info: model params     = 1.41 B
0.00.041.530 I print_info: general.name     = 1.4B
0.00.041.531 I print_info: vocab type       = BPE
0.00.041.531 I print_info: n_vocab          = 50304
0.00.041.531 I print_info: n_merges         = 50009
0.00.041.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.532 I print_info: LF token         = 187 'Ċ'
0.00.041.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.537 I print_info: max token length = 1024
0.00.041.537 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.534.746 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.768 I load_tensors: offloading output layer to GPU
0.00.534.769 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.807 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.808 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.536.170 I llama_init_from_model: n_seq_max     = 1
0.00.536.173 I llama_init_from_model: n_ctx         = 2048
0.00.536.174 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.536.174 I llama_init_from_model: n_batch       = 2048
0.00.536.175 I llama_init_from_model: n_ubatch      = 512
0.00.536.175 I llama_init_from_model: flash_attn    = 0
0.00.536.177 I llama_init_from_model: freq_base     = 10000.0
0.00.536.178 I llama_init_from_model: freq_scale    = 1
0.00.536.181 I ggml_metal_init: allocating
0.00.536.258 I ggml_metal_init: found device: Apple M4
0.00.536.274 I ggml_metal_init: picking default device: Apple M4
0.00.538.282 I ggml_metal_init: using embedded metal library
0.00.544.641 I ggml_metal_init: GPU name:   Apple M4
0.00.544.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.544.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.544.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.544.660 I ggml_metal_init: simdgroup reduction   = true
0.00.544.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.544.661 I ggml_metal_init: has residency sets    = true
0.00.544.661 I ggml_metal_init: has bfloat            = true
0.00.544.662 I ggml_metal_init: use bfloat            = true
0.00.544.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.544.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.565.284 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.380 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.619.388 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.619.425 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.949 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.623.951 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.623.952 I llama_init_from_model: graph nodes  = 967
0.00.623.952 I llama_init_from_model: graph splits = 2
0.00.623.961 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.087 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.532 I main: llama threadpool init, n_threads = 4
0.00.673.579 I 
0.00.673.603 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.606 I 
0.00.673.720 I sampler seed: 1234
0.00.673.725 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.673.744 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.673.745 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.673.745 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.475.541 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.475.542 I llama_perf_context_print:        load time =     661.18 ms
0.01.475.543 I llama_perf_context_print: prompt eval time =      57.99 ms /     7 tokens (    8.28 ms per token,   120.71 tokens per second)
0.01.475.544 I llama_perf_context_print:        eval time =     740.78 ms /    63 runs   (   11.76 ms per token,    85.05 tokens per second)
0.01.475.545 I llama_perf_context_print:       total time =     802.76 ms /    70 tokens
0.01.475.778 I ggml_metal_free: deallocating

real	0m1.494s
user	0m0.112s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.274 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.545 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.553 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.554 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.555 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.561 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.183 I llama_model_loader: - type  f32:  194 tensors
0.00.026.184 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.184 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.185 I print_info: file format = GGUF V3 (latest)
0.00.026.185 I print_info: file type   = Q5_K - Medium
0.00.026.186 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.178 I load: special tokens cache size = 25
0.00.040.049 I load: token to piece cache size = 0.2984 MB
0.00.040.052 I print_info: arch             = gptneox
0.00.040.053 I print_info: vocab_only       = 0
0.00.040.053 I print_info: n_ctx_train      = 2048
0.00.040.053 I print_info: n_embd           = 2048
0.00.040.053 I print_info: n_layer          = 24
0.00.040.057 I print_info: n_head           = 16
0.00.040.057 I print_info: n_head_kv        = 16
0.00.040.058 I print_info: n_rot            = 32
0.00.040.058 I print_info: n_swa            = 0
0.00.040.058 I print_info: n_embd_head_k    = 128
0.00.040.058 I print_info: n_embd_head_v    = 128
0.00.040.059 I print_info: n_gqa            = 1
0.00.040.060 I print_info: n_embd_k_gqa     = 2048
0.00.040.060 I print_info: n_embd_v_gqa     = 2048
0.00.040.061 I print_info: f_norm_eps       = 1.0e-05
0.00.040.061 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.061 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.062 I print_info: f_logit_scale    = 0.0e+00
0.00.040.062 I print_info: n_ff             = 8192
0.00.040.064 I print_info: n_expert         = 0
0.00.040.064 I print_info: n_expert_used    = 0
0.00.040.064 I print_info: causal attn      = 1
0.00.040.064 I print_info: pooling type     = 0
0.00.040.065 I print_info: rope type        = 2
0.00.040.067 I print_info: rope scaling     = linear
0.00.040.068 I print_info: freq_base_train  = 10000.0
0.00.040.068 I print_info: freq_scale_train = 1
0.00.040.068 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.068 I print_info: rope_finetuned   = unknown
0.00.040.068 I print_info: ssm_d_conv       = 0
0.00.040.068 I print_info: ssm_d_inner      = 0
0.00.040.069 I print_info: ssm_d_state      = 0
0.00.040.069 I print_info: ssm_dt_rank      = 0
0.00.040.069 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.069 I print_info: model type       = 1.4B
0.00.040.069 I print_info: model params     = 1.41 B
0.00.040.070 I print_info: general.name     = 1.4B
0.00.040.070 I print_info: vocab type       = BPE
0.00.040.070 I print_info: n_vocab          = 50304
0.00.040.070 I print_info: n_merges         = 50009
0.00.040.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: LF token         = 187 'Ċ'
0.00.040.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.075 I print_info: max token length = 1024
0.00.040.075 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.636 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.657 I load_tensors: offloading output layer to GPU
0.00.615.658 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.697 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.615.698 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.617.089 I llama_init_from_model: n_seq_max     = 1
0.00.617.100 I llama_init_from_model: n_ctx         = 2048
0.00.617.101 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.617.101 I llama_init_from_model: n_batch       = 2048
0.00.617.102 I llama_init_from_model: n_ubatch      = 512
0.00.617.102 I llama_init_from_model: flash_attn    = 0
0.00.617.104 I llama_init_from_model: freq_base     = 10000.0
0.00.617.105 I llama_init_from_model: freq_scale    = 1
0.00.617.107 I ggml_metal_init: allocating
0.00.617.189 I ggml_metal_init: found device: Apple M4
0.00.617.206 I ggml_metal_init: picking default device: Apple M4
0.00.619.241 I ggml_metal_init: using embedded metal library
0.00.625.830 I ggml_metal_init: GPU name:   Apple M4
0.00.625.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.835 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.836 I ggml_metal_init: simdgroup reduction   = true
0.00.625.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.836 I ggml_metal_init: has residency sets    = true
0.00.625.836 I ggml_metal_init: has bfloat            = true
0.00.625.837 I ggml_metal_init: use bfloat            = true
0.00.625.837 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.840 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.606 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.480 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.489 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.377 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.379 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.379 I llama_init_from_model: graph nodes  = 967
0.00.698.380 I llama_init_from_model: graph splits = 2
0.00.698.384 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.457 I main: llama threadpool init, n_threads = 4
0.00.754.505 I 
0.00.754.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.531 I 
0.00.754.648 I sampler seed: 1234
0.00.754.653 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.701 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.701 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.650.295 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.650.295 I llama_perf_context_print:        load time =     744.38 ms
0.01.650.296 I llama_perf_context_print: prompt eval time =      65.05 ms /     7 tokens (    9.29 ms per token,   107.62 tokens per second)
0.01.650.297 I llama_perf_context_print:        eval time =     827.59 ms /    63 runs   (   13.14 ms per token,    76.12 tokens per second)
0.01.650.297 I llama_perf_context_print:       total time =     896.64 ms /    70 tokens
0.01.650.498 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.112s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.715 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.716 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.716 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.999 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.000 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.001 I llama_model_loader: - type  f32:  194 tensors
0.00.027.001 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.002 I print_info: file format = GGUF V3 (latest)
0.00.027.002 I print_info: file type   = Q6_K
0.00.027.003 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.943 I load: special tokens cache size = 25
0.00.040.987 I load: token to piece cache size = 0.2984 MB
0.00.040.990 I print_info: arch             = gptneox
0.00.040.990 I print_info: vocab_only       = 0
0.00.040.990 I print_info: n_ctx_train      = 2048
0.00.040.991 I print_info: n_embd           = 2048
0.00.040.991 I print_info: n_layer          = 24
0.00.040.994 I print_info: n_head           = 16
0.00.040.995 I print_info: n_head_kv        = 16
0.00.040.995 I print_info: n_rot            = 32
0.00.040.997 I print_info: n_swa            = 0
0.00.040.997 I print_info: n_embd_head_k    = 128
0.00.040.997 I print_info: n_embd_head_v    = 128
0.00.040.998 I print_info: n_gqa            = 1
0.00.040.999 I print_info: n_embd_k_gqa     = 2048
0.00.041.000 I print_info: n_embd_v_gqa     = 2048
0.00.041.000 I print_info: f_norm_eps       = 1.0e-05
0.00.041.001 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.001 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.001 I print_info: f_logit_scale    = 0.0e+00
0.00.041.002 I print_info: n_ff             = 8192
0.00.041.002 I print_info: n_expert         = 0
0.00.041.002 I print_info: n_expert_used    = 0
0.00.041.002 I print_info: causal attn      = 1
0.00.041.002 I print_info: pooling type     = 0
0.00.041.008 I print_info: rope type        = 2
0.00.041.010 I print_info: rope scaling     = linear
0.00.041.012 I print_info: freq_base_train  = 10000.0
0.00.041.012 I print_info: freq_scale_train = 1
0.00.041.012 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.013 I print_info: rope_finetuned   = unknown
0.00.041.013 I print_info: ssm_d_conv       = 0
0.00.041.013 I print_info: ssm_d_inner      = 0
0.00.041.013 I print_info: ssm_d_state      = 0
0.00.041.013 I print_info: ssm_dt_rank      = 0
0.00.041.013 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.014 I print_info: model type       = 1.4B
0.00.041.014 I print_info: model params     = 1.41 B
0.00.041.014 I print_info: general.name     = 1.4B
0.00.041.015 I print_info: vocab type       = BPE
0.00.041.015 I print_info: n_vocab          = 50304
0.00.041.015 I print_info: n_merges         = 50009
0.00.041.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.016 I print_info: LF token         = 187 'Ċ'
0.00.041.017 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.018 I print_info: max token length = 1024
0.00.041.019 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.699 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.717 I load_tensors: offloading output layer to GPU
0.00.664.718 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.754 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.664.755 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.666.188 I llama_init_from_model: n_seq_max     = 1
0.00.666.191 I llama_init_from_model: n_ctx         = 2048
0.00.666.192 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.666.192 I llama_init_from_model: n_batch       = 2048
0.00.666.192 I llama_init_from_model: n_ubatch      = 512
0.00.666.193 I llama_init_from_model: flash_attn    = 0
0.00.666.196 I llama_init_from_model: freq_base     = 10000.0
0.00.666.196 I llama_init_from_model: freq_scale    = 1
0.00.666.199 I ggml_metal_init: allocating
0.00.666.274 I ggml_metal_init: found device: Apple M4
0.00.666.287 I ggml_metal_init: picking default device: Apple M4
0.00.667.899 I ggml_metal_init: using embedded metal library
0.00.674.718 I ggml_metal_init: GPU name:   Apple M4
0.00.674.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.724 I ggml_metal_init: simdgroup reduction   = true
0.00.674.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.724 I ggml_metal_init: has residency sets    = true
0.00.674.725 I ggml_metal_init: has bfloat            = true
0.00.674.725 I ggml_metal_init: use bfloat            = true
0.00.674.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.166 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.379 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.385 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.870 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.872 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.872 I llama_init_from_model: graph nodes  = 967
0.00.742.872 I llama_init_from_model: graph splits = 2
0.00.742.879 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.223 I main: llama threadpool init, n_threads = 4
0.00.795.273 I 
0.00.795.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.300 I 
0.00.795.405 I sampler seed: 1234
0.00.795.409 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.430 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.430 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.707.444 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.707.445 I llama_perf_context_print:        load time =     784.75 ms
0.01.707.449 I llama_perf_context_print: prompt eval time =      57.63 ms /     7 tokens (    8.23 ms per token,   121.47 tokens per second)
0.01.707.449 I llama_perf_context_print:        eval time =     851.49 ms /    63 runs   (   13.52 ms per token,    73.99 tokens per second)
0.01.707.450 I llama_perf_context_print:       total time =     912.95 ms /    70 tokens
0.01.707.711 I ggml_metal_free: deallocating

real	0m1.726s
user	0m0.110s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.537 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.247 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.250 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.261 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.264 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.266 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.268 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.274 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.333 I llama_model_loader: - type  f32:  194 tensors
0.00.056.334 I llama_model_loader: - type  f16:   98 tensors
0.00.056.334 I print_info: file format = GGUF V3 (latest)
0.00.056.335 I print_info: file type   = all F32 (guessed)
0.00.056.336 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.495 I load: special tokens cache size = 25
0.00.077.576 I load: token to piece cache size = 0.2984 MB
0.00.077.580 I print_info: arch             = gptneox
0.00.077.580 I print_info: vocab_only       = 0
0.00.077.580 I print_info: n_ctx_train      = 2048
0.00.077.580 I print_info: n_embd           = 2048
0.00.077.581 I print_info: n_layer          = 24
0.00.077.584 I print_info: n_head           = 16
0.00.077.585 I print_info: n_head_kv        = 16
0.00.077.585 I print_info: n_rot            = 32
0.00.077.588 I print_info: n_swa            = 0
0.00.077.588 I print_info: n_embd_head_k    = 128
0.00.077.589 I print_info: n_embd_head_v    = 128
0.00.077.590 I print_info: n_gqa            = 1
0.00.077.590 I print_info: n_embd_k_gqa     = 2048
0.00.077.591 I print_info: n_embd_v_gqa     = 2048
0.00.077.592 I print_info: f_norm_eps       = 1.0e-05
0.00.077.592 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.593 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.593 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.593 I print_info: f_logit_scale    = 0.0e+00
0.00.077.594 I print_info: n_ff             = 8192
0.00.077.596 I print_info: n_expert         = 0
0.00.077.596 I print_info: n_expert_used    = 0
0.00.077.596 I print_info: causal attn      = 1
0.00.077.596 I print_info: pooling type     = 0
0.00.077.596 I print_info: rope type        = 2
0.00.077.597 I print_info: rope scaling     = linear
0.00.077.597 I print_info: freq_base_train  = 10000.0
0.00.077.597 I print_info: freq_scale_train = 1
0.00.077.598 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.598 I print_info: rope_finetuned   = unknown
0.00.077.599 I print_info: ssm_d_conv       = 0
0.00.077.599 I print_info: ssm_d_inner      = 0
0.00.077.599 I print_info: ssm_d_state      = 0
0.00.077.599 I print_info: ssm_dt_rank      = 0
0.00.077.599 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.599 I print_info: model type       = 1.4B
0.00.077.600 I print_info: model params     = 1.41 B
0.00.077.600 I print_info: general.name     = 1.4B
0.00.077.600 I print_info: vocab type       = BPE
0.00.077.601 I print_info: n_vocab          = 50304
0.00.077.601 I print_info: n_merges         = 50009
0.00.077.601 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.602 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.602 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.603 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.603 I print_info: LF token         = 187 'Ċ'
0.00.077.603 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.604 I print_info: max token length = 1024
0.00.077.607 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.384.384 I load_tensors: offloading 24 repeating layers to GPU
0.01.384.387 I load_tensors: offloading output layer to GPU
0.01.384.388 I load_tensors: offloaded 25/25 layers to GPU
0.01.384.413 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.384.417 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.385.492 I llama_init_from_model: n_seq_max     = 1
0.01.385.493 I llama_init_from_model: n_ctx         = 128
0.01.385.493 I llama_init_from_model: n_ctx_per_seq = 128
0.01.385.494 I llama_init_from_model: n_batch       = 128
0.01.385.494 I llama_init_from_model: n_ubatch      = 128
0.01.385.494 I llama_init_from_model: flash_attn    = 0
0.01.385.495 I llama_init_from_model: freq_base     = 10000.0
0.01.385.495 I llama_init_from_model: freq_scale    = 1
0.01.385.495 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.385.496 I ggml_metal_init: allocating
0.01.385.561 I ggml_metal_init: found device: Apple M4
0.01.385.567 I ggml_metal_init: picking default device: Apple M4
0.01.386.729 I ggml_metal_init: using embedded metal library
0.01.390.657 I ggml_metal_init: GPU name:   Apple M4
0.01.390.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.390.660 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.390.660 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.390.661 I ggml_metal_init: simdgroup reduction   = true
0.01.390.661 I ggml_metal_init: simdgroup matrix mul. = true
0.01.390.661 I ggml_metal_init: has residency sets    = true
0.01.390.661 I ggml_metal_init: has bfloat            = true
0.01.390.661 I ggml_metal_init: use bfloat            = true
0.01.390.662 I ggml_metal_init: hasUnifiedMemory      = true
0.01.390.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.402.514 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.404.262 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.404.264 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.404.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.405.958 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.405.960 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.405.960 I llama_init_from_model: graph nodes  = 967
0.01.405.960 I llama_init_from_model: graph splits = 2
0.01.405.962 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.405.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.440.092 I 
0.01.440.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.440.132 I perplexity: tokenizing the input ..
0.01.445.570 I perplexity: tokenization took 5.437 ms
0.01.445.575 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.563.900 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.565.228 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.565.261 I llama_perf_context_print:        load time =    1415.83 ms
0.01.565.262 I llama_perf_context_print: prompt eval time =     118.06 ms /   128 tokens (    0.92 ms per token,  1084.23 tokens per second)
0.01.565.262 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.565.263 I llama_perf_context_print:       total time =     125.17 ms /   129 tokens
0.01.565.595 I ggml_metal_free: deallocating

real	0m1.786s
user	0m0.100s
sys	0m0.246s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.416 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.832 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.864 I llama_model_loader: - type  f32:  194 tensors
0.00.025.864 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.865 I print_info: file format = GGUF V3 (latest)
0.00.025.865 I print_info: file type   = Q8_0
0.00.025.866 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.000 I load: special tokens cache size = 25
0.00.042.683 I load: token to piece cache size = 0.2984 MB
0.00.042.685 I print_info: arch             = gptneox
0.00.042.686 I print_info: vocab_only       = 0
0.00.042.686 I print_info: n_ctx_train      = 2048
0.00.042.686 I print_info: n_embd           = 2048
0.00.042.686 I print_info: n_layer          = 24
0.00.042.689 I print_info: n_head           = 16
0.00.042.690 I print_info: n_head_kv        = 16
0.00.042.690 I print_info: n_rot            = 32
0.00.042.690 I print_info: n_swa            = 0
0.00.042.691 I print_info: n_embd_head_k    = 128
0.00.042.691 I print_info: n_embd_head_v    = 128
0.00.042.692 I print_info: n_gqa            = 1
0.00.042.692 I print_info: n_embd_k_gqa     = 2048
0.00.042.693 I print_info: n_embd_v_gqa     = 2048
0.00.042.694 I print_info: f_norm_eps       = 1.0e-05
0.00.042.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.694 I print_info: f_logit_scale    = 0.0e+00
0.00.042.695 I print_info: n_ff             = 8192
0.00.042.695 I print_info: n_expert         = 0
0.00.042.695 I print_info: n_expert_used    = 0
0.00.042.696 I print_info: causal attn      = 1
0.00.042.696 I print_info: pooling type     = 0
0.00.042.696 I print_info: rope type        = 2
0.00.042.696 I print_info: rope scaling     = linear
0.00.042.699 I print_info: freq_base_train  = 10000.0
0.00.042.699 I print_info: freq_scale_train = 1
0.00.042.700 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.700 I print_info: rope_finetuned   = unknown
0.00.042.700 I print_info: ssm_d_conv       = 0
0.00.042.700 I print_info: ssm_d_inner      = 0
0.00.042.700 I print_info: ssm_d_state      = 0
0.00.042.700 I print_info: ssm_dt_rank      = 0
0.00.042.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.701 I print_info: model type       = 1.4B
0.00.042.701 I print_info: model params     = 1.41 B
0.00.042.701 I print_info: general.name     = 1.4B
0.00.042.702 I print_info: vocab type       = BPE
0.00.042.702 I print_info: n_vocab          = 50304
0.00.042.702 I print_info: n_merges         = 50009
0.00.042.702 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.706 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.706 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.707 I print_info: LF token         = 187 'Ċ'
0.00.042.707 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.707 I print_info: max token length = 1024
0.00.042.708 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.903.187 I load_tensors: offloading 24 repeating layers to GPU
0.00.903.193 I load_tensors: offloading output layer to GPU
0.00.903.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.903.220 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.903.222 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.904.503 I llama_init_from_model: n_seq_max     = 1
0.00.904.505 I llama_init_from_model: n_ctx         = 128
0.00.904.505 I llama_init_from_model: n_ctx_per_seq = 128
0.00.904.505 I llama_init_from_model: n_batch       = 128
0.00.904.506 I llama_init_from_model: n_ubatch      = 128
0.00.904.506 I llama_init_from_model: flash_attn    = 0
0.00.904.507 I llama_init_from_model: freq_base     = 10000.0
0.00.904.507 I llama_init_from_model: freq_scale    = 1
0.00.904.508 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.904.509 I ggml_metal_init: allocating
0.00.904.524 I ggml_metal_init: found device: Apple M4
0.00.904.531 I ggml_metal_init: picking default device: Apple M4
0.00.905.768 I ggml_metal_init: using embedded metal library
0.00.911.555 I ggml_metal_init: GPU name:   Apple M4
0.00.911.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.911.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.911.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.911.560 I ggml_metal_init: simdgroup reduction   = true
0.00.911.560 I ggml_metal_init: simdgroup matrix mul. = true
0.00.911.560 I ggml_metal_init: has residency sets    = true
0.00.911.561 I ggml_metal_init: has bfloat            = true
0.00.911.561 I ggml_metal_init: use bfloat            = true
0.00.911.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.911.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.927.598 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.930.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.930.350 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.930.385 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.932.998 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.933.000 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.933.000 I llama_init_from_model: graph nodes  = 967
0.00.933.000 I llama_init_from_model: graph splits = 2
0.00.933.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.933.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.961.794 I 
0.00.961.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.961.848 I perplexity: tokenizing the input ..
0.00.968.669 I perplexity: tokenization took 6.819 ms
0.00.968.675 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.107.762 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.109.098 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.109.119 I llama_perf_context_print:        load time =     952.37 ms
0.01.109.120 I llama_perf_context_print: prompt eval time =     138.48 ms /   128 tokens (    1.08 ms per token,   924.34 tokens per second)
0.01.109.121 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.109.121 I llama_perf_context_print:       total time =     147.33 ms /   129 tokens
0.01.109.539 I ggml_metal_free: deallocating

real	0m1.124s
user	0m0.080s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.150 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.336 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.338 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.339 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.339 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.341 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.341 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.342 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.342 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.343 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.347 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.067 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.070 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.070 I llama_model_loader: - type  f32:  194 tensors
0.00.026.070 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.071 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.071 I print_info: file format = GGUF V3 (latest)
0.00.026.072 I print_info: file type   = Q4_0
0.00.026.076 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.351 I load: special tokens cache size = 25
0.00.040.284 I load: token to piece cache size = 0.2984 MB
0.00.040.287 I print_info: arch             = gptneox
0.00.040.287 I print_info: vocab_only       = 0
0.00.040.287 I print_info: n_ctx_train      = 2048
0.00.040.287 I print_info: n_embd           = 2048
0.00.040.288 I print_info: n_layer          = 24
0.00.040.290 I print_info: n_head           = 16
0.00.040.291 I print_info: n_head_kv        = 16
0.00.040.292 I print_info: n_rot            = 32
0.00.040.292 I print_info: n_swa            = 0
0.00.040.292 I print_info: n_embd_head_k    = 128
0.00.040.292 I print_info: n_embd_head_v    = 128
0.00.040.293 I print_info: n_gqa            = 1
0.00.040.294 I print_info: n_embd_k_gqa     = 2048
0.00.040.295 I print_info: n_embd_v_gqa     = 2048
0.00.040.296 I print_info: f_norm_eps       = 1.0e-05
0.00.040.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.298 I print_info: f_logit_scale    = 0.0e+00
0.00.040.299 I print_info: n_ff             = 8192
0.00.040.299 I print_info: n_expert         = 0
0.00.040.299 I print_info: n_expert_used    = 0
0.00.040.299 I print_info: causal attn      = 1
0.00.040.299 I print_info: pooling type     = 0
0.00.040.299 I print_info: rope type        = 2
0.00.040.300 I print_info: rope scaling     = linear
0.00.040.300 I print_info: freq_base_train  = 10000.0
0.00.040.300 I print_info: freq_scale_train = 1
0.00.040.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.301 I print_info: rope_finetuned   = unknown
0.00.040.301 I print_info: ssm_d_conv       = 0
0.00.040.301 I print_info: ssm_d_inner      = 0
0.00.040.301 I print_info: ssm_d_state      = 0
0.00.040.301 I print_info: ssm_dt_rank      = 0
0.00.040.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.302 I print_info: model type       = 1.4B
0.00.040.302 I print_info: model params     = 1.41 B
0.00.040.302 I print_info: general.name     = 1.4B
0.00.040.303 I print_info: vocab type       = BPE
0.00.040.303 I print_info: n_vocab          = 50304
0.00.040.303 I print_info: n_merges         = 50009
0.00.040.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.304 I print_info: LF token         = 187 'Ċ'
0.00.040.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.305 I print_info: max token length = 1024
0.00.040.310 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.582.466 I load_tensors: offloading 24 repeating layers to GPU
0.00.582.478 I load_tensors: offloading output layer to GPU
0.00.582.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.582.512 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.582.513 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.584.069 I llama_init_from_model: n_seq_max     = 1
0.00.584.073 I llama_init_from_model: n_ctx         = 128
0.00.584.073 I llama_init_from_model: n_ctx_per_seq = 128
0.00.584.074 I llama_init_from_model: n_batch       = 128
0.00.584.074 I llama_init_from_model: n_ubatch      = 128
0.00.584.075 I llama_init_from_model: flash_attn    = 0
0.00.584.076 I llama_init_from_model: freq_base     = 10000.0
0.00.584.077 I llama_init_from_model: freq_scale    = 1
0.00.584.078 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.584.087 I ggml_metal_init: allocating
0.00.584.179 I ggml_metal_init: found device: Apple M4
0.00.584.192 I ggml_metal_init: picking default device: Apple M4
0.00.586.110 I ggml_metal_init: using embedded metal library
0.00.592.648 I ggml_metal_init: GPU name:   Apple M4
0.00.592.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.656 I ggml_metal_init: simdgroup reduction   = true
0.00.592.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.657 I ggml_metal_init: has residency sets    = true
0.00.592.657 I ggml_metal_init: has bfloat            = true
0.00.592.657 I ggml_metal_init: use bfloat            = true
0.00.592.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.267 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.614.645 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.614.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.617.797 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.617.799 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.617.800 I llama_init_from_model: graph nodes  = 967
0.00.617.800 I llama_init_from_model: graph splits = 2
0.00.617.804 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.617.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.068 I 
0.00.642.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.160 I perplexity: tokenizing the input ..
0.00.649.394 I perplexity: tokenization took 7.229 ms
0.00.649.402 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.786 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.787.110 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.787.138 I llama_perf_context_print:        load time =     631.91 ms
0.00.787.139 I llama_perf_context_print: prompt eval time =     135.52 ms /   128 tokens (    1.06 ms per token,   944.52 tokens per second)
0.00.787.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.140 I llama_perf_context_print:       total time =     145.07 ms /   129 tokens
0.00.787.568 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.666 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.682 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.682 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.481 I llama_model_loader: - type  f32:  194 tensors
0.00.026.481 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.482 I print_info: file format = GGUF V3 (latest)
0.00.026.482 I print_info: file type   = Q4_1
0.00.026.483 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.632 I load: special tokens cache size = 25
0.00.040.689 I load: token to piece cache size = 0.2984 MB
0.00.040.691 I print_info: arch             = gptneox
0.00.040.691 I print_info: vocab_only       = 0
0.00.040.691 I print_info: n_ctx_train      = 2048
0.00.040.692 I print_info: n_embd           = 2048
0.00.040.692 I print_info: n_layer          = 24
0.00.040.695 I print_info: n_head           = 16
0.00.040.695 I print_info: n_head_kv        = 16
0.00.040.696 I print_info: n_rot            = 32
0.00.040.696 I print_info: n_swa            = 0
0.00.040.697 I print_info: n_embd_head_k    = 128
0.00.040.697 I print_info: n_embd_head_v    = 128
0.00.040.698 I print_info: n_gqa            = 1
0.00.040.699 I print_info: n_embd_k_gqa     = 2048
0.00.040.699 I print_info: n_embd_v_gqa     = 2048
0.00.040.700 I print_info: f_norm_eps       = 1.0e-05
0.00.040.700 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.701 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.701 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.701 I print_info: f_logit_scale    = 0.0e+00
0.00.040.701 I print_info: n_ff             = 8192
0.00.040.702 I print_info: n_expert         = 0
0.00.040.702 I print_info: n_expert_used    = 0
0.00.040.703 I print_info: causal attn      = 1
0.00.040.703 I print_info: pooling type     = 0
0.00.040.704 I print_info: rope type        = 2
0.00.040.704 I print_info: rope scaling     = linear
0.00.040.704 I print_info: freq_base_train  = 10000.0
0.00.040.704 I print_info: freq_scale_train = 1
0.00.040.705 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.709 I print_info: rope_finetuned   = unknown
0.00.040.710 I print_info: ssm_d_conv       = 0
0.00.040.710 I print_info: ssm_d_inner      = 0
0.00.040.710 I print_info: ssm_d_state      = 0
0.00.040.710 I print_info: ssm_dt_rank      = 0
0.00.040.710 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.711 I print_info: model type       = 1.4B
0.00.040.711 I print_info: model params     = 1.41 B
0.00.040.711 I print_info: general.name     = 1.4B
0.00.040.712 I print_info: vocab type       = BPE
0.00.040.712 I print_info: n_vocab          = 50304
0.00.040.713 I print_info: n_merges         = 50009
0.00.040.714 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.714 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.714 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.714 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.714 I print_info: LF token         = 187 'Ċ'
0.00.040.714 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.715 I print_info: max token length = 1024
0.00.040.715 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.534 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.548 I load_tensors: offloading output layer to GPU
0.00.619.549 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.583 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.619.584 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.621.241 I llama_init_from_model: n_seq_max     = 1
0.00.621.245 I llama_init_from_model: n_ctx         = 128
0.00.621.246 I llama_init_from_model: n_ctx_per_seq = 128
0.00.621.246 I llama_init_from_model: n_batch       = 128
0.00.621.247 I llama_init_from_model: n_ubatch      = 128
0.00.621.247 I llama_init_from_model: flash_attn    = 0
0.00.621.249 I llama_init_from_model: freq_base     = 10000.0
0.00.621.250 I llama_init_from_model: freq_scale    = 1
0.00.621.250 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.272 I ggml_metal_init: allocating
0.00.621.354 I ggml_metal_init: found device: Apple M4
0.00.621.393 I ggml_metal_init: picking default device: Apple M4
0.00.623.231 I ggml_metal_init: using embedded metal library
0.00.629.873 I ggml_metal_init: GPU name:   Apple M4
0.00.629.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.879 I ggml_metal_init: simdgroup reduction   = true
0.00.629.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.879 I ggml_metal_init: has residency sets    = true
0.00.629.880 I ggml_metal_init: has bfloat            = true
0.00.629.880 I ggml_metal_init: use bfloat            = true
0.00.629.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.588 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.030 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.076 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.654.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.654.353 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.654.353 I llama_init_from_model: graph nodes  = 967
0.00.654.354 I llama_init_from_model: graph splits = 2
0.00.654.357 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.515 I 
0.00.682.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.614 I perplexity: tokenizing the input ..
0.00.689.268 I perplexity: tokenization took 6.651 ms
0.00.689.273 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.367 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.820.717 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.820.744 I llama_perf_context_print:        load time =     673.63 ms
0.00.820.745 I llama_perf_context_print: prompt eval time =     129.86 ms /   128 tokens (    1.01 ms per token,   985.68 tokens per second)
0.00.820.746 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.748 I llama_perf_context_print:       total time =     138.24 ms /   129 tokens
0.00.821.125 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.294 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.354 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.133 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.133 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.134 I llama_model_loader: - type  f32:  194 tensors
0.00.026.134 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.135 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.135 I print_info: file format = GGUF V3 (latest)
0.00.026.136 I print_info: file type   = Q5_0
0.00.026.138 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.352 I load: special tokens cache size = 25
0.00.040.262 I load: token to piece cache size = 0.2984 MB
0.00.040.265 I print_info: arch             = gptneox
0.00.040.266 I print_info: vocab_only       = 0
0.00.040.266 I print_info: n_ctx_train      = 2048
0.00.040.266 I print_info: n_embd           = 2048
0.00.040.266 I print_info: n_layer          = 24
0.00.040.269 I print_info: n_head           = 16
0.00.040.270 I print_info: n_head_kv        = 16
0.00.040.270 I print_info: n_rot            = 32
0.00.040.270 I print_info: n_swa            = 0
0.00.040.271 I print_info: n_embd_head_k    = 128
0.00.040.271 I print_info: n_embd_head_v    = 128
0.00.040.273 I print_info: n_gqa            = 1
0.00.040.274 I print_info: n_embd_k_gqa     = 2048
0.00.040.275 I print_info: n_embd_v_gqa     = 2048
0.00.040.275 I print_info: f_norm_eps       = 1.0e-05
0.00.040.282 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.282 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.283 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.283 I print_info: f_logit_scale    = 0.0e+00
0.00.040.288 I print_info: n_ff             = 8192
0.00.040.288 I print_info: n_expert         = 0
0.00.040.288 I print_info: n_expert_used    = 0
0.00.040.289 I print_info: causal attn      = 1
0.00.040.289 I print_info: pooling type     = 0
0.00.040.289 I print_info: rope type        = 2
0.00.040.289 I print_info: rope scaling     = linear
0.00.040.291 I print_info: freq_base_train  = 10000.0
0.00.040.291 I print_info: freq_scale_train = 1
0.00.040.291 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.291 I print_info: rope_finetuned   = unknown
0.00.040.291 I print_info: ssm_d_conv       = 0
0.00.040.291 I print_info: ssm_d_inner      = 0
0.00.040.292 I print_info: ssm_d_state      = 0
0.00.040.292 I print_info: ssm_dt_rank      = 0
0.00.040.293 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.295 I print_info: model type       = 1.4B
0.00.040.295 I print_info: model params     = 1.41 B
0.00.040.295 I print_info: general.name     = 1.4B
0.00.040.296 I print_info: vocab type       = BPE
0.00.040.296 I print_info: n_vocab          = 50304
0.00.040.296 I print_info: n_merges         = 50009
0.00.040.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.296 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.297 I print_info: LF token         = 187 'Ċ'
0.00.040.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.297 I print_info: max token length = 1024
0.00.040.299 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.025 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.036 I load_tensors: offloading output layer to GPU
0.00.683.037 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.066 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.683.068 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.684.627 I llama_init_from_model: n_seq_max     = 1
0.00.684.632 I llama_init_from_model: n_ctx         = 128
0.00.684.632 I llama_init_from_model: n_ctx_per_seq = 128
0.00.684.633 I llama_init_from_model: n_batch       = 128
0.00.684.633 I llama_init_from_model: n_ubatch      = 128
0.00.684.634 I llama_init_from_model: flash_attn    = 0
0.00.684.636 I llama_init_from_model: freq_base     = 10000.0
0.00.684.637 I llama_init_from_model: freq_scale    = 1
0.00.684.637 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.640 I ggml_metal_init: allocating
0.00.684.684 I ggml_metal_init: found device: Apple M4
0.00.684.696 I ggml_metal_init: picking default device: Apple M4
0.00.686.372 I ggml_metal_init: using embedded metal library
0.00.692.142 I ggml_metal_init: GPU name:   Apple M4
0.00.692.147 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.148 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.150 I ggml_metal_init: simdgroup reduction   = true
0.00.692.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.150 I ggml_metal_init: has residency sets    = true
0.00.692.151 I ggml_metal_init: has bfloat            = true
0.00.692.151 I ggml_metal_init: use bfloat            = true
0.00.692.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.154 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.751 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.454 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.716.460 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.716.518 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.922 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.719.924 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.719.925 I llama_init_from_model: graph nodes  = 967
0.00.719.925 I llama_init_from_model: graph splits = 2
0.00.719.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.719.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.867 I 
0.00.750.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.964 I perplexity: tokenizing the input ..
0.00.757.958 I perplexity: tokenization took 6.99 ms
0.00.757.970 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.417 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.907.565 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.907.589 I llama_perf_context_print:        load time =     740.57 ms
0.00.907.593 I llama_perf_context_print: prompt eval time =     147.58 ms /   128 tokens (    1.15 ms per token,   867.34 tokens per second)
0.00.907.594 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.594 I llama_perf_context_print:       total time =     156.73 ms /   129 tokens
0.00.907.939 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.081s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.982 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.616 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.634 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.634 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.428 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.429 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.431 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.431 I llama_model_loader: - type  f32:  194 tensors
0.00.024.431 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.432 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.432 I print_info: file format = GGUF V3 (latest)
0.00.024.433 I print_info: file type   = Q5_1
0.00.024.437 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.566 I load: special tokens cache size = 25
0.00.038.477 I load: token to piece cache size = 0.2984 MB
0.00.038.479 I print_info: arch             = gptneox
0.00.038.480 I print_info: vocab_only       = 0
0.00.038.480 I print_info: n_ctx_train      = 2048
0.00.038.480 I print_info: n_embd           = 2048
0.00.038.480 I print_info: n_layer          = 24
0.00.038.483 I print_info: n_head           = 16
0.00.038.484 I print_info: n_head_kv        = 16
0.00.038.484 I print_info: n_rot            = 32
0.00.038.484 I print_info: n_swa            = 0
0.00.038.484 I print_info: n_embd_head_k    = 128
0.00.038.485 I print_info: n_embd_head_v    = 128
0.00.038.485 I print_info: n_gqa            = 1
0.00.038.486 I print_info: n_embd_k_gqa     = 2048
0.00.038.487 I print_info: n_embd_v_gqa     = 2048
0.00.038.487 I print_info: f_norm_eps       = 1.0e-05
0.00.038.488 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.488 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.488 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.488 I print_info: f_logit_scale    = 0.0e+00
0.00.038.489 I print_info: n_ff             = 8192
0.00.038.489 I print_info: n_expert         = 0
0.00.038.489 I print_info: n_expert_used    = 0
0.00.038.489 I print_info: causal attn      = 1
0.00.038.490 I print_info: pooling type     = 0
0.00.038.492 I print_info: rope type        = 2
0.00.038.492 I print_info: rope scaling     = linear
0.00.038.492 I print_info: freq_base_train  = 10000.0
0.00.038.493 I print_info: freq_scale_train = 1
0.00.038.493 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.493 I print_info: rope_finetuned   = unknown
0.00.038.494 I print_info: ssm_d_conv       = 0
0.00.038.494 I print_info: ssm_d_inner      = 0
0.00.038.494 I print_info: ssm_d_state      = 0
0.00.038.494 I print_info: ssm_dt_rank      = 0
0.00.038.494 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.494 I print_info: model type       = 1.4B
0.00.038.495 I print_info: model params     = 1.41 B
0.00.038.495 I print_info: general.name     = 1.4B
0.00.038.495 I print_info: vocab type       = BPE
0.00.038.495 I print_info: n_vocab          = 50304
0.00.038.496 I print_info: n_merges         = 50009
0.00.038.496 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.496 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.496 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.497 I print_info: LF token         = 187 'Ċ'
0.00.038.497 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.497 I print_info: max token length = 1024
0.00.038.498 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.160 I load_tensors: offloading output layer to GPU
0.00.609.161 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.195 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.609.197 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.772 I llama_init_from_model: n_seq_max     = 1
0.00.610.774 I llama_init_from_model: n_ctx         = 128
0.00.610.775 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.776 I llama_init_from_model: n_batch       = 128
0.00.610.776 I llama_init_from_model: n_ubatch      = 128
0.00.610.777 I llama_init_from_model: flash_attn    = 0
0.00.610.779 I llama_init_from_model: freq_base     = 10000.0
0.00.610.780 I llama_init_from_model: freq_scale    = 1
0.00.610.780 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.783 I ggml_metal_init: allocating
0.00.610.919 I ggml_metal_init: found device: Apple M4
0.00.610.936 I ggml_metal_init: picking default device: Apple M4
0.00.612.920 I ggml_metal_init: using embedded metal library
0.00.619.492 I ggml_metal_init: GPU name:   Apple M4
0.00.619.495 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.497 I ggml_metal_init: simdgroup reduction   = true
0.00.619.498 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.498 I ggml_metal_init: has residency sets    = true
0.00.619.498 I ggml_metal_init: has bfloat            = true
0.00.619.498 I ggml_metal_init: use bfloat            = true
0.00.619.499 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.759 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.197 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.201 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.242 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.621 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.623 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.623 I llama_init_from_model: graph nodes  = 967
0.00.643.624 I llama_init_from_model: graph splits = 2
0.00.643.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.890 I 
0.00.675.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.993 I perplexity: tokenizing the input ..
0.00.683.156 I perplexity: tokenization took 7.161 ms
0.00.683.161 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.374 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.830.516 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.830.538 I llama_perf_context_print:        load time =     666.90 ms
0.00.830.539 I llama_perf_context_print: prompt eval time =     145.98 ms /   128 tokens (    1.14 ms per token,   876.81 tokens per second)
0.00.830.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.540 I llama_perf_context_print:       total time =     154.65 ms /   129 tokens
0.00.830.881 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.078s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.964 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.700 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.700 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.701 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.706 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.708 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.528 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.456 I llama_model_loader: - type  f32:  194 tensors
0.00.025.456 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.456 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.457 I print_info: file format = GGUF V3 (latest)
0.00.025.457 I print_info: file type   = Q2_K - Medium
0.00.025.458 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.613 I load: special tokens cache size = 25
0.00.039.694 I load: token to piece cache size = 0.2984 MB
0.00.039.697 I print_info: arch             = gptneox
0.00.039.697 I print_info: vocab_only       = 0
0.00.039.697 I print_info: n_ctx_train      = 2048
0.00.039.697 I print_info: n_embd           = 2048
0.00.039.698 I print_info: n_layer          = 24
0.00.039.700 I print_info: n_head           = 16
0.00.039.701 I print_info: n_head_kv        = 16
0.00.039.701 I print_info: n_rot            = 32
0.00.039.702 I print_info: n_swa            = 0
0.00.039.702 I print_info: n_embd_head_k    = 128
0.00.039.704 I print_info: n_embd_head_v    = 128
0.00.039.705 I print_info: n_gqa            = 1
0.00.039.706 I print_info: n_embd_k_gqa     = 2048
0.00.039.707 I print_info: n_embd_v_gqa     = 2048
0.00.039.707 I print_info: f_norm_eps       = 1.0e-05
0.00.039.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.708 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.708 I print_info: f_logit_scale    = 0.0e+00
0.00.039.710 I print_info: n_ff             = 8192
0.00.039.710 I print_info: n_expert         = 0
0.00.039.710 I print_info: n_expert_used    = 0
0.00.039.711 I print_info: causal attn      = 1
0.00.039.711 I print_info: pooling type     = 0
0.00.039.711 I print_info: rope type        = 2
0.00.039.711 I print_info: rope scaling     = linear
0.00.039.711 I print_info: freq_base_train  = 10000.0
0.00.039.712 I print_info: freq_scale_train = 1
0.00.039.712 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.712 I print_info: rope_finetuned   = unknown
0.00.039.712 I print_info: ssm_d_conv       = 0
0.00.039.712 I print_info: ssm_d_inner      = 0
0.00.039.713 I print_info: ssm_d_state      = 0
0.00.039.713 I print_info: ssm_dt_rank      = 0
0.00.039.713 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.713 I print_info: model type       = 1.4B
0.00.039.717 I print_info: model params     = 1.41 B
0.00.039.717 I print_info: general.name     = 1.4B
0.00.039.718 I print_info: vocab type       = BPE
0.00.039.722 I print_info: n_vocab          = 50304
0.00.039.723 I print_info: n_merges         = 50009
0.00.039.723 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: LF token         = 187 'Ċ'
0.00.039.726 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: max token length = 1024
0.00.039.726 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.337.627 I load_tensors: offloading 24 repeating layers to GPU
0.00.337.641 I load_tensors: offloading output layer to GPU
0.00.337.642 I load_tensors: offloaded 25/25 layers to GPU
0.00.337.674 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.337.676 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.339.415 I llama_init_from_model: n_seq_max     = 1
0.00.339.418 I llama_init_from_model: n_ctx         = 128
0.00.339.418 I llama_init_from_model: n_ctx_per_seq = 128
0.00.339.419 I llama_init_from_model: n_batch       = 128
0.00.339.419 I llama_init_from_model: n_ubatch      = 128
0.00.339.420 I llama_init_from_model: flash_attn    = 0
0.00.339.422 I llama_init_from_model: freq_base     = 10000.0
0.00.339.423 I llama_init_from_model: freq_scale    = 1
0.00.339.423 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.339.425 I ggml_metal_init: allocating
0.00.339.501 I ggml_metal_init: found device: Apple M4
0.00.339.514 I ggml_metal_init: picking default device: Apple M4
0.00.341.339 I ggml_metal_init: using embedded metal library
0.00.347.013 I ggml_metal_init: GPU name:   Apple M4
0.00.347.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.025 I ggml_metal_init: simdgroup reduction   = true
0.00.347.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.026 I ggml_metal_init: has residency sets    = true
0.00.347.026 I ggml_metal_init: has bfloat            = true
0.00.347.027 I ggml_metal_init: use bfloat            = true
0.00.347.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.033 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.303 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.371.815 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.371.820 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.371.860 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.375.221 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.375.222 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.375.223 I llama_init_from_model: graph nodes  = 967
0.00.375.223 I llama_init_from_model: graph splits = 2
0.00.375.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.375.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.402.701 I 
0.00.402.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.402.800 I perplexity: tokenizing the input ..
0.00.410.142 I perplexity: tokenization took 7.338 ms
0.00.410.151 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.543.622 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.544.968 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.544.993 I llama_perf_context_print:        load time =     392.73 ms
0.00.544.994 I llama_perf_context_print: prompt eval time =     132.51 ms /   128 tokens (    1.04 ms per token,   965.99 tokens per second)
0.00.544.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.544.995 I llama_perf_context_print:       total time =     142.30 ms /   129 tokens
0.00.545.370 I ggml_metal_free: deallocating

real	0m0.561s
user	0m0.082s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.179 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.812 I llama_model_loader: - type  f32:  194 tensors
0.00.024.812 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.813 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.813 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.813 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.814 I print_info: file format = GGUF V3 (latest)
0.00.024.814 I print_info: file type   = Q3_K - Medium
0.00.024.815 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.694 I load: special tokens cache size = 25
0.00.038.783 I load: token to piece cache size = 0.2984 MB
0.00.038.786 I print_info: arch             = gptneox
0.00.038.786 I print_info: vocab_only       = 0
0.00.038.786 I print_info: n_ctx_train      = 2048
0.00.038.786 I print_info: n_embd           = 2048
0.00.038.787 I print_info: n_layer          = 24
0.00.038.789 I print_info: n_head           = 16
0.00.038.790 I print_info: n_head_kv        = 16
0.00.038.790 I print_info: n_rot            = 32
0.00.038.790 I print_info: n_swa            = 0
0.00.038.791 I print_info: n_embd_head_k    = 128
0.00.038.792 I print_info: n_embd_head_v    = 128
0.00.038.793 I print_info: n_gqa            = 1
0.00.038.793 I print_info: n_embd_k_gqa     = 2048
0.00.038.794 I print_info: n_embd_v_gqa     = 2048
0.00.038.795 I print_info: f_norm_eps       = 1.0e-05
0.00.038.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.795 I print_info: f_logit_scale    = 0.0e+00
0.00.038.796 I print_info: n_ff             = 8192
0.00.038.796 I print_info: n_expert         = 0
0.00.038.797 I print_info: n_expert_used    = 0
0.00.038.797 I print_info: causal attn      = 1
0.00.038.797 I print_info: pooling type     = 0
0.00.038.797 I print_info: rope type        = 2
0.00.038.797 I print_info: rope scaling     = linear
0.00.038.798 I print_info: freq_base_train  = 10000.0
0.00.038.798 I print_info: freq_scale_train = 1
0.00.038.798 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.799 I print_info: rope_finetuned   = unknown
0.00.038.799 I print_info: ssm_d_conv       = 0
0.00.038.799 I print_info: ssm_d_inner      = 0
0.00.038.799 I print_info: ssm_d_state      = 0
0.00.038.799 I print_info: ssm_dt_rank      = 0
0.00.038.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.800 I print_info: model type       = 1.4B
0.00.038.800 I print_info: model params     = 1.41 B
0.00.038.800 I print_info: general.name     = 1.4B
0.00.038.801 I print_info: vocab type       = BPE
0.00.038.803 I print_info: n_vocab          = 50304
0.00.038.803 I print_info: n_merges         = 50009
0.00.038.803 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: LF token         = 187 'Ċ'
0.00.038.805 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.805 I print_info: max token length = 1024
0.00.038.805 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.453.306 I load_tensors: offloading 24 repeating layers to GPU
0.00.453.316 I load_tensors: offloading output layer to GPU
0.00.453.317 I load_tensors: offloaded 25/25 layers to GPU
0.00.453.352 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.453.353 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.455.021 I llama_init_from_model: n_seq_max     = 1
0.00.455.027 I llama_init_from_model: n_ctx         = 128
0.00.455.027 I llama_init_from_model: n_ctx_per_seq = 128
0.00.455.028 I llama_init_from_model: n_batch       = 128
0.00.455.028 I llama_init_from_model: n_ubatch      = 128
0.00.455.028 I llama_init_from_model: flash_attn    = 0
0.00.455.029 I llama_init_from_model: freq_base     = 10000.0
0.00.455.030 I llama_init_from_model: freq_scale    = 1
0.00.455.030 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.455.032 I ggml_metal_init: allocating
0.00.455.077 I ggml_metal_init: found device: Apple M4
0.00.455.088 I ggml_metal_init: picking default device: Apple M4
0.00.457.285 I ggml_metal_init: using embedded metal library
0.00.463.742 I ggml_metal_init: GPU name:   Apple M4
0.00.463.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.463.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.463.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.463.753 I ggml_metal_init: simdgroup reduction   = true
0.00.463.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.463.754 I ggml_metal_init: has residency sets    = true
0.00.463.754 I ggml_metal_init: has bfloat            = true
0.00.463.754 I ggml_metal_init: use bfloat            = true
0.00.463.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.463.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.483.273 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.486.836 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.486.840 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.486.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.490.209 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.490.211 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.490.211 I llama_init_from_model: graph nodes  = 967
0.00.490.212 I llama_init_from_model: graph splits = 2
0.00.490.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.490.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.665 I 
0.00.520.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.743 I perplexity: tokenizing the input ..
0.00.527.923 I perplexity: tokenization took 7.178 ms
0.00.527.929 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.668.443 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.669.869 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.669.889 I llama_perf_context_print:        load time =     511.60 ms
0.00.669.891 I llama_perf_context_print: prompt eval time =     139.71 ms /   128 tokens (    1.09 ms per token,   916.19 tokens per second)
0.00.669.892 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.669.893 I llama_perf_context_print:       total time =     149.23 ms /   129 tokens
0.00.670.268 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.079s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.163 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.119 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.121 I llama_model_loader: - type  f32:  194 tensors
0.00.026.122 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.122 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.122 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.123 I print_info: file format = GGUF V3 (latest)
0.00.026.123 I print_info: file type   = Q4_K - Medium
0.00.026.124 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.323 I load: special tokens cache size = 25
0.00.040.345 I load: token to piece cache size = 0.2984 MB
0.00.040.347 I print_info: arch             = gptneox
0.00.040.347 I print_info: vocab_only       = 0
0.00.040.348 I print_info: n_ctx_train      = 2048
0.00.040.348 I print_info: n_embd           = 2048
0.00.040.348 I print_info: n_layer          = 24
0.00.040.351 I print_info: n_head           = 16
0.00.040.352 I print_info: n_head_kv        = 16
0.00.040.352 I print_info: n_rot            = 32
0.00.040.353 I print_info: n_swa            = 0
0.00.040.353 I print_info: n_embd_head_k    = 128
0.00.040.356 I print_info: n_embd_head_v    = 128
0.00.040.356 I print_info: n_gqa            = 1
0.00.040.357 I print_info: n_embd_k_gqa     = 2048
0.00.040.358 I print_info: n_embd_v_gqa     = 2048
0.00.040.359 I print_info: f_norm_eps       = 1.0e-05
0.00.040.359 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.359 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.359 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.360 I print_info: f_logit_scale    = 0.0e+00
0.00.040.360 I print_info: n_ff             = 8192
0.00.040.360 I print_info: n_expert         = 0
0.00.040.361 I print_info: n_expert_used    = 0
0.00.040.361 I print_info: causal attn      = 1
0.00.040.361 I print_info: pooling type     = 0
0.00.040.361 I print_info: rope type        = 2
0.00.040.361 I print_info: rope scaling     = linear
0.00.040.363 I print_info: freq_base_train  = 10000.0
0.00.040.363 I print_info: freq_scale_train = 1
0.00.040.363 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.364 I print_info: rope_finetuned   = unknown
0.00.040.364 I print_info: ssm_d_conv       = 0
0.00.040.364 I print_info: ssm_d_inner      = 0
0.00.040.364 I print_info: ssm_d_state      = 0
0.00.040.364 I print_info: ssm_dt_rank      = 0
0.00.040.364 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.364 I print_info: model type       = 1.4B
0.00.040.365 I print_info: model params     = 1.41 B
0.00.040.365 I print_info: general.name     = 1.4B
0.00.040.365 I print_info: vocab type       = BPE
0.00.040.366 I print_info: n_vocab          = 50304
0.00.040.366 I print_info: n_merges         = 50009
0.00.040.366 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.366 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.368 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.368 I print_info: LF token         = 187 'Ċ'
0.00.040.368 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.368 I print_info: max token length = 1024
0.00.040.369 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.518.310 I load_tensors: offloading 24 repeating layers to GPU
0.00.518.327 I load_tensors: offloading output layer to GPU
0.00.518.328 I load_tensors: offloaded 25/25 layers to GPU
0.00.518.359 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.518.360 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.520.101 I llama_init_from_model: n_seq_max     = 1
0.00.520.105 I llama_init_from_model: n_ctx         = 128
0.00.520.105 I llama_init_from_model: n_ctx_per_seq = 128
0.00.520.106 I llama_init_from_model: n_batch       = 128
0.00.520.106 I llama_init_from_model: n_ubatch      = 128
0.00.520.106 I llama_init_from_model: flash_attn    = 0
0.00.520.109 I llama_init_from_model: freq_base     = 10000.0
0.00.520.109 I llama_init_from_model: freq_scale    = 1
0.00.520.110 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.520.112 I ggml_metal_init: allocating
0.00.520.205 I ggml_metal_init: found device: Apple M4
0.00.520.218 I ggml_metal_init: picking default device: Apple M4
0.00.521.835 I ggml_metal_init: using embedded metal library
0.00.528.256 I ggml_metal_init: GPU name:   Apple M4
0.00.528.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.262 I ggml_metal_init: simdgroup reduction   = true
0.00.528.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.262 I ggml_metal_init: has residency sets    = true
0.00.528.263 I ggml_metal_init: has bfloat            = true
0.00.528.263 I ggml_metal_init: use bfloat            = true
0.00.528.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.271 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.549.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.549.771 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.549.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.552.891 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.552.893 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.552.893 I llama_init_from_model: graph nodes  = 967
0.00.552.893 I llama_init_from_model: graph splits = 2
0.00.552.896 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.552.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.777 I 
0.00.582.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.876 I perplexity: tokenizing the input ..
0.00.590.418 I perplexity: tokenization took 7.539 ms
0.00.590.426 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.675 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.733.821 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.733.844 I llama_perf_context_print:        load time =     572.60 ms
0.00.733.845 I llama_perf_context_print: prompt eval time =     141.29 ms /   128 tokens (    1.10 ms per token,   905.94 tokens per second)
0.00.733.846 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.846 I llama_perf_context_print:       total time =     151.07 ms /   129 tokens
0.00.734.180 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.080s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.710 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.900 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.900 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.699 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.567 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.569 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.570 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.570 I llama_model_loader: - type  f32:  194 tensors
0.00.024.571 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.571 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.571 I print_info: file format = GGUF V3 (latest)
0.00.024.572 I print_info: file type   = Q5_K - Medium
0.00.024.576 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.725 I load: special tokens cache size = 25
0.00.038.722 I load: token to piece cache size = 0.2984 MB
0.00.038.725 I print_info: arch             = gptneox
0.00.038.725 I print_info: vocab_only       = 0
0.00.038.726 I print_info: n_ctx_train      = 2048
0.00.038.726 I print_info: n_embd           = 2048
0.00.038.726 I print_info: n_layer          = 24
0.00.038.728 I print_info: n_head           = 16
0.00.038.729 I print_info: n_head_kv        = 16
0.00.038.729 I print_info: n_rot            = 32
0.00.038.729 I print_info: n_swa            = 0
0.00.038.729 I print_info: n_embd_head_k    = 128
0.00.038.730 I print_info: n_embd_head_v    = 128
0.00.038.732 I print_info: n_gqa            = 1
0.00.038.733 I print_info: n_embd_k_gqa     = 2048
0.00.038.733 I print_info: n_embd_v_gqa     = 2048
0.00.038.739 I print_info: f_norm_eps       = 1.0e-05
0.00.038.741 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.742 I print_info: f_logit_scale    = 0.0e+00
0.00.038.745 I print_info: n_ff             = 8192
0.00.038.746 I print_info: n_expert         = 0
0.00.038.747 I print_info: n_expert_used    = 0
0.00.038.747 I print_info: causal attn      = 1
0.00.038.747 I print_info: pooling type     = 0
0.00.038.747 I print_info: rope type        = 2
0.00.038.747 I print_info: rope scaling     = linear
0.00.038.747 I print_info: freq_base_train  = 10000.0
0.00.038.748 I print_info: freq_scale_train = 1
0.00.038.748 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.748 I print_info: rope_finetuned   = unknown
0.00.038.748 I print_info: ssm_d_conv       = 0
0.00.038.748 I print_info: ssm_d_inner      = 0
0.00.038.748 I print_info: ssm_d_state      = 0
0.00.038.748 I print_info: ssm_dt_rank      = 0
0.00.038.749 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.749 I print_info: model type       = 1.4B
0.00.038.749 I print_info: model params     = 1.41 B
0.00.038.750 I print_info: general.name     = 1.4B
0.00.038.750 I print_info: vocab type       = BPE
0.00.038.750 I print_info: n_vocab          = 50304
0.00.038.750 I print_info: n_merges         = 50009
0.00.038.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: LF token         = 187 'Ċ'
0.00.038.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.752 I print_info: max token length = 1024
0.00.038.752 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.076 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.091 I load_tensors: offloading output layer to GPU
0.00.587.092 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.128 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.129 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.588.825 I llama_init_from_model: n_seq_max     = 1
0.00.588.829 I llama_init_from_model: n_ctx         = 128
0.00.588.829 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.830 I llama_init_from_model: n_batch       = 128
0.00.588.830 I llama_init_from_model: n_ubatch      = 128
0.00.588.830 I llama_init_from_model: flash_attn    = 0
0.00.588.832 I llama_init_from_model: freq_base     = 10000.0
0.00.588.833 I llama_init_from_model: freq_scale    = 1
0.00.588.833 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.839 I ggml_metal_init: allocating
0.00.588.972 I ggml_metal_init: found device: Apple M4
0.00.588.986 I ggml_metal_init: picking default device: Apple M4
0.00.590.957 I ggml_metal_init: using embedded metal library
0.00.597.444 I ggml_metal_init: GPU name:   Apple M4
0.00.597.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.448 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.449 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.450 I ggml_metal_init: simdgroup reduction   = true
0.00.597.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.450 I ggml_metal_init: has residency sets    = true
0.00.597.450 I ggml_metal_init: has bfloat            = true
0.00.597.451 I ggml_metal_init: use bfloat            = true
0.00.597.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.702 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.082 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.086 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.131 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.301 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.303 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.304 I llama_init_from_model: graph nodes  = 967
0.00.621.304 I llama_init_from_model: graph splits = 2
0.00.621.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.617 I 
0.00.656.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.701 I perplexity: tokenizing the input ..
0.00.663.731 I perplexity: tokenization took 7.026 ms
0.00.663.742 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.858 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.803.194 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.803.216 I llama_perf_context_print:        load time =     647.90 ms
0.00.803.217 I llama_perf_context_print: prompt eval time =     137.19 ms /   128 tokens (    1.07 ms per token,   933.01 tokens per second)
0.00.803.218 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.218 I llama_perf_context_print:       total time =     146.60 ms /   129 tokens
0.00.803.578 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.296 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.207 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.211 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.218 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.219 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.219 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.219 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.220 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.221 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.221 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.221 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.222 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.222 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.222 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.223 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.225 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.225 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.872 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.872 I llama_model_loader: - type  f32:  194 tensors
0.00.025.872 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.873 I print_info: file format = GGUF V3 (latest)
0.00.025.874 I print_info: file type   = Q6_K
0.00.025.874 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.008 I load: special tokens cache size = 25
0.00.039.792 I load: token to piece cache size = 0.2984 MB
0.00.039.795 I print_info: arch             = gptneox
0.00.039.795 I print_info: vocab_only       = 0
0.00.039.795 I print_info: n_ctx_train      = 2048
0.00.039.795 I print_info: n_embd           = 2048
0.00.039.795 I print_info: n_layer          = 24
0.00.039.798 I print_info: n_head           = 16
0.00.039.799 I print_info: n_head_kv        = 16
0.00.039.799 I print_info: n_rot            = 32
0.00.039.799 I print_info: n_swa            = 0
0.00.039.800 I print_info: n_embd_head_k    = 128
0.00.039.800 I print_info: n_embd_head_v    = 128
0.00.039.800 I print_info: n_gqa            = 1
0.00.039.801 I print_info: n_embd_k_gqa     = 2048
0.00.039.802 I print_info: n_embd_v_gqa     = 2048
0.00.039.802 I print_info: f_norm_eps       = 1.0e-05
0.00.039.803 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.803 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.803 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.803 I print_info: f_logit_scale    = 0.0e+00
0.00.039.804 I print_info: n_ff             = 8192
0.00.039.804 I print_info: n_expert         = 0
0.00.039.804 I print_info: n_expert_used    = 0
0.00.039.804 I print_info: causal attn      = 1
0.00.039.804 I print_info: pooling type     = 0
0.00.039.805 I print_info: rope type        = 2
0.00.039.805 I print_info: rope scaling     = linear
0.00.039.805 I print_info: freq_base_train  = 10000.0
0.00.039.805 I print_info: freq_scale_train = 1
0.00.039.806 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.808 I print_info: rope_finetuned   = unknown
0.00.039.808 I print_info: ssm_d_conv       = 0
0.00.039.808 I print_info: ssm_d_inner      = 0
0.00.039.808 I print_info: ssm_d_state      = 0
0.00.039.808 I print_info: ssm_dt_rank      = 0
0.00.039.809 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.809 I print_info: model type       = 1.4B
0.00.039.809 I print_info: model params     = 1.41 B
0.00.039.810 I print_info: general.name     = 1.4B
0.00.039.810 I print_info: vocab type       = BPE
0.00.039.810 I print_info: n_vocab          = 50304
0.00.039.810 I print_info: n_merges         = 50009
0.00.039.811 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.811 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: LF token         = 187 'Ċ'
0.00.039.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: max token length = 1024
0.00.039.816 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.275.323 I load_tensors: offloading 24 repeating layers to GPU
0.00.275.328 I load_tensors: offloading output layer to GPU
0.00.275.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.275.352 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.275.358 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.276.459 I llama_init_from_model: n_seq_max     = 1
0.00.276.461 I llama_init_from_model: n_ctx         = 128
0.00.276.462 I llama_init_from_model: n_ctx_per_seq = 128
0.00.276.462 I llama_init_from_model: n_batch       = 128
0.00.276.463 I llama_init_from_model: n_ubatch      = 128
0.00.276.463 I llama_init_from_model: flash_attn    = 0
0.00.276.464 I llama_init_from_model: freq_base     = 10000.0
0.00.276.464 I llama_init_from_model: freq_scale    = 1
0.00.276.465 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.276.468 I ggml_metal_init: allocating
0.00.276.521 I ggml_metal_init: found device: Apple M4
0.00.276.532 I ggml_metal_init: picking default device: Apple M4
0.00.277.900 I ggml_metal_init: using embedded metal library
0.00.283.600 I ggml_metal_init: GPU name:   Apple M4
0.00.283.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.283.604 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.283.605 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.283.606 I ggml_metal_init: simdgroup reduction   = true
0.00.283.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.283.606 I ggml_metal_init: has residency sets    = true
0.00.283.606 I ggml_metal_init: has bfloat            = true
0.00.283.607 I ggml_metal_init: use bfloat            = true
0.00.283.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.283.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.299.747 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.303.144 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.303.148 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.303.190 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.306.329 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.306.331 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.306.331 I llama_init_from_model: graph nodes  = 967
0.00.306.332 I llama_init_from_model: graph splits = 2
0.00.306.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.306.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.339.345 I 
0.00.339.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.339.380 I perplexity: tokenizing the input ..
0.00.346.580 I perplexity: tokenization took 7.197 ms
0.00.346.587 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.479.854 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.481.284 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.481.308 I llama_perf_context_print:        load time =     329.05 ms
0.00.481.309 I llama_perf_context_print: prompt eval time =     132.31 ms /   128 tokens (    1.03 ms per token,   967.44 tokens per second)
0.00.481.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.481.310 I llama_perf_context_print:       total time =     141.96 ms /   129 tokens
0.00.481.693 I ggml_metal_free: deallocating

real	0m0.497s
user	0m0.077s
sys	0m0.094s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.229 I build: 4784 (b95c8af3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.789 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.772 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.779 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.781 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.781 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.783 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.785 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.786 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.788 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.495 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.496 I llama_model_loader: - type  f32:  194 tensors
0.00.033.496 I llama_model_loader: - type  f16:   98 tensors
0.00.033.497 I print_info: file format = GGUF V3 (latest)
0.00.033.497 I print_info: file type   = all F32 (guessed)
0.00.033.499 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.042.056 I load: special tokens cache size = 25
0.00.048.172 I load: token to piece cache size = 0.2984 MB
0.00.048.177 I print_info: arch             = gptneox
0.00.048.178 I print_info: vocab_only       = 0
0.00.048.178 I print_info: n_ctx_train      = 2048
0.00.048.178 I print_info: n_embd           = 2048
0.00.048.178 I print_info: n_layer          = 24
0.00.048.182 I print_info: n_head           = 16
0.00.048.182 I print_info: n_head_kv        = 16
0.00.048.183 I print_info: n_rot            = 32
0.00.048.183 I print_info: n_swa            = 0
0.00.048.183 I print_info: n_embd_head_k    = 128
0.00.048.183 I print_info: n_embd_head_v    = 128
0.00.048.184 I print_info: n_gqa            = 1
0.00.048.185 I print_info: n_embd_k_gqa     = 2048
0.00.048.185 I print_info: n_embd_v_gqa     = 2048
0.00.048.186 I print_info: f_norm_eps       = 1.0e-05
0.00.048.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.188 I print_info: f_logit_scale    = 0.0e+00
0.00.048.189 I print_info: n_ff             = 8192
0.00.048.189 I print_info: n_expert         = 0
0.00.048.189 I print_info: n_expert_used    = 0
0.00.048.189 I print_info: causal attn      = 1
0.00.048.189 I print_info: pooling type     = 0
0.00.048.189 I print_info: rope type        = 2
0.00.048.190 I print_info: rope scaling     = linear
0.00.048.190 I print_info: freq_base_train  = 10000.0
0.00.048.190 I print_info: freq_scale_train = 1
0.00.048.190 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.191 I print_info: rope_finetuned   = unknown
0.00.048.191 I print_info: ssm_d_conv       = 0
0.00.048.191 I print_info: ssm_d_inner      = 0
0.00.048.191 I print_info: ssm_d_state      = 0
0.00.048.191 I print_info: ssm_dt_rank      = 0
0.00.048.191 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.191 I print_info: model type       = 1.4B
0.00.048.192 I print_info: model params     = 1.41 B
0.00.048.192 I print_info: general.name     = 1.4B
0.00.048.192 I print_info: vocab type       = BPE
0.00.048.193 I print_info: n_vocab          = 50304
0.00.048.193 I print_info: n_merges         = 50009
0.00.048.193 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.193 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.193 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.193 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.194 I print_info: LF token         = 187 'Ċ'
0.00.048.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.194 I print_info: max token length = 1024
0.00.048.195 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.245.773 I load_tensors: offloading 24 repeating layers to GPU
0.01.245.777 I load_tensors: offloading output layer to GPU
0.01.245.777 I load_tensors: offloaded 25/25 layers to GPU
0.01.245.806 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.245.809 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.246.493 I llama_init_from_model: n_seq_max     = 1
0.01.246.495 I llama_init_from_model: n_ctx         = 128
0.01.246.495 I llama_init_from_model: n_ctx_per_seq = 128
0.01.246.495 I llama_init_from_model: n_batch       = 128
0.01.246.496 I llama_init_from_model: n_ubatch      = 128
0.01.246.496 I llama_init_from_model: flash_attn    = 0
0.01.246.497 I llama_init_from_model: freq_base     = 10000.0
0.01.246.497 I llama_init_from_model: freq_scale    = 1
0.01.246.497 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.246.501 I ggml_metal_init: allocating
0.01.246.579 I ggml_metal_init: found device: Apple M4
0.01.246.587 I ggml_metal_init: picking default device: Apple M4
0.01.247.759 I ggml_metal_init: using embedded metal library
0.01.251.657 I ggml_metal_init: GPU name:   Apple M4
0.01.251.660 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.251.661 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.251.661 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.251.661 I ggml_metal_init: simdgroup reduction   = true
0.01.251.662 I ggml_metal_init: simdgroup matrix mul. = true
0.01.251.662 I ggml_metal_init: has residency sets    = true
0.01.251.662 I ggml_metal_init: has bfloat            = true
0.01.251.662 I ggml_metal_init: use bfloat            = true
0.01.251.663 I ggml_metal_init: hasUnifiedMemory      = true
0.01.251.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.262.973 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.264.713 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.264.715 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.264.741 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.266.434 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.266.436 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.266.436 I llama_init_from_model: graph nodes  = 967
0.01.266.436 I llama_init_from_model: graph splits = 2
0.01.266.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.266.438 I 
0.01.266.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.266.477 I compute_imatrix: tokenizing the input ..
0.01.270.662 I compute_imatrix: tokenization took 4.185 ms
0.01.270.665 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.537.043 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.539.907 I llama_perf_context_print:        load time =    1519.25 ms
0.01.539.908 I llama_perf_context_print: prompt eval time =     264.61 ms /   128 tokens (    2.07 ms per token,   483.73 tokens per second)
0.01.539.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.539.909 I llama_perf_context_print:       total time =    1522.11 ms /   129 tokens
0.01.540.473 I ggml_metal_free: deallocating

real	0m1.745s
user	0m0.102s
sys	0m0.230s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4784 (b95c8af3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147208080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1472087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147208d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147209320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1472098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147209e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14720a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14720a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14720af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14720b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14720b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14720be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14720c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14720d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14720d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14720e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14720e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14720eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14720f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14720fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1472104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147210c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147211320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147211bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1472122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1472125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147212bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147213820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147213d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147304100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147304570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1473049e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147304e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1473052c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147305730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147305fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147306470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147306940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147306e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1473072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1473077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147307c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147308150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147308620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147308af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147308f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1473093d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147309d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147309fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14730a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14730a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14730ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14730b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14730b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14730ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14730c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14730c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14730c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14730cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14730d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14730d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14730dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14730e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14730e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14730eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14730efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14730f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14730f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14730fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1473103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1473108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147310da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1473112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1473117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147311d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147312300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1473128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147312e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147313410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1473139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147313f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147314520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147314ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147315080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147315630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147315be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147316190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147316740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147316cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1473172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147317850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147317e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1473183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147318960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147318f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1473194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147319a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1473099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14731a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14731a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14731aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14731b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14731b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14731bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14731c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14731c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14731ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14731d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14731d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14731dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14731e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14731e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14731eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14731f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14731f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14731fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1473203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1473208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147320da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1473212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1473217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147321ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1473221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1473226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147322ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1473230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1473235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147323aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147323fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1473244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1473249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147324ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1473253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1473258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147325da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1473262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1473267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147326ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1473271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1473276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147327ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1473280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1473285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147328aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147328fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1473294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1473299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147329ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14732a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14732a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14732ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14732b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14732b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14732bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14732c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14732c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14732cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14732d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14732d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14732daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14732dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14732e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14732e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14732eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14732f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14732f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14732fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1473302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1473307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147330ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1473311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1473316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147331ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1473320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1473325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147332aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147332fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1473334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1473339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147333ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1473343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1473348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147334da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1473352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1473357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147335ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1473361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1473366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147336ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108704580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1087049f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108704e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1087052d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108705740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108705bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108706020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108706490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108706900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108706d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1087071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108707650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108707ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108707f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1087083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108708810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108708c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1087090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108709560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1087099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10870a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10870a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10870aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10870b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10870ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10870bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10870c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10870cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10870d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10870d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10870dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10870e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10870e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10870ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10870f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10870f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10870fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108710420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1087109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108711530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108711ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108712090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1087131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108713750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108713d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1087142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108714860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1087153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108715970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108715f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1087164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108716a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108717030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1087175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108717b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108718140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1087186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108718ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108719250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108719800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108719db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10871a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10871a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10871aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10871b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10871ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10871bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10871c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10871cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10871d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10871d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10871dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10871e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10871e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10871eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10871f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10871f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10871fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1087200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1087205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108720aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108720fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1087214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1087219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108721ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1087223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1087228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108722da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1087232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1087237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x108723ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1087241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1087246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x108724ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1087250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1087255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x108725aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x108725fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1087264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108726eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1087275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108727cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108728410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1087286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108728ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108729180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108729790 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.711.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125f09830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125f09ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125f0a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125f0a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125f0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125f0ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125f0b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125f0b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125f0bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125f0c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125f0c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125f0cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125f0de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125f0e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125f0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125f0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125f0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125f102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125f10a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125f111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125f118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125f11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125f12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125f130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125f133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125f13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125f13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125f140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125f145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125f14b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125f14f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125f15230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125f156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125f15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125f16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125f16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125f16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125f16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125f17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125f17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125f17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125f18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125f18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125f18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125f19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125f195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125f19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125f19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125f1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125f1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125f1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125f1b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125f1b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125f1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125f1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125f1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125f1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125f1d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125f1db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125f1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125f1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125f1edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125f1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125f1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125f1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125f20040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125f204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125f20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125f20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125f21370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125f218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125f21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125f22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125f228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125f22e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125f23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125f238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125f23df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125f24340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125f24890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125f24de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125f25330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125f25880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125f25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125f26320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125f26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125f26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125f27310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125f27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125f28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125f28850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125f28da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125f292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125f29840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125f29d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125f2a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125f2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125f2ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125f2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125f2b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125f2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125f2c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125f2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125f2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125f2d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125f2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125f2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125f2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125f2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125f2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125f2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125f2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125f2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125f2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125f30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125f307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125f30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125f310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125f31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125f31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125f32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125f32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125f32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125f33140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125f335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125f33a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125f33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125f343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125f34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125f34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125f351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125f35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125f35ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125f35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125f36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125f368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125f36d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125f37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125f376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125f37b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125f37fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125f38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125f38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125f38dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125f39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125f39700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125f39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125f3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125f3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125f3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125f3ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125f3b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125f3b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125f3bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125f3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125f3c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125f3c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125f3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125f3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125f3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125f3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125f3e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125f3e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125f3ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125f3eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125f3f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125f3f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125f3fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125f40160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125f40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125f40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125f40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125f413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125f41880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125f41d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125f421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125f42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125f42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125f42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125f43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125f438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125f43d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125f44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125f446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125f44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125f45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125f454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125f459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125f45f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125f46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125f469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125f46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125f472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125f47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125f486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125f48b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125f48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125f49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125f49a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125f4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125f4a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125f4ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125f4b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125f4b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125f4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125f4c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125f4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125f4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125f4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125f4d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125f4dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125f4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125f4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125f4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125f4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125f4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125f4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125f50220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125f50770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125f50cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125f51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125f51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125f51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125f52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125f52750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125f52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125f531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125f53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125f53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125f541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125f54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125f54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125f551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125f55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125f55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125f561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125f56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125f56c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125f571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125f57700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125f57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125f581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125f586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125f58c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125f59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125f596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125f59c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125f5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125f5a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125f5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125f5b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125f5b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125f5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125f5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125f5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125f5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125f5d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125f5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125f5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125f5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125f5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125f5ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125f5ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125f5f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125f5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125f5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125f601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125f60640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125f60ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125f60f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125f61420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125f618c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125f61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125f62200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125f626a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x125f62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x125f62fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x125f63480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x125f63920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x125f63dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x125f64260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x125f64700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x125f64ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x125f65040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x125f654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125f65a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125f66150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125f66870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125f66f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125f676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125f67970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125f68160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125f68420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125f68a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e04530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e08490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e08ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e09b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e0a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e0aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e0b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e0bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e0e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e0f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e0fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e10fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e12ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e13160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e14690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e16b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e19ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e1d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e1f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e23810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e24d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e28d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e29d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e2acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e2b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e2c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e2c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e2ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e2d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e2d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e2dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e2e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e2fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e30110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e30ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e31390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e31830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e31cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e32f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e33890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e33d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e34b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e34fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e35d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e36b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e37010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e38290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e38730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e39070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e39e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e3a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e3ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e3beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e3cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e3d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e3df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e3e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e3ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e3f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e40410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e40d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e41690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e41b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e41fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e42470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e42db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e43250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e44030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e44970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e44ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e47b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e48900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e49700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e4a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e4b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e4d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e51180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e51c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e52170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e53160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e53c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e54150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e54bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e55140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e55690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e56680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e56bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e57bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e58110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e58660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e58bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e59100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e59650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e59ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e5a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e5ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e5b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e5b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e5bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e5c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e5c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e5cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e5d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e5d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e5dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e5e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e5ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e5fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e5ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e60d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e61230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x125e62010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x125e624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x125e62950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x125e62df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x125e63290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x125e63730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x125e63bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x125e64070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x125e64510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x125e649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e64f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e65620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e66460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e66b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e66e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e67f00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.812s
user	0m0.282s
sys	0m0.289s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4784 (b95c8af3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b70cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b70d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b70d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b70df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b70e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b70ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b70f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b70f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b70fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b7100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b7105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b710aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b7115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b711d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b712ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b7133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b714200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b7149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b7150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b715810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b715f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b7167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b716ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b7171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b7177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b718970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b718c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b7190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b719390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b719c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b71a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b71a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b71a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b71ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b71b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b71b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b71bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b71bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b71c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b71c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b71cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b71d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b71d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b71dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b71e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b71ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b71f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b71f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b71fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b720410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b720a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b721210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b7216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b721b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b721e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b722c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b722ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b723810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b723cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b724150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b7245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b724a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b724f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b7253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b725870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b725d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b7261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b726650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b726ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b7270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b727640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b727b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b7280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b728630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b728b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b7290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b729620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b729b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b72a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b72a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b72ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b72b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b72b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b72bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b72c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b72c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b72cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b72d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b72d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b72db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b72e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b72e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b71e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b72ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b72f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b72f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b72fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b7301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b730730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b730c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b7311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b731720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b731c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b7321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b732710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b732c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b7331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b733700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b733ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b734040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b7344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b734980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b734e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b7352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b735760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b735c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b7360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b736540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b7369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b736e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b737320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b7377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b737c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b738100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b7385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b738a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b738ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b739380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b739820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b739cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b73a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b73a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b73aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b73af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b73b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b73b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b73bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b73c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b73c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b73cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b73cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b73d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b73d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b73dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b73e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b73e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b73eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b73f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b73f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b73f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b73fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b740280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b740720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b740bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b741060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b741500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b7419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b741e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b7422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b742780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b742c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b7430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b743560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b743a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b743ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b744340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b7447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b744c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b745120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b7455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b745a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b745f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b7463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b746840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b746ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b747180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b747620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b747ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b747f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b748400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b7488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b748d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b7491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b749680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b749b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b749fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b74a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b74a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b74ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b74b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b74b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b74be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b74c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b74c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b74cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b74d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b74db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b74dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b74e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b74e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b74eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b74f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b74fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b74ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b750470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b750c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b751170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b7516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b751c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b752160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b7526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b752c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b753150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b7536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b753bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b754140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b754690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b754be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b755130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b755680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b755bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b756120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b756670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b756bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b757110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b757660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b757bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b758100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b758650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b758ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b7590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b759640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b759b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b75a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b75a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b75ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b75b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b75b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b75bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b75c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b75c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b75cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b75d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b75d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b75db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b75e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b75e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b75eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b75f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b75f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b75fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b760080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b7605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b760b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b761070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b7615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b761b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b762060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b7625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b762b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b763050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b7635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b763a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b763ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b764380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b764820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b764cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b765160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b765600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b765aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b765f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b7663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b766880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b766d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b7671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b767660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b767b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15b767fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15b768440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15b7688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15b768d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15b769220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15b7696c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15b769b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15b76a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15b76a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15b76a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b76ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b76b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b76bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b76c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b76cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b76cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b76d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b76d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b76de90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b6096b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b609b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b609f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b60a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b60a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b60ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b60b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b60b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b60ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b60bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b60c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b60caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b60d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b60dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b60e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b60eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b60f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b60fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b610200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b6109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b6110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b611810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b611f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b612650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b612d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b613030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b6132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b613760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b613bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b614040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b614540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b614a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b614ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b6155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b6164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b6169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b616ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b6173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b6178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b617dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b6182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b6187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b618c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b6190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b619510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b619980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b61a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b61a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b61ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b61afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b61b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b61bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b61c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b61c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b61c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b61d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b61d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b61da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b61df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b61e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b61e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b61ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b61f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b61f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b61faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b61ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b620430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b6208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b621810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b621d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b6222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b6232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b6237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b6247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b6257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b625d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b626270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b6267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b627260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b6277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b627d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b6287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b629790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b629ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b62a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b62a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b62acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b62b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b62b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b62bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b62c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b62c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b62ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b62d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b62d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b62dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b62e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b62e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b62eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b62efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b62f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b62f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b62fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b6306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b631030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b6314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b631970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b631e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b6322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b632750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b632bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b633090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b633530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b6339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b633e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b634310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b6347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b634c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b6350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b635590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b635a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b635ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b636370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b636810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b636cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b637150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b6375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b637a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b637f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b6383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b638870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b638d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b6391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b639650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b639af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b639f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b63a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b63a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b63ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b63b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b63b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b63bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b63bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b63c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b63c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b63cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b63d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b63d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b63dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b63e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b63e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b63e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b63f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b63f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b63fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b6400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b640550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b6409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b640e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b641330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b6417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b641c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b642110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b6425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b642ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b643390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b643830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b643cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b644170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b644610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b644f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b6453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b645940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b645e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b6463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b647810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b647e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b648610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b648ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b648d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b649380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b649990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b64a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b64a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b64aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b64af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b64b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b64bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b64c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b64c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b64cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b64d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b64d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b64dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b64e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b64e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b64ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b64f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b64f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b64fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b650170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b6506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b650c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b651160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b6516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b651c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b652150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b6526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b652bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b653140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b653690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b653be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b654130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b654680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b654bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b655120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b655670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b655bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b656110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b656660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b656bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b657100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b657650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b657ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b6580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b658640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b658b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b6590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b659630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b659b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b65a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b65a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b65ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b65b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b65b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b65bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b65c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b65c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b65cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b65d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b65d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b65db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b65e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b65e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b65e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b65ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b65f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b65f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b65fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b6600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b660590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b660a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b660ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b661370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b661810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b661cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b662150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b6625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15b662a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15b662f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15b6633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15b663870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15b663d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15b6641b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15b664650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15b664af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15b664f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15b665430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b665980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b6660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b6667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b666ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b6678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b6680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b668370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b668980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15fe046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15fe04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15fe04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15fe05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15fe058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15fe05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15fe06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15fe065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15fe06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15fe06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15fe07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15fe079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15fe08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15fe08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15fe094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15fe09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15fe0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15fe0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15fe0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15fe0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15fe0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15fe0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15fe0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15fe0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15fe0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15fe0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15fe0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15fe0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15fe0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15fe0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15fe0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15fe0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15fe0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15fe10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15fe104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15fe10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15fe10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15fe11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15fe11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15fe11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15fe11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15fe123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15fe12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15fe12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15fe13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15fe13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15fe13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15fe13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15fe142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15fe14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15fe14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15fe15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15fe154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15fe15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15fe15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15fe161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15fe16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15fe16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15fe170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15fe17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15fe179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15fe17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15fe18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15fe18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15fe18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15fe18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15fe19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15fe198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15fe19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15fe1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15fe1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15fe1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15fe1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15fe1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15fe1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15fe1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15fe1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15fe1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15fe1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15fe1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15fe1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15fe1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15fe1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15fe1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15fe1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15fe1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15fe1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15fe1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15fe1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15fe1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15fe1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15fe20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15fe207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15fe20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15fe21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15fe21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15fe21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15fe21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15fe22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15fe22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15fe23110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15fe236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15fe23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15fe24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15fe247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15fe24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15fe25330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15fe258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15fe25e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15fe26440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15fe269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15fe26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15fe27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15fe27b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15fe28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15fe28500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15fe28a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15fe28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15fe29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15fe29900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15fe29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15fe2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15fe2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15fe2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15fe2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15fe2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15fe2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15fe2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15fe2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15fe2cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15fe2d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15fe2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15fe2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15fe2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15fe2e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15fe2e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15fe2ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15fe2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15fe2f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15fe2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15fe30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15fe30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15fe30c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15fe31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15fe31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15fe31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15fe32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15fe32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15fe32a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15fe32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15fe33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15fe33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15fe33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15fe34300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15fe34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15fe34d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15fe35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15fe35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15fe35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15fe36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15fe36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15fe36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15fe37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15fe37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15fe37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15fe37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15fe38400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15fe38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15fe38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15fe39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15fe39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15fe39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15fe3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15fe3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15fe3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15fe3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15fe3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15fe3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15fe3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15fe3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15fe3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15fe3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15fe3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15fe3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15fe3de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15fe3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15fe3e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15fe3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15fe3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15fe3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15fe3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15fe40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15fe40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15fe40b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15fe410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15fe41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15fe41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c804230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c8046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c804b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c804f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c8053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c805860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c805cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c806140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15c8065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c806a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c806fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c807420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c807890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c8083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c8086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c808960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c808dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c809240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c8096b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c809b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c809f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c80a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c80a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c80ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c80b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c80b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c80ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c80bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c80c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c80c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c80cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c80d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c80d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c80d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c80ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c80e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c80e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c80eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c80ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c80f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c80f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c80fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c810130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c8105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c810a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c810e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c8112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c811760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c811bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c812040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c8124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c812920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c812d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c813200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c813670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c813ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c813f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c8143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c814830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c814ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c815110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c815580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c8159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c815e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c8162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c816740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c816bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c817020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c817490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c817900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15c817d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15c8181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c818650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c818ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c818f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c8193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c819810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c819c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c81a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c81a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c81a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c81ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c81b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c81b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c81bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15c81c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15c81c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15c81c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15c81cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15c81d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15c81d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15c81daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15c81df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15c81e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15c81e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c81ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c81f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c81fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c820550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c820c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c820f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c8211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c821ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.954s
user	0m0.232s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
