### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.88 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.72 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.45 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.52 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.04 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.27 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.56 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.55 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.70 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 224.76 sec*proc (28 tests)

Total Test time (real) = 224.77 sec

real	3m44.812s
user	7m36.133s
sys	0m6.318s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.31 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.59 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.44 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.26 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.95 sec*proc (28 tests)

Total Test time (real) =  51.97 sec

real	0m51.981s
user	1m12.703s
sys	0m5.611s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.071 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.072 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.237 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.247 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.248 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.249 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.250 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.251 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.252 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.252 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.253 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.254 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.257 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.258 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.259 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.259 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.260 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.260 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.261 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.003 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.005 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.006 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.006 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.007 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.026.007 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.007 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.026.008 I llama_model_loader: - type  f32:  124 tensors
0.00.026.009 I llama_model_loader: - type  f16:   73 tensors
0.00.030.624 I llm_load_vocab: special tokens cache size = 5
0.00.032.700 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.032.728 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.032.730 I llm_load_print_meta: arch             = bert
0.00.032.730 I llm_load_print_meta: vocab type       = WPM
0.00.032.731 I llm_load_print_meta: n_vocab          = 30522
0.00.032.731 I llm_load_print_meta: n_merges         = 0
0.00.032.731 I llm_load_print_meta: vocab_only       = 0
0.00.032.731 I llm_load_print_meta: n_ctx_train      = 512
0.00.032.732 I llm_load_print_meta: n_embd           = 384
0.00.032.732 I llm_load_print_meta: n_layer          = 12
0.00.032.735 I llm_load_print_meta: n_head           = 12
0.00.032.736 I llm_load_print_meta: n_head_kv        = 12
0.00.032.737 I llm_load_print_meta: n_rot            = 32
0.00.032.737 I llm_load_print_meta: n_swa            = 0
0.00.032.737 I llm_load_print_meta: n_embd_head_k    = 32
0.00.032.737 I llm_load_print_meta: n_embd_head_v    = 32
0.00.032.738 I llm_load_print_meta: n_gqa            = 1
0.00.032.739 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.032.740 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.032.741 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.032.741 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.032.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.032.742 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.032.742 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.032.743 I llm_load_print_meta: n_ff             = 1536
0.00.032.743 I llm_load_print_meta: n_expert         = 0
0.00.032.744 I llm_load_print_meta: n_expert_used    = 0
0.00.032.744 I llm_load_print_meta: causal attn      = 0
0.00.032.744 I llm_load_print_meta: pooling type     = 2
0.00.032.744 I llm_load_print_meta: rope type        = 2
0.00.032.746 I llm_load_print_meta: rope scaling     = linear
0.00.032.747 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.032.748 I llm_load_print_meta: freq_scale_train = 1
0.00.032.748 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.032.749 I llm_load_print_meta: rope_finetuned   = unknown
0.00.032.749 I llm_load_print_meta: ssm_d_conv       = 0
0.00.032.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.032.751 I llm_load_print_meta: ssm_d_state      = 0
0.00.032.751 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.032.751 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.032.751 I llm_load_print_meta: model type       = 33M
0.00.032.752 I llm_load_print_meta: model ftype      = F16
0.00.032.752 I llm_load_print_meta: model params     = 33.21 M
0.00.032.753 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.032.753 I llm_load_print_meta: general.name     = Bge Small
0.00.032.754 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.032.756 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.032.756 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.032.757 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.032.757 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.032.757 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.032.757 I llm_load_print_meta: max token length = 21
0.00.034.729 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.729 I llm_load_tensors: offloading output layer to GPU
0.00.034.730 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.756 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.758 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.350 I llama_new_context_with_model: n_ctx         = 512
0.00.035.350 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.350 I llama_new_context_with_model: n_batch       = 2048
0.00.035.351 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.351 I llama_new_context_with_model: flash_attn    = 0
0.00.035.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.352 I llama_new_context_with_model: freq_scale    = 1
0.00.035.353 I ggml_metal_init: allocating
0.00.035.363 I ggml_metal_init: found device: Apple M4
0.00.035.367 I ggml_metal_init: picking default device: Apple M4
0.00.036.207 I ggml_metal_init: using embedded metal library
0.00.040.265 I ggml_metal_init: GPU name:   Apple M4
0.00.040.268 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.268 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.269 I ggml_metal_init: simdgroup reduction   = true
0.00.040.269 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.269 I ggml_metal_init: has bfloat            = true
0.00.040.269 I ggml_metal_init: use bfloat            = true
0.00.040.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.715 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.053.409 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.412 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.415 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.248 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.054.249 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.054.250 I llama_new_context_with_model: graph nodes  = 429
0.00.054.250 I llama_new_context_with_model: graph splits = 2
0.00.054.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.349 I 
0.00.061.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.046 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.104 I llama_perf_context_print:        load time =      45.27 ms
0.00.067.105 I llama_perf_context_print: prompt eval time =       4.92 ms /     9 tokens (    0.55 ms per token,  1830.76 tokens per second)
0.00.067.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.107 I llama_perf_context_print:       total time =       5.75 ms /    10 tokens
0.00.067.233 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.048s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.403 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.505 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.510 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.511 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.511 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.511 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.512 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.512 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.513 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.513 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.513 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.517 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.517 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.517 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.518 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.518 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.518 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.518 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.729 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.730 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.730 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.730 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.731 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.731 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.731 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.732 I llama_model_loader: - type  f32:  124 tensors
0.00.014.732 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.194 I llm_load_vocab: special tokens cache size = 5
0.00.018.474 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.483 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.484 I llm_load_print_meta: arch             = bert
0.00.018.485 I llm_load_print_meta: vocab type       = WPM
0.00.018.485 I llm_load_print_meta: n_vocab          = 30522
0.00.018.485 I llm_load_print_meta: n_merges         = 0
0.00.018.485 I llm_load_print_meta: vocab_only       = 0
0.00.018.486 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.486 I llm_load_print_meta: n_embd           = 384
0.00.018.486 I llm_load_print_meta: n_layer          = 12
0.00.018.488 I llm_load_print_meta: n_head           = 12
0.00.018.489 I llm_load_print_meta: n_head_kv        = 12
0.00.018.489 I llm_load_print_meta: n_rot            = 32
0.00.018.489 I llm_load_print_meta: n_swa            = 0
0.00.018.489 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.489 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.490 I llm_load_print_meta: n_gqa            = 1
0.00.018.491 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.491 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.492 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.492 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.492 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.492 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.493 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.493 I llm_load_print_meta: n_ff             = 1536
0.00.018.493 I llm_load_print_meta: n_expert         = 0
0.00.018.493 I llm_load_print_meta: n_expert_used    = 0
0.00.018.493 I llm_load_print_meta: causal attn      = 0
0.00.018.494 I llm_load_print_meta: pooling type     = 2
0.00.018.494 I llm_load_print_meta: rope type        = 2
0.00.018.494 I llm_load_print_meta: rope scaling     = linear
0.00.018.494 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.495 I llm_load_print_meta: freq_scale_train = 1
0.00.018.495 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.495 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.495 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.495 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.495 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.495 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.496 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.496 I llm_load_print_meta: model type       = 33M
0.00.018.496 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.496 I llm_load_print_meta: model params     = 33.21 M
0.00.018.497 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.497 I llm_load_print_meta: general.name     = Bge Small
0.00.018.497 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.498 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.498 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.498 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.498 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.498 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.498 I llm_load_print_meta: max token length = 21
0.00.019.805 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.806 I llm_load_tensors: offloading output layer to GPU
0.00.019.806 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.816 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.816 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.184 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.185 I llama_new_context_with_model: n_ctx         = 512
0.00.020.185 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.185 I llama_new_context_with_model: n_batch       = 2048
0.00.020.185 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.185 I llama_new_context_with_model: flash_attn    = 0
0.00.020.186 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.186 I llama_new_context_with_model: freq_scale    = 1
0.00.020.187 I ggml_metal_init: allocating
0.00.020.190 I ggml_metal_init: found device: Apple M4
0.00.020.192 I ggml_metal_init: picking default device: Apple M4
0.00.020.839 I ggml_metal_init: using embedded metal library
0.00.023.395 I ggml_metal_init: GPU name:   Apple M4
0.00.023.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.398 I ggml_metal_init: simdgroup reduction   = true
0.00.023.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.398 I ggml_metal_init: has bfloat            = true
0.00.023.399 I ggml_metal_init: use bfloat            = true
0.00.023.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.621 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.034.144 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.146 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.154 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.800 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.801 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.801 I llama_new_context_with_model: graph nodes  = 429
0.00.034.801 I llama_new_context_with_model: graph splits = 2
0.00.034.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.816 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.229 I 
0.00.040.260 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.787 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.301 I llama_perf_context_print:        load time =      30.82 ms
0.00.045.302 I llama_perf_context_print: prompt eval time =       4.38 ms /     9 tokens (    0.49 ms per token,  2053.39 tokens per second)
0.00.045.303 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.303 I llama_perf_context_print:       total time =       5.07 ms /    10 tokens
0.00.045.464 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.147 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.644 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.821 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.828 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.829 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.830 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.831 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.832 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.833 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.833 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.834 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.834 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.838 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.838 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.839 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.825 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.826 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.827 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.827 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.827 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.828 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.828 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.828 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.829 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.829 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.830 I llama_model_loader: - type  f32:   40 tensors
0.00.049.830 I llama_model_loader: - type  f16:   30 tensors
0.00.067.721 W llm_load_vocab: empty token at index 5
0.00.072.227 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.484 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.516 I llm_load_vocab: special tokens cache size = 5
0.00.334.041 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.334.047 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.334.047 I llm_load_print_meta: arch             = jina-bert-v2
0.00.334.048 I llm_load_print_meta: vocab type       = BPE
0.00.334.048 I llm_load_print_meta: n_vocab          = 61056
0.00.334.048 I llm_load_print_meta: n_merges         = 39382
0.00.334.048 I llm_load_print_meta: vocab_only       = 0
0.00.334.051 I llm_load_print_meta: n_ctx_train      = 8192
0.00.334.051 I llm_load_print_meta: n_embd           = 384
0.00.334.051 I llm_load_print_meta: n_layer          = 4
0.00.334.058 I llm_load_print_meta: n_head           = 12
0.00.334.060 I llm_load_print_meta: n_head_kv        = 12
0.00.334.061 I llm_load_print_meta: n_rot            = 32
0.00.334.061 I llm_load_print_meta: n_swa            = 0
0.00.334.061 I llm_load_print_meta: n_embd_head_k    = 32
0.00.334.061 I llm_load_print_meta: n_embd_head_v    = 32
0.00.334.062 I llm_load_print_meta: n_gqa            = 1
0.00.334.064 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.334.065 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.334.066 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.334.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.334.067 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.334.067 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.334.067 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.334.068 I llm_load_print_meta: n_ff             = 1536
0.00.334.069 I llm_load_print_meta: n_expert         = 0
0.00.334.069 I llm_load_print_meta: n_expert_used    = 0
0.00.334.069 I llm_load_print_meta: causal attn      = 0
0.00.334.070 I llm_load_print_meta: pooling type     = -1
0.00.334.070 I llm_load_print_meta: rope type        = -1
0.00.334.070 I llm_load_print_meta: rope scaling     = linear
0.00.334.070 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.334.070 I llm_load_print_meta: freq_scale_train = 1
0.00.334.071 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.334.071 I llm_load_print_meta: rope_finetuned   = unknown
0.00.334.071 I llm_load_print_meta: ssm_d_conv       = 0
0.00.334.071 I llm_load_print_meta: ssm_d_inner      = 0
0.00.334.071 I llm_load_print_meta: ssm_d_state      = 0
0.00.334.071 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.334.071 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.334.072 I llm_load_print_meta: model type       = 33M
0.00.334.077 I llm_load_print_meta: model ftype      = F16
0.00.334.078 I llm_load_print_meta: model params     = 32.90 M
0.00.334.078 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.334.078 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.334.079 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.334.079 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.334.079 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.334.079 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.334.080 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.334.081 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.334.081 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.334.082 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.334.082 I llm_load_print_meta: max token length = 45
0.00.335.285 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.335.286 I llm_load_tensors: offloading output layer to GPU
0.00.335.286 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.335.309 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.310 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.336.224 I llama_new_context_with_model: n_seq_max     = 1
0.00.336.225 I llama_new_context_with_model: n_ctx         = 8192
0.00.336.225 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.336.225 I llama_new_context_with_model: n_batch       = 2048
0.00.336.225 I llama_new_context_with_model: n_ubatch      = 2048
0.00.336.226 I llama_new_context_with_model: flash_attn    = 0
0.00.336.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.336.226 I llama_new_context_with_model: freq_scale    = 1
0.00.336.227 I ggml_metal_init: allocating
0.00.336.230 I ggml_metal_init: found device: Apple M4
0.00.336.232 I ggml_metal_init: picking default device: Apple M4
0.00.337.214 I ggml_metal_init: using embedded metal library
0.00.339.795 I ggml_metal_init: GPU name:   Apple M4
0.00.339.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.797 I ggml_metal_init: simdgroup reduction   = true
0.00.339.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.798 I ggml_metal_init: has bfloat            = true
0.00.339.798 I ggml_metal_init: use bfloat            = true
0.00.339.798 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.799 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.393 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.351.897 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.351.899 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.903 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.352.519 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.352.520 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.352.520 I llama_new_context_with_model: graph nodes  = 154
0.00.352.520 I llama_new_context_with_model: graph splits = 2
0.00.352.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.168 I 
0.00.365.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.495 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.496 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.510 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.510 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.526 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.526 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.078 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.701 I llama_perf_context_print:        load time =     340.52 ms
0.00.369.702 I llama_perf_context_print: prompt eval time =       3.61 ms /    62 tokens (    0.06 ms per token, 17155.51 tokens per second)
0.00.369.703 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.703 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.369.907 I ggml_metal_free: deallocating

real	0m1.100s
user	0m0.340s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.222 I main: llama backend init
0.00.000.228 I main: load the model and apply lora adapter, if any
0.00.032.655 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.287 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.323 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.323 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.324 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.325 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.333 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.334 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.586 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.592 I llama_model_loader: - type  f32:  194 tensors
0.00.063.593 I llama_model_loader: - type  f16:   98 tensors
0.00.097.024 I llm_load_vocab: special tokens cache size = 25
0.00.104.230 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.104.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.104.233 I llm_load_print_meta: arch             = gptneox
0.00.104.233 I llm_load_print_meta: vocab type       = BPE
0.00.104.233 I llm_load_print_meta: n_vocab          = 50304
0.00.104.233 I llm_load_print_meta: n_merges         = 50009
0.00.104.234 I llm_load_print_meta: vocab_only       = 0
0.00.104.234 I llm_load_print_meta: n_ctx_train      = 2048
0.00.104.234 I llm_load_print_meta: n_embd           = 2048
0.00.104.234 I llm_load_print_meta: n_layer          = 24
0.00.104.237 I llm_load_print_meta: n_head           = 16
0.00.104.240 I llm_load_print_meta: n_head_kv        = 16
0.00.104.240 I llm_load_print_meta: n_rot            = 32
0.00.104.240 I llm_load_print_meta: n_swa            = 0
0.00.104.241 I llm_load_print_meta: n_embd_head_k    = 128
0.00.104.241 I llm_load_print_meta: n_embd_head_v    = 128
0.00.104.241 I llm_load_print_meta: n_gqa            = 1
0.00.104.242 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.104.243 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.104.243 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.104.244 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.104.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.104.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.104.244 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.104.245 I llm_load_print_meta: n_ff             = 8192
0.00.104.245 I llm_load_print_meta: n_expert         = 0
0.00.104.245 I llm_load_print_meta: n_expert_used    = 0
0.00.104.245 I llm_load_print_meta: causal attn      = 1
0.00.104.245 I llm_load_print_meta: pooling type     = 0
0.00.104.246 I llm_load_print_meta: rope type        = 2
0.00.104.246 I llm_load_print_meta: rope scaling     = linear
0.00.104.247 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.104.247 I llm_load_print_meta: freq_scale_train = 1
0.00.104.247 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.104.248 I llm_load_print_meta: rope_finetuned   = unknown
0.00.104.248 I llm_load_print_meta: ssm_d_conv       = 0
0.00.104.250 I llm_load_print_meta: ssm_d_inner      = 0
0.00.104.250 I llm_load_print_meta: ssm_d_state      = 0
0.00.104.250 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.104.250 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.104.250 I llm_load_print_meta: model type       = 1.4B
0.00.104.251 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.104.256 I llm_load_print_meta: model params     = 1.41 B
0.00.104.257 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.104.257 I llm_load_print_meta: general.name     = 1.4B
0.00.104.257 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.104.257 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.104.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.104.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.104.258 I llm_load_print_meta: LF token         = 128 ''
0.00.104.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.104.258 I llm_load_print_meta: max token length = 1024
0.00.106.923 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.106.923 I llm_load_tensors: offloading output layer to GPU
0.00.106.923 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.106.941 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.106.943 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.107.913 I llama_new_context_with_model: n_seq_max     = 1
0.00.107.914 I llama_new_context_with_model: n_ctx         = 2048
0.00.107.914 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.107.914 I llama_new_context_with_model: n_batch       = 2048
0.00.107.915 I llama_new_context_with_model: n_ubatch      = 512
0.00.107.915 I llama_new_context_with_model: flash_attn    = 0
0.00.107.915 I llama_new_context_with_model: freq_base     = 10000.0
0.00.107.916 I llama_new_context_with_model: freq_scale    = 1
0.00.107.916 I ggml_metal_init: allocating
0.00.107.925 I ggml_metal_init: found device: Apple M4
0.00.107.929 I ggml_metal_init: picking default device: Apple M4
0.00.108.626 I ggml_metal_init: using embedded metal library
0.00.118.159 I ggml_metal_init: GPU name:   Apple M4
0.00.118.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.118.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.118.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.118.162 I ggml_metal_init: simdgroup reduction   = true
0.00.118.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.118.162 I ggml_metal_init: has bfloat            = true
0.00.118.162 I ggml_metal_init: use bfloat            = true
0.00.118.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.118.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.142.269 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.164.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.164.259 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.164.280 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.165.298 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.165.299 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.165.300 I llama_new_context_with_model: graph nodes  = 967
0.00.165.300 I llama_new_context_with_model: graph splits = 2
0.00.165.325 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.165.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.165.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.247.753 I main: llama threadpool init, n_threads = 4
0.00.247.792 I 
0.00.247.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.247.832 I 
0.00.247.908 I sampler seed: 1234
0.00.247.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.247.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.247.938 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.247.939 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.218.382 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.02.218.383 I llama_perf_context_print:        load time =     215.09 ms
0.02.218.384 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.02.218.386 I llama_perf_context_print:        eval time =    1913.07 ms /    63 runs   (   30.37 ms per token,    32.93 tokens per second)
0.02.218.386 I llama_perf_context_print:       total time =    1970.63 ms /    70 tokens
0.02.218.590 I ggml_metal_free: deallocating

real	0m2.535s
user	0m0.147s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.640 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.031.866 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.527 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.536 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.537 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.538 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.546 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.547 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.550 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.653 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.544 I llama_model_loader: - type  f32:  194 tensors
0.00.062.544 I llama_model_loader: - type  f16:   98 tensors
0.00.091.409 I llm_load_vocab: special tokens cache size = 25
0.00.098.229 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.232 I llm_load_print_meta: arch             = gptneox
0.00.098.233 I llm_load_print_meta: vocab type       = BPE
0.00.098.233 I llm_load_print_meta: n_vocab          = 50304
0.00.098.233 I llm_load_print_meta: n_merges         = 50009
0.00.098.233 I llm_load_print_meta: vocab_only       = 0
0.00.098.233 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.233 I llm_load_print_meta: n_embd           = 2048
0.00.098.234 I llm_load_print_meta: n_layer          = 24
0.00.098.237 I llm_load_print_meta: n_head           = 16
0.00.098.238 I llm_load_print_meta: n_head_kv        = 16
0.00.098.238 I llm_load_print_meta: n_rot            = 32
0.00.098.238 I llm_load_print_meta: n_swa            = 0
0.00.098.238 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.238 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.239 I llm_load_print_meta: n_gqa            = 1
0.00.098.239 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.240 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.241 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.241 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.241 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.241 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.241 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.242 I llm_load_print_meta: n_ff             = 8192
0.00.098.242 I llm_load_print_meta: n_expert         = 0
0.00.098.242 I llm_load_print_meta: n_expert_used    = 0
0.00.098.242 I llm_load_print_meta: causal attn      = 1
0.00.098.242 I llm_load_print_meta: pooling type     = 0
0.00.098.243 I llm_load_print_meta: rope type        = 2
0.00.098.244 I llm_load_print_meta: rope scaling     = linear
0.00.098.244 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.244 I llm_load_print_meta: freq_scale_train = 1
0.00.098.245 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.245 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.245 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.245 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.247 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.247 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.247 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.247 I llm_load_print_meta: model type       = 1.4B
0.00.098.248 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.248 I llm_load_print_meta: model params     = 1.41 B
0.00.098.249 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.249 I llm_load_print_meta: general.name     = 1.4B
0.00.098.249 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.249 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.249 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.249 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.253 I llm_load_print_meta: LF token         = 128 ''
0.00.098.253 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.254 I llm_load_print_meta: max token length = 1024
0.00.100.858 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.858 I llm_load_tensors: offloading output layer to GPU
0.00.100.858 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.869 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.870 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.816 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.817 I llama_new_context_with_model: n_ctx         = 128
0.00.101.817 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.101.817 I llama_new_context_with_model: n_batch       = 128
0.00.101.817 I llama_new_context_with_model: n_ubatch      = 128
0.00.101.817 I llama_new_context_with_model: flash_attn    = 0
0.00.101.818 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.818 I llama_new_context_with_model: freq_scale    = 1
0.00.101.818 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.101.819 I ggml_metal_init: allocating
0.00.101.821 I ggml_metal_init: found device: Apple M4
0.00.101.824 I ggml_metal_init: picking default device: Apple M4
0.00.102.444 I ggml_metal_init: using embedded metal library
0.00.104.961 I ggml_metal_init: GPU name:   Apple M4
0.00.104.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.104.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.104.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.104.964 I ggml_metal_init: simdgroup reduction   = true
0.00.104.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.104.964 I ggml_metal_init: has bfloat            = true
0.00.104.965 I ggml_metal_init: use bfloat            = true
0.00.104.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.104.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.116.340 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.116.343 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.116.359 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.280 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.117.281 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.117.281 I llama_new_context_with_model: graph nodes  = 967
0.00.117.281 I llama_new_context_with_model: graph splits = 2
0.00.117.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.117.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.005.861 I 
0.01.005.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.005.961 I perplexity: tokenizing the input ..
0.01.019.440 I perplexity: tokenization took 13.475 ms
0.01.019.446 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.142.063 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.143.927 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.143.951 I llama_perf_context_print:        load time =     973.98 ms
0.01.143.953 I llama_perf_context_print: prompt eval time =     121.66 ms /   128 tokens (    0.95 ms per token,  1052.10 tokens per second)
0.01.143.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.143.955 I llama_perf_context_print:       total time =     138.10 ms /   129 tokens
0.01.144.813 I ggml_metal_free: deallocating

real	0m1.338s
user	0m0.128s
sys	0m0.207s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.227 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.111 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.112 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.112 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.112 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.182 I llama_model_loader: - type  f32:  194 tensors
0.00.034.182 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.256 I llm_load_vocab: special tokens cache size = 25
0.00.062.228 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.233 I llm_load_print_meta: arch             = gptneox
0.00.062.233 I llm_load_print_meta: vocab type       = BPE
0.00.062.234 I llm_load_print_meta: n_vocab          = 50304
0.00.062.236 I llm_load_print_meta: n_merges         = 50009
0.00.062.236 I llm_load_print_meta: vocab_only       = 0
0.00.062.236 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.237 I llm_load_print_meta: n_embd           = 2048
0.00.062.237 I llm_load_print_meta: n_layer          = 24
0.00.062.242 I llm_load_print_meta: n_head           = 16
0.00.062.243 I llm_load_print_meta: n_head_kv        = 16
0.00.062.246 I llm_load_print_meta: n_rot            = 32
0.00.062.246 I llm_load_print_meta: n_swa            = 0
0.00.062.246 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.246 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.247 I llm_load_print_meta: n_gqa            = 1
0.00.062.248 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.249 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.250 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.250 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.250 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.252 I llm_load_print_meta: n_ff             = 8192
0.00.062.252 I llm_load_print_meta: n_expert         = 0
0.00.062.252 I llm_load_print_meta: n_expert_used    = 0
0.00.062.252 I llm_load_print_meta: causal attn      = 1
0.00.062.252 I llm_load_print_meta: pooling type     = 0
0.00.062.253 I llm_load_print_meta: rope type        = 2
0.00.062.253 I llm_load_print_meta: rope scaling     = linear
0.00.062.253 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.254 I llm_load_print_meta: freq_scale_train = 1
0.00.062.254 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.254 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.254 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.254 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.254 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.254 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.255 I llm_load_print_meta: model type       = 1.4B
0.00.062.255 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.256 I llm_load_print_meta: model params     = 1.41 B
0.00.062.256 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.256 I llm_load_print_meta: general.name     = 1.4B
0.00.062.256 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.257 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.257 I llm_load_print_meta: LF token         = 128 ''
0.00.062.257 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.257 I llm_load_print_meta: max token length = 1024
0.00.064.234 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.234 I llm_load_tensors: offloading output layer to GPU
0.00.064.235 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.246 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.247 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.176 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.177 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.177 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.177 I llama_new_context_with_model: n_batch       = 2048
0.00.065.178 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.178 I llama_new_context_with_model: flash_attn    = 0
0.00.065.178 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.179 I llama_new_context_with_model: freq_scale    = 1
0.00.065.179 I ggml_metal_init: allocating
0.00.065.186 I ggml_metal_init: found device: Apple M4
0.00.065.188 I ggml_metal_init: picking default device: Apple M4
0.00.065.987 I ggml_metal_init: using embedded metal library
0.00.068.562 I ggml_metal_init: GPU name:   Apple M4
0.00.068.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.565 I ggml_metal_init: simdgroup reduction   = true
0.00.068.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.566 I ggml_metal_init: has bfloat            = true
0.00.068.566 I ggml_metal_init: use bfloat            = true
0.00.068.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.078 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.745 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.754 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.778 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.934 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.938 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.938 I llama_new_context_with_model: graph nodes  = 967
0.00.105.938 I llama_new_context_with_model: graph splits = 2
0.00.105.957 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.344.483 I main: llama threadpool init, n_threads = 4
0.01.344.524 I 
0.01.344.572 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.344.574 I 
0.01.344.880 I sampler seed: 1234
0.01.344.885 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.344.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.344.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.344.940 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.443.778 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.02.443.778 I llama_perf_context_print:        load time =    1334.25 ms
0.02.443.779 I llama_perf_context_print: prompt eval time =      50.18 ms /     7 tokens (    7.17 ms per token,   139.50 tokens per second)
0.02.443.780 I llama_perf_context_print:        eval time =    1045.63 ms /    63 runs   (   16.60 ms per token,    60.25 tokens per second)
0.02.443.780 I llama_perf_context_print:       total time =    1099.30 ms /    70 tokens
0.02.443.973 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.121s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.143 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.895 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.027 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.027 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.028 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.028 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.030 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.034 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.372 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.372 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.373 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.373 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.374 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.374 I llama_model_loader: - type  f32:  194 tensors
0.00.035.375 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.808 I llm_load_vocab: special tokens cache size = 25
0.00.067.936 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.939 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.939 I llm_load_print_meta: arch             = gptneox
0.00.067.939 I llm_load_print_meta: vocab type       = BPE
0.00.067.939 I llm_load_print_meta: n_vocab          = 50304
0.00.067.939 I llm_load_print_meta: n_merges         = 50009
0.00.067.940 I llm_load_print_meta: vocab_only       = 0
0.00.067.940 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.940 I llm_load_print_meta: n_embd           = 2048
0.00.067.940 I llm_load_print_meta: n_layer          = 24
0.00.067.943 I llm_load_print_meta: n_head           = 16
0.00.067.944 I llm_load_print_meta: n_head_kv        = 16
0.00.067.944 I llm_load_print_meta: n_rot            = 32
0.00.067.944 I llm_load_print_meta: n_swa            = 0
0.00.067.945 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.947 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.948 I llm_load_print_meta: n_gqa            = 1
0.00.067.949 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.951 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.951 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.952 I llm_load_print_meta: n_ff             = 8192
0.00.067.952 I llm_load_print_meta: n_expert         = 0
0.00.067.953 I llm_load_print_meta: n_expert_used    = 0
0.00.067.953 I llm_load_print_meta: causal attn      = 1
0.00.067.953 I llm_load_print_meta: pooling type     = 0
0.00.067.953 I llm_load_print_meta: rope type        = 2
0.00.067.954 I llm_load_print_meta: rope scaling     = linear
0.00.067.954 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.954 I llm_load_print_meta: freq_scale_train = 1
0.00.067.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.955 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.955 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.955 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.955 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.956 I llm_load_print_meta: model type       = 1.4B
0.00.067.956 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.956 I llm_load_print_meta: model params     = 1.41 B
0.00.067.957 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.957 I llm_load_print_meta: general.name     = 1.4B
0.00.067.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.958 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.958 I llm_load_print_meta: LF token         = 128 ''
0.00.067.958 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.958 I llm_load_print_meta: max token length = 1024
0.00.069.620 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.620 I llm_load_tensors: offloading output layer to GPU
0.00.069.621 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.630 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.632 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.481 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.482 I llama_new_context_with_model: n_ctx         = 128
0.00.070.482 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.482 I llama_new_context_with_model: n_batch       = 128
0.00.070.482 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.482 I llama_new_context_with_model: flash_attn    = 0
0.00.070.483 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.483 I llama_new_context_with_model: freq_scale    = 1
0.00.070.483 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.484 I ggml_metal_init: allocating
0.00.070.486 I ggml_metal_init: found device: Apple M4
0.00.070.488 I ggml_metal_init: picking default device: Apple M4
0.00.071.070 I ggml_metal_init: using embedded metal library
0.00.073.556 I ggml_metal_init: GPU name:   Apple M4
0.00.073.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.559 I ggml_metal_init: simdgroup reduction   = true
0.00.073.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.559 I ggml_metal_init: has bfloat            = true
0.00.073.559 I ggml_metal_init: use bfloat            = true
0.00.073.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.698 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.024 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.027 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.041 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.958 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.959 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.959 I llama_new_context_with_model: graph nodes  = 967
0.00.085.960 I llama_new_context_with_model: graph splits = 2
0.00.085.972 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.900.062 I 
0.00.900.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.900.130 I perplexity: tokenizing the input ..
0.00.908.415 I perplexity: tokenization took 8.284 ms
0.00.908.422 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.032.449 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.033.651 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.033.668 I llama_perf_context_print:        load time =     887.16 ms
0.01.033.669 I llama_perf_context_print: prompt eval time =     123.80 ms /   128 tokens (    0.97 ms per token,  1033.90 tokens per second)
0.01.033.669 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.033.670 I llama_perf_context_print:       total time =     133.61 ms /   129 tokens
0.01.034.067 I ggml_metal_free: deallocating

real	0m1.055s
user	0m0.097s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.017.916 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.044.043 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.058.443 I llama_model_loader: - type  f32:  194 tensors
0.00.058.443 I llama_model_loader: - type q4_0:   97 tensors
0.00.058.444 I llama_model_loader: - type q6_K:    1 tensors
0.00.100.027 I llm_load_vocab: special tokens cache size = 25
0.00.109.415 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.109.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.109.419 I llm_load_print_meta: arch             = gptneox
0.00.109.420 I llm_load_print_meta: vocab type       = BPE
0.00.109.420 I llm_load_print_meta: n_vocab          = 50304
0.00.109.420 I llm_load_print_meta: n_merges         = 50009
0.00.109.421 I llm_load_print_meta: vocab_only       = 0
0.00.109.421 I llm_load_print_meta: n_ctx_train      = 2048
0.00.109.421 I llm_load_print_meta: n_embd           = 2048
0.00.109.421 I llm_load_print_meta: n_layer          = 24
0.00.109.425 I llm_load_print_meta: n_head           = 16
0.00.109.426 I llm_load_print_meta: n_head_kv        = 16
0.00.109.426 I llm_load_print_meta: n_rot            = 32
0.00.109.427 I llm_load_print_meta: n_swa            = 0
0.00.109.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.109.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.109.428 I llm_load_print_meta: n_gqa            = 1
0.00.109.429 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.109.430 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.109.430 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.109.431 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.109.431 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.109.431 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.109.431 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.109.432 I llm_load_print_meta: n_ff             = 8192
0.00.109.432 I llm_load_print_meta: n_expert         = 0
0.00.109.433 I llm_load_print_meta: n_expert_used    = 0
0.00.109.433 I llm_load_print_meta: causal attn      = 1
0.00.109.433 I llm_load_print_meta: pooling type     = 0
0.00.109.433 I llm_load_print_meta: rope type        = 2
0.00.109.433 I llm_load_print_meta: rope scaling     = linear
0.00.109.436 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.109.436 I llm_load_print_meta: freq_scale_train = 1
0.00.109.436 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.109.437 I llm_load_print_meta: rope_finetuned   = unknown
0.00.109.437 I llm_load_print_meta: ssm_d_conv       = 0
0.00.109.437 I llm_load_print_meta: ssm_d_inner      = 0
0.00.109.437 I llm_load_print_meta: ssm_d_state      = 0
0.00.109.437 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.109.437 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.109.438 I llm_load_print_meta: model type       = 1.4B
0.00.109.438 I llm_load_print_meta: model ftype      = Q4_0
0.00.109.439 I llm_load_print_meta: model params     = 1.41 B
0.00.109.439 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.109.439 I llm_load_print_meta: general.name     = 1.4B
0.00.109.440 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.109.441 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.109.442 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.109.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.109.443 I llm_load_print_meta: LF token         = 128 ''
0.00.109.443 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.109.443 I llm_load_print_meta: max token length = 1024
0.00.112.065 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.112.065 I llm_load_tensors: offloading output layer to GPU
0.00.112.065 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.112.077 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.112.078 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.113.320 I llama_new_context_with_model: n_seq_max     = 1
0.00.113.321 I llama_new_context_with_model: n_ctx         = 2048
0.00.113.321 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.113.321 I llama_new_context_with_model: n_batch       = 2048
0.00.113.322 I llama_new_context_with_model: n_ubatch      = 512
0.00.113.322 I llama_new_context_with_model: flash_attn    = 0
0.00.113.322 I llama_new_context_with_model: freq_base     = 10000.0
0.00.113.323 I llama_new_context_with_model: freq_scale    = 1
0.00.113.324 I ggml_metal_init: allocating
0.00.113.331 I ggml_metal_init: found device: Apple M4
0.00.113.333 I ggml_metal_init: picking default device: Apple M4
0.00.114.136 I ggml_metal_init: using embedded metal library
0.00.117.484 I ggml_metal_init: GPU name:   Apple M4
0.00.117.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.487 I ggml_metal_init: simdgroup reduction   = true
0.00.117.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.487 I ggml_metal_init: has bfloat            = true
0.00.117.487 I ggml_metal_init: use bfloat            = true
0.00.117.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.128.147 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.151.584 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.151.593 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.151.622 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.152.692 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.152.693 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.152.694 I llama_new_context_with_model: graph nodes  = 967
0.00.152.694 I llama_new_context_with_model: graph splits = 2
0.00.152.711 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.152.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.152.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.864.186 I main: llama threadpool init, n_threads = 4
0.00.864.284 I 
0.00.864.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.864.381 I 
0.00.864.926 I sampler seed: 1234
0.00.864.939 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.865.006 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.865.014 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.865.014 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.551.991 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.551.992 I llama_perf_context_print:        load time =     846.25 ms
0.01.551.992 I llama_perf_context_print: prompt eval time =      50.65 ms /     7 tokens (    7.24 ms per token,   138.20 tokens per second)
0.01.551.993 I llama_perf_context_print:        eval time =     633.38 ms /    63 runs   (   10.05 ms per token,    99.47 tokens per second)
0.01.551.993 I llama_perf_context_print:       total time =     687.82 ms /    70 tokens
0.01.552.171 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.155s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.845 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.979 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.982 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.984 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.985 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.986 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.744 I llama_model_loader: - type  f32:  194 tensors
0.00.024.744 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.745 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.212 I llm_load_vocab: special tokens cache size = 25
0.00.051.222 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.225 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.225 I llm_load_print_meta: arch             = gptneox
0.00.051.225 I llm_load_print_meta: vocab type       = BPE
0.00.051.225 I llm_load_print_meta: n_vocab          = 50304
0.00.051.226 I llm_load_print_meta: n_merges         = 50009
0.00.051.226 I llm_load_print_meta: vocab_only       = 0
0.00.051.226 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.226 I llm_load_print_meta: n_embd           = 2048
0.00.051.226 I llm_load_print_meta: n_layer          = 24
0.00.051.229 I llm_load_print_meta: n_head           = 16
0.00.051.230 I llm_load_print_meta: n_head_kv        = 16
0.00.051.230 I llm_load_print_meta: n_rot            = 32
0.00.051.230 I llm_load_print_meta: n_swa            = 0
0.00.051.230 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.230 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.231 I llm_load_print_meta: n_gqa            = 1
0.00.051.232 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.233 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.233 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.233 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.234 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.235 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.235 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.236 I llm_load_print_meta: n_ff             = 8192
0.00.051.237 I llm_load_print_meta: n_expert         = 0
0.00.051.237 I llm_load_print_meta: n_expert_used    = 0
0.00.051.237 I llm_load_print_meta: causal attn      = 1
0.00.051.237 I llm_load_print_meta: pooling type     = 0
0.00.051.237 I llm_load_print_meta: rope type        = 2
0.00.051.237 I llm_load_print_meta: rope scaling     = linear
0.00.051.238 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.238 I llm_load_print_meta: freq_scale_train = 1
0.00.051.238 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.239 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.239 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.239 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.239 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.239 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.239 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.239 I llm_load_print_meta: model type       = 1.4B
0.00.051.240 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.240 I llm_load_print_meta: model params     = 1.41 B
0.00.051.241 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.241 I llm_load_print_meta: general.name     = 1.4B
0.00.051.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: LF token         = 128 ''
0.00.051.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.242 I llm_load_print_meta: max token length = 1024
0.00.053.163 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.163 I llm_load_tensors: offloading output layer to GPU
0.00.053.164 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.174 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.176 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.116 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.117 I llama_new_context_with_model: n_ctx         = 128
0.00.054.117 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.117 I llama_new_context_with_model: n_batch       = 128
0.00.054.118 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.118 I llama_new_context_with_model: flash_attn    = 0
0.00.054.118 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.118 I llama_new_context_with_model: freq_scale    = 1
0.00.054.119 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.119 I ggml_metal_init: allocating
0.00.054.122 I ggml_metal_init: found device: Apple M4
0.00.054.124 I ggml_metal_init: picking default device: Apple M4
0.00.054.698 I ggml_metal_init: using embedded metal library
0.00.057.023 I ggml_metal_init: GPU name:   Apple M4
0.00.057.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.025 I ggml_metal_init: simdgroup reduction   = true
0.00.057.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.026 I ggml_metal_init: has bfloat            = true
0.00.057.026 I ggml_metal_init: use bfloat            = true
0.00.057.026 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.933 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.253 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.255 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.269 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.186 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.187 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.188 I llama_new_context_with_model: graph nodes  = 967
0.00.069.188 I llama_new_context_with_model: graph splits = 2
0.00.069.200 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.846 I 
0.00.554.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.899 I perplexity: tokenizing the input ..
0.00.562.888 I perplexity: tokenization took 7.986 ms
0.00.562.892 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.685.741 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.687.086 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.687.097 I llama_perf_context_print:        load time =     544.99 ms
0.00.687.098 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.96 tokens per second)
0.00.687.099 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.687.099 I llama_perf_context_print:       total time =     132.26 ms /   129 tokens
0.00.687.414 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.077s
sys	0m0.096s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.016.341 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.497 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.057 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.387 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.387 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.388 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.388 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.389 I llama_model_loader: - type  f32:  194 tensors
0.00.039.389 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.389 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.338 I llm_load_vocab: special tokens cache size = 25
0.00.073.512 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.515 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.515 I llm_load_print_meta: arch             = gptneox
0.00.073.516 I llm_load_print_meta: vocab type       = BPE
0.00.073.516 I llm_load_print_meta: n_vocab          = 50304
0.00.073.516 I llm_load_print_meta: n_merges         = 50009
0.00.073.516 I llm_load_print_meta: vocab_only       = 0
0.00.073.516 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.516 I llm_load_print_meta: n_embd           = 2048
0.00.073.517 I llm_load_print_meta: n_layer          = 24
0.00.073.520 I llm_load_print_meta: n_head           = 16
0.00.073.521 I llm_load_print_meta: n_head_kv        = 16
0.00.073.521 I llm_load_print_meta: n_rot            = 32
0.00.073.521 I llm_load_print_meta: n_swa            = 0
0.00.073.522 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.522 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.523 I llm_load_print_meta: n_gqa            = 1
0.00.073.524 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.524 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.526 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.527 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.527 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.527 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.527 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.529 I llm_load_print_meta: n_ff             = 8192
0.00.073.530 I llm_load_print_meta: n_expert         = 0
0.00.073.530 I llm_load_print_meta: n_expert_used    = 0
0.00.073.530 I llm_load_print_meta: causal attn      = 1
0.00.073.530 I llm_load_print_meta: pooling type     = 0
0.00.073.530 I llm_load_print_meta: rope type        = 2
0.00.073.530 I llm_load_print_meta: rope scaling     = linear
0.00.073.531 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.531 I llm_load_print_meta: freq_scale_train = 1
0.00.073.531 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.531 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.532 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.532 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.532 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.532 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.532 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.532 I llm_load_print_meta: model type       = 1.4B
0.00.073.533 I llm_load_print_meta: model ftype      = Q4_1
0.00.073.533 I llm_load_print_meta: model params     = 1.41 B
0.00.073.537 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.073.537 I llm_load_print_meta: general.name     = 1.4B
0.00.073.537 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.537 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.538 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.538 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.538 I llm_load_print_meta: LF token         = 128 ''
0.00.073.538 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.539 I llm_load_print_meta: max token length = 1024
0.00.075.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.843 I llm_load_tensors: offloading output layer to GPU
0.00.075.844 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.854 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.075.856 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.077.005 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.006 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.007 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.007 I llama_new_context_with_model: n_batch       = 2048
0.00.077.007 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.007 I llama_new_context_with_model: flash_attn    = 0
0.00.077.008 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.008 I llama_new_context_with_model: freq_scale    = 1
0.00.077.009 I ggml_metal_init: allocating
0.00.077.016 I ggml_metal_init: found device: Apple M4
0.00.077.018 I ggml_metal_init: picking default device: Apple M4
0.00.077.754 I ggml_metal_init: using embedded metal library
0.00.080.755 I ggml_metal_init: GPU name:   Apple M4
0.00.080.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.759 I ggml_metal_init: simdgroup reduction   = true
0.00.080.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.759 I ggml_metal_init: has bfloat            = true
0.00.080.759 I ggml_metal_init: use bfloat            = true
0.00.080.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.385 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.114.457 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.462 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.480 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.503 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.505 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.505 I llama_new_context_with_model: graph nodes  = 967
0.00.115.505 I llama_new_context_with_model: graph splits = 2
0.00.115.521 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.115.662 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.506 I main: llama threadpool init, n_threads = 4
0.00.812.543 I 
0.00.812.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.576 I 
0.00.812.788 I sampler seed: 1234
0.00.812.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.831 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.831 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.552.908 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62445.03 tokens per second)
0.01.552.909 I llama_perf_context_print:        load time =     796.16 ms
0.01.552.910 I llama_perf_context_print: prompt eval time =      46.13 ms /     7 tokens (    6.59 ms per token,   151.74 tokens per second)
0.01.552.911 I llama_perf_context_print:        eval time =     691.10 ms /    63 runs   (   10.97 ms per token,    91.16 tokens per second)
0.01.552.911 I llama_perf_context_print:       total time =     740.40 ms /    70 tokens
0.01.553.107 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.124s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.162 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.137 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.042 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.042 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.043 I llama_model_loader: - type  f32:  194 tensors
0.00.024.043 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.043 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.525 I llm_load_vocab: special tokens cache size = 25
0.00.050.531 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.536 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.536 I llm_load_print_meta: arch             = gptneox
0.00.050.536 I llm_load_print_meta: vocab type       = BPE
0.00.050.536 I llm_load_print_meta: n_vocab          = 50304
0.00.050.537 I llm_load_print_meta: n_merges         = 50009
0.00.050.538 I llm_load_print_meta: vocab_only       = 0
0.00.050.539 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.540 I llm_load_print_meta: n_embd           = 2048
0.00.050.540 I llm_load_print_meta: n_layer          = 24
0.00.050.543 I llm_load_print_meta: n_head           = 16
0.00.050.543 I llm_load_print_meta: n_head_kv        = 16
0.00.050.543 I llm_load_print_meta: n_rot            = 32
0.00.050.544 I llm_load_print_meta: n_swa            = 0
0.00.050.544 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.544 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.545 I llm_load_print_meta: n_gqa            = 1
0.00.050.546 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.548 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.548 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.549 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.549 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.549 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.550 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.550 I llm_load_print_meta: n_ff             = 8192
0.00.050.552 I llm_load_print_meta: n_expert         = 0
0.00.050.552 I llm_load_print_meta: n_expert_used    = 0
0.00.050.552 I llm_load_print_meta: causal attn      = 1
0.00.050.552 I llm_load_print_meta: pooling type     = 0
0.00.050.552 I llm_load_print_meta: rope type        = 2
0.00.050.553 I llm_load_print_meta: rope scaling     = linear
0.00.050.553 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.553 I llm_load_print_meta: freq_scale_train = 1
0.00.050.554 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.554 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.554 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.554 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.554 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.554 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.554 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.555 I llm_load_print_meta: model type       = 1.4B
0.00.050.555 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.556 I llm_load_print_meta: model params     = 1.41 B
0.00.050.556 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.556 I llm_load_print_meta: general.name     = 1.4B
0.00.050.556 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.557 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.557 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.557 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.558 I llm_load_print_meta: LF token         = 128 ''
0.00.050.558 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.558 I llm_load_print_meta: max token length = 1024
0.00.052.277 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.277 I llm_load_tensors: offloading output layer to GPU
0.00.052.277 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.283 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.284 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.190 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.190 I llama_new_context_with_model: n_ctx         = 128
0.00.053.190 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.191 I llama_new_context_with_model: n_batch       = 128
0.00.053.191 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.191 I llama_new_context_with_model: flash_attn    = 0
0.00.053.191 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.192 I llama_new_context_with_model: freq_scale    = 1
0.00.053.192 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.192 I ggml_metal_init: allocating
0.00.053.197 I ggml_metal_init: found device: Apple M4
0.00.053.199 I ggml_metal_init: picking default device: Apple M4
0.00.053.756 I ggml_metal_init: using embedded metal library
0.00.056.068 I ggml_metal_init: GPU name:   Apple M4
0.00.056.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.070 I ggml_metal_init: simdgroup reduction   = true
0.00.056.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.070 I ggml_metal_init: has bfloat            = true
0.00.056.071 I ggml_metal_init: use bfloat            = true
0.00.056.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.829 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.111 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.114 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.129 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.982 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.983 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.983 I llama_new_context_with_model: graph nodes  = 967
0.00.067.984 I llama_new_context_with_model: graph splits = 2
0.00.067.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.526 I 
0.00.630.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.572 I perplexity: tokenizing the input ..
0.00.638.775 I perplexity: tokenization took 8.2 ms
0.00.638.778 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.763 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.762.936 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.762.962 I llama_perf_context_print:        load time =     621.36 ms
0.00.762.963 I llama_perf_context_print: prompt eval time =     122.76 ms /   128 tokens (    0.96 ms per token,  1042.71 tokens per second)
0.00.762.964 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.965 I llama_perf_context_print:       total time =     132.44 ms /   129 tokens
0.00.763.388 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.079s
sys	0m0.097s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.684 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.041 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.045 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.048 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.051 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.057 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.830 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.833 I llama_model_loader: - type  f32:  194 tensors
0.00.025.833 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.833 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.433 I llm_load_vocab: special tokens cache size = 25
0.00.052.522 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.525 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.526 I llm_load_print_meta: arch             = gptneox
0.00.052.526 I llm_load_print_meta: vocab type       = BPE
0.00.052.526 I llm_load_print_meta: n_vocab          = 50304
0.00.052.526 I llm_load_print_meta: n_merges         = 50009
0.00.052.527 I llm_load_print_meta: vocab_only       = 0
0.00.052.527 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.527 I llm_load_print_meta: n_embd           = 2048
0.00.052.527 I llm_load_print_meta: n_layer          = 24
0.00.052.530 I llm_load_print_meta: n_head           = 16
0.00.052.530 I llm_load_print_meta: n_head_kv        = 16
0.00.052.531 I llm_load_print_meta: n_rot            = 32
0.00.052.532 I llm_load_print_meta: n_swa            = 0
0.00.052.532 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.532 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.533 I llm_load_print_meta: n_gqa            = 1
0.00.052.534 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.534 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.535 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.535 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.535 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.536 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.537 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.538 I llm_load_print_meta: n_ff             = 8192
0.00.052.538 I llm_load_print_meta: n_expert         = 0
0.00.052.538 I llm_load_print_meta: n_expert_used    = 0
0.00.052.540 I llm_load_print_meta: causal attn      = 1
0.00.052.541 I llm_load_print_meta: pooling type     = 0
0.00.052.541 I llm_load_print_meta: rope type        = 2
0.00.052.541 I llm_load_print_meta: rope scaling     = linear
0.00.052.542 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.542 I llm_load_print_meta: freq_scale_train = 1
0.00.052.542 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.542 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.542 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.543 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.543 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.543 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.543 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.543 I llm_load_print_meta: model type       = 1.4B
0.00.052.544 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.544 I llm_load_print_meta: model params     = 1.41 B
0.00.052.545 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.549 I llm_load_print_meta: general.name     = 1.4B
0.00.052.549 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.550 I llm_load_print_meta: LF token         = 128 ''
0.00.052.550 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.550 I llm_load_print_meta: max token length = 1024
0.00.054.529 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.529 I llm_load_tensors: offloading output layer to GPU
0.00.054.529 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.540 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.541 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.453 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.454 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.454 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.454 I llama_new_context_with_model: n_batch       = 2048
0.00.055.454 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.454 I llama_new_context_with_model: flash_attn    = 0
0.00.055.455 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.455 I llama_new_context_with_model: freq_scale    = 1
0.00.055.455 I ggml_metal_init: allocating
0.00.055.459 I ggml_metal_init: found device: Apple M4
0.00.055.461 I ggml_metal_init: picking default device: Apple M4
0.00.056.072 I ggml_metal_init: using embedded metal library
0.00.058.439 I ggml_metal_init: GPU name:   Apple M4
0.00.058.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.442 I ggml_metal_init: simdgroup reduction   = true
0.00.058.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.442 I ggml_metal_init: has bfloat            = true
0.00.058.442 I ggml_metal_init: use bfloat            = true
0.00.058.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.284 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.666 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.674 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.694 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.747 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.748 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.749 I llama_new_context_with_model: graph nodes  = 967
0.00.088.749 I llama_new_context_with_model: graph splits = 2
0.00.088.764 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.973.313 I main: llama threadpool init, n_threads = 4
0.00.973.394 I 
0.00.973.466 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.973.468 I 
0.00.973.997 I sampler seed: 1234
0.00.974.007 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.974.043 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.974.046 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.974.046 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.775.777 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.775.778 I llama_perf_context_print:        load time =     964.62 ms
0.01.775.778 I llama_perf_context_print: prompt eval time =      54.87 ms /     7 tokens (    7.84 ms per token,   127.58 tokens per second)
0.01.775.779 I llama_perf_context_print:        eval time =     743.83 ms /    63 runs   (   11.81 ms per token,    84.70 tokens per second)
0.01.775.781 I llama_perf_context_print:       total time =     802.47 ms /    70 tokens
0.01.776.005 I ggml_metal_free: deallocating

real	0m1.792s
user	0m0.120s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.917 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.949 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.940 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.941 I llama_model_loader: - type  f32:  194 tensors
0.00.024.941 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.941 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.132 I llm_load_vocab: special tokens cache size = 25
0.00.052.116 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.118 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.119 I llm_load_print_meta: arch             = gptneox
0.00.052.119 I llm_load_print_meta: vocab type       = BPE
0.00.052.119 I llm_load_print_meta: n_vocab          = 50304
0.00.052.120 I llm_load_print_meta: n_merges         = 50009
0.00.052.120 I llm_load_print_meta: vocab_only       = 0
0.00.052.120 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.120 I llm_load_print_meta: n_embd           = 2048
0.00.052.120 I llm_load_print_meta: n_layer          = 24
0.00.052.123 I llm_load_print_meta: n_head           = 16
0.00.052.124 I llm_load_print_meta: n_head_kv        = 16
0.00.052.124 I llm_load_print_meta: n_rot            = 32
0.00.052.124 I llm_load_print_meta: n_swa            = 0
0.00.052.124 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.125 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.125 I llm_load_print_meta: n_gqa            = 1
0.00.052.126 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.127 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.128 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.128 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.128 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.129 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.129 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.129 I llm_load_print_meta: n_ff             = 8192
0.00.052.130 I llm_load_print_meta: n_expert         = 0
0.00.052.130 I llm_load_print_meta: n_expert_used    = 0
0.00.052.130 I llm_load_print_meta: causal attn      = 1
0.00.052.130 I llm_load_print_meta: pooling type     = 0
0.00.052.130 I llm_load_print_meta: rope type        = 2
0.00.052.131 I llm_load_print_meta: rope scaling     = linear
0.00.052.131 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.133 I llm_load_print_meta: freq_scale_train = 1
0.00.052.133 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.133 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.134 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.134 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.134 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.134 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.134 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.134 I llm_load_print_meta: model type       = 1.4B
0.00.052.135 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.135 I llm_load_print_meta: model params     = 1.41 B
0.00.052.137 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.137 I llm_load_print_meta: general.name     = 1.4B
0.00.052.137 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.137 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.138 I llm_load_print_meta: LF token         = 128 ''
0.00.052.138 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.139 I llm_load_print_meta: max token length = 1024
0.00.053.719 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.719 I llm_load_tensors: offloading output layer to GPU
0.00.053.720 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.730 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.731 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.587 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.588 I llama_new_context_with_model: n_ctx         = 128
0.00.054.589 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.589 I llama_new_context_with_model: n_batch       = 128
0.00.054.589 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.589 I llama_new_context_with_model: flash_attn    = 0
0.00.054.589 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.590 I llama_new_context_with_model: freq_scale    = 1
0.00.054.590 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.591 I ggml_metal_init: allocating
0.00.054.594 I ggml_metal_init: found device: Apple M4
0.00.054.596 I ggml_metal_init: picking default device: Apple M4
0.00.055.165 I ggml_metal_init: using embedded metal library
0.00.057.614 I ggml_metal_init: GPU name:   Apple M4
0.00.057.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.617 I ggml_metal_init: simdgroup reduction   = true
0.00.057.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.617 I ggml_metal_init: has bfloat            = true
0.00.057.617 I ggml_metal_init: use bfloat            = true
0.00.057.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.688 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.935 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.938 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.954 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.822 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.823 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.824 I llama_new_context_with_model: graph nodes  = 967
0.00.069.824 I llama_new_context_with_model: graph splits = 2
0.00.069.836 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.956 I 
0.00.696.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.001 I perplexity: tokenizing the input ..
0.00.705.212 I perplexity: tokenization took 8.209 ms
0.00.705.215 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.319 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.841.490 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.841.510 I llama_perf_context_print:        load time =     687.03 ms
0.00.841.511 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   949.01 tokens per second)
0.00.841.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.513 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.842.015 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.080s
sys	0m0.119s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.016.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.031.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.681 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.682 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.329 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.620 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.621 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.042.621 I llama_model_loader: - type  f32:  194 tensors
0.00.042.622 I llama_model_loader: - type q5_1:   97 tensors
0.00.042.622 I llama_model_loader: - type q6_K:    1 tensors
0.00.072.949 I llm_load_vocab: special tokens cache size = 25
0.00.084.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.558 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.558 I llm_load_print_meta: arch             = gptneox
0.00.084.559 I llm_load_print_meta: vocab type       = BPE
0.00.084.559 I llm_load_print_meta: n_vocab          = 50304
0.00.084.559 I llm_load_print_meta: n_merges         = 50009
0.00.084.560 I llm_load_print_meta: vocab_only       = 0
0.00.084.560 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.560 I llm_load_print_meta: n_embd           = 2048
0.00.084.560 I llm_load_print_meta: n_layer          = 24
0.00.084.563 I llm_load_print_meta: n_head           = 16
0.00.084.565 I llm_load_print_meta: n_head_kv        = 16
0.00.084.565 I llm_load_print_meta: n_rot            = 32
0.00.084.565 I llm_load_print_meta: n_swa            = 0
0.00.084.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.566 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.567 I llm_load_print_meta: n_gqa            = 1
0.00.084.568 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.569 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.569 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.570 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.570 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.570 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.570 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.571 I llm_load_print_meta: n_ff             = 8192
0.00.084.572 I llm_load_print_meta: n_expert         = 0
0.00.084.572 I llm_load_print_meta: n_expert_used    = 0
0.00.084.574 I llm_load_print_meta: causal attn      = 1
0.00.084.577 I llm_load_print_meta: pooling type     = 0
0.00.084.577 I llm_load_print_meta: rope type        = 2
0.00.084.577 I llm_load_print_meta: rope scaling     = linear
0.00.084.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.578 I llm_load_print_meta: freq_scale_train = 1
0.00.084.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.579 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.579 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.580 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.580 I llm_load_print_meta: model type       = 1.4B
0.00.084.581 I llm_load_print_meta: model ftype      = Q5_1
0.00.084.581 I llm_load_print_meta: model params     = 1.41 B
0.00.084.582 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.084.582 I llm_load_print_meta: general.name     = 1.4B
0.00.084.582 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.583 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.583 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.588 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.589 I llm_load_print_meta: LF token         = 128 ''
0.00.084.589 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.589 I llm_load_print_meta: max token length = 1024
0.00.087.386 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.386 I llm_load_tensors: offloading output layer to GPU
0.00.087.386 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.398 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.087.399 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.088.900 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.902 I llama_new_context_with_model: n_ctx         = 2048
0.00.088.902 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.088.902 I llama_new_context_with_model: n_batch       = 2048
0.00.088.903 I llama_new_context_with_model: n_ubatch      = 512
0.00.088.903 I llama_new_context_with_model: flash_attn    = 0
0.00.088.904 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.904 I llama_new_context_with_model: freq_scale    = 1
0.00.088.904 I ggml_metal_init: allocating
0.00.088.909 I ggml_metal_init: found device: Apple M4
0.00.088.912 I ggml_metal_init: picking default device: Apple M4
0.00.089.793 I ggml_metal_init: using embedded metal library
0.00.093.437 I ggml_metal_init: GPU name:   Apple M4
0.00.093.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.441 I ggml_metal_init: simdgroup reduction   = true
0.00.093.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.443 I ggml_metal_init: has bfloat            = true
0.00.093.443 I ggml_metal_init: use bfloat            = true
0.00.093.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.213 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.126.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.716 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.734 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.764 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.765 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.765 I llama_new_context_with_model: graph nodes  = 967
0.00.127.765 I llama_new_context_with_model: graph splits = 2
0.00.127.780 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.909 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.910 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.939.396 I main: llama threadpool init, n_threads = 4
0.00.939.475 I 
0.00.939.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.939.558 I 
0.00.940.113 I sampler seed: 1234
0.00.940.122 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.940.194 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.940.218 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.940.218 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.783.272 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.01.783.272 I llama_perf_context_print:        load time =     922.67 ms
0.01.783.274 I llama_perf_context_print: prompt eval time =      42.88 ms /     7 tokens (    6.12 ms per token,   163.27 tokens per second)
0.01.783.274 I llama_perf_context_print:        eval time =     797.00 ms /    63 runs   (   12.65 ms per token,    79.05 tokens per second)
0.01.783.275 I llama_perf_context_print:       total time =     843.88 ms /    70 tokens
0.01.783.481 I ggml_metal_free: deallocating

real	0m1.813s
user	0m0.140s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.910 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.830 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.831 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.833 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.835 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.663 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.664 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.665 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.665 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.666 I llama_model_loader: - type  f32:  194 tensors
0.00.023.667 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.667 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.167 I llm_load_vocab: special tokens cache size = 25
0.00.050.082 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.084 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.085 I llm_load_print_meta: arch             = gptneox
0.00.050.085 I llm_load_print_meta: vocab type       = BPE
0.00.050.085 I llm_load_print_meta: n_vocab          = 50304
0.00.050.085 I llm_load_print_meta: n_merges         = 50009
0.00.050.085 I llm_load_print_meta: vocab_only       = 0
0.00.050.086 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.086 I llm_load_print_meta: n_embd           = 2048
0.00.050.086 I llm_load_print_meta: n_layer          = 24
0.00.050.089 I llm_load_print_meta: n_head           = 16
0.00.050.090 I llm_load_print_meta: n_head_kv        = 16
0.00.050.090 I llm_load_print_meta: n_rot            = 32
0.00.050.090 I llm_load_print_meta: n_swa            = 0
0.00.050.090 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.090 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.092 I llm_load_print_meta: n_gqa            = 1
0.00.050.093 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.093 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.094 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.094 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.094 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.095 I llm_load_print_meta: n_ff             = 8192
0.00.050.096 I llm_load_print_meta: n_expert         = 0
0.00.050.098 I llm_load_print_meta: n_expert_used    = 0
0.00.050.098 I llm_load_print_meta: causal attn      = 1
0.00.050.098 I llm_load_print_meta: pooling type     = 0
0.00.050.098 I llm_load_print_meta: rope type        = 2
0.00.050.099 I llm_load_print_meta: rope scaling     = linear
0.00.050.099 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.099 I llm_load_print_meta: freq_scale_train = 1
0.00.050.099 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.100 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.100 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.100 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.100 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.100 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.100 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.101 I llm_load_print_meta: model type       = 1.4B
0.00.050.101 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.101 I llm_load_print_meta: model params     = 1.41 B
0.00.050.107 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.107 I llm_load_print_meta: general.name     = 1.4B
0.00.050.108 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.108 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.108 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.108 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: LF token         = 128 ''
0.00.050.109 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.109 I llm_load_print_meta: max token length = 1024
0.00.052.054 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.054 I llm_load_tensors: offloading output layer to GPU
0.00.052.054 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.065 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.066 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.980 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.981 I llama_new_context_with_model: n_ctx         = 128
0.00.052.981 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.981 I llama_new_context_with_model: n_batch       = 128
0.00.052.981 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.982 I llama_new_context_with_model: flash_attn    = 0
0.00.052.982 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.982 I llama_new_context_with_model: freq_scale    = 1
0.00.052.983 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.983 I ggml_metal_init: allocating
0.00.052.986 I ggml_metal_init: found device: Apple M4
0.00.052.988 I ggml_metal_init: picking default device: Apple M4
0.00.053.545 I ggml_metal_init: using embedded metal library
0.00.055.914 I ggml_metal_init: GPU name:   Apple M4
0.00.055.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.915 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.916 I ggml_metal_init: simdgroup reduction   = true
0.00.055.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.916 I ggml_metal_init: has bfloat            = true
0.00.055.916 I ggml_metal_init: use bfloat            = true
0.00.055.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.493 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.787 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.789 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.803 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.709 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.710 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.711 I llama_new_context_with_model: graph nodes  = 967
0.00.067.711 I llama_new_context_with_model: graph splits = 2
0.00.067.723 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.453 I 
0.00.794.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.527 I perplexity: tokenizing the input ..
0.00.802.729 I perplexity: tokenization took 8.199 ms
0.00.802.736 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.937.642 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.938.791 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.938.801 I llama_perf_context_print:        load time =     785.54 ms
0.00.938.802 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.44 tokens per second)
0.00.938.803 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.938.804 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.939.129 I ggml_metal_free: deallocating

real	0m0.952s
user	0m0.078s
sys	0m0.128s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.016.367 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.114 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.022.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.963 I llama_model_loader: - type  f32:  194 tensors
0.00.030.964 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.964 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.713 I llm_load_vocab: special tokens cache size = 25
0.00.057.691 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.693 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.694 I llm_load_print_meta: arch             = gptneox
0.00.057.694 I llm_load_print_meta: vocab type       = BPE
0.00.057.694 I llm_load_print_meta: n_vocab          = 50304
0.00.057.694 I llm_load_print_meta: n_merges         = 50009
0.00.057.695 I llm_load_print_meta: vocab_only       = 0
0.00.057.695 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.695 I llm_load_print_meta: n_embd           = 2048
0.00.057.695 I llm_load_print_meta: n_layer          = 24
0.00.057.698 I llm_load_print_meta: n_head           = 16
0.00.057.701 I llm_load_print_meta: n_head_kv        = 16
0.00.057.701 I llm_load_print_meta: n_rot            = 32
0.00.057.701 I llm_load_print_meta: n_swa            = 0
0.00.057.702 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.702 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.703 I llm_load_print_meta: n_gqa            = 1
0.00.057.709 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.709 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.710 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.711 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.711 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.711 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.711 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.712 I llm_load_print_meta: n_ff             = 8192
0.00.057.712 I llm_load_print_meta: n_expert         = 0
0.00.057.712 I llm_load_print_meta: n_expert_used    = 0
0.00.057.713 I llm_load_print_meta: causal attn      = 1
0.00.057.713 I llm_load_print_meta: pooling type     = 0
0.00.057.713 I llm_load_print_meta: rope type        = 2
0.00.057.713 I llm_load_print_meta: rope scaling     = linear
0.00.057.713 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.714 I llm_load_print_meta: freq_scale_train = 1
0.00.057.714 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.714 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.717 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.717 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.717 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.717 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.717 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.718 I llm_load_print_meta: model type       = 1.4B
0.00.057.718 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.057.718 I llm_load_print_meta: model params     = 1.41 B
0.00.057.719 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.057.719 I llm_load_print_meta: general.name     = 1.4B
0.00.057.719 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.719 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.719 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.719 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.720 I llm_load_print_meta: LF token         = 128 ''
0.00.057.720 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.720 I llm_load_print_meta: max token length = 1024
0.00.059.614 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.614 I llm_load_tensors: offloading output layer to GPU
0.00.059.614 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.625 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.059.626 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.060.519 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.520 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.520 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.520 I llama_new_context_with_model: n_batch       = 2048
0.00.060.521 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.521 I llama_new_context_with_model: flash_attn    = 0
0.00.060.521 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.522 I llama_new_context_with_model: freq_scale    = 1
0.00.060.522 I ggml_metal_init: allocating
0.00.060.529 I ggml_metal_init: found device: Apple M4
0.00.060.533 I ggml_metal_init: picking default device: Apple M4
0.00.061.121 I ggml_metal_init: using embedded metal library
0.00.063.438 I ggml_metal_init: GPU name:   Apple M4
0.00.063.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.441 I ggml_metal_init: simdgroup reduction   = true
0.00.063.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.441 I ggml_metal_init: has bfloat            = true
0.00.063.441 I ggml_metal_init: use bfloat            = true
0.00.063.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.532 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.093.916 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.922 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.941 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.015 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.016 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.017 I llama_new_context_with_model: graph nodes  = 967
0.00.095.017 I llama_new_context_with_model: graph splits = 2
0.00.095.032 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.457.778 I main: llama threadpool init, n_threads = 4
0.00.457.819 I 
0.00.457.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.457.865 I 
0.00.458.152 I sampler seed: 1234
0.00.458.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.458.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.458.170 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.458.170 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.135.990 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.135.991 I llama_perf_context_print:        load time =     441.41 ms
0.01.135.992 I llama_perf_context_print: prompt eval time =      35.89 ms /     7 tokens (    5.13 ms per token,   195.04 tokens per second)
0.01.135.993 I llama_perf_context_print:        eval time =     638.97 ms /    63 runs   (   10.14 ms per token,    98.60 tokens per second)
0.01.135.994 I llama_perf_context_print:       total time =     678.22 ms /    70 tokens
0.01.136.201 I ggml_metal_free: deallocating

real	0m1.154s
user	0m0.111s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.684 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.237 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.238 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.239 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.906 I llama_model_loader: - type  f32:  194 tensors
0.00.023.906 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.906 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.907 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.078 I llm_load_vocab: special tokens cache size = 25
0.00.050.973 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.976 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.976 I llm_load_print_meta: arch             = gptneox
0.00.050.976 I llm_load_print_meta: vocab type       = BPE
0.00.050.977 I llm_load_print_meta: n_vocab          = 50304
0.00.050.977 I llm_load_print_meta: n_merges         = 50009
0.00.050.977 I llm_load_print_meta: vocab_only       = 0
0.00.050.977 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.977 I llm_load_print_meta: n_embd           = 2048
0.00.050.977 I llm_load_print_meta: n_layer          = 24
0.00.050.980 I llm_load_print_meta: n_head           = 16
0.00.050.983 I llm_load_print_meta: n_head_kv        = 16
0.00.050.983 I llm_load_print_meta: n_rot            = 32
0.00.050.983 I llm_load_print_meta: n_swa            = 0
0.00.050.983 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.983 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.984 I llm_load_print_meta: n_gqa            = 1
0.00.050.985 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.986 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.986 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.987 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.987 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.987 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.987 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.988 I llm_load_print_meta: n_ff             = 8192
0.00.050.988 I llm_load_print_meta: n_expert         = 0
0.00.050.988 I llm_load_print_meta: n_expert_used    = 0
0.00.050.988 I llm_load_print_meta: causal attn      = 1
0.00.050.988 I llm_load_print_meta: pooling type     = 0
0.00.050.989 I llm_load_print_meta: rope type        = 2
0.00.050.989 I llm_load_print_meta: rope scaling     = linear
0.00.050.989 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.990 I llm_load_print_meta: freq_scale_train = 1
0.00.050.990 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.991 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.991 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.991 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.991 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.991 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.991 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.992 I llm_load_print_meta: model type       = 1.4B
0.00.050.992 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.993 I llm_load_print_meta: model params     = 1.41 B
0.00.050.993 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.993 I llm_load_print_meta: general.name     = 1.4B
0.00.050.994 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.994 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.994 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.994 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.994 I llm_load_print_meta: LF token         = 128 ''
0.00.050.995 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.995 I llm_load_print_meta: max token length = 1024
0.00.052.897 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.897 I llm_load_tensors: offloading output layer to GPU
0.00.052.897 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.907 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.909 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.812 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.813 I llama_new_context_with_model: n_ctx         = 128
0.00.053.813 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.813 I llama_new_context_with_model: n_batch       = 128
0.00.053.813 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.814 I llama_new_context_with_model: flash_attn    = 0
0.00.053.814 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.814 I llama_new_context_with_model: freq_scale    = 1
0.00.053.814 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.815 I ggml_metal_init: allocating
0.00.053.818 I ggml_metal_init: found device: Apple M4
0.00.053.820 I ggml_metal_init: picking default device: Apple M4
0.00.054.375 I ggml_metal_init: using embedded metal library
0.00.056.788 I ggml_metal_init: GPU name:   Apple M4
0.00.056.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.791 I ggml_metal_init: simdgroup reduction   = true
0.00.056.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.791 I ggml_metal_init: has bfloat            = true
0.00.056.791 I ggml_metal_init: use bfloat            = true
0.00.056.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.088 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.107 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.016 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.017 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.017 I llama_new_context_with_model: graph nodes  = 967
0.00.069.018 I llama_new_context_with_model: graph splits = 2
0.00.069.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.568 I 
0.00.381.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.724 I perplexity: tokenizing the input ..
0.00.389.799 I perplexity: tokenization took 8.073 ms
0.00.389.803 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.522.260 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.523.421 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.523.445 I llama_perf_context_print:        load time =     371.87 ms
0.00.523.445 I llama_perf_context_print: prompt eval time =     132.23 ms /   128 tokens (    1.03 ms per token,   968.01 tokens per second)
0.00.523.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.523.446 I llama_perf_context_print:       total time =     141.89 ms /   129 tokens
0.00.523.985 I ggml_metal_free: deallocating

real	0m0.540s
user	0m0.080s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.538 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.084 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.031.089 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.099 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.099 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.999 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.000 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.040.000 I llama_model_loader: - type  f32:  194 tensors
0.00.040.001 I llama_model_loader: - type q3_K:   25 tensors
0.00.040.001 I llama_model_loader: - type q4_K:   71 tensors
0.00.040.001 I llama_model_loader: - type q5_K:    1 tensors
0.00.040.001 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.465 I llm_load_vocab: special tokens cache size = 25
0.00.074.026 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.030 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.030 I llm_load_print_meta: arch             = gptneox
0.00.074.030 I llm_load_print_meta: vocab type       = BPE
0.00.074.031 I llm_load_print_meta: n_vocab          = 50304
0.00.074.031 I llm_load_print_meta: n_merges         = 50009
0.00.074.031 I llm_load_print_meta: vocab_only       = 0
0.00.074.031 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.032 I llm_load_print_meta: n_embd           = 2048
0.00.074.032 I llm_load_print_meta: n_layer          = 24
0.00.074.038 I llm_load_print_meta: n_head           = 16
0.00.074.039 I llm_load_print_meta: n_head_kv        = 16
0.00.074.039 I llm_load_print_meta: n_rot            = 32
0.00.074.039 I llm_load_print_meta: n_swa            = 0
0.00.074.040 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.040 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.041 I llm_load_print_meta: n_gqa            = 1
0.00.074.042 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.043 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.044 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.044 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.045 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.045 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.045 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.046 I llm_load_print_meta: n_ff             = 8192
0.00.074.046 I llm_load_print_meta: n_expert         = 0
0.00.074.046 I llm_load_print_meta: n_expert_used    = 0
0.00.074.047 I llm_load_print_meta: causal attn      = 1
0.00.074.047 I llm_load_print_meta: pooling type     = 0
0.00.074.047 I llm_load_print_meta: rope type        = 2
0.00.074.047 I llm_load_print_meta: rope scaling     = linear
0.00.074.048 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.048 I llm_load_print_meta: freq_scale_train = 1
0.00.074.048 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.049 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.049 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.051 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.051 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.051 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.051 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.052 I llm_load_print_meta: model type       = 1.4B
0.00.074.052 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.074.053 I llm_load_print_meta: model params     = 1.41 B
0.00.074.053 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.074.053 I llm_load_print_meta: general.name     = 1.4B
0.00.074.054 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.054 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.054 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.054 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.055 I llm_load_print_meta: LF token         = 128 ''
0.00.074.061 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.061 I llm_load_print_meta: max token length = 1024
0.00.076.517 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.518 I llm_load_tensors: offloading output layer to GPU
0.00.076.518 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.524 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.076.525 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.077.984 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.985 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.985 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.985 I llama_new_context_with_model: n_batch       = 2048
0.00.077.986 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.986 I llama_new_context_with_model: flash_attn    = 0
0.00.077.987 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.987 I llama_new_context_with_model: freq_scale    = 1
0.00.077.988 I ggml_metal_init: allocating
0.00.077.992 I ggml_metal_init: found device: Apple M4
0.00.077.995 I ggml_metal_init: picking default device: Apple M4
0.00.078.788 I ggml_metal_init: using embedded metal library
0.00.082.446 I ggml_metal_init: GPU name:   Apple M4
0.00.082.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.448 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.449 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.449 I ggml_metal_init: simdgroup reduction   = true
0.00.082.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.450 I ggml_metal_init: has bfloat            = true
0.00.082.450 I ggml_metal_init: use bfloat            = true
0.00.082.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.927 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.116.011 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.021 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.039 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.084 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.085 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.085 I llama_new_context_with_model: graph nodes  = 967
0.00.117.086 I llama_new_context_with_model: graph splits = 2
0.00.117.103 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.517 I main: llama threadpool init, n_threads = 4
0.00.625.556 I 
0.00.625.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.591 I 
0.00.625.813 I sampler seed: 1234
0.00.625.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.839 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.839 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.839 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.378.442 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.378.442 I llama_perf_context_print:        load time =     614.97 ms
0.01.378.443 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.64 tokens per second)
0.01.378.444 I llama_perf_context_print:        eval time =     705.18 ms /    63 runs   (   11.19 ms per token,    89.34 tokens per second)
0.01.378.444 I llama_perf_context_print:       total time =     752.93 ms /    70 tokens
0.01.378.628 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.122s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.661 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.559 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.560 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.562 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.386 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.387 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.387 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.388 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.388 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.389 I llama_model_loader: - type  f32:  194 tensors
0.00.023.389 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.390 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.390 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.390 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.963 I llm_load_vocab: special tokens cache size = 25
0.00.050.007 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.009 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.010 I llm_load_print_meta: arch             = gptneox
0.00.050.010 I llm_load_print_meta: vocab type       = BPE
0.00.050.010 I llm_load_print_meta: n_vocab          = 50304
0.00.050.010 I llm_load_print_meta: n_merges         = 50009
0.00.050.010 I llm_load_print_meta: vocab_only       = 0
0.00.050.011 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.011 I llm_load_print_meta: n_embd           = 2048
0.00.050.011 I llm_load_print_meta: n_layer          = 24
0.00.050.014 I llm_load_print_meta: n_head           = 16
0.00.050.014 I llm_load_print_meta: n_head_kv        = 16
0.00.050.015 I llm_load_print_meta: n_rot            = 32
0.00.050.015 I llm_load_print_meta: n_swa            = 0
0.00.050.015 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.015 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.016 I llm_load_print_meta: n_gqa            = 1
0.00.050.017 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.022 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.023 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.023 I llm_load_print_meta: n_ff             = 8192
0.00.050.023 I llm_load_print_meta: n_expert         = 0
0.00.050.024 I llm_load_print_meta: n_expert_used    = 0
0.00.050.024 I llm_load_print_meta: causal attn      = 1
0.00.050.024 I llm_load_print_meta: pooling type     = 0
0.00.050.024 I llm_load_print_meta: rope type        = 2
0.00.050.025 I llm_load_print_meta: rope scaling     = linear
0.00.050.026 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.027 I llm_load_print_meta: freq_scale_train = 1
0.00.050.027 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.027 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.027 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.028 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.028 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.028 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.030 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.030 I llm_load_print_meta: model type       = 1.4B
0.00.050.030 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.031 I llm_load_print_meta: model params     = 1.41 B
0.00.050.031 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.031 I llm_load_print_meta: general.name     = 1.4B
0.00.050.032 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.035 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.035 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.035 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.036 I llm_load_print_meta: LF token         = 128 ''
0.00.050.036 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.036 I llm_load_print_meta: max token length = 1024
0.00.051.612 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.612 I llm_load_tensors: offloading output layer to GPU
0.00.051.612 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.622 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.623 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.471 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.472 I llama_new_context_with_model: n_ctx         = 128
0.00.052.472 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.472 I llama_new_context_with_model: n_batch       = 128
0.00.052.472 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.472 I llama_new_context_with_model: flash_attn    = 0
0.00.052.473 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.473 I llama_new_context_with_model: freq_scale    = 1
0.00.052.473 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.474 I ggml_metal_init: allocating
0.00.052.477 I ggml_metal_init: found device: Apple M4
0.00.052.479 I ggml_metal_init: picking default device: Apple M4
0.00.053.059 I ggml_metal_init: using embedded metal library
0.00.055.401 I ggml_metal_init: GPU name:   Apple M4
0.00.055.403 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.404 I ggml_metal_init: simdgroup reduction   = true
0.00.055.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.404 I ggml_metal_init: has bfloat            = true
0.00.055.404 I ggml_metal_init: use bfloat            = true
0.00.055.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.310 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.601 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.604 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.619 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.473 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.474 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.474 I llama_new_context_with_model: graph nodes  = 967
0.00.067.474 I llama_new_context_with_model: graph splits = 2
0.00.067.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.711 I 
0.00.502.747 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.758 I perplexity: tokenizing the input ..
0.00.510.791 I perplexity: tokenization took 8.03 ms
0.00.510.794 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.085 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.644.261 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.644.285 I llama_perf_context_print:        load time =     494.05 ms
0.00.644.287 I llama_perf_context_print: prompt eval time =     132.06 ms /   128 tokens (    1.03 ms per token,   969.29 tokens per second)
0.00.644.289 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.289 I llama_perf_context_print:       total time =     141.57 ms /   129 tokens
0.00.644.820 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.079s
sys	0m0.096s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.013.841 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.030.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.129 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.350 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.041.876 I llama_model_loader: - type  f32:  194 tensors
0.00.041.876 I llama_model_loader: - type q4_K:   61 tensors
0.00.041.876 I llama_model_loader: - type q5_K:   24 tensors
0.00.041.877 I llama_model_loader: - type q6_K:   13 tensors
0.00.076.620 I llm_load_vocab: special tokens cache size = 25
0.00.086.741 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.745 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.745 I llm_load_print_meta: arch             = gptneox
0.00.086.746 I llm_load_print_meta: vocab type       = BPE
0.00.086.746 I llm_load_print_meta: n_vocab          = 50304
0.00.086.746 I llm_load_print_meta: n_merges         = 50009
0.00.086.746 I llm_load_print_meta: vocab_only       = 0
0.00.086.747 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.747 I llm_load_print_meta: n_embd           = 2048
0.00.086.747 I llm_load_print_meta: n_layer          = 24
0.00.086.751 I llm_load_print_meta: n_head           = 16
0.00.086.752 I llm_load_print_meta: n_head_kv        = 16
0.00.086.752 I llm_load_print_meta: n_rot            = 32
0.00.086.753 I llm_load_print_meta: n_swa            = 0
0.00.086.753 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.754 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.755 I llm_load_print_meta: n_gqa            = 1
0.00.086.756 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.757 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.757 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.758 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.758 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.758 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.761 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.762 I llm_load_print_meta: n_ff             = 8192
0.00.086.762 I llm_load_print_meta: n_expert         = 0
0.00.086.764 I llm_load_print_meta: n_expert_used    = 0
0.00.086.764 I llm_load_print_meta: causal attn      = 1
0.00.086.764 I llm_load_print_meta: pooling type     = 0
0.00.086.764 I llm_load_print_meta: rope type        = 2
0.00.086.764 I llm_load_print_meta: rope scaling     = linear
0.00.086.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.765 I llm_load_print_meta: freq_scale_train = 1
0.00.086.766 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.766 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.766 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.771 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.771 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.771 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.771 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.772 I llm_load_print_meta: model type       = 1.4B
0.00.086.772 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.086.773 I llm_load_print_meta: model params     = 1.41 B
0.00.086.774 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.086.775 I llm_load_print_meta: general.name     = 1.4B
0.00.086.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.775 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.775 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.776 I llm_load_print_meta: LF token         = 128 ''
0.00.086.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.777 I llm_load_print_meta: max token length = 1024
0.00.089.565 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.565 I llm_load_tensors: offloading output layer to GPU
0.00.089.566 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.578 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.089.579 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.090.892 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.894 I llama_new_context_with_model: n_ctx         = 2048
0.00.090.894 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.090.894 I llama_new_context_with_model: n_batch       = 2048
0.00.090.895 I llama_new_context_with_model: n_ubatch      = 512
0.00.090.895 I llama_new_context_with_model: flash_attn    = 0
0.00.090.895 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.896 I llama_new_context_with_model: freq_scale    = 1
0.00.090.896 I ggml_metal_init: allocating
0.00.090.900 I ggml_metal_init: found device: Apple M4
0.00.090.903 I ggml_metal_init: picking default device: Apple M4
0.00.091.739 I ggml_metal_init: using embedded metal library
0.00.095.215 I ggml_metal_init: GPU name:   Apple M4
0.00.095.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.218 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.220 I ggml_metal_init: simdgroup reduction   = true
0.00.095.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.221 I ggml_metal_init: has bfloat            = true
0.00.095.221 I ggml_metal_init: use bfloat            = true
0.00.095.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.339 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.128.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.128.786 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.808 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.129.811 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.129.813 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.129.813 I llama_new_context_with_model: graph nodes  = 967
0.00.129.813 I llama_new_context_with_model: graph splits = 2
0.00.129.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.129.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.129.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.425 I main: llama threadpool init, n_threads = 4
0.00.748.505 I 
0.00.748.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.579 I 
0.00.748.887 I sampler seed: 1234
0.00.748.894 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.922 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.924 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.511.364 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.511.365 I llama_perf_context_print:        load time =     734.58 ms
0.01.511.365 I llama_perf_context_print: prompt eval time =      47.75 ms /     7 tokens (    6.82 ms per token,   146.60 tokens per second)
0.01.511.366 I llama_perf_context_print:        eval time =     711.68 ms /    63 runs   (   11.30 ms per token,    88.52 tokens per second)
0.01.511.367 I llama_perf_context_print:       total time =     762.94 ms /    70 tokens
0.01.511.559 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.147s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.525 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.405 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.406 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.407 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.409 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.376 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.295 I llama_model_loader: - type  f32:  194 tensors
0.00.024.296 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.296 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.296 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.734 I llm_load_vocab: special tokens cache size = 25
0.00.050.740 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.743 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.744 I llm_load_print_meta: arch             = gptneox
0.00.050.744 I llm_load_print_meta: vocab type       = BPE
0.00.050.744 I llm_load_print_meta: n_vocab          = 50304
0.00.050.744 I llm_load_print_meta: n_merges         = 50009
0.00.050.745 I llm_load_print_meta: vocab_only       = 0
0.00.050.745 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.745 I llm_load_print_meta: n_embd           = 2048
0.00.050.745 I llm_load_print_meta: n_layer          = 24
0.00.050.748 I llm_load_print_meta: n_head           = 16
0.00.050.748 I llm_load_print_meta: n_head_kv        = 16
0.00.050.749 I llm_load_print_meta: n_rot            = 32
0.00.050.749 I llm_load_print_meta: n_swa            = 0
0.00.050.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.749 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.750 I llm_load_print_meta: n_gqa            = 1
0.00.050.751 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.752 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.753 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.753 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.754 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.754 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.756 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.756 I llm_load_print_meta: n_ff             = 8192
0.00.050.756 I llm_load_print_meta: n_expert         = 0
0.00.050.756 I llm_load_print_meta: n_expert_used    = 0
0.00.050.757 I llm_load_print_meta: causal attn      = 1
0.00.050.757 I llm_load_print_meta: pooling type     = 0
0.00.050.757 I llm_load_print_meta: rope type        = 2
0.00.050.757 I llm_load_print_meta: rope scaling     = linear
0.00.050.757 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.758 I llm_load_print_meta: freq_scale_train = 1
0.00.050.758 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.758 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.758 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.758 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.759 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.759 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.759 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.759 I llm_load_print_meta: model type       = 1.4B
0.00.050.760 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.760 I llm_load_print_meta: model params     = 1.41 B
0.00.050.765 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.765 I llm_load_print_meta: general.name     = 1.4B
0.00.050.765 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.765 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.766 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.766 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.766 I llm_load_print_meta: LF token         = 128 ''
0.00.050.766 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.767 I llm_load_print_meta: max token length = 1024
0.00.052.758 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.758 I llm_load_tensors: offloading output layer to GPU
0.00.052.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.769 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.770 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.676 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.677 I llama_new_context_with_model: n_ctx         = 128
0.00.053.677 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.677 I llama_new_context_with_model: n_batch       = 128
0.00.053.677 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.677 I llama_new_context_with_model: flash_attn    = 0
0.00.053.678 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.678 I llama_new_context_with_model: freq_scale    = 1
0.00.053.678 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.679 I ggml_metal_init: allocating
0.00.053.682 I ggml_metal_init: found device: Apple M4
0.00.053.684 I ggml_metal_init: picking default device: Apple M4
0.00.054.241 I ggml_metal_init: using embedded metal library
0.00.056.541 I ggml_metal_init: GPU name:   Apple M4
0.00.056.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.543 I ggml_metal_init: simdgroup reduction   = true
0.00.056.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.543 I ggml_metal_init: has bfloat            = true
0.00.056.544 I ggml_metal_init: use bfloat            = true
0.00.056.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.545 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.005 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.009 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.026 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.883 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.884 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.884 I llama_new_context_with_model: graph nodes  = 967
0.00.068.885 I llama_new_context_with_model: graph splits = 2
0.00.068.897 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.786 I 
0.00.557.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.834 I perplexity: tokenizing the input ..
0.00.565.498 I perplexity: tokenization took 7.662 ms
0.00.565.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.700.115 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.701.338 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.701.357 I llama_perf_context_print:        load time =     548.26 ms
0.00.701.358 I llama_perf_context_print: prompt eval time =     134.39 ms /   128 tokens (    1.05 ms per token,   952.42 tokens per second)
0.00.701.359 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.701.359 I llama_perf_context_print:       total time =     143.57 ms /   129 tokens
0.00.701.838 I ggml_metal_free: deallocating

real	0m0.717s
user	0m0.079s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.682 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.542 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.544 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.544 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.545 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.549 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.550 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.604 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.494 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.495 I llama_model_loader: - type  f32:  194 tensors
0.00.024.495 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.496 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.153 I llm_load_vocab: special tokens cache size = 25
0.00.051.140 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.142 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.143 I llm_load_print_meta: arch             = gptneox
0.00.051.143 I llm_load_print_meta: vocab type       = BPE
0.00.051.144 I llm_load_print_meta: n_vocab          = 50304
0.00.051.144 I llm_load_print_meta: n_merges         = 50009
0.00.051.144 I llm_load_print_meta: vocab_only       = 0
0.00.051.144 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.144 I llm_load_print_meta: n_embd           = 2048
0.00.051.144 I llm_load_print_meta: n_layer          = 24
0.00.051.147 I llm_load_print_meta: n_head           = 16
0.00.051.148 I llm_load_print_meta: n_head_kv        = 16
0.00.051.148 I llm_load_print_meta: n_rot            = 32
0.00.051.148 I llm_load_print_meta: n_swa            = 0
0.00.051.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.149 I llm_load_print_meta: n_gqa            = 1
0.00.051.150 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.151 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.152 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.152 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.154 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.155 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.155 I llm_load_print_meta: n_ff             = 8192
0.00.051.155 I llm_load_print_meta: n_expert         = 0
0.00.051.155 I llm_load_print_meta: n_expert_used    = 0
0.00.051.157 I llm_load_print_meta: causal attn      = 1
0.00.051.159 I llm_load_print_meta: pooling type     = 0
0.00.051.159 I llm_load_print_meta: rope type        = 2
0.00.051.159 I llm_load_print_meta: rope scaling     = linear
0.00.051.159 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.160 I llm_load_print_meta: freq_scale_train = 1
0.00.051.160 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.160 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.160 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.160 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.160 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.160 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.161 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.161 I llm_load_print_meta: model type       = 1.4B
0.00.051.162 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.166 I llm_load_print_meta: model params     = 1.41 B
0.00.051.166 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.167 I llm_load_print_meta: general.name     = 1.4B
0.00.051.168 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.168 I llm_load_print_meta: LF token         = 128 ''
0.00.051.169 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.169 I llm_load_print_meta: max token length = 1024
0.00.053.211 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.211 I llm_load_tensors: offloading output layer to GPU
0.00.053.212 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.222 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.223 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.125 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.126 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.126 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.126 I llama_new_context_with_model: n_batch       = 2048
0.00.054.126 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.126 I llama_new_context_with_model: flash_attn    = 0
0.00.054.127 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.127 I llama_new_context_with_model: freq_scale    = 1
0.00.054.128 I ggml_metal_init: allocating
0.00.054.131 I ggml_metal_init: found device: Apple M4
0.00.054.133 I ggml_metal_init: picking default device: Apple M4
0.00.054.731 I ggml_metal_init: using embedded metal library
0.00.057.064 I ggml_metal_init: GPU name:   Apple M4
0.00.057.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.068 I ggml_metal_init: simdgroup reduction   = true
0.00.057.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.068 I ggml_metal_init: has bfloat            = true
0.00.057.068 I ggml_metal_init: use bfloat            = true
0.00.057.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.034 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.658 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.663 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.621 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.622 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.623 I llama_new_context_with_model: graph nodes  = 967
0.00.087.623 I llama_new_context_with_model: graph splits = 2
0.00.087.638 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.778 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.386 I main: llama threadpool init, n_threads = 4
0.00.711.423 I 
0.00.711.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.454 I 
0.00.711.754 I sampler seed: 1234
0.00.711.760 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.773 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.561.927 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.561.928 I llama_perf_context_print:        load time =     702.70 ms
0.01.561.928 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.58 tokens per second)
0.01.561.932 I llama_perf_context_print:        eval time =     795.68 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.561.933 I llama_perf_context_print:       total time =     850.54 ms /    70 tokens
0.01.562.142 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.947 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.010 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.011 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.011 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.800 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.802 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.802 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.802 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.803 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.803 I llama_model_loader: - type  f32:  194 tensors
0.00.023.804 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.804 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.021 I llm_load_vocab: special tokens cache size = 25
0.00.051.070 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.073 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.074 I llm_load_print_meta: arch             = gptneox
0.00.051.074 I llm_load_print_meta: vocab type       = BPE
0.00.051.074 I llm_load_print_meta: n_vocab          = 50304
0.00.051.074 I llm_load_print_meta: n_merges         = 50009
0.00.051.074 I llm_load_print_meta: vocab_only       = 0
0.00.051.075 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.075 I llm_load_print_meta: n_embd           = 2048
0.00.051.075 I llm_load_print_meta: n_layer          = 24
0.00.051.078 I llm_load_print_meta: n_head           = 16
0.00.051.079 I llm_load_print_meta: n_head_kv        = 16
0.00.051.079 I llm_load_print_meta: n_rot            = 32
0.00.051.079 I llm_load_print_meta: n_swa            = 0
0.00.051.081 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.081 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.082 I llm_load_print_meta: n_gqa            = 1
0.00.051.083 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.084 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.084 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.086 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.086 I llm_load_print_meta: n_ff             = 8192
0.00.051.087 I llm_load_print_meta: n_expert         = 0
0.00.051.087 I llm_load_print_meta: n_expert_used    = 0
0.00.051.087 I llm_load_print_meta: causal attn      = 1
0.00.051.087 I llm_load_print_meta: pooling type     = 0
0.00.051.087 I llm_load_print_meta: rope type        = 2
0.00.051.088 I llm_load_print_meta: rope scaling     = linear
0.00.051.088 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.088 I llm_load_print_meta: freq_scale_train = 1
0.00.051.088 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.089 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.089 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.089 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.089 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.089 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.089 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.091 I llm_load_print_meta: model type       = 1.4B
0.00.051.091 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.092 I llm_load_print_meta: model params     = 1.41 B
0.00.051.092 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.092 I llm_load_print_meta: general.name     = 1.4B
0.00.051.093 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.093 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.093 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.093 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.094 I llm_load_print_meta: LF token         = 128 ''
0.00.051.094 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.095 I llm_load_print_meta: max token length = 1024
0.00.053.133 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.133 I llm_load_tensors: offloading output layer to GPU
0.00.053.134 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.144 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.145 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.099 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.100 I llama_new_context_with_model: n_ctx         = 128
0.00.054.100 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.100 I llama_new_context_with_model: n_batch       = 128
0.00.054.101 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.101 I llama_new_context_with_model: flash_attn    = 0
0.00.054.101 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.101 I llama_new_context_with_model: freq_scale    = 1
0.00.054.102 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.102 I ggml_metal_init: allocating
0.00.054.106 I ggml_metal_init: found device: Apple M4
0.00.054.108 I ggml_metal_init: picking default device: Apple M4
0.00.054.684 I ggml_metal_init: using embedded metal library
0.00.057.134 I ggml_metal_init: GPU name:   Apple M4
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.137 I ggml_metal_init: simdgroup reduction   = true
0.00.057.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.137 I ggml_metal_init: has bfloat            = true
0.00.057.137 I ggml_metal_init: use bfloat            = true
0.00.057.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.185 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.520 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.535 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.533 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.534 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.534 I llama_new_context_with_model: graph nodes  = 967
0.00.069.534 I llama_new_context_with_model: graph splits = 2
0.00.069.547 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.199 I 
0.00.634.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.249 I perplexity: tokenizing the input ..
0.00.642.134 I perplexity: tokenization took 7.884 ms
0.00.642.142 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.883 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.784.056 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.784.073 I llama_perf_context_print:        load time =     625.25 ms
0.00.784.074 I llama_perf_context_print: prompt eval time =     140.52 ms /   128 tokens (    1.10 ms per token,   910.92 tokens per second)
0.00.784.075 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.075 I llama_perf_context_print:       total time =     149.88 ms /   129 tokens
0.00.784.478 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.080s
sys	0m0.113s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.433 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.433 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.434 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.434 I llama_model_loader: - type  f32:  194 tensors
0.00.025.434 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.634 I llm_load_vocab: special tokens cache size = 25
0.00.052.623 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.626 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.626 I llm_load_print_meta: arch             = gptneox
0.00.052.627 I llm_load_print_meta: vocab type       = BPE
0.00.052.627 I llm_load_print_meta: n_vocab          = 50304
0.00.052.627 I llm_load_print_meta: n_merges         = 50009
0.00.052.627 I llm_load_print_meta: vocab_only       = 0
0.00.052.627 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.628 I llm_load_print_meta: n_embd           = 2048
0.00.052.628 I llm_load_print_meta: n_layer          = 24
0.00.052.631 I llm_load_print_meta: n_head           = 16
0.00.052.633 I llm_load_print_meta: n_head_kv        = 16
0.00.052.634 I llm_load_print_meta: n_rot            = 32
0.00.052.634 I llm_load_print_meta: n_swa            = 0
0.00.052.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.635 I llm_load_print_meta: n_gqa            = 1
0.00.052.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.639 I llm_load_print_meta: n_ff             = 8192
0.00.052.639 I llm_load_print_meta: n_expert         = 0
0.00.052.639 I llm_load_print_meta: n_expert_used    = 0
0.00.052.639 I llm_load_print_meta: causal attn      = 1
0.00.052.643 I llm_load_print_meta: pooling type     = 0
0.00.052.643 I llm_load_print_meta: rope type        = 2
0.00.052.644 I llm_load_print_meta: rope scaling     = linear
0.00.052.645 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.645 I llm_load_print_meta: freq_scale_train = 1
0.00.052.646 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.646 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.646 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.646 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.646 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.650 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.651 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.651 I llm_load_print_meta: model type       = 1.4B
0.00.052.652 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.653 I llm_load_print_meta: model params     = 1.41 B
0.00.052.653 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.653 I llm_load_print_meta: general.name     = 1.4B
0.00.052.653 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.654 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.654 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.654 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.654 I llm_load_print_meta: LF token         = 128 ''
0.00.052.654 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.654 I llm_load_print_meta: max token length = 1024
0.00.054.783 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.783 I llm_load_tensors: offloading output layer to GPU
0.00.054.783 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.794 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.795 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.736 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.736 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.736 I llama_new_context_with_model: n_batch       = 2048
0.00.055.737 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.737 I llama_new_context_with_model: flash_attn    = 0
0.00.055.737 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.738 I llama_new_context_with_model: freq_scale    = 1
0.00.055.738 I ggml_metal_init: allocating
0.00.055.742 I ggml_metal_init: found device: Apple M4
0.00.055.745 I ggml_metal_init: picking default device: Apple M4
0.00.056.386 I ggml_metal_init: using embedded metal library
0.00.058.857 I ggml_metal_init: GPU name:   Apple M4
0.00.058.858 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.859 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.859 I ggml_metal_init: simdgroup reduction   = true
0.00.058.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.861 I ggml_metal_init: has bfloat            = true
0.00.058.861 I ggml_metal_init: use bfloat            = true
0.00.058.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.073 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.081 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.087 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.107 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.185 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.187 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.187 I llama_new_context_with_model: graph nodes  = 967
0.00.090.187 I llama_new_context_with_model: graph splits = 2
0.00.090.203 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.352 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.025 I main: llama threadpool init, n_threads = 4
0.00.749.065 I 
0.00.749.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.095 I 
0.00.749.328 I sampler seed: 1234
0.00.749.333 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.344 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.346 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.633.530 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.633.531 I llama_perf_context_print:        load time =     739.05 ms
0.01.633.532 I llama_perf_context_print: prompt eval time =      54.54 ms /     7 tokens (    7.79 ms per token,   128.36 tokens per second)
0.01.633.533 I llama_perf_context_print:        eval time =     826.52 ms /    63 runs   (   13.12 ms per token,    76.22 tokens per second)
0.01.633.533 I llama_perf_context_print:       total time =     884.51 ms /    70 tokens
0.01.633.725 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.112s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4381 (b92a14a8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.991 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.722 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.722 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.723 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.723 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.723 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.724 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.725 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.731 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.731 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.731 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.584 I llama_model_loader: - type  f32:  194 tensors
0.00.024.584 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.069 I llm_load_vocab: special tokens cache size = 25
0.00.050.959 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.962 I llm_load_print_meta: arch             = gptneox
0.00.050.962 I llm_load_print_meta: vocab type       = BPE
0.00.050.962 I llm_load_print_meta: n_vocab          = 50304
0.00.050.963 I llm_load_print_meta: n_merges         = 50009
0.00.050.963 I llm_load_print_meta: vocab_only       = 0
0.00.050.963 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.963 I llm_load_print_meta: n_embd           = 2048
0.00.050.963 I llm_load_print_meta: n_layer          = 24
0.00.050.966 I llm_load_print_meta: n_head           = 16
0.00.050.967 I llm_load_print_meta: n_head_kv        = 16
0.00.050.969 I llm_load_print_meta: n_rot            = 32
0.00.050.969 I llm_load_print_meta: n_swa            = 0
0.00.050.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.970 I llm_load_print_meta: n_gqa            = 1
0.00.050.971 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.972 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.973 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.973 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.974 I llm_load_print_meta: n_ff             = 8192
0.00.050.974 I llm_load_print_meta: n_expert         = 0
0.00.050.974 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.974 I llm_load_print_meta: pooling type     = 0
0.00.050.974 I llm_load_print_meta: rope type        = 2
0.00.050.980 I llm_load_print_meta: rope scaling     = linear
0.00.050.980 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.980 I llm_load_print_meta: freq_scale_train = 1
0.00.050.981 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.981 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.981 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.981 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.982 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.982 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.982 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.982 I llm_load_print_meta: model type       = 1.4B
0.00.050.982 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.983 I llm_load_print_meta: model params     = 1.41 B
0.00.050.983 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.983 I llm_load_print_meta: general.name     = 1.4B
0.00.050.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.984 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.986 I llm_load_print_meta: LF token         = 128 ''
0.00.050.986 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.986 I llm_load_print_meta: max token length = 1024
0.00.052.969 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.970 I llm_load_tensors: offloading output layer to GPU
0.00.052.970 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.980 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.982 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.871 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.872 I llama_new_context_with_model: n_ctx         = 128
0.00.053.872 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.872 I llama_new_context_with_model: n_batch       = 128
0.00.053.873 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.873 I llama_new_context_with_model: flash_attn    = 0
0.00.053.873 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.873 I llama_new_context_with_model: freq_scale    = 1
0.00.053.874 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.874 I ggml_metal_init: allocating
0.00.053.877 I ggml_metal_init: found device: Apple M4
0.00.053.879 I ggml_metal_init: picking default device: Apple M4
0.00.054.442 I ggml_metal_init: using embedded metal library
0.00.056.784 I ggml_metal_init: GPU name:   Apple M4
0.00.056.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.786 I ggml_metal_init: simdgroup reduction   = true
0.00.056.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.786 I ggml_metal_init: has bfloat            = true
0.00.056.786 I ggml_metal_init: use bfloat            = true
0.00.056.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.555 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.889 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.891 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.905 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.839 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.840 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.841 I llama_new_context_with_model: graph nodes  = 967
0.00.068.841 I llama_new_context_with_model: graph splits = 2
0.00.068.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.260.514 I 
0.00.260.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.260.568 I perplexity: tokenizing the input ..
0.00.267.891 I perplexity: tokenization took 7.319 ms
0.00.267.894 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.408.298 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.409.540 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.409.562 I llama_perf_context_print:        load time =     250.52 ms
0.00.409.563 I llama_perf_context_print: prompt eval time =     140.17 ms /   128 tokens (    1.10 ms per token,   913.16 tokens per second)
0.00.409.566 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.409.567 I llama_perf_context_print:       total time =     149.05 ms /   129 tokens
0.00.410.037 I ggml_metal_free: deallocating

real	0m0.426s
user	0m0.078s
sys	0m0.049s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4381 (b92a14a8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10110a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10110a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10110af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10110b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10110bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10110c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10110c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10110cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10110d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10110d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10110db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10110e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10110eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10110f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10110fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101110270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101110990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1011110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1011117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101111fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1011126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101112de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101113500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101113da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1011144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101114780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101114d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101115a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101115f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101116200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1011166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101116960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1011171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101117730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1011179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101117e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101118330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1011187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101118c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101119110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1011195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101119a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101119ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10111a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10111a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10111ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10111b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10111bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10111c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10111c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10111cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10111d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10111d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10111dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10111e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10111ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10111f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10111f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10111f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1011201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1011204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101120940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101120de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101121280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101121720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101121bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101122060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101122500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1011229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101122e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1011232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101123780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101123c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101124170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1011246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101124c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101125160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1011256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101125c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101126150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1011266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101126bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101127140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101127690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101127be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101128130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101128680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101128bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101129120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101129670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101129bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10112a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10112a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10112abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10112b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10112b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10112bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10111b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10112c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10112c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10112cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10112d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10112d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10112dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10112e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10112e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10112ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10112f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10112f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10112fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101130230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101130780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101130cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101131170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101131610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101131ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101131f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1011323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101132890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101132d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1011331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101133670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101133b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101133fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101134450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1011348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101134d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101135230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1011356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101135b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101136010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1011364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101136950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101136df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101137290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101137730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101137bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101138070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101138510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1011389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101138e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1011392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101139790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101139c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10113a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10113a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10113aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10113aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10113b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10113b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10113bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10113c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10113c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10113ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10113cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10113d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10113d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10113dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10113e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10113e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10113ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10113ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10113f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10113f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10113fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1011401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101140690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101140b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101140fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101141470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101141910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101141db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101142250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1011426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101142b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101143030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1011434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101143970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101143e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1011442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101144750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101144bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101145090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101145530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1011459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101145e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101146310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1011467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101146c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1011470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101147590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101147a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101147ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101148420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101148970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101148ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101149410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1011496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101149ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10114a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10114a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10114b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10114b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10114b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10114be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10114c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10114cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10114d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10114d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10114da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10114e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10114e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10114ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10114f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10114f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10114fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1011501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101150720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101150c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1011511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101151710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101151c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1011521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101152700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101152c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1011531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1011536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101153c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101154190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1011546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101154c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101155180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1011556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101155c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101156170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1011566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101156c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101157160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1011576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101157c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101158150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1011586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101158bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101159140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101159690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101159be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10115a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10115a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10115abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10115b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10115b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10115bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10115c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10115c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10115cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10115d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10115d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10115dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10115e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10115e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10115eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10115f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10115f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10115fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1011600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101160620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101160b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101161010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1011614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101161950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101161df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101162290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101162730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101162bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101163070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101163510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1011639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101163e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1011642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101164790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101164c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1011650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101165620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101165d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101166460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101166b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1011672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101167560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101167d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101168010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101168620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.157 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.145.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106204ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106205150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1062055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106205a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106205ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106206310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106206780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106206bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106207060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106207570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1062079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106208060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106208b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106209330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106209b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10620a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10620a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10620b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10620b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10620bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10620c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10620cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10620d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10620dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10620e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10620e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10620e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10620ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10620f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10620f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10620fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10620ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106210410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1062106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106210b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106210fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106211420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106211890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106211d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106212170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1062125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106212a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106212ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106213330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1062137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106213c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106214080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1062144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106214960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106214dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106215240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1062156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106215b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106215f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106216400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106216870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106216de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1062172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106217750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106217bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106218030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1062184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106218910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106218d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1062191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106219660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106219ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106219f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10621a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10621a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10621ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10621b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10621b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10621b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10621be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10621c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10621c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10621cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10621d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10621d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10621d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10621dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10621e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10621e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10621eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10621ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10621f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10621f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10621fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1062200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106220550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1062209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106220e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1062212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106221710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106221b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106221ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106222460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1062228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106222d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1062231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106223620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106223a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106223f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106224370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1062247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106224c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1062250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106225530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1062259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106225e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106226280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1062266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106226b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106226fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106227440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1062278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106227d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106228190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106228600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106228a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106228ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106229350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1062297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106229c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10622a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10622a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10622a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10622adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10622b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10622b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10622bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10622bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10622c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10622c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10622cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10622d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10622d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10622da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10622dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10622e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10622e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10622ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10622f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10622f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10622f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10622fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106230240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1062306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106230b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106230f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106231400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106231870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106231ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106232150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1062325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106232a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106232ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106233310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106233780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106233bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106234060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1062344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106234940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106234db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106235220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106235690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106235b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106235f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1062363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106236850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106236cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106237130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1062375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106237a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106237e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1062382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106238760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106238bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106239040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1062394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106239920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106239d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10623a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10623a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10623aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10623af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10623b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10623b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10623bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10623c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10623c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10623c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10623ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10623d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10623d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10623dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10623e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10623e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10623e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10623ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10623f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10623f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10623fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10623ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1062403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106240810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106240da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106241210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106241680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1062421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106242490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106242750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106242bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106243030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1062434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106243910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106243d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1062441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106244660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106244ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106244f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1062453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106245820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106245c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106246100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106246570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1062469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106246e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1062472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106247730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106247ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106248010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106248480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1062488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106248d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1062491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106249640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106249ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106249f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10624a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10624a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10624ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10624b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10624b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10624b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10624be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10624c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10624c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10624cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10624cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10624d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10624d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10624dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10624e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10624e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10624ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10624ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10624f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10624f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10624fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1062500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106250530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1062509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106250e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106251280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1062516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106251b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106251fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106252440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1062528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106252d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106253190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106253600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106253a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106253ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106254350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1062547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106254c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1062550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106255510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106255980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106255df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106256860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106256f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1062576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106257dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106258080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1062584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106258af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106259100 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1067044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1067056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1067063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1067078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1067083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10670a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10670a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10670b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10670b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10670bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10670c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10670cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10670d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10670db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10670de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10670e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10670e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10670e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10670ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10670f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10670f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10670fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10670ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1067107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1067110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1067119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1067138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1067141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1067157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1067160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1067185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10671a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10671a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10671a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10671adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10671b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10671b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10671bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10671bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10671c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10671c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10671ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10671d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10671d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10671da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10671de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10671e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10671e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10671ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10671f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10671f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10671f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10671fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1067213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1067229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1067232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106723bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106724490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106724900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106724d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1067251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106725650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106725ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106725f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1067263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106726810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106726c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1067270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106727560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1067279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106727e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1067282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106728b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106729000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106729470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1067298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106729d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10672a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10672a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10672aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10672af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10672b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10672b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10672bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10672c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10672c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10672c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10672ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10672d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10672d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10672db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10672dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10672e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10672e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10672ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10672f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10672f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10672fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10672fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106730360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1067307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106730c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1067310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106731520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106731990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106731e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106732270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1067326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106732b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106732fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106733430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1067338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106734180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1067345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106734ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106735340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1067357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106735c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106736500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106736970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1067376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106737b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106737fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106738410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106738880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106738cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106739160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1067395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106739a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106739eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10673a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10673a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10673ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10673b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10673b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10673b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10673bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10673c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10673c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10673cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10673cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10673d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10673d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10673dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10673e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10673e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10673ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10673ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10673f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10673f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10673fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1067405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106740a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106741a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106741cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106741f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106742400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106742870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106742ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106743150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1067435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106743a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106743ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106744310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106744bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106745060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1067454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106745940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106745db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106746b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106746f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1067473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106747850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106747cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1067485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1067492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106749760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106749bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10674a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10674a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10674a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10674b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10674b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10674b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10674be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10674c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10674c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10674cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10674cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10674d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10674d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10674dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10674e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10674e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10674ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10674ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10674f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10674f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10674fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1067500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1067509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106750e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106751280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1067516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106751b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106751fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1067528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106752d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106753190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106753600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106753ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106754350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1067547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106754c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1067550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106755510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106755980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1067563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106756b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106757230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106757950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106758680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106758c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.796s
user	0m0.297s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4381 (b92a14a8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c80a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c80a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c80ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c80b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c80b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c80bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c80c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c80cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c80d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c80d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c80dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c80dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c80ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c80f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c80fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c8101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c8108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c810ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c811710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c812d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c813440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c813ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c814400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c8146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c814cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c815e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c8165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c8168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c818710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c818bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c819050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c8194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c819990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c819e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c81a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c81a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c81aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c81b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c81bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c81c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c81c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c81cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c81d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c81d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c81df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c81e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c81ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c81f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c81f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c81f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c8203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c820d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c8211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c821b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c822440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c8228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c822d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c8236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c823b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c8240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c824b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c8250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c8255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c825b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c826090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c8265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c827080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c8275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c828070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c8285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c828b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c829060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c8295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c829b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c82a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c82a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c82aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c82b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c82b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c82bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c81b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c82bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c82c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c82cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c82d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c82d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c82dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c82e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c82e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c82ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c82f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c82f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c82fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c8306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c830c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c8310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c831550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c8319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c8327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c833110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c8335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c833a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c834390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c834830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c834cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c835170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c835610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c835f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c8363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c836d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c8371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c837670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c837b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c837fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c838450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c8388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c838d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c839230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c839b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c83a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c83a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c83a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c83adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c83b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c83b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c83bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c83c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c83c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c83c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c83ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c83d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c83d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c83dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c83e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c83e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c83ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c83eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c83f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c83f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c83fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c840130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c8405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c840a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c840f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c8413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c841cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c842f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c8438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c843d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c8441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c844690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c845470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c845910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c8466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c847030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c8474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c847970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c847e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c8488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c848e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c849610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c849c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c84a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c84a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c84b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c84b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c84b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c84bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c84c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c84cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c84d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c84d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c84d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c84e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c84e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c84ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c84f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c84f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c84fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c850110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c850660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c851100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c851650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c8520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c852640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c852b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c8530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c853630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c853b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c8540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c854620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c854b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c8550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c855610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c8560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c856600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c856b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c8570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c8575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c857b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c858090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c8585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c858b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c859080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c8595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c859b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c85a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c85a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c85ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c85b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c85b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c85bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c85c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c85c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c85caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c85d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c85d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c85dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c85e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c85e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c85ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c85f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c85f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c85fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c860010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c860560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c860ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c860f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c8613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c861890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c861d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c8621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c862670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c862b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c862fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c8638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c863d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c864230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c8646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c865010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c865560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c865c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c8663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c866ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c8671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c8674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c867c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c867f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c868560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b7056e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b705b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b705fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b706430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b7068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b706d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b707180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b7075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b707a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b707fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b708430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b708ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b7095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b709d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b70a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b70acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b70b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b70baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b70c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b70c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b70d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b70d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b70df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b70e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b70ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b70f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b70f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b70f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b70fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b710050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b7104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b7109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b710e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b711120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b711590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b711a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b711e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b7122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b712750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b712bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b713030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b7134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b713910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b713d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b7141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b714660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b714ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b714f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b7153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b715820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b715c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b716100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b716570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b7169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b716e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b7172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b717830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b717d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b7181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b718610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b718a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b718ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b719360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b7197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b719c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b71a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b71a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b71a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b71ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b71b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b71b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b71bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b71bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b71c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b71c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b71cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b71d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b71d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b71da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b71ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b71e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b71e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b71ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b71f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b71f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b71f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b71fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b720250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b7206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b720b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b720fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b721410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b721880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b721cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b722160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b7225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b722eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b723320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b723790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b723c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b724070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b7244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b724950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b724dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b725230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b7256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b725b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b725f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b7263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b726860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b726cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b727140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b7275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b727a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b727e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b728300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b728770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b728be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b729050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b7294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b729930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b729da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b72a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b72a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b72aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b72af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b72b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b72b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b72bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b72c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b72c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b72ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b72ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b72d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b72d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b72dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b72e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b72e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b72e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b72ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b72f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b72f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b72fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b72ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b7303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b730820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b730c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b731100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b731570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b7319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b731e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b7322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b732730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b732ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b733010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b733480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b7338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b733d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b7341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b734640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b734ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b734f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b735390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b735800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b735c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b7360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b736550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b7369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b736e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b7372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b737710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b737b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b738460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b7388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b738d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b7391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b739620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b739a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b739f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b73a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b73a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b73ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b73b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b73b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b73b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b73be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b73c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b73c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b73cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b73cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b73d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b73d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b73dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b73e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b73e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b73ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b73eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b73f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b73f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b73fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b7400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b740510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b740980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b740df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b741260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b7417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b7420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b742c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b742ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b7431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b743610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b743a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b743ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b744360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b7447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b744c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b7450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b745520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b745e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b746270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b7466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b746b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b746fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b747430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b7478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b747d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b748180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b7485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b748a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b748ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b749340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b7497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b749c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b74a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b74a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b74a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b74ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b74b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b74b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b74bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b74bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b74c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b74c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b74ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b74d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b74d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b74da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b74deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b74e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b74e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b74ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b74f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b74f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b74f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b74fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b750230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b750b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b750f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b7513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b751860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b751cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b752140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b7525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b752a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b752e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b753300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b753770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b753be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b754050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b7544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b754930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b754da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b755210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b755680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b755af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b755f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b7563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b756840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b7572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b7579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b7580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b758810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b758ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b759540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b759b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11eb046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11eb04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11eb04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11eb05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11eb058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11eb05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11eb06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11eb065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11eb06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11eb06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11eb07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11eb079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11eb08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11eb08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11eb094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11eb09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11eb0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11eb0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11eb0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11eb0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11eb0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11eb0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11eb0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11eb0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11eb0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11eb0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11eb0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11eb0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11eb0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11eb0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11eb0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11eb0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11eb0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11eb10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11eb104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11eb10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11eb10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11eb11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11eb11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11eb11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11eb11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11eb123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11eb12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11eb12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11eb13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11eb13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11eb13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11eb13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11eb142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11eb14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11eb14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11eb15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11eb154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11eb15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11eb15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11eb161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11eb16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11eb16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11eb170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11eb17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11eb179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11eb17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11eb18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11eb18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11eb18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11eb18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11eb19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11eb198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11eb19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11eb1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11eb1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11eb1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11eb1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11eb1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11eb1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11eb1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11eb1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11eb1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11eb1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11eb1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11eb1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11eb1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11eb1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11eb1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11eb1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11eb1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11eb1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11eb1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11eb1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11eb1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11eb1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11eb20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11eb207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11eb20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11eb21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11eb21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11eb21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11eb21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11eb22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11eb226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11eb22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11eb22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11eb23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11eb23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11eb23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11eb24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11eb245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11eb24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11eb24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11eb25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11eb25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11eb25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11eb26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11eb264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11eb26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11eb26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11eb27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11eb276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11eb27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11eb27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11eb283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11eb28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11eb28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11eb29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11eb295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11eb29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11eb29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11eb2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11eb2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11eb2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11eb2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11eb2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11eb2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11eb2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11eb2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11eb2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11eb2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11eb2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11eb2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11eb2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11eb2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11eb2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11eb2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11eb2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11eb2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11eb2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11eb2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11eb2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11eb30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11eb304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11eb30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11eb30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11eb311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11eb31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11eb31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11eb31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11eb323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11eb32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11eb32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11eb33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11eb33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11eb339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11eb33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11eb342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11eb34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11eb34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11eb35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11eb35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11eb358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11eb35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11eb361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11eb36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11eb36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11eb36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11eb37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11eb37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11eb37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11eb380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11eb38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11eb389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11eb38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11eb392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11eb39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11eb39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11eb39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11eb3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11eb3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11eb3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11eb3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11eb3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11eb3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11eb3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11eb3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11eb3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11eb3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11eb3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11eb3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11eb3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11eb3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11eb3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11eb3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11eb3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11eb3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11eb3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11eb3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11eb3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11eb40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11eb40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11eb40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11eb41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11eb41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11eb41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11eb420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11eb42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11eb429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11eb42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11eb43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11eb43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11eb43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11eb43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11eb44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11eb448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11eb44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11eb451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11eb45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11eb45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11eb45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11eb46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11eb467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11eb46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11eb470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11eb47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11eb47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11eb47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11eb48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11eb486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11eb48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11eb48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11eb49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11eb498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11eb49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11eb4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11eb4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11eb4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11eb4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11eb4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11eb4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11eb4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11eb4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11eb4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11eb4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11eb4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11eb4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11eb4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11eb4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11eb4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11eb4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11eb4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11eb4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11eb4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11eb4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11eb4fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11eb50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11eb50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11eb50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11eb50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11eb513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11eb51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11eb51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11eb52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11eb52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11eb529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11eb52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11eb532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11eb53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11eb53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11eb54020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11eb54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11eb54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11eb54d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11eb551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11eb55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11eb55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11eb56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11eb56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11eb57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11eb57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11eb57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11eb581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11eb587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11eb58dd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.947s
user	0m0.245s
sys	0m0.151s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.15 user         0.04 sys
```
